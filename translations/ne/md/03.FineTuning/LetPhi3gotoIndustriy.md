<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "743d7e9cb9c4e8ea642d77bee657a7fa",
  "translation_date": "2025-07-17T09:54:53+00:00",
  "source_file": "md/03.FineTuning/LetPhi3gotoIndustriy.md",
  "language_code": "ne"
}
-->
# **Phi-3 लाई उद्योग विशेषज्ञ बनाऔं**

Phi-3 मोडेललाई उद्योगमा प्रयोग गर्नका लागि, तपाईंले उद्योगको व्यवसाय डेटा Phi-3 मोडेलमा थप्नुपर्छ। हामीसँग दुई विकल्पहरू छन्, पहिलो RAG (Retrieval Augmented Generation) र दोस्रो Fine Tuning।

## **RAG र Fine-Tuning को तुलना**

### **Retrieval Augmented Generation**

RAG भनेको डेटा पुनःप्राप्ति + पाठ सिर्जना हो। उद्यमको संरचित र असंरचित डेटा भेक्टर डाटाबेसमा संग्रहित हुन्छ। सम्बन्धित सामग्री खोज्दा, सम्बन्धित सारांश र सामग्री पत्ता लगाइन्छ र सन्दर्भ तयार पारिन्छ, र LLM/SLM को पाठ पूर्ति क्षमता सँग मिलाएर सामग्री सिर्जना गरिन्छ।

### **Fine-tuning**

Fine-tuning भनेको कुनै मोडेलको सुधारमा आधारित हुन्छ। यसले मोडेल एल्गोरिदमबाट सुरु गर्न आवश्यक पर्दैन, तर डेटा निरन्तर सङ्कलन गर्नुपर्छ। यदि तपाईं उद्योग अनुप्रयोगहरूमा थप सटीक शब्दावली र भाषा अभिव्यक्ति चाहनुहुन्छ भने, fine-tuning तपाईंको राम्रो विकल्प हो। तर यदि तपाईंको डेटा बारम्बार परिवर्तन हुन्छ भने, fine-tuning जटिल हुन सक्छ।

### **कसरी छनौट गर्ने**

1. यदि हाम्रो उत्तरमा बाह्य डेटा समावेश गर्न आवश्यक छ भने, RAG उत्तम विकल्प हो।

2. यदि तपाईंलाई स्थिर र सटीक उद्योग ज्ञान चाहिन्छ भने, fine-tuning राम्रो विकल्प हुनेछ। RAG सम्बन्धित सामग्री तान्न प्राथमिकता दिन्छ तर सधैं विशेषज्ञता सम्बन्धी सूक्ष्मता समात्न सक्दैन।

3. Fine-tuning को लागि उच्च गुणस्तरको डेटा सेट आवश्यक हुन्छ, र यदि डेटा सानो क्षेत्रमै सीमित छ भने धेरै फरक पर्दैन। RAG बढी लचिलो हुन्छ।

4. Fine-tuning एउटा कालो बाकस जस्तो हुन्छ, यसको आन्तरिक प्रक्रिया बुझ्न गाह्रो हुन्छ। तर RAG ले डेटा स्रोत सजिलै पत्ता लगाउन मद्दत गर्छ, जसले भ्रम वा सामग्री त्रुटिहरूलाई प्रभावकारी रूपमा समायोजन गर्न र राम्रो पारदर्शिता प्रदान गर्न सक्छ।

### **परिदृश्यहरू**

1. ठाडो उद्योगहरूमा विशेष पेशागत शब्दावली र अभिव्यक्तिहरू आवश्यक पर्दा, ***Fine-tuning*** उत्तम विकल्प हुन्छ।

2. QA प्रणालीमा, विभिन्न ज्ञान बिन्दुहरूको संयोजन आवश्यक पर्दा, ***RAG*** उत्तम विकल्प हुन्छ।

3. स्वचालित व्यवसाय प्रवाहको संयोजनमा ***RAG + Fine-tuning*** उत्तम विकल्प हो।

## **RAG कसरी प्रयोग गर्ने**

![rag](../../../../translated_images/ne/rag.2014adc59e6f6007.png)

भेक्टर डाटाबेस भनेको गणितीय रूपमा संग्रहित डेटा हो। भेक्टर डाटाबेसले मेसिन लर्निङ मोडेलहरूलाई अघिल्लो इनपुटहरू सम्झन सजिलो बनाउँछ, जसले खोज, सिफारिस, र पाठ सिर्जना जस्ता प्रयोगहरूलाई समर्थन गर्न मेसिन लर्निङ प्रयोग गर्न सक्षम बनाउँछ। डेटा ठ्याक्कै मेल खानुको सट्टा समानता मेट्रिक्समा आधारित पहिचान गर्न सकिन्छ, जसले कम्प्युटर मोडेलहरूलाई डेटा सन्दर्भ बुझ्न मद्दत गर्छ।

भेक्टर डाटाबेस RAG कार्यान्वयनको मुख्य कुञ्जी हो। हामी text-embedding-3, jina-ai-embedding जस्ता भेक्टर मोडेलहरू मार्फत डेटा भेक्टर भण्डारणमा रूपान्तरण गर्न सक्छौं।

RAG एप्लिकेसन बनाउन थप जान्न [https://github.com/microsoft/Phi-3CookBook](https://github.com/microsoft/Phi-3CookBook?WT.mc_id=aiml-138114-kinfeylo) मा जानुहोस्।

## **Fine-tuning कसरी प्रयोग गर्ने**

Fine-tuning मा सामान्यतया प्रयोग गरिने एल्गोरिदमहरू Lora र QLora हुन्। कसरी छनौट गर्ने?
- [यो नमूना नोटबुकबाट थप जान्नुहोस्](../../../../code/04.Finetuning/Phi_3_Inference_Finetuning.ipynb)
- [Python FineTuning नमूना उदाहरण](../../../../code/04.Finetuning/FineTrainingScript.py)

### **Lora र QLora**

![lora](../../../../translated_images/ne/qlora.e6446c988ee04ca0.png)

LoRA (Low-Rank Adaptation) र QLoRA (Quantized Low-Rank Adaptation) दुवै Parameter Efficient Fine Tuning (PEFT) प्रयोग गरेर ठूलो भाषा मोडेलहरू (LLMs) लाई fine-tune गर्ने प्रविधिहरू हुन्। PEFT प्रविधिहरू परम्परागत विधिहरू भन्दा मोडेलहरूलाई बढी कुशलतापूर्वक प्रशिक्षण गर्न डिजाइन गरिएका छन्।  
LoRA एउटा स्वतन्त्र fine-tuning प्रविधि हो जसले वजन अपडेट म्याट्रिक्समा कम-रैंक अनुमान लागू गरेर मेमोरी खपत घटाउँछ। यसले छिटो प्रशिक्षण समय दिन्छ र परम्परागत fine-tuning विधिहरूको नजिकको प्रदर्शन कायम राख्छ।

QLoRA LoRA को विस्तारित संस्करण हो जसले मेमोरी उपयोग अझ कम गर्न क्वान्टाइजेसन प्रविधिहरू समावेश गर्छ। QLoRA ले पूर्व-प्रशिक्षित LLM का वजन प्यारामिटरहरूलाई 4-बिट प्रिसिजनमा क्वान्टाइज गर्छ, जुन LoRA भन्दा मेमोरीमा बढी कुशल हुन्छ। तर, QLoRA प्रशिक्षण LoRA भन्दा करिब ३०% ढिलो हुन्छ किनभने थप क्वान्टाइजेसन र डिक्वान्टाइजेसन चरणहरू हुन्छन्।

QLoRA ले LoRA लाई सहायकको रूपमा प्रयोग गर्छ क्वान्टाइजेसन त्रुटिहरूलाई सुधार गर्न। QLoRA ले साना, सजिलै उपलब्ध GPU हरूमा अर्बौं प्यारामिटर भएका विशाल मोडेलहरू fine-tune गर्न सक्षम बनाउँछ। उदाहरणका लागि, QLoRA ले ७०B प्यारामिटर मोडेललाई ३६ GPU को आवश्यकता पर्ने अवस्थामा मात्र २ GPU मा fine-tune गर्न सक्छ।

**अस्वीकरण**:  
यो दस्तावेज AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरी अनुवाद गरिएको हो। हामी शुद्धताका लागि प्रयासरत छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटि वा अशुद्धता हुन सक्छ। मूल दस्तावेज यसको मूल भाषामा नै अधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीका लागि व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न कुनै पनि गलतफहमी वा गलत व्याख्याका लागि हामी जिम्मेवार छैनौं।