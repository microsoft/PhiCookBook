<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-09T07:04:04+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "ne"
}
-->
# Phi-3 लाई स्थानीय रूपमा सुरु गर्ने

यो मार्गदर्शकले तपाईँलाई Ollama प्रयोग गरेर Phi-3 मोडेल स्थानीय रूपमा चलाउन वातावरण सेटअप गर्न मद्दत गर्नेछ। तपाईँले मोडेल चलाउन विभिन्न तरिकाहरू प्रयोग गर्न सक्नुहुन्छ, जस्तै GitHub Codespaces, VS Code Dev Containers, वा आफ्नो स्थानीय वातावरण।

## वातावरण सेटअप

### GitHub Codespaces

तपाईँ यो टेम्प्लेट GitHub Codespaces प्रयोग गरेर भर्चुअल रूपमा चलाउन सक्नुहुन्छ। बटनले तपाईँको ब्राउजरमा वेब-आधारित VS Code सत्र खोल्नेछ:

1. टेम्प्लेट खोल्नुहोस् (यसमा केही मिनेट लाग्न सक्छ):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. टर्मिनल विन्डो खोल्नुहोस्

### VS Code Dev Containers

⚠️ यो विकल्प तब मात्र काम गर्नेछ जब तपाईँको Docker Desktop मा कम्तीमा १६ जीबी RAM छुट्याइएको छ। यदि तपाईँसँग १६ जीबी भन्दा कम RAM छ भने, तपाईँ [GitHub Codespaces विकल्प](../../../../../md/01.Introduction/01) वा [स्थानीय रूपमा सेटअप](../../../../../md/01.Introduction/01) गर्न सक्नुहुन्छ।

सम्बन्धित विकल्प भनेको VS Code Dev Containers हो, जसले प्रोजेक्ट तपाईँको स्थानीय VS Code मा [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) प्रयोग गरेर खोल्नेछ:

1. Docker Desktop सुरु गर्नुहोस् (यदि स्थापना गरिएको छैन भने स्थापना गर्नुहोस्)
2. प्रोजेक्ट खोल्नुहोस्:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. खुल्ने VS Code विन्डोमा, प्रोजेक्ट फाइलहरू देखिएपछि (यसमा केही मिनेट लाग्न सक्छ) टर्मिनल विन्डो खोल्नुहोस्।
4. [डिप्लोयमेन्ट चरणहरू](../../../../../md/01.Introduction/01) सँग जारी राख्नुहोस्

### स्थानीय वातावरण

1. निम्न उपकरणहरू स्थापना भएको सुनिश्चित गर्नुहोस्:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## मोडेल परीक्षण गर्नुहोस्

1. Ollama लाई phi3:mini मोडेल डाउनलोड र चलाउन भन्नुहोस्:

    ```shell
    ollama run phi3:mini
    ```

    मोडेल डाउनलोड गर्न केही मिनेट लाग्नेछ।

2. आउटपुटमा "success" देखिएपछि, तपाईँ सो मोडेलमा सन्देश पठाउन सक्नुहुन्छ।

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. केही सेकेन्ड पछि, तपाईँ मोडेलबाट प्रतिक्रिया स्ट्रिम देख्नुपर्नेछ।

4. भाषा मोडेलसँग प्रयोग गरिएका विभिन्न प्रविधिहरू सिक्न, Python नोटबुक [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) खोल्नुहोस् र प्रत्येक सेल चलाउनुहोस्। यदि तपाईँले 'phi3:mini' बाहेकको मोडेल प्रयोग गर्नुभयो भने, फाइलको माथि `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` परिवर्तन गर्न सक्नुहुन्छ, र प्रणाली सन्देश वा फ्यू-शट उदाहरणहरू पनि आवश्यक परे थप्न सक्नुहुन्छ।

**अस्वीकरण**:  
यो दस्तावेज AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरी अनुवाद गरिएको हो। हामी सटीकता को लागि प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटि वा अशुद्धि हुन सक्छ। मूल दस्तावेज यसको मूल भाषामा आधिकारिक स्रोत मानिनु पर्छ। महत्वपूर्ण जानकारीका लागि व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याको लागि हामी जिम्मेवार छैनौं।