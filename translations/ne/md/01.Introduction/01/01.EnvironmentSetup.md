<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-07-16T18:08:42+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "ne"
}
-->
# Phi-3 लाई स्थानीय रूपमा सुरु गर्ने तरिका

यो मार्गदर्शनले तपाईंलाई Ollama प्रयोग गरेर Phi-3 मोडेल चलाउनको लागि स्थानीय वातावरण सेटअप गर्न मद्दत गर्नेछ। तपाईं मोडेललाई विभिन्न तरिकाले चलाउन सक्नुहुन्छ, जस्तै GitHub Codespaces, VS Code Dev Containers, वा आफ्नो स्थानीय वातावरणमा।

## वातावरण सेटअप

### GitHub Codespaces

तपाईं GitHub Codespaces प्रयोग गरेर यो टेम्प्लेटलाई भर्चुअल रूपमा चलाउन सक्नुहुन्छ। तलको बटनले तपाईंको ब्राउजरमा वेब-आधारित VS Code खोल्नेछ:

1. टेम्प्लेट खोल्नुहोस् (यसमा केही मिनेट लाग्न सक्छ):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. टर्मिनल विन्डो खोल्नुहोस्

### VS Code Dev Containers

⚠️ यो विकल्प तब मात्र काम गर्नेछ जब तपाईंको Docker Desktop मा कम्तीमा १६ जीबी RAM छुट्याइएको छ। यदि तपाईंको RAM १६ जीबी भन्दा कम छ भने, तपाईं [GitHub Codespaces विकल्प](../../../../../md/01.Introduction/01) वा [स्थानीय रूपमा सेटअप](../../../../../md/01.Introduction/01) गर्न सक्नुहुन्छ।

सम्बन्धित विकल्प हो VS Code Dev Containers, जसले तपाईंको स्थानीय VS Code मा [Dev Containers एक्सटेन्सन](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) प्रयोग गरी प्रोजेक्ट खोल्नेछ:

1. Docker Desktop सुरु गर्नुहोस् (यदि पहिले इन्स्टल गरिएको छैन भने इन्स्टल गर्नुहोस्)
2. प्रोजेक्ट खोल्नुहोस्:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. खुल्ने VS Code विन्डोमा, प्रोजेक्ट फाइलहरू देखिएपछि (यसमा केही मिनेट लाग्न सक्छ) टर्मिनल विन्डो खोल्नुहोस्।
4. [डिप्लोयमेन्ट चरणहरू](../../../../../md/01.Introduction/01) अनुसार अगाडि बढ्नुहोस्

### स्थानीय वातावरण

1. तलका उपकरणहरू इन्स्टल गरिएको छ भनी सुनिश्चित गर्नुहोस्:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## मोडेल परीक्षण गर्नुहोस्

1. Ollama लाई phi3:mini मोडेल डाउनलोड गरी चलाउन भन्नुहोस्:

    ```shell
    ollama run phi3:mini
    ```

    मोडेल डाउनलोड गर्न केही मिनेट लाग्नेछ।

2. आउटपुटमा "success" देखिएपछि, तपाईं सो मोडेललाई प्रॉम्प्टबाट सन्देश पठाउन सक्नुहुन्छ।

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. केही सेकेन्डपछि, मोडेलबाट प्रतिक्रिया स्ट्रिम देखिन थाल्नेछ।

4. भाषा मोडेलसँग प्रयोग गरिएका विभिन्न प्रविधिहरू सिक्नको लागि, Python नोटबुक [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) खोल्नुहोस् र प्रत्येक सेल चलाउनुहोस्। यदि तपाईंले 'phi3:mini' बाहेकको मोडेल प्रयोग गर्नुभएको छ भने, पहिलो सेलमा `MODEL_NAME` परिवर्तन गर्नुहोस्।

5. Python बाट phi3:mini मोडेलसँग कुराकानी गर्न, Python फाइल [chat.py](../../../../../code/01.Introduce/chat.py) खोल्नुहोस् र चलाउनुहोस्। आवश्यक अनुसार फाइलको माथिल्लो भागमा `MODEL_NAME` परिवर्तन गर्न सक्नुहुन्छ, साथै सिस्टम सन्देश वा केही उदाहरणहरू थप्न पनि सक्नुहुन्छ।

**अस्वीकरण**:  
यो दस्तावेज AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरी अनुवाद गरिएको हो। हामी शुद्धताका लागि प्रयासरत छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटि वा अशुद्धता हुन सक्छ। मूल दस्तावेज यसको मूल भाषामा नै अधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीका लागि व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न कुनै पनि गलतफहमी वा गलत व्याख्याका लागि हामी जिम्मेवार छैनौं।