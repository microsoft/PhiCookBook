<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:43:00+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "ne"
}
-->
# उल्लेखित प्रमुख प्रविधिहरू

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12 मा आधारित हार्डवेयर-त्वरित मेसिन लर्निङका लागि एक कम-स्तरीय API।
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia द्वारा विकास गरिएको समानान्तर कम्प्युटिङ प्लेटफर्म र एप्लिकेसन प्रोग्रामिङ इन्टरफेस (API) मोडेल, जसले ग्राफिक्स प्रोसेसिङ युनिटहरू (GPUs) मा सामान्य प्रयोजन प्रोसेसिङ सक्षम बनाउँछ।
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - मेसिन लर्निङ मोडेलहरूलाई प्रतिनिधित्व गर्न डिजाइन गरिएको खुला ढाँचा, जसले विभिन्न ML फ्रेमवर्कहरू बीच अन्तरक्रियाशीलता प्रदान गर्छ।
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - मेसिन लर्निङ मोडेलहरू प्रतिनिधित्व र अपडेट गर्न प्रयोग हुने ढाँचा, विशेष गरी साना भाषा मोडेलहरूका लागि उपयोगी जुन CPU मा 4-8bit क्वान्टाइजेसनसहित प्रभावकारी रूपमा चल्न सक्छन्।

## DirectML

DirectML एक कम-स्तरीय API हो जसले हार्डवेयर-त्वरित मेसिन लर्निङ सक्षम बनाउँछ। यो DirectX 12 मा आधारित छ जसले GPU तिब्रता प्रयोग गर्छ र विक्रेता-स्वतन्त्र छ, जसको अर्थ विभिन्न GPU विक्रेताहरूमा काम गर्न कोड परिवर्तन आवश्यक पर्दैन। यो मुख्य रूपमा GPU मा मोडेल प्रशिक्षण र पूर्वानुमान कार्यभारका लागि प्रयोग गरिन्छ।

हार्डवेयर समर्थनको कुरा गर्दा, DirectML विभिन्न GPU हरूसँग काम गर्न डिजाइन गरिएको हो, जसमा AMD को एकीकृत र पृथक GPU हरू, Intel को एकीकृत GPU हरू, र NVIDIA को पृथक GPU हरू समावेश छन्। यो Windows AI Platform को हिस्सा हो र Windows 10 र 11 मा समर्थित छ, जसले कुनै पनि Windows उपकरणमा मोडेल प्रशिक्षण र पूर्वानुमान गर्न अनुमति दिन्छ।

DirectML सम्बन्धी अपडेटहरू र अवसरहरू पनि आएका छन्, जस्तै १५० सम्म ONNX अपरेटरहरूको समर्थन र ONNX runtime र WinML दुवैले यसको प्रयोग गर्नु। यसलाई प्रमुख एकीकृत हार्डवेयर विक्रेताहरू (IHVs) द्वारा समर्थन गरिएको छ, जसले विभिन्न मेटाकमाण्डहरू कार्यान्वयन गर्छन्।

## CUDA

CUDA, जसको पूरा नाम Compute Unified Device Architecture हो, Nvidia द्वारा सिर्जना गरिएको समानान्तर कम्प्युटिङ प्लेटफर्म र एप्लिकेसन प्रोग्रामिङ इन्टरफेस (API) मोडेल हो। यसले सफ्टवेयर विकासकर्ताहरूलाई CUDA-सक्षम GPU प्रयोग गरेर सामान्य प्रयोजन प्रोसेसिङ गर्न अनुमति दिन्छ — जसलाई GPGPU (General-Purpose computing on Graphics Processing Units) भनिन्छ। CUDA Nvidia को GPU तिब्रता सक्षम पार्ने मुख्य प्रविधि हो र मेसिन लर्निङ, वैज्ञानिक कम्प्युटिङ, र भिडियो प्रशोधन लगायत विभिन्न क्षेत्रहरूमा व्यापक रूपमा प्रयोग हुन्छ।

CUDA को हार्डवेयर समर्थन Nvidia का GPU हरूसँग सीमित छ, किनकि यो Nvidia को स्वामित्वमा रहेको प्रविधि हो। प्रत्येक आर्किटेक्चरले CUDA टूलकिटका विशिष्ट संस्करणहरू समर्थन गर्छ, जसले विकासकर्ताहरूलाई CUDA एप्लिकेसनहरू निर्माण र चलाउन आवश्यक पुस्तकालय र उपकरणहरू प्रदान गर्छ।

## ONNX

ONNX (Open Neural Network Exchange) मेसिन लर्निङ मोडेलहरू प्रतिनिधित्व गर्न डिजाइन गरिएको खुला ढाँचा हो। यसले विस्तारयोग्य कम्प्युटेशन ग्राफ मोडेलको परिभाषा, साथै निर्मित अपरेटरहरू र मानक डाटा प्रकारहरूको परिभाषा प्रदान गर्छ। ONNX ले विकासकर्ताहरूलाई विभिन्न ML फ्रेमवर्कहरू बीच मोडेलहरू सार्न अनुमति दिन्छ, जसले अन्तरक्रियाशीलता सक्षम पार्छ र AI एप्लिकेसनहरू सिर्जना र तैनाथ गर्न सजिलो बनाउँछ।

Phi3 mini ले ONNX Runtime सँग CPU र GPU मा विभिन्न उपकरणहरूमा चल्न सक्छ, जसमा सर्भर प्लेटफर्महरू, Windows, Linux र Mac डेस्कटपहरू, र मोबाइल CPU हरू समावेश छन्।  
हामीले थपेका अनुकूलित कन्फिगरेसनहरू हुन्

- int4 DML का लागि ONNX मोडेल: AWQ मार्फत int4 मा क्वान्टाइज गरिएको  
- fp16 CUDA का लागि ONNX मोडेल  
- int4 CUDA का लागि ONNX मोडेल: RTN मार्फत int4 मा क्वान्टाइज गरिएको  
- int4 CPU र मोबाइलका लागि ONNX मोडेल: RTN मार्फत int4 मा क्वान्टाइज गरिएको  

## Llama.cpp

Llama.cpp C++ मा लेखिएको खुला स्रोत सफ्टवेयर लाइब्रेरी हो। यसले विभिन्न ठूलो भाषा मोडेलहरू (LLMs) मा पूर्वानुमान गर्दछ, जसमा Llama पनि समावेश छ। ggml लाइब्रेरी (एक सामान्य प्रयोजन टेन्सर लाइब्रेरी) सँगै विकास गरिएको, llama.cpp ले मूल Python कार्यान्वयनको तुलनामा छिटो पूर्वानुमान र कम मेमोरी प्रयोग प्रदान गर्ने लक्ष्य राख्छ। यसले हार्डवेयर अनुकूलन, क्वान्टाइजेसन समर्थन गर्छ र सरल API र उदाहरणहरू उपलब्ध गराउँछ। यदि तपाईं प्रभावकारी LLM पूर्वानुमानमा रुचि राख्नुहुन्छ भने, llama.cpp अन्वेषण गर्न योग्य छ किनकि Phi3 ले Llama.cpp चलाउन सक्छ।

## GGUF

GGUF (Generic Graph Update Format) मेसिन लर्निङ मोडेलहरू प्रतिनिधित्व र अपडेट गर्न प्रयोग हुने ढाँचा हो। यो विशेष गरी साना भाषा मोडेलहरू (SLMs) का लागि उपयोगी छ जुन CPU मा 4-8bit क्वान्टाइजेसनसहित प्रभावकारी रूपमा चल्न सक्छन्। GGUF छिटो प्रोटोटाइपिङ र एज उपकरणहरूमा वा CI/CD पाइपलाइन जस्ता ब्याच कामहरूमा मोडेलहरू चलाउन लाभदायक छ।

**अस्वीकरण**:  
यो दस्तावेज AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरी अनुवाद गरिएको हो। हामी शुद्धताका लागि प्रयासरत छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटि वा अशुद्धता हुन सक्छ। मूल दस्तावेज यसको मूल भाषामा नै अधिकारिक स्रोत मानिनु पर्छ। महत्वपूर्ण जानकारीका लागि व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न कुनै पनि गलतफहमी वा गलत व्याख्याका लागि हामी जिम्मेवार छैनौं।