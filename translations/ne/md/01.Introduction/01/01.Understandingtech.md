<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:18:31+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "ne"
}
-->
# मुख्य प्रविधिहरूमा समावेश छन्

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12 माथि आधारित हार्डवेयर-एक्सेलेरेटेड मेशिन लर्निङका लागि एक लो-लेभल API।
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia द्वारा विकास गरिएको समानान्तर कम्प्युटिङ प्लेटफर्म र एप्लिकेसन प्रोग्रामिङ इन्टरफेस (API) मोडेल, जसले GPU मा सामान्य प्रयोजन प्रोसेसिङ सम्भव बनाउँछ।
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - मेशिन लर्निङ मोडेलहरू प्रतिनिधित्व गर्न डिजाइन गरिएको खुला फर्म्याट, जसले विभिन्न ML फ्रेमवर्कहरूबीच अन्तरक्रियाशीलता प्रदान गर्छ।
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - मेशिन लर्निङ मोडेलहरू प्रतिनिधित्व र अपडेट गर्न प्रयोग हुने फर्म्याट, विशेष गरी साना भाषा मोडेलहरूका लागि उपयोगी जुन CPU मा 4-8bit क्वान्टाइजेशनसहित प्रभावकारी रूपमा चल्न सक्छ।

## DirectML

DirectML हार्डवेयर-एक्सेलेरेटेड मेशिन लर्निङ सक्षम गर्ने एक लो-लेभल API हो। यो DirectX 12 माथि निर्माण गरिएको हो GPU एक्सेलेरेसनको लागि र विभिन्न GPU विक्रेताहरूमा काम गर्न कोड परिवर्तन आवश्यक नपर्ने विक्रेता-स्वतन्त्र हो। यो मुख्य रूपमा GPU मा मोडेल प्रशिक्षण र इन्फरेन्सिङ कार्यभारका लागि प्रयोग गरिन्छ।

हार्डवेयर समर्थनको हिसाबले, DirectML धेरै प्रकारका GPU सँग काम गर्न डिजाइन गरिएको हो, जसमा AMD को इन्टिग्रेटेड र डिस्क्रिट GPU, Intel को इन्टिग्रेटेड GPU, र NVIDIA को डिस्क्रिट GPU समावेश छन्। यो Windows AI Platform को हिस्सा हो र Windows 10 र 11 मा समर्थित छ, जसले कुनै पनि Windows उपकरणमा मोडेल प्रशिक्षण र इन्फरेन्सिङ सम्भव बनाउँछ।

DirectML सँग सम्बन्धित अपडेट र अवसरहरू पनि आएका छन्, जस्तै 150 सम्म ONNX अपरेटरहरूको समर्थन र ONNX runtime र WinML दुवै द्वारा प्रयोग। यो प्रमुख Integrated Hardware Vendors (IHVs) द्वारा समर्थन गरिएको छ, जसले विभिन्न मेटाकमाण्डहरू कार्यान्वयन गर्छन्।

## CUDA

CUDA, जसको पूरा नाम Compute Unified Device Architecture हो, Nvidia द्वारा सिर्जना गरिएको समानान्तर कम्प्युटिङ प्लेटफर्म र API मोडेल हो। यसले सफ्टवेयर विकासकर्ताहरूलाई CUDA-सक्षम GPU प्रयोग गरी सामान्य प्रयोजन प्रोसेसिङ (GPGPU) गर्न अनुमति दिन्छ। CUDA Nvidia को GPU एक्सेलेरेसनको मुख्य चालक हो र यो विभिन्न क्षेत्रहरूमा व्यापक रूपमा प्रयोग हुन्छ, जस्तै मेशिन लर्निङ, वैज्ञानिक कम्प्युटिङ, र भिडियो प्रोसेसिङ।

CUDA को हार्डवेयर समर्थन Nvidia का GPU हरूमा सीमित छ किनकि यो Nvidia को स्वामित्वमा रहेको प्रविधि हो। प्रत्येक आर्किटेक्चरले CUDA टुलकिटका विशिष्ट संस्करणहरू समर्थन गर्छ, जसले विकासकर्ताहरूलाई CUDA एप्लिकेसन बनाउन र चलाउन आवश्यक लाइब्रेरी र उपकरणहरू प्रदान गर्छ।

## ONNX

ONNX (Open Neural Network Exchange) एक खुला फर्म्याट हो जुन मेशिन लर्निङ मोडेलहरू प्रतिनिधित्व गर्न डिजाइन गरिएको हो। यसले विस्तारयोग्य कम्प्युटेसन ग्राफ मोडेलको परिभाषा र बिल्ट-इन अपरेटरहरू र मानक डेटा प्रकारहरूको परिभाषा प्रदान गर्छ। ONNX ले विकासकर्ताहरूलाई विभिन्न ML फ्रेमवर्कहरूबीच मोडेल सार्न अनुमति दिन्छ, जसले अन्तरक्रियाशीलता बढाउँछ र AI एप्लिकेसनहरू बनाउन र तैनाथ गर्न सजिलो बनाउँछ।

Phi3 mini CPU र GPU मा ONNX Runtime सँग चल्न सक्छ, विभिन्न उपकरणहरूमा, जस्तै सर्भर प्लेटफर्महरू, Windows, Linux र Mac डेस्कटपहरू, र मोबाइल CPU हरूमा।
हामीले थपेका अप्टिमाइज्ड कन्फिगरेसनहरू छन्

- int4 DML का लागि ONNX मोडेल: AWQ मार्फत int4 मा क्वान्टाइज गरिएको
- fp16 CUDA का लागि ONNX मोडेल
- int4 CUDA का लागि ONNX मोडेल: RTN मार्फत int4 मा क्वान्टाइज गरिएको
- int4 CPU र मोबाइलका लागि ONNX मोडेल: RTN मार्फत int4 मा क्वान्टाइज गरिएको

## Llama.cpp

Llama.cpp C++ मा लेखिएको खुला स्रोत सफ्टवेयर लाइब्रेरी हो। यसले विभिन्न ठूलो भाषा मोडेलहरू (LLMs) मा इन्फरेन्स गर्दछ, जसमा Llama पनि समावेश छ। ggml लाइब्रेरी (सामान्य प्रयोजन टेन्सर लाइब्रेरी) सँगै विकास गरिएको, llama.cpp ले मूल Python कार्यान्वयनको तुलनामा छिटो इन्फरेन्स र कम मेमोरी प्रयोग गर्ने लक्ष्य राख्छ। यसले हार्डवेयर अप्टिमाइजेसन, क्वान्टाइजेसन समर्थन गर्छ र सरल API र उदाहरणहरू प्रदान गर्छ। यदि तपाईं प्रभावकारी LLM इन्फरेन्समा रुचि राख्नुहुन्छ भने, llama.cpp अन्वेषण गर्न योग्य छ किनकि Phi3 ले Llama.cpp चलाउन सक्छ।

## GGUF

GGUF (Generic Graph Update Format) मेशिन लर्निङ मोडेलहरू प्रतिनिधित्व र अपडेट गर्न प्रयोग हुने फर्म्याट हो। यो विशेष गरी साना भाषा मोडेलहरू (SLMs) का लागि उपयोगी छ जुन CPU मा 4-8bit क्वान्टाइजेसनसहित प्रभावकारी रूपमा चल्न सक्छ। GGUF छिटो प्रोटोटाइपिङ र एज उपकरणहरूमा वा CI/CD पाइपलाइन जस्ता ब्याच कामहरूमा मोडेलहरू चलाउन लाभदायक छ।

**अस्वीकरण**:  
यो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) को प्रयोग गरी अनुवाद गरिएको हो। हामी शुद्धताको लागि प्रयासरत छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटि वा अशुद्धता हुनसक्छ। मूल दस्तावेज़ यसको मूल भाषामा नै प्रामाणिक स्रोत मानिनु पर्छ। महत्वपूर्ण जानकारीको लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न कुनै पनि गलतफहमी वा गलत व्याख्याको लागि हामी जिम्मेवार छैनौं।