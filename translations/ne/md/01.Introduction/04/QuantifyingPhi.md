<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-05T02:25:36+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ne"
}
-->
# **Phi परिवारको क्वान्टाइजेसन**

मोडेल क्वान्टाइजेसनले न्यूरल नेटवर्क मोडेलका प्यारामिटरहरू (जस्तै वजन र एक्टिवेसन मानहरू) लाई ठूलो मान दायराबाट (सामान्यतया निरन्तर मान दायराबाट) सानो सीमित मान दायरामा म्याप गर्ने प्रक्रियालाई जनाउँछ। यस प्रविधिले मोडेलको आकार र गणनात्मक जटिलता घटाउन सक्छ र मोबाइल उपकरणहरू वा एम्बेडेड सिस्टम जस्ता स्रोत-सीमित वातावरणमा मोडेलको सञ्चालन दक्षता सुधार्न सक्छ। मोडेल क्वान्टाइजेसनले प्यारामिटरहरूको प्रेसिजन घटाएर कम्प्रेसन हासिल गर्छ, तर यसले निश्चित मात्रामा प्रेसिजनको ह्रास पनि परिचय गराउँछ। त्यसैले क्वान्टाइजेसन प्रक्रियामा मोडेलको आकार, गणनात्मक जटिलता, र प्रेसिजनको सन्तुलन बनाउनु आवश्यक हुन्छ। सामान्य क्वान्टाइजेसन विधिहरूमा फिक्स्ड-पोइन्ट क्वान्टाइजेसन, फ्लोटिङ-पोइन्ट क्वान्टाइजेसन आदिमा पर्छ। तपाईं विशिष्ट परिदृश्य र आवश्यकताअनुसार उपयुक्त क्वान्टाइजेसन रणनीति छनोट गर्न सक्नुहुन्छ।

हामी GenAI मोडेललाई एज उपकरणहरूमा डिप्लोय गर्न र थप उपकरणहरुलाई GenAI परिदृश्यहरूमा ल्याउन चाहन्छौं, जस्तै मोबाइल उपकरणहरू, AI PC/Copilot+PC, र परम्परागत IoT उपकरणहरू। क्वान्टाइज गरिएको मोडेलमार्फत, हामी यसलाई फरक-फरक उपकरणहरूसँग मेल गर्दै विभिन्न एज उपकरणहरूमा डिप्लोय गर्न सक्छौं। हार्डवेयर निर्माताहरूले प्रदान गरेको मोडेल एक्सेलेरेशन फ्रेमवर्क र क्वान्टाइजेसन मोडेलसँग संयुक्त रूपमा हामी राम्रो SLM अनुप्रयोग परिदृश्यहरू बनाउन सक्छौं।

क्वान्टाइजेसन परिदृश्यमा, हामीसँग फरक प्रेसिजनहरू हुन्छन् (INT4, INT8, FP16, FP32)। तल सामान्यतया प्रयोग गरिने क्वान्टाइजेसन प्रेसिजनहरूको व्याख्या दिइएको छ

### **INT4**

INT4 क्वान्टाइजेसन एक कटिङ क्वान्टाइजेसन विधि हो जसले मोडेलका वजन र एक्टिभेसन मानहरूलाई 4-बिट इन्टिजरमा क्वान्टाइज गर्छ। सानो प्रतिनिधित्व दायराको र कम प्रेसिजनको कारण INT4 क्वान्टाइजेसनले सामान्यतया ठूलो प्रेसिजन ह्रास निम्त्याउन सक्छ। यद्यपि, INT8 क्वान्टाइजेसनको तुलनामा, INT4 क्वान्टाइजेसनले मोडेलको भण्डारण आवश्यकताहरू र गणनात्मक जटिलतालाई अझ कम गर्न सक्छ। ध्यान दिनुपर्ने कुरा के हो भने व्यावहारिक प्रयोगमा INT4 क्वान्टाइजेसन अपेक्षाकृत कम पाइन्छ, किनकि अत्यधिक कम प्रेसिजनले मोडेल प्रदर्शनमा उल्लेखनीय дег्रेडेसन निम्त्याउन सक्छ। साथै, सबै हार्डवेयरले INT4 अपरेसनहरू समर्थन गर्दैनन्, त्यसैले क्वान्टाइजेसन विधि छनोट गर्दा हार्डवेयर अनुकूलतालाई विचार गर्नुपर्ने हुन्छ।

### **INT8**

INT8 क्वान्टाइजेसन भनेको मोडेलका वजन र एक्टिभेसनहरूलाई फ्लोटिङ पोइन्ट संख्याबाट 8-बिट इन्टिजरहरूमा रूपान्तरण गर्ने प्रक्रिया हो। INT8 इन्टिजरहरूले प्रतिनिधित्व गर्ने संख्यात्मक दायरा सानो र कम ठ्याक्काश्ट हुन्छ भने पनि यसले भण्डारण र गणनाको आवश्यकतालाई उल्लेखनीय रूपमा घटाउन सक्छ। INT8 क्वान्टाइजेसनमा, मोडेलका वजन र एक्टिभेसन मानहरू स्केलिङ र अफसेट सहितको क्वान्टाइजेसन प्रक्रियाबाट गुज्रन्छन् ताकि मौलिक फ्लोटिङ पोइन्ट जानकारीलाई सकेसम्म सुरक्षित राख्न सकियोस्। इन्फरेन्स क्रममा, यी क्वान्टाइज्ड मानहरू गणनाका लागि फेरि फ्लोटिङ पोइन्टमा डिक्वान्टाइज गरिन्छन्, र त्यसपछि अर्को चरणका लागि पुन: INT8 मा क्वान्टाइज गरिन्छ। यस विधिले धेरै अनुप्रयोगहरूमा पर्याप्त शुद्धता प्रदान गर्न सक्छ र उच्च गणनात्मक दक्षता कायम राख्छ।

### **FP16**

FP16 ढाँचा, अर्थात् 16-बिट फ्लोटिङ पोइन्ट संख्याहरू (float16), ले 32-बिट फ्लोटिङ पोइन्ट संख्याहरू (float32) को तुलनामा मेमोरी फुटप्रिन्टलाई आधा घटाउँछ, जुन ठूला स्केलको डीप लर्निङ अनुप्रयोगहरूमा महत्वपूर्ण फाइदा हो। FP16 ढाँचाले एउटै GPU मेमोरी सीमाभित्र ठूलो मोडेलहरू लोड गर्न वा अधिक डाटा प्रोसेस गर्न अनुमति दिन्छ। आधुनिक GPU हार्डवेयरले FP16 अपरेसनहरू समर्थन गर्न जारी राखेकाले, FP16 ढाँचा प्रयोग गर्दा कम्प्युटिङ स्पीडमा पनि सुधार आउन सक्छ। यद्यपि, FP16 ढाँचाका आफ्नै सीमाहरू पनि छन्, अर्थात् कम प्रेसिजन, जसले केही अवस्थामा संख्यात्मक अस्थिरता वा प्रेसिजन ह्रास निम्त्याउन सक्छ।

### **FP32**

FP32 ढाँचाले उच्च प्रेसिजन प्रदान गर्दछ र फराकिलो मान दायरालाई सही रूपमा प्रतिनिधित्व गर्न सक्छ। जटिल गणितीय अपरेसनहरू गरिनुपर्ने वा उच्च-प्रेसिजन परिणाम आवश्यक पर्ने परिदृश्यहरूमा FP32 ढाँचा प्राथमिकता दिइन्छ। यद्यपि, उच्च प्रेसिजनले बढी मेमोरी प्रयोग र लामो गणना समयको पनि अर्थ राख्छ। ठूलो-स्केल डीप लर्निङ मोडेलहरूको लागि, विशेष गरी जब धेरै मोडेल प्यारामिटरहरू र ठूलो मात्रामा डाटा हुन्छ, FP32 ढाँचाले GPU मेमोरी अपर्याप्त हुन वा इन्फरेन्स स्पीड घट्न सक्छ।

मोबाइल उपकरणहरू वा IoT उपकरणहरूमा, हामी Phi-3.x मोडेलहरूलाई INT4 मा रूपान्तरण गर्न सक्छौं, जबकि AI PC / Copilot PC उच्च प्रेसिजनहरू जस्तै INT8, FP16, FP32 प्रयोग गर्न सक्छन्।

वर्तमानमा, फरक हार्डवेयर निर्माताहरूले जेनेरेटिभ मोडेलहरू समर्थन गर्नका लागि फरक फ्रेमवर्कहरू उपलब्ध गराएका छन्, जस्तै Intel को OpenVINO, Qualcomm को QNN, Apple को MLX, र Nvidia को CUDA, आदि, जसलाई मोडेल क्वान्टाइजेसनसँग मिलाएर लोकल डिप्लोयमेन्ट पूरा गर्न सकिन्छ।

प्रविधिको सन्दर्भमा, क्वान्टाइजेसनपछि हामीसँग फरक ढाँचा समर्थनहरू हुन्छन्, जस्तै PyTorch / TensorFlow ढाँचाहरू, GGUF, र ONNX। मैले GGUF र ONNX बीच ढाँचा तुलना र प्रयोग परिदृश्यहरू गरेको छु। यहाँ म ONNX क्वान्टाइजेसन ढाँचालाई सिफारिस गर्दछु, जुन मोडेल फ्रेमवर्कदेखि हार्डवेयरसम्म राम्रो समर्थन गर्छ। यस अध्यायमा, हामी ONNX Runtime for GenAI, OpenVINO, र Apple MLX मा मोडेल क्वान्टाइजेसनमा केन्द्रित हुनेछौं (यदि तपाईंको कुनै राम्रो तरिका छ भने, तपाईं PR पेश गरेर हामीलाई दिन सक्नुहुन्छ)

**यो अध्यायमा समावेश छन्**

1. [llama.cpp प्रयोग गरी Phi-3.5 / 4 क्वान्टाइजिङ](./UsingLlamacppQuantifyingPhi.md)

2. [Generative AI extensions for onnxruntime प्रयोग गरी Phi-3.5 / 4 क्वान्टाइजिङ](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO प्रयोग गरी Phi-3.5 / 4 क्वान्टाइजिङ](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX Framework प्रयोग गरी Phi-3.5 / 4 क्वान्टाइजिङ](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
अस्वीकरण:
यो दस्तावेज AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरी अनुवाद गरिएको हो। हामी शुद्धताको लागि प्रयास गर्दछौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटि वा असत्यता हुनसक्छ। मूल भाषामा रहेको दस्तावेजलाई प्रमाणिक/अधिकृत स्रोत मान्नुहोस्। महत्वपूर्ण जानकारीका लागि पेशेवर मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याका लागि हामी जिम्मेवार छैनौं।
<!-- CO-OP TRANSLATOR DISCLAIMER END -->