<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-05-09T13:21:24+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ne"
}
-->
# **Phi परिवारको क्वान्टिफाइङ**

मोडेल क्वान्टाइजेसन भन्नाले न्यूरल नेटवर्क मोडेलमा रहेका प्यारामिटरहरू (जस्तै तौल र एक्टिभेसन मानहरू) लाई ठूलो मानको दायराबाट (सामान्यतया निरन्तर मान दायरा) सानो सीमित मान दायरामा म्याप गर्ने प्रक्रिया हो। यस प्रविधिले मोडेलको आकार र गणनात्मक जटिलता कम गर्न सक्छ र मोबाइल उपकरण वा एम्बेडेड सिस्टमजस्ता स्रोत सीमित वातावरणमा मोडेलको सञ्चालन क्षमता सुधार्न मद्दत गर्छ। मोडेल क्वान्टाइजेसनले प्यारामिटरहरूको प्रेसिजन घटाएर कम्प्रेसन गर्छ, तर यसले केही प्रेसिजन घाटा पनि ल्याउँछ। त्यसैले क्वान्टाइजेसन प्रक्रियामा मोडेलको आकार, गणनात्मक जटिलता र प्रेसिजनबीच सन्तुलन मिलाउन आवश्यक हुन्छ। सामान्य क्वान्टाइजेसन विधिहरूमा फिक्स्ड-प्वाइन्ट क्वान्टाइजेसन, फ्लोटिङ-प्वाइन्ट क्वान्टाइजेसन आदि समावेश छन्। तपाईंले विशेष परिस्थिती र आवश्यकताअनुसार उपयुक्त क्वान्टाइजेसन रणनीति छान्न सक्नुहुन्छ।

हामी GenAI मोडेललाई एज उपकरणहरूमा डिप्लोय गर्न चाहन्छौं र मोबाइल उपकरण, AI PC/Copilot+PC, र परम्परागत IoT उपकरणहरू जस्ता धेरै उपकरणहरूलाई GenAI परिदृश्यमा ल्याउन चाहन्छौं। क्वान्टाइजेसन मोडेल मार्फत, हामी विभिन्न एज उपकरणहरूमा फरक-फरक उपकरण अनुसार मोडेल डिप्लोय गर्न सक्छौं। हार्डवेयर निर्माताहरूले प्रदान गरेको मोडेल एक्सेलेरेशन फ्रेमवर्क र क्वान्टाइजेसन मोडेलसँग संयोजन गरेर हामी राम्रो SLM अनुप्रयोग परिदृश्य निर्माण गर्न सक्छौं।

क्वान्टाइजेसन परिदृश्यमा, हामीसँग विभिन्न प्रेसिजनहरू (INT4, INT8, FP16, FP32) छन्। तल सामान्यतया प्रयोग हुने क्वान्टाइजेसन प्रेसिजनहरूको व्याख्या गरिएको छ।

### **INT4**

INT4 क्वान्टाइजेसन एक कडा क्वान्टाइजेसन विधि हो जसले मोडेलका तौल र एक्टिभेसन मानहरूलाई 4-बिट इन्टिजरमा क्वान्टाइज गर्छ। INT4 क्वान्टाइजेसनले सानो प्रतिनिधित्व दायराका कारण र कम प्रेसिजनका कारण प्रायः ठूलो प्रेसिजन घाटा ल्याउँछ। तर, INT8 क्वान्टाइजेसनको तुलनामा, INT4 ले मोडेलको भण्डारण आवश्यकताहरू र गणनात्मक जटिलता अझ कम गर्न सक्छ। ध्यान दिनुपर्ने कुरा के हो भने व्यावहारिक प्रयोगमा INT4 क्वान्टाइजेसन कमै हुन्छ किनकि अत्यन्त कम प्रेसिजनले मोडेल प्रदर्शनमा ठूलो गिरावट ल्याउन सक्छ। साथै, सबै हार्डवेयरले INT4 अपरेसन समर्थन गर्दैनन्, त्यसैले क्वान्टाइजेसन विधि छान्दा हार्डवेयर अनुकूलता विचार गर्नुपर्छ।

### **INT8**

INT8 क्वान्टाइजेसनले मोडेलका तौल र एक्टिभेसनहरूलाई फ्लोटिङ पोइन्ट नम्बरबाट 8-बिट इन्टिजरमा रूपान्तरण गर्ने प्रक्रिया हो। INT8 इन्टिजरहरूले प्रतिनिधित्व गर्ने संख्यात्मक दायरा सानो र कम प्रेसिजनयुक्त भए तापनि, यसले भण्डारण र गणना आवश्यकताहरूलाई उल्लेखनीय रूपमा घटाउन सक्छ। INT8 क्वान्टाइजेसनमा, मोडेलका तौल र एक्टिभेसन मानहरू स्केलिङ र अफसेट सहित क्वान्टाइजेसन प्रक्रियाबाट गुज्रन्छन् ताकि मूल फ्लोटिङ पोइन्ट जानकारी सकेसम्म जोगाउन सकियोस्। इन्फरेन्सको क्रममा यी क्वान्टाइज गरिएको मानहरूलाई पुन: फ्लोटिङ पोइन्टमा डिक्वान्टाइज गरिन्छ गणनाका लागि, र त्यसपछि अर्को चरणका लागि फेरि INT8 मा क्वान्टाइज गरिन्छ। यस विधिले अधिकांश अनुप्रयोगहरूमा पर्याप्त प्रेसिजन प्रदान गर्न सक्छ र उच्च गणनात्मक दक्षता कायम राख्छ।

### **FP16**

FP16 फर्म्याट, अर्थात् 16-बिट फ्लोटिङ पोइन्ट नम्बरहरू (float16), ले 32-बिट फ्लोटिङ पोइन्ट नम्बरहरू (float32) को तुलनामा मेमोरी खपत आधा घटाउँछ, जुन ठूलो स्तरको डीप लर्निङ अनुप्रयोगहरूमा महत्वपूर्ण फाइदा हो। FP16 फर्म्याटले समान GPU मेमोरी सीमाभित्र ठूलो मोडेलहरू लोड गर्न वा बढी डेटा प्रक्रिया गर्न अनुमति दिन्छ। आधुनिक GPU हार्डवेयरहरूले FP16 अपरेसनहरू समर्थन गर्दै गएसँगै, FP16 प्रयोगले कम्प्युटिङ स्पीडमा पनि सुधार ल्याउन सक्छ। तर, FP16 फर्म्याटका आफ्ना कमीकमजोरीहरू छन्, जस्तै कम प्रेसिजन, जसले केही अवस्थामा संख्यात्मक अस्थिरता वा प्रेसिजन घाटा ल्याउन सक्छ।

### **FP32**

FP32 फर्म्याटले उच्च प्रेसिजन प्रदान गर्छ र फराकिलो मान दायरा सही तरिकाले प्रतिनिधित्व गर्न सक्छ। जटिल गणितीय अपरेसनहरू गर्ने वा उच्च प्रेसिजन आवश्यक पर्ने परिदृश्यहरूमा FP32 फर्म्याट प्राथमिकता दिइन्छ। तर, उच्च प्रेसिजनले बढी मेमोरी खपत र लामो गणना समय पनि जनाउँछ। ठूलो स्तरको डीप लर्निङ मोडेलहरूमा, विशेष गरी जब धेरै मोडेल प्यारामिटरहरू र विशाल डेटा हुन्छ, FP32 फर्म्याटले GPU मेमोरी अभाव वा इन्फरेन्स गति घटाउने समस्या ल्याउन सक्छ।

मोबाइल उपकरण वा IoT उपकरणहरूमा हामी Phi-3.x मोडेलहरूलाई INT4 मा रूपान्तरण गर्न सक्छौं, भने AI PC / Copilot PC ले उच्च प्रेसिजन जस्तै INT8, FP16, FP32 प्रयोग गर्न सक्छ।

हाल, विभिन्न हार्डवेयर निर्माताहरूले जेनरेटिभ मोडेलहरूलाई समर्थन गर्न विभिन्न फ्रेमवर्कहरू उपलब्ध गराएका छन्, जस्तै Intel को OpenVINO, Qualcomm को QNN, Apple को MLX, र Nvidia को CUDA, आदि, जुन मोडेल क्वान्टाइजेसनसँग संयोजन गरेर स्थानीय डिप्लोयमेन्ट पूरा गर्न सकिन्छ।

प्रविधिको हिसाबले, हामीसँग क्वान्टाइजेसनपछि विभिन्न फर्म्याट समर्थनहरू छन्, जस्तै PyTorch / Tensorflow फर्म्याट, GGUF, र ONNX। मैले GGUF र ONNX बीच फर्म्याट तुलना र अनुप्रयोग परिदृश्यहरू गरेको छु। यहाँ म ONNX क्वान्टाइजेसन फर्म्याट सिफारिस गर्छु, जसले मोडेल फ्रेमवर्कदेखि हार्डवेयरसम्म राम्रो समर्थन पाएको छ। यस अध्यायमा, हामी ONNX Runtime for GenAI, OpenVINO, र Apple MLX मार्फत मोडेल क्वान्टाइजेसनमा केन्द्रित हुनेछौं (यदि तपाईंसँग राम्रो तरिका छ भने, PR मार्फत दिन सक्नुहुन्छ)।

**यस अध्यायमा समावेश छन्**

1. [llama.cpp प्रयोग गरेर Phi-3.5 / 4 क्वान्टाइजिङ](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntime का लागि Generative AI एक्सटेन्सनहरू प्रयोग गरेर Phi-3.5 / 4 क्वान्टाइजिङ](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO प्रयोग गरेर Phi-3.5 / 4 क्वान्टाइजिङ](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX Framework प्रयोग गरेर Phi-3.5 / 4 क्वान्टाइजिङ](./UsingAppleMLXQuantifyingPhi.md)

**अस्वीकरण**:  
यो दस्तावेज AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) को प्रयोग गरी अनुवाद गरिएको हो। हामी शुद्धताका लागि प्रयासरत छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटि वा असत्यता हुन सक्छ। मूल दस्तावेज यसको मूल भाषामा आधिकारिक स्रोतको रूपमा मानिनेछ। महत्वपूर्ण जानकारीको लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न कुनै पनि गलतफहमी वा गलत व्याख्याका लागि हामी जिम्मेवार छैनौं।