<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-05T16:54:31+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ne"
}
-->
# **फाइ परिवार क्वान्टिफाइङ**

मोडेल क्वान्टिफिकेसन भनेको न्यूरल नेटवर्क मोडेलका प्यारामिटरहरू (जस्तै तौल र सक्रियता मानहरू) लाई ठूलो मान दायराबाट (सामान्यतया निरन्तर मान दायरा) सानो सीमित मान दायरामा म्याप गर्ने प्रक्रिया हो। यो प्रविधिले मोडेलको आकार र गणनात्मक जटिलता कम गर्न सक्छ र मोबाइल उपकरणहरू वा एम्बेडेड सिस्टम जस्ता स्रोत-सीमित वातावरणमा मोडेलको सञ्चालन दक्षता सुधार गर्न सक्छ। मोडेल क्वान्टिफिकेसनले प्यारामिटरहरूको सटीकता कम गरेर संकुचन हासिल गर्छ, तर यसले निश्चित सटीकता हानी पनि ल्याउँछ। त्यसैले, क्वान्टिफिकेसन प्रक्रियामा मोडेल आकार, गणनात्मक जटिलता, र सटीकता बीच सन्तुलन आवश्यक हुन्छ। सामान्य क्वान्टिफिकेसन विधिहरूमा फिक्स्ड-प्वाइन्ट क्वान्टिफिकेसन, फ्लोटिङ-प्वाइन्ट क्वान्टिफिकेसन आदि समावेश छन्। तपाईंले विशेष परिदृश्य र आवश्यकताअनुसार उपयुक्त क्वान्टिफिकेसन रणनीति चयन गर्न सक्नुहुन्छ।

हामी GenAI मोडेललाई एज उपकरणहरूमा तैनाथ गर्न चाहन्छौं र अझ धेरै उपकरणहरूलाई GenAI परिदृश्यहरूमा ल्याउन चाहन्छौं, जस्तै मोबाइल उपकरणहरू, AI PC/Copilot+PC, र पारम्परिक IoT उपकरणहरू। क्वान्टिफिकेसन मोडेल मार्फत, हामी यसलाई विभिन्न उपकरणहरूसँग आधारित विभिन्न एज उपकरणहरूमा तैनाथ गर्न सक्छौं। हार्डवेयर निर्माताहरूले प्रदान गरेको मोडेल छिटो गराउने फ्रेमवर्क र क्वान्टिफिकेसन मोडेलसँग मिलाएर हामी राम्रो SLM अनुप्रयोग परिदृश्यहरू निर्माण गर्न सक्छौं।

क्वान्टिफिकेसन परिदृश्यमा हामीसँग विभिन्न सटीकताहरू (INT4, INT8, FP16, FP32) छन्। तल सामान्य रूपमा प्रयोग हुने क्वान्टिफिकेसन सटीकताहरूको व्याख्या छ।

### **INT4**

INT4 क्वान्टिफिकेसन एक चरम क्वान्टिफिकेसन विधि हो जसले मोडेलका तौल र सक्रियता मानहरूलाई ४-बिट पूर्णासङ्ख्यामा क्वान्टिफाइ गर्दछ। INT4 क्वान्टिफिकेसन प्रायः सानो प्रतिनिधित्व दायराका कारण र कम सटीकताका कारण ठूलो सटीकता हानिको परिणाम दिन सक्छ। तर, INT8 क्वान्टिफिकेसनसँग तुलना गर्दा, INT4 क्वान्टिफिकेसनले मोडेलको भण्डारण आवश्यकताहरू र गणनात्मक जटिलता अझ बढि कम गर्न सक्छ। ध्यान दिनुपर्ने कुरा के हो भने, प्रायोगिक अनुप्रयोगहरूमा INT4 क्वान्टिफिकेसन तुलनात्मक रूपमा दुर्लभ छ, किनभने अत्यधिक कम सटीकताले मोडेल प्रदर्शनमा ठूलो गिरावट ल्याउन सक्छ। थप रूपमा, सबै हार्डवेयरहरूले INT4 अपरेशन्स समर्थन गर्दैनन्, त्यसैले क्वान्टिफिकेसन विधि छनौट गर्दा हार्डवेयर अनुकूलता विचार गर्नुपर्दछ।

### **INT8**

INT8 क्वान्टिफिकेसन भनेको मोडेलका तौल र सक्रियतालाई फ्लोटिङ प्वाइन्ट नम्बरबाट ८-बिट पूर्णासङ्ख्यामा रूपान्तरण गर्ने प्रक्रिया हो। यद्यपि INT8 पूर्णासङ्ख्याहरुले प्रतिनिधित्व गर्ने संख्यात्मक दायरा सानो र कम सटीक हुन्छ, यसले भण्डारण र गणनाको आवश्यकतालाई उल्लेखनीय रूपमा घटाउन सक्छ। INT8 क्वान्टिफिकेसनमा, मोडेलका तौल र सक्रियता मानहरू क्वान्टिफिकेसन प्रक्रियाबाट जाँदैछन्, जसमा स्केलिङ र अफसेट समावेश छ, जसले मूल फ्लोटिङ प्वाइन्ट जानकारी जति सम्भव भए जति जगेर्ना गर्छ। इन्फरेन्सको क्रममा, यी क्वान्टिफाइड मानहरू पुनः फ्लोटिङ प्वाइन्ट नम्बरमा डिक्वान्टाइज गरिन्छन्, अनि पुनः अर्को चरणका लागि INT8 मा क्वान्टिफाइ गरिन्छ। यस विधिले अधिकांश अनुप्रयोगहरूमा पर्याप्त सटीकता प्रदान गर्न सक्छ र उच्च गणनात्मक दक्षता पनि कायम राख्छ।

### **FP16**

FP16 ढाँचा, अर्थात् १६-बिट फ्लोटिङ प्वाइन्ट नम्बरहरू (float16), ३२-बिट फ्लोटिङ प्वाइन्ट नम्बरहरू (float32) को तुलनामा मेमोरीको खपत आधामा घटाउँछ, जसले ठूलो परिमाणमा दीप लर्निङ अनुप्रयोगहरूमा महत्त्वपूर्ण फाइदा पुर्‍याउँछ। FP16 ढाँचाले एउटै GPU मेमोरी सीमाभित्र ठूलो मोडेलहरू लोड गर्न वा अधिक डेटा प्रक्रिया गर्न अनुमति दिन्छ। आधुनिक GPU हार्डवेयरहरूले FP16 अपरेशन्सलाई निरन्तर समर्थन गरिरहेकाले, FP16 ढाँचाको प्रयोगले गणनाको गति सुधार पनि ल्याउन सक्छ। तर FP16 ढाँचाका आफ्नै सीमाहरू पनि छन्, जस्तै कम सटीकता जसले केही अवस्थामा संख्यात्मक अस्थिरता वा सटीकता हानी पुर्‍याउन सक्छ।

### **FP32**

FP32 ढाँचाले उच्च सटीकता प्रदान गर्छ र फराकिलो मान दायरालाई सही रूपमा प्रतिनिधित्व गर्न सक्छ। जहाँ जटिल गणितीय अपरेशन्स गरिन्छ वा उच्च सटीक नतिजा आवश्यक हुन्छ, त्यहाँ FP32 ढाँचा प्राथमिकता पाइन्छ। तर उच्च सटीकताले बढी मेमोरी खपत र लामो गणना समय पनि जनाउँछ। ठूलो परिमाणको दीप लर्निङ मोडेलहरूमा, विशेष गरी जब धेरै मोडेल प्यारामिटरहरू र ठूलो डेटा हुन्छ, FP32 ढाँचाले GPU मेमोरी अपर्याप्त वा इन्फरेन्सको गति घटाउन सक्छ।

मोबाइल उपकरणहरू वा IoT उपकरणहरूमा, हामी Phi-3.x मोडेलहरूलाई INT4 मा रूपान्तरण गर्न सक्छौं, जबकि AI PC / Copilot PC मा उच्च सटीकताहरू जस्तै INT8, FP16, FP32 प्रयोग गर्न सकिन्छ।

हाल, फरक हार्डवेयर निर्माताहरूले जनरेटिभ मोडेलहरूलाई समर्थन गर्न फरक फ्रेमवर्कहरू छन्, जस्तै Intel को OpenVINO, Qualcomm को QNN, Apple को MLX, र Nvidia को CUDA आदि, मोडेल क्वान्टिफिकेसनसँग मिलाएर स्थानीय तैनाथी पूरा गर्न।

प्रविधिको हिसाबले, हामीसँग क्वान्टिफिकेसन पछि फरक ढाँचाहरूको समर्थन छ, जस्तै PyTorch / TensorFlow ढाँचा, GGUF, र ONNX। मैले GGUF र ONNX बीचमा ढाँचा तुलना र अनुप्रयोग परिदृश्यहरू गरेको छु। यहाँ म ONNX क्वान्टिफिकेसन ढाँचाको सिफारिस गर्दछु, जसले मोडेल फ्रेमवर्कदेखि हार्डवेयरसम्म राम्रो समर्थन प्राप्त गरेको छ। यस अध्यायमा, हामी GenAI का लागि ONNX Runtime, OpenVINO, र Apple MLX मार्फत मोडेल क्वान्टिफिकेसनमा केन्द्रित हुनेछौं (यदि तपाईंसँग राम्रो तरिका छ भने, PR पठाएर हामीलाई दिन सक्नुहुन्छ)।

**यस अध्यायमा समावेश छन्**

1. [llama.cpp प्रयोग गरेर Phi-3.5 / 4 क्वान्टिफाइङ](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntime का लागि जनरेटिभ AI विस्तारहरू प्रयोग गरेर Phi-3.5 / 4 क्वान्टिफाइङ](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO प्रयोग गरेर Phi-3.5 / 4 क्वान्टिफाइङ](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX Framework प्रयोग गरेर Phi-3.5 / 4 क्वान्टिफाइङ](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**अस्वीकरण**:  
यो दस्तावेज [Co-op Translator](https://github.com/Azure/co-op-translator) नामक एआई अनुवाद सेवाको प्रयोग गरेर अनुवाद गरिएको हो। हामी शुद्धताका लागि प्रयासरत छौं भने पनि, कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटि वा अशुद्धि हुनसक्छ। मूल दस्तावेज यसको आफ्नै भाषामा अधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीका लागि पेशेवर मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगले उत्पन्न भएका कुनै पनि गलत बुझाइ वा व्याख्याका लागि हामी जिम्मेवार छैनौं।
<!-- CO-OP TRANSLATOR DISCLAIMER END -->