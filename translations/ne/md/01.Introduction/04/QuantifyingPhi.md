<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-07-16T21:44:38+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ne"
}
-->
# **Phi परिवारको क्वान्टिफिकेशन**

मोडेल क्वान्टाइजेशन भनेको न्यूरल नेटवर्क मोडेलका प्यारामिटरहरू (जस्तै तौल र सक्रियता मानहरू) लाई ठूलो मान दायराबाट (सामान्यतया निरन्तर मान दायरा) सानो सीमित मान दायरामा रूपान्तरण गर्ने प्रक्रिया हो। यो प्रविधिले मोडेलको आकार र गणनात्मक जटिलता घटाउन सक्छ र मोबाइल उपकरणहरू वा एम्बेडेड प्रणालीहरू जस्ता स्रोत सीमित वातावरणमा मोडेलको सञ्चालन दक्षता सुधार गर्न मद्दत गर्छ। मोडेल क्वान्टाइजेशनले प्यारामिटरहरूको सटीकता घटाएर कम्प्रेसन हासिल गर्छ, तर यसले केही सटीकता ह्रास पनि ल्याउँछ। त्यसैले, क्वान्टाइजेशन प्रक्रियामा मोडेलको आकार, गणनात्मक जटिलता, र सटीकताको सन्तुलन आवश्यक हुन्छ। सामान्य क्वान्टाइजेशन विधिहरूमा fixed-point क्वान्टाइजेशन, floating-point क्वान्टाइजेशन आदि समावेश छन्। तपाईंले विशेष परिदृश्य र आवश्यकताअनुसार उपयुक्त क्वान्टाइजेशन रणनीति छनोट गर्न सक्नुहुन्छ।

हामी GenAI मोडेललाई edge उपकरणहरूमा तैनाथ गर्न चाहन्छौं र थप उपकरणहरूलाई GenAI परिदृश्यमा ल्याउन चाहन्छौं, जस्तै मोबाइल उपकरणहरू, AI PC/Copilot+PC, र परम्परागत IoT उपकरणहरू। क्वान्टाइजेशन मोडेलमार्फत, हामी यसलाई विभिन्न edge उपकरणहरूमा फरक-फरक उपकरणहरूका आधारमा तैनाथ गर्न सक्छौं। हार्डवेयर निर्माताहरूले प्रदान गरेको मोडेल एक्सेलेरेशन फ्रेमवर्क र क्वान्टाइजेशन मोडेलसँग मिलाएर, हामी राम्रो SLM अनुप्रयोग परिदृश्यहरू निर्माण गर्न सक्छौं।

क्वान्टाइजेशन परिदृश्यमा, हामीसँग विभिन्न सटीकताहरू छन् (INT4, INT8, FP16, FP32)। तल सामान्यतया प्रयोग गरिने क्वान्टाइजेशन सटीकताहरूको व्याख्या गरिएको छ।

### **INT4**

INT4 क्वान्टाइजेशन एक चरम क्वान्टाइजेशन विधि हो जसले मोडेलका तौल र सक्रियता मानहरूलाई 4-बिट पूर्णांकमा रूपान्तरण गर्छ। INT4 क्वान्टाइजेशनले सानो प्रतिनिधित्व दायराका कारण र कम सटीकताका कारण प्रायः ठूलो सटीकता ह्रास ल्याउँछ। तर, INT8 क्वान्टाइजेशनको तुलनामा, INT4 क्वान्टाइजेशनले मोडेलको भण्डारण आवश्यकताहरू र गणनात्मक जटिलता अझै कम गर्न सक्छ। ध्यान दिनुपर्ने कुरा के हो भने, व्यावहारिक प्रयोगमा INT4 क्वान्टाइजेशन तुलनात्मक रूपमा कम प्रयोग हुन्छ, किनभने धेरै कम सटीकताले मोडेल प्रदर्शनमा ठूलो गिरावट ल्याउन सक्छ। साथै, सबै हार्डवेयरले INT4 अपरेसन समर्थन गर्दैनन्, त्यसैले क्वान्टाइजेशन विधि छनोट गर्दा हार्डवेयर अनुकूलता विचार गर्नुपर्छ।

### **INT8**

INT8 क्वान्टाइजेशन भनेको मोडेलका तौल र सक्रियता मानहरूलाई फ्लोटिङ पोइन्टबाट 8-बिट पूर्णांकमा रूपान्तरण गर्ने प्रक्रिया हो। INT8 पूर्णांकहरूले प्रतिनिधित्व गर्ने संख्यात्मक दायरा सानो र कम सटीक भए तापनि, यसले भण्डारण र गणना आवश्यकताहरूमा उल्लेखनीय कमी ल्याउन सक्छ। INT8 क्वान्टाइजेशनमा, मोडेलका तौल र सक्रियता मानहरू स्केलिङ र अफसेट सहित क्वान्टाइजेशन प्रक्रियाबाट गुज्रन्छन्, जसले मूल फ्लोटिङ पोइन्ट जानकारीलाई सकेसम्म जोगाउँछ। इन्फरेन्सको क्रममा, यी क्वान्टाइज्ड मानहरूलाई पुनः फ्लोटिङ पोइन्टमा डिक्वान्टाइज गरिन्छ र गणनाका लागि प्रयोग गरिन्छ, त्यसपछि अर्को चरणका लागि फेरि INT8 मा क्वान्टाइज गरिन्छ। यो विधिले अधिकांश अनुप्रयोगहरूमा पर्याप्त सटीकता प्रदान गर्न सक्छ र उच्च गणनात्मक दक्षता कायम राख्छ।

### **FP16**

FP16 ढाँचा, अर्थात् 16-बिट फ्लोटिङ पोइन्ट नम्बरहरू (float16), 32-बिट फ्लोटिङ पोइन्ट नम्बरहरू (float32) को तुलनामा मेमोरी खपत आधा घटाउँछ, जसले ठूलो स्तरको गहिरो सिकाइ अनुप्रयोगहरूमा महत्वपूर्ण फाइदा पुर्‍याउँछ। FP16 ढाँचाले समान GPU मेमोरी सीमाभित्र ठूलो मोडेलहरू लोड गर्न वा बढी डाटा प्रशोधन गर्न अनुमति दिन्छ। आधुनिक GPU हार्डवेयरहरूले FP16 अपरेसनलाई निरन्तर समर्थन गरिरहेकाले, FP16 ढाँचाको प्रयोगले गणनाको गति पनि सुधार गर्न सक्छ। तर, FP16 ढाँचाको आफ्नै कमजोरी छ, अर्थात् कम सटीकता, जसले केही अवस्थामा संख्यात्मक अस्थिरता वा सटीकता ह्रास निम्त्याउन सक्छ।

### **FP32**

FP32 ढाँचाले उच्च सटीकता प्रदान गर्छ र फराकिलो मान दायरा सही रूपमा प्रतिनिधित्व गर्न सक्छ। जटिल गणितीय अपरेसनहरू गरिनुपर्ने वा उच्च सटीक परिणाम आवश्यक पर्ने परिदृश्यहरूमा FP32 ढाँचा प्राथमिकता दिइन्छ। तर, उच्च सटीकताले बढी मेमोरी प्रयोग र लामो गणना समय पनि जनाउँछ। ठूलो स्तरको गहिरो सिकाइ मोडेलहरूमा, विशेष गरी जब धेरै मोडेल प्यारामिटरहरू र ठूलो मात्रामा डाटा हुन्छ, FP32 ढाँचाले GPU मेमोरी अभाव वा इन्फरेन्स गति घटाउन सक्छ।

मोबाइल उपकरणहरू वा IoT उपकरणहरूमा, हामी Phi-3.x मोडेलहरूलाई INT4 मा रूपान्तरण गर्न सक्छौं, जबकि AI PC / Copilot PC मा उच्च सटीकता जस्तै INT8, FP16, FP32 प्रयोग गर्न सकिन्छ।

हाल, विभिन्न हार्डवेयर निर्माताहरूले जेनेरेटिभ मोडेलहरूलाई समर्थन गर्न फरक-फरक फ्रेमवर्कहरू विकास गरेका छन्, जस्तै Intel को OpenVINO, Qualcomm को QNN, Apple को MLX, र Nvidia को CUDA आदि, जसलाई मोडेल क्वान्टाइजेशनसँग मिलाएर स्थानीय तैनाथी पूरा गर्न सकिन्छ।

प्रविधिको हिसाबले, क्वान्टाइजेशन पछि हामीसँग PyTorch / Tensorflow ढाँचा, GGUF, र ONNX जस्ता फरक ढाँचाहरूको समर्थन छ। मैले GGUF र ONNX बीच ढाँचा तुलना र अनुप्रयोग परिदृश्यहरू गरेको छु। यहाँ म ONNX क्वान्टाइजेशन ढाँचाको सिफारिस गर्छु, जसले मोडेल फ्रेमवर्कदेखि हार्डवेयरसम्म राम्रो समर्थन पाउँछ। यस अध्यायमा, हामी GenAI का लागि ONNX Runtime, OpenVINO, र Apple MLX प्रयोग गरेर मोडेल क्वान्टाइजेशनमा केन्द्रित हुनेछौं (यदि तपाईंसँग राम्रो तरिका छ भने, PR मार्फत हामीलाई दिन सक्नुहुन्छ)।

**यस अध्यायमा समावेश छ**

1. [llama.cpp प्रयोग गरेर Phi-3.5 / 4 क्वान्टाइजेशन](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntime का लागि Generative AI एक्सटेन्सन प्रयोग गरेर Phi-3.5 / 4 क्वान्टाइजेशन](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO प्रयोग गरेर Phi-3.5 / 4 क्वान्टाइजेशन](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX Framework प्रयोग गरेर Phi-3.5 / 4 क्वान्टाइजेशन](./UsingAppleMLXQuantifyingPhi.md)

**अस्वीकरण**:  
यो दस्तावेज AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरी अनुवाद गरिएको हो। हामी शुद्धताका लागि प्रयासरत छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटि वा अशुद्धता हुनसक्छ। मूल दस्तावेज यसको मूल भाषामा नै अधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीका लागि व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न कुनै पनि गलतफहमी वा गलत व्याख्याका लागि हामी जिम्मेवार छैनौं।