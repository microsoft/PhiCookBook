# Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper

## अवलोकन

Interactive Phi 3 Mini 4K Instruct Chatbot एउटा उपकरण हो जसले प्रयोगकर्ताहरूलाई Microsoft Phi 3 Mini 4K instruct डेमो सँग पाठ वा अडियो इनपुट प्रयोग गरी अन्तरक्रिया गर्न अनुमति दिन्छ। यो च्याटबोट अनुवाद, मौसम अपडेट, र सामान्य जानकारी सङ्कलन जस्ता विभिन्न कार्यहरूको लागि प्रयोग गर्न सकिन्छ।

### सुरु गर्ने तरिका

यो च्याटबोट प्रयोग गर्न, तल दिइएका निर्देशनहरू पालना गर्नुहोस्:

1. नयाँ [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) खोल्नुहोस्।
2. नोटबुकको मुख्य विन्डोमा, तपाईले पाठ इनपुट बक्स र "Send" बटनसहितको च्याटबक्स इन्टरफेस देख्नु हुनेछ।
3. पाठ-आधारित च्याटबोट प्रयोग गर्न, आफ्नो सन्देश पाठ इनपुट बक्समा टाइप गर्नुहोस् र "Send" बटनमा क्लिक गर्नुहोस्। च्याटबोटले नोटबुकभित्रै प्ले गर्न मिल्ने अडियो फाइलसँग प्रतिक्रिया दिनेछ।

**सूचना**: यो उपकरणका लागि GPU र Microsoft Phi-3 र OpenAI Whisper मोडेलहरूमा पहुँच आवश्यक छ, जुन भाषण पहिचान र अनुवादका लागि प्रयोग गरिन्छ।

### GPU आवश्यकताहरू

यो डेमो चलाउनका लागि तपाईलाई १२ जीबी GPU मेमोरी आवश्यक छ।

**Microsoft-Phi-3-Mini-4K instruct** डेमो GPU मा चलाउँदा आवश्यक मेमोरी विभिन्न तत्वहरूमा निर्भर गर्नेछ, जस्तै इनपुट डाटाको आकार (अडियो वा पाठ), अनुवादमा प्रयोग हुने भाषा, मोडेलको गति, र GPU मा उपलब्ध मेमोरी।

सामान्यतया, Whisper मोडेल GPU मा चलाउन डिजाइन गरिएको हो। Whisper मोडेल चलाउनको लागि सिफारिस गरिएको न्यूनतम GPU मेमोरी ८ जीबी हो, तर आवश्यक भए ठूलो मेमोरी पनि सम्हाल्न सक्छ।

ध्यान दिनुहोस् कि ठूलो मात्रा डाटा वा उच्च अनुरोधहरूको मात्रा मोडेलमा चलाउँदा बढी GPU मेमोरी आवश्यक पर्न सक्छ वा प्रदर्शन समस्याहरू आउन सक्छ। तपाईंको विशिष्ट आवश्यकताहरूको लागि उपयुक्त संयोजन पत्ता लगाउन विभिन्न कन्फिगरेसनहरूमा परीक्षण गर्नु र मेमोरी उपयोग निगरानी गर्नु सिफारिस गरिन्छ।

## Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper को लागि E2E नमुना

[Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) शीर्षकको जुपिटर नोटबुकले Microsoft Phi 3 Mini 4K instruct डेमो कसरी अडियो वा लेखिएको पाठ इनपुटबाट टेक्स्ट उत्पन्न गर्न प्रयोग गर्ने देखाउँछ। नोटबुकले केही फङ्क्सनहरू परिभाषित गर्दछ:

1. `tts_file_name(text)`: यो फङ्क्सनले उत्पन्न अडियो फाइल भण्डारण गर्न इनपुट पाठको आधारमा फाइल नाम उत्पन्न गर्दछ।
1. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: यसले Edge TTS API प्रयोग गरी इनपुट पाठका टुक्राहरूको सूचीबाट अडियो फाइल उत्पन्न गर्दछ। इनपुट प्यारामिटरहरूमा टुक्राहरूको सूची, भाषण गति, आवाजको नाम, र उत्पन्न अडियो फाइल भण्डारण गर्ने पथ समावेश छन्।
1. `talk(input_text)`: यो फङ्क्सनले Edge TTS API प्रयोग गरी अडियो फाइल उत्पन्न गरेर /content/audio निर्देशनिकामा एउटा बेतरतीब फाइल नाममा सुरक्षित गर्छ। इनपुट प्यारामिटर इनपुट भाषणमा रूपान्तरण गरिने पाठ हो।
1. `run_text_prompt(message, chat_history)`: यो फङ्क्सनले Microsoft Phi 3 Mini 4K instruct डेमो प्रयोग गरी सन्देश इनपुटबाट अडियो फाइल उत्पन्न गर्छ र त्यसलाई च्याट इतिहासमा थप्छ।
1. `run_audio_prompt(audio, chat_history)`: यो फङ्क्सनले Whisper मोडेल API प्रयोग गरी अडियो फाइललाई पाठमा रुपान्तरण गर्छ र त्यसलाई `run_text_prompt()` फङ्क्सनमा पास गर्छ।
1. कोडले एउटा Gradio एप सुरु गर्छ जसले प्रयोगकर्ताहरूलाई सन्देश टाइप गरेर वा अडियो फाइलहरू अपलोड गरेर Phi 3 Mini 4K instruct डेमोसँग अन्तरक्रिया गर्न अनुमति दिन्छ। आउटपुट एप भित्रै पाठ सन्देशको रूपमा देखाइन्छ।

## समस्या समाधान

Cuda GPU ड्राइभरहरू स्थापना गर्दै

1. तपाईको Linux एप्लिकेसनहरू अद्यावधिक गर्नुहोस्।

    ```bash
    sudo apt update
    ```

1. Cuda ड्राइभरहरू स्थापना गर्नुहोस्।

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. cuda ड्राइभर स्थान दर्ता गर्नुहोस्।

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. Nvidia GPU मेमोरी साइज जाँच्नुहोस् (आवश्यक १२GB GPU मेमोरी)।

    ```bash
    nvidia-smi
    ```

1. खाली क्यास: यदि तपाईं PyTorch प्रयोग गर्दै हुनुहुन्छ भने, torch.cuda.empty_cache() कल गर्नुहोस् जसले सबै अप्रयुक्त क्यास गरिएको मेमोरी रिलीज गर्छ ताकि अन्य GPU एप्लिकेसनहरूले प्रयोग गर्न सकून्।

    ```python
    torch.cuda.empty_cache() 
    ```

1. Nvidia Cuda जाँच्दै

    ```bash
    nvcc --version
    ```

1. Hugging Face टोकन बनाउन तलका कार्यहरू गर्नुहोस्।

    - [Hugging Face Token Settings page](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo) मा जानुहोस्।
    - **New token** छान्नुहोस्।
    - तपाईंले प्रयोग गर्न चाहेको परियोजनाको **Name** लेख्नुहोस्।
    - **Type** लाई **Write** छान्नुहोस्।

> [!NOTE]
>
> यदि तपाईंलाई तलको त्रुटि आउँछ:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> यसलाई समाधान गर्न, आफ्नो टर्मिनल भित्र तलको आदेश टाइप गर्नुहोस्।
>
> ```bash
> sudo ldconfig
> ```

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**अस्वीकरण**:  
यो दस्तावेज AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) मार्फत अनुवाद गरिएको हो। हामी सहीताको प्रयास गर्छौं भने पनि, कृपया बुझ्नुहोस् कि स्वचालित अनुवादमा त्रुटि वा अशुद्धि हुन सक्नेछ। मूल भाषामा रहेको दस्तावेजलाई अधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीको लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याबारे हामी जिम्मेवार छैनौं।
<!-- CO-OP TRANSLATOR DISCLAIMER END -->