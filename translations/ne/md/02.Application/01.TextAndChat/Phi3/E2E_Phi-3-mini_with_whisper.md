<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "006e8cf75211d3297f24e1b22e38955f",
  "translation_date": "2025-07-17T02:16:48+00:00",
  "source_file": "md/02.Application/01.TextAndChat/Phi3/E2E_Phi-3-mini_with_whisper.md",
  "language_code": "ne"
}
-->
# Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper

## अवलोकन

Interactive Phi 3 Mini 4K Instruct Chatbot एउटा उपकरण हो जसले प्रयोगकर्ताहरूलाई Microsoft Phi 3 Mini 4K instruct डेमो सँग पाठ वा अडियो इनपुट मार्फत अन्तरक्रिया गर्न अनुमति दिन्छ। यो च्याटबोट अनुवाद, मौसम अपडेट, र सामान्य जानकारी सङ्कलन जस्ता विभिन्न कार्यहरूका लागि प्रयोग गर्न सकिन्छ।

### सुरु गर्ने तरिका

यो च्याटबोट प्रयोग गर्न, तलका निर्देशनहरू पालना गर्नुहोस्:

1. नयाँ [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) खोल्नुहोस्
2. नोटबुकको मुख्य विन्डोमा, तपाईंले एउटा च्याटबक्स इन्टरफेस देख्नुहुनेछ जसमा पाठ इनपुट बक्स र "Send" बटन हुन्छ।
3. पाठ-आधारित च्याटबोट प्रयोग गर्न, आफ्नो सन्देश पाठ इनपुट बक्समा टाइप गर्नुहोस् र "Send" बटनमा क्लिक गर्नुहोस्। च्याटबोटले अडियो फाइलको रूपमा जवाफ दिनेछ जुन नोटबुक भित्रै प्ले गर्न सकिन्छ।

**Note**: यो उपकरण चलाउन GPU र Microsoft Phi-3 तथा OpenAI Whisper मोडेलहरूमा पहुँच आवश्यक छ, जुन भाषण पहिचान र अनुवादका लागि प्रयोग हुन्छ।

### GPU आवश्यकताहरू

यो डेमो चलाउन तपाईंलाई १२ जीबी GPU मेमोरी चाहिन्छ।

**Microsoft-Phi-3-Mini-4K instruct** डेमो GPU मा चलाउँदा आवश्यक मेमोरी इनपुट डाटा (अडियो वा पाठ) को आकार, अनुवादको भाषा, मोडेलको गति, र GPU मा उपलब्ध मेमोरी जस्ता विभिन्न कारकहरूमा निर्भर गर्दछ।

सामान्यतया, Whisper मोडेल GPU मा चलाउन डिजाइन गरिएको हो। Whisper मोडेल चलाउन सिफारिस गरिएको न्यूनतम GPU मेमोरी ८ जीबी हो, तर आवश्यक परे ठूलो मेमोरी पनि सम्हाल्न सक्छ।

ध्यान दिनुहोस् कि ठूलो मात्रामा डाटा वा धेरै अनुरोधहरू मोडेलमा पठाउँदा थप GPU मेमोरी आवश्यक पर्न सक्छ र प्रदर्शनमा समस्या आउन सक्छ। तपाईंको प्रयोग केस विभिन्न कन्फिगरेसनमा परीक्षण गरी मेमोरी प्रयोग अनुगमन गर्न सिफारिस गरिन्छ ताकि तपाईंको आवश्यकताका लागि उपयुक्त सेटिङहरू निर्धारण गर्न सकियोस्।

## Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper को E2E नमूना

[Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) नामक जुपिटर नोटबुकले Microsoft Phi 3 Mini 4K instruct डेमो कसरी अडियो वा लेखिएको पाठ इनपुटबाट पाठ उत्पन्न गर्न प्रयोग गर्ने देखाउँछ। नोटबुकले केही फङ्सनहरू परिभाषित गरेको छ:

1. `tts_file_name(text)`: यो फङ्सनले इनपुट पाठको आधारमा अडियो फाइल सुरक्षित गर्नको लागि फाइल नाम बनाउँछ।
1. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: यो फङ्सनले Edge TTS API प्रयोग गरी इनपुट पाठका टुक्राहरूको सूचीबाट अडियो फाइल बनाउँछ। इनपुट प्यारामिटरहरूमा टुक्राहरूको सूची, भाषण गति, आवाजको नाम, र अडियो फाइल सुरक्षित गर्ने पथ समावेश छन्।
1. `talk(input_text)`: यो फङ्सनले Edge TTS API प्रयोग गरी अडियो फाइल बनाउँछ र /content/audio डाइरेक्टरीमा यादृच्छिक फाइल नाममा सुरक्षित गर्छ। इनपुट प्यारामिटर इनपुट पाठ हो जुन भाषणमा रूपान्तरण गरिन्छ।
1. `run_text_prompt(message, chat_history)`: यो फङ्सनले Microsoft Phi 3 Mini 4K instruct डेमो प्रयोग गरी सन्देश इनपुटबाट अडियो फाइल बनाउँछ र यसलाई च्याट इतिहासमा थप्छ।
1. `run_audio_prompt(audio, chat_history)`: यो फङ्सनले Whisper मोडेल API प्रयोग गरी अडियो फाइललाई पाठमा रूपान्तरण गर्छ र त्यसलाई `run_text_prompt()` फङ्सनमा पठाउँछ।
1. कोडले Gradio एप सुरु गर्छ जसले प्रयोगकर्ताहरूलाई सन्देश टाइप गरेर वा अडियो फाइल अपलोड गरेर Phi 3 Mini 4K instruct डेमो सँग अन्तरक्रिया गर्न अनुमति दिन्छ। आउटपुट एप भित्रै पाठ सन्देशको रूपमा देखाइन्छ।

## समस्या समाधान

Cuda GPU ड्राइभरहरू स्थापना गर्ने

1. आफ्नो Linux एप्लिकेसनहरू अपडेट गर्नुहोस्

    ```bash
    sudo apt update
    ```

1. Cuda ड्राइभरहरू स्थापना गर्नुहोस्

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. Cuda ड्राइभर स्थान दर्ता गर्नुहोस्

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. Nvidia GPU मेमोरी साइज जाँच गर्नुहोस् (१२GB GPU मेमोरी आवश्यक)

    ```bash
    nvidia-smi
    ```

1. क्यास खाली गर्नुहोस्: यदि तपाईं PyTorch प्रयोग गर्दै हुनुहुन्छ भने, torch.cuda.empty_cache() कल गरेर सबै अप्रयुक्त क्यास गरिएको मेमोरी खाली गर्न सक्नुहुन्छ जसले अन्य GPU एप्लिकेसनहरूले प्रयोग गर्न सकून्।

    ```python
    torch.cuda.empty_cache() 
    ```

1. Nvidia Cuda जाँच गर्नुहोस्

    ```bash
    nvcc --version
    ```

1. Hugging Face टोकन बनाउन तलका कार्यहरू गर्नुहोस्।

    - [Hugging Face Token Settings page](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo) मा जानुहोस्।
    - **New token** चयन गर्नुहोस्।
    - प्रयोग गर्न चाहेको परियोजनाको **Name** प्रविष्ट गर्नुहोस्।
    - **Type** लाई **Write** मा चयन गर्नुहोस्।

> **Note**
>
> यदि तपाईंले तलको त्रुटि देख्नुभयो भने:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> यसलाई समाधान गर्न, आफ्नो टर्मिनल भित्र तलको कमाण्ड टाइप गर्नुहोस्।
>
> ```bash
> sudo ldconfig
> ```

**अस्वीकरण**:  
यो दस्तावेज AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरी अनुवाद गरिएको हो। हामी शुद्धताका लागि प्रयासरत छौं भने पनि, कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटि वा अशुद्धता हुन सक्छ। मूल दस्तावेज यसको मूल भाषामा नै अधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीका लागि व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न कुनै पनि गलतफहमी वा गलत व्याख्याका लागि हामी जिम्मेवार छैनौं।