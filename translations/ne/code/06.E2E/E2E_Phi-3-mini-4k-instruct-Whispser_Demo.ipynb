{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## इन्टरएक्टिभ फाई ३ मिनी ४के इन्स्ट्रक्ट च्याटबोट विथ व्हिस्पर\n",
    "\n",
    "### परिचय:\n",
    "इन्टरएक्टिभ फाई ३ मिनी ४के इन्स्ट्रक्ट च्याटबोट एउटा उपकरण हो जसले प्रयोगकर्तालाई माइक्रोसफ्ट फाई ३ मिनी ४के इन्स्ट्रक्ट डेमोमा पाठ वा अडियो इनपुट प्रयोग गरेर अन्तरक्रिया गर्न अनुमति दिन्छ। यो च्याटबोट अनुवाद, मौसम अपडेट, र सामान्य जानकारी संकलन जस्ता विभिन्न कार्यहरूको लागि प्रयोग गर्न सकिन्छ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[तपाईंको Huggingface पहुँच टोकन बनाउनुहोस्](https://huggingface.co/settings/tokens)\n",
    "\n",
    "नयाँ टोकन बनाउनुहोस्  \n",
    "नयाँ नाम दिनुहोस्  \n",
    "लेखन अनुमति चयन गर्नुहोस्  \n",
    "टोकनलाई प्रतिलिपि गर्नुहोस् र सुरक्षित स्थानमा सुरक्षित गर्नुहोस्  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "यो Python कोडले दुई मुख्य कार्यहरू गर्दछ: `os` मोड्युल आयात गर्नु र वातावरण चर सेट गर्नु।\n",
    "\n",
    "1. `os` मोड्युल आयात गर्नु:\n",
    "   - Python मा `os` मोड्युलले अपरेटिङ सिस्टमसँग अन्तरक्रिया गर्नको लागि सुविधा प्रदान गर्दछ। यसले विभिन्न अपरेटिङ सिस्टमसँग सम्बन्धित कार्यहरू जस्तै वातावरण चरहरू पहुँच गर्नु, फाइल र डाइरेक्टरीहरूसँग काम गर्नु आदि गर्न अनुमति दिन्छ।\n",
    "   - यस कोडमा, `os` मोड्युललाई `import` स्टेटमेन्ट प्रयोग गरेर आयात गरिएको छ। यो स्टेटमेन्टले `os` मोड्युलको कार्यक्षमता हालको Python स्क्रिप्टमा प्रयोग गर्न उपलब्ध गराउँछ।\n",
    "\n",
    "2. वातावरण चर सेट गर्नु:\n",
    "   - वातावरण चर भनेको अपरेटिङ सिस्टममा चल्ने प्रोग्रामहरूले पहुँच गर्न सक्ने मान हो। यो कन्फिगरेसन सेटिङहरू वा अन्य जानकारी भण्डारण गर्नको लागि प्रयोग गरिन्छ, जसलाई धेरै प्रोग्रामहरूले प्रयोग गर्न सक्छन्।\n",
    "   - यस कोडमा, नयाँ वातावरण चर `os.environ` डिक्सनरी प्रयोग गरेर सेट गरिएको छ। डिक्सनरीको कुञ्जी `'HF_TOKEN'` हो, र मान `HUGGINGFACE_TOKEN` चरबाट असाइन गरिएको छ।\n",
    "   - `HUGGINGFACE_TOKEN` चर यस कोड स्निपेटको माथि परिभाषित गरिएको छ, र यसलाई `#@param` सिन्ट्याक्स प्रयोग गरेर `\"hf_**************\"` स्ट्रिङ मान असाइन गरिएको छ। यो सिन्ट्याक्स प्रायः Jupyter नोटबुकहरूमा प्रयोग गरिन्छ, जसले प्रयोगकर्तालाई इनपुट दिन र नोटबुक इन्टरफेसमा सीधा प्यारामिटर कन्फिगरेसन गर्न अनुमति दिन्छ।\n",
    "   - `'HF_TOKEN'` वातावरण चर सेट गरेर, यसलाई प्रोग्रामका अन्य भागहरू वा सोही अपरेटिङ सिस्टममा चल्ने अन्य प्रोग्रामहरूले पहुँच गर्न सक्छन्।\n",
    "\n",
    "समग्रमा, यो कोडले `os` मोड्युल आयात गर्दछ र `'HF_TOKEN'` नामको वातावरण चर सेट गर्दछ, जसको मान `HUGGINGFACE_TOKEN` चरमा प्रदान गरिएको मान हो।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "यो कोड स्निपेटले `clear_output` नामको एक फङ्क्सन परिभाषित गर्दछ, जुन Jupyter Notebook वा IPython मा हालको सेलको आउटपुट हटाउन प्रयोग गरिन्छ। अब कोडलाई टुक्रा-टुक्रामा बुझौं र यसको कार्यक्षमता स्पष्ट गरौं:\n",
    "\n",
    "`clear_output` फङ्क्सनले `wait` नामको एक प्यारामिटर लिन्छ, जुन एक बूलियन मान हो। डिफल्ट रूपमा, `wait` लाई `False` मा सेट गरिएको छ। यो प्यारामिटरले निर्धारण गर्दछ कि नयाँ आउटपुट उपलब्ध नभएसम्म हालको आउटपुट हटाउन पर्खनु पर्ने हो कि होइन।\n",
    "\n",
    "फङ्क्सन आफैं हालको सेलको आउटपुट हटाउन प्रयोग गरिन्छ। Jupyter Notebook वा IPython मा, जब कुनै सेलले आउटपुट उत्पादन गर्छ, जस्तै प्रिन्ट गरिएको टेक्स्ट वा ग्राफिकल प्लटहरू, त्यो आउटपुट सेलको तल देखिन्छ। `clear_output` फङ्क्सनले उक्त आउटपुट हटाउन अनुमति दिन्छ।\n",
    "\n",
    "फङ्क्सनको कार्यान्वयन कोड स्निपेटमा प्रदान गरिएको छैन, जसलाई `...` द्वारा संकेत गरिएको छ। `...` ले आउटपुट हटाउने वास्तविक कोडको लागि प्लेसहोल्डरको प्रतिनिधित्व गर्दछ। फङ्क्सनको कार्यान्वयनले Jupyter Notebook वा IPython API सँग अन्तरक्रिया गरेर सेलको हालको आउटपुट हटाउन समावेश गर्न सक्छ।\n",
    "\n",
    "समग्रमा, यो फङ्क्सनले Jupyter Notebook वा IPython मा हालको सेलको आउटपुट हटाउनको लागि एक सुविधाजनक तरिका प्रदान गर्दछ, जसले इन्टरएक्टिभ कोडिङ सेसनहरूमा देखाइएको आउटपुट व्यवस्थापन र अपडेट गर्न सजिलो बनाउँछ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "एज टीटीएस सेवा प्रयोग गरेर पाठ-देखि-भाषण (TTS) गर्ने प्रक्रिया। अब हामी सम्बन्धित कार्यान्वयनहरू एक-एक गरी हेर्नेछौं:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: यो कार्यले इनपुट मान लिन्छ र TTS आवाजको गति स्ट्रिङ गणना गर्छ। इनपुट मानले भाषणको चाहिएको गति जनाउँछ, जहाँ 1 ले सामान्य गति जनाउँछ। कार्यले इनपुट मानबाट 1 घटाउँछ, त्यसलाई 100 ले गुणा गर्छ, र इनपुट मान 1 भन्दा ठूलो वा बराबर छ कि छैन भन्ने आधारमा चिह्न निर्धारण गर्छ। कार्यले \"{sign}{rate}\" ढाँचामा गति स्ट्रिङ फिर्ता गर्छ।\n",
    "\n",
    "2. `make_chunks(input_text, language)`: यो कार्यले इनपुट पाठ र भाषा लिन्छ। यो इनपुट पाठलाई भाषा-विशिष्ट नियमहरूका आधारमा खण्डहरूमा विभाजन गर्छ। यस कार्यान्वयनमा, यदि भाषा \"English\" हो भने, कार्यले पाठलाई प्रत्येक पूर्णविराम (\".\") मा विभाजन गर्छ र कुनै पनि अगाडि वा पछाडि भएको खाली ठाउँ हटाउँछ। त्यसपछि यो प्रत्येक खण्डमा पूर्णविराम थप्छ र फिल्टर गरिएको खण्डहरूको सूची फिर्ता गर्छ।\n",
    "\n",
    "3. `tts_file_name(text)`: यो कार्यले TTS अडियो फाइलको नाम इनपुट पाठको आधारमा उत्पन्न गर्छ। यो पाठमा केही परिवर्तनहरू गर्छ: अन्त्यमा भएको पूर्णविराम (यदि छ भने) हटाउँछ, पाठलाई सानो अक्षरमा रूपान्तरण गर्छ, अगाडि र पछाडि भएको खाली ठाउँ हटाउँछ, र खाली ठाउँलाई अन्डरस्कोरले प्रतिस्थापन गर्छ। त्यसपछि यो पाठलाई अधिकतम 25 अक्षरमा छोट्याउँछ (यदि लामो छ भने) वा पाठ खाली भएमा पूर्ण पाठ प्रयोग गर्छ। अन्ततः, यो [`uuid`] मोड्युल प्रयोग गरेर एउटा र्यान्डम स्ट्रिङ उत्पन्न गर्छ र छोट्याइएको पाठसँग मिलाएर \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\" ढाँचामा फाइल नाम बनाउँछ।\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: यो कार्यले धेरै अडियो फाइलहरूलाई एकल अडियो फाइलमा मर्ज गर्छ। यो अडियो फाइल पथहरूको सूची र आउटपुट पथलाई प्यारामिटरको रूपमा लिन्छ। कार्यले [`AudioSegment`] नामक खाली वस्तु आरम्भ गर्छ। त्यसपछि यो प्रत्येक अडियो फाइल पथमा पुनरावृत्ति गर्छ, `pydub` पुस्तकालयको `AudioSegment.from_file()` विधि प्रयोग गरेर अडियो फाइल लोड गर्छ, र हालको अडियो फाइललाई [`merged_audio`] वस्तुमा थप्छ। अन्ततः, यो मर्ज गरिएको अडियोलाई निर्दिष्ट आउटपुट पथमा MP3 ढाँचामा निर्यात गर्छ।\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: यो कार्य एज टीटीएस सेवा प्रयोग गरेर TTS सञ्चालन गर्छ। यो पाठ खण्डहरूको सूची, भाषणको गति, आवाजको नाम, र सेभ पथलाई प्यारामिटरको रूपमा लिन्छ। यदि खण्डहरूको संख्या 1 भन्दा बढी छ भने, कार्यले व्यक्तिगत खण्ड अडियो फाइलहरू भण्डारण गर्नका लागि एउटा डाइरेक्टरी बनाउँछ। त्यसपछि यो प्रत्येक खण्डमा पुनरावृत्ति गर्छ, `calculate_rate_string()` कार्य, आवाजको नाम, र खण्ड पाठ प्रयोग गरेर एज टीटीएस आदेश निर्माण गर्छ, र `os.system()` कार्य प्रयोग गरेर आदेश कार्यान्वयन गर्छ। यदि आदेश सफलतापूर्वक कार्यान्वयन भयो भने, यो उत्पन्न अडियो फाइलको पथलाई सूचीमा थप्छ। सबै खण्डहरू प्रशोधन गरेपछि, यो व्यक्तिगत अडियो फाइलहरूलाई `merge_audio_files()` कार्य प्रयोग गरेर मर्ज गर्छ र मर्ज गरिएको अडियोलाई निर्दिष्ट सेभ पथमा भण्डारण गर्छ। यदि केवल एक खण्ड छ भने, यो सिधै एज टीटीएस आदेश उत्पन्न गर्छ र अडियोलाई सेभ पथमा भण्डारण गर्छ। अन्ततः, यो उत्पन्न अडियो फाइलको सेभ पथ फिर्ता गर्छ।\n",
    "\n",
    "6. `random_audio_name_generate()`: यो कार्य [`uuid`] मोड्युल प्रयोग गरेर एउटा र्यान्डम अडियो फाइल नाम उत्पन्न गर्छ। यो एउटा र्यान्डम UUID उत्पन्न गर्छ, यसलाई स्ट्रिङमा रूपान्तरण गर्छ, पहिलो 8 अक्षर लिन्छ, \".mp3\" एक्सटेन्सन थप्छ, र र्यान्डम अडियो फाइल नाम फिर्ता गर्छ।\n",
    "\n",
    "7. `talk(input_text)`: यो कार्य TTS सञ्चालन गर्ने मुख्य प्रवेश बिन्दु हो। यो इनपुट पाठलाई प्यारामिटरको रूपमा लिन्छ। यो पहिले इनपुट पाठको लम्बाइ जाँच गर्छ कि यो लामो वाक्य हो (600 अक्षर वा सोभन्दा बढी) कि होइन। लम्बाइ र `translate_text_flag` भेरिएबलको मानको आधारमा, यो भाषा निर्धारण गर्छ र `make_chunks()` कार्य प्रयोग गरेर पाठ खण्डहरूको सूची उत्पन्न गर्छ। त्यसपछि यो `random_audio_name_generate()` कार्य प्रयोग गरेर अडियो फाइलको सेभ पथ उत्पन्न गर्छ। अन्ततः, यो `edge_free_tts()` कार्यलाई TTS सञ्चालन गर्न बोलाउँछ र उत्पन्न अडियो फाइलको सेभ पथ फिर्ता गर्छ।\n",
    "\n",
    "समग्रमा, यी कार्यहरूले इनपुट पाठलाई खण्डहरूमा विभाजन गर्न, अडियो फाइलको नाम उत्पन्न गर्न, एज टीटीएस सेवा प्रयोग गरेर TTS सञ्चालन गर्न, र व्यक्तिगत अडियो फाइलहरूलाई एकल अडियो फाइलमा मर्ज गर्न मिलेर काम गर्छ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "दुई फङ्सनहरू: convert_to_text र run_text_prompt को कार्यान्वयन, साथै दुई कक्षाहरू: str र Audio को घोषणा।\n",
    "\n",
    "convert_to_text फङ्सनले audio_path लाई इनपुटको रूपमा लिन्छ र whisper_model नामक मोडेल प्रयोग गरेर अडियोलाई टेक्स्टमा ट्रान्सक्राइब गर्छ। फङ्सनले पहिलोमा जाँच गर्छ कि gpu फ्ल्याग True सेट गरिएको छ कि छैन। यदि True छ भने, whisper_model लाई word_timestamps=True, fp16=True, language='English', र task='translate' जस्ता केही प्यारामिटरहरूसँग प्रयोग गरिन्छ। यदि gpu फ्ल्याग False छ भने, whisper_model लाई fp16=False को साथ प्रयोग गरिन्छ। परिणामी ट्रान्सक्रिप्शनलाई 'scan.txt' नामक फाइलमा सुरक्षित गरिन्छ र टेक्स्टको रूपमा फिर्ता गरिन्छ।\n",
    "\n",
    "run_text_prompt फङ्सनले message र chat_history लाई इनपुटको रूपमा लिन्छ। यसले phi_demo फङ्सन प्रयोग गरेर इनपुट सन्देशको आधारमा च्याटबोटबाट प्रतिक्रिया उत्पन्न गर्छ। उत्पन्न गरिएको प्रतिक्रियालाई talk फङ्सनमा पठाइन्छ, जसले प्रतिक्रियालाई अडियो फाइलमा रूपान्तरण गर्छ र फाइलको पथ फिर्ता गर्छ। Audio कक्षा अडियो फाइललाई देखाउन र बजाउन प्रयोग गरिन्छ। अडियोलाई IPython.display मोड्युलको display फङ्सन प्रयोग गरेर देखाइन्छ, र Audio वस्तु autoplay=True प्यारामिटरको साथ सिर्जना गरिन्छ, जसले गर्दा अडियो स्वतः बज्न सुरु हुन्छ। chat_history लाई इनपुट सन्देश र उत्पन्न गरिएको प्रतिक्रियासँग अपडेट गरिन्छ, र खाली स्ट्रिङ र अपडेट गरिएको chat_history फिर्ता गरिन्छ।\n",
    "\n",
    "str कक्षा Python मा एक बिल्ट-इन कक्षा हो जसले क्यारेक्टरहरूको अनुक्रमलाई प्रतिनिधित्व गर्छ। यसले स्ट्रिङहरूलाई ह्यान्डल गर्न र काम गर्न विभिन्न विधिहरू प्रदान गर्छ, जस्तै capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill, र अन्य। यी विधिहरूले खोज्ने, प्रतिस्थापन गर्ने, फर्म्याट गर्ने, र स्ट्रिङहरूलाई ह्यान्डल गर्ने जस्ता कार्यहरू गर्न अनुमति दिन्छ।\n",
    "\n",
    "Audio कक्षा एक कस्टम कक्षा हो जसले अडियो वस्तुलाई प्रतिनिधित्व गर्छ। यो Jupyter Notebook वातावरणमा अडियो प्लेयर सिर्जना गर्न प्रयोग गरिन्छ। कक्षाले विभिन्न प्यारामिटरहरू स्वीकार गर्छ, जस्तै data, filename, url, embed, rate, autoplay, र normalize। data प्यारामिटर numpy array, स्याम्पलहरूको सूची, filename वा URL प्रतिनिधित्व गर्ने स्ट्रिङ, वा raw PCM data हुन सक्छ। filename प्यारामिटरले अडियो डेटा लोड गर्न स्थानीय फाइल निर्दिष्ट गर्न प्रयोग गरिन्छ, र url प्यारामिटरले अडियो डेटा डाउनलोड गर्न URL निर्दिष्ट गर्न प्रयोग गरिन्छ। embed प्यारामिटरले अडियो डेटा data URI प्रयोग गरेर एम्बेड गरिनु पर्ने वा मूल स्रोतबाट सन्दर्भित गरिनु पर्ने निर्धारण गर्छ। rate प्यारामिटरले अडियो डेटा को स्याम्पलिंग दर निर्दिष्ट गर्छ। autoplay प्यारामिटरले अडियो स्वतः बज्न सुरु हुने निर्धारण गर्छ। normalize प्यारामिटरले अडियो डेटा अधिकतम सम्भावित दायरामा पुनःस्केल गरिनु पर्ने निर्धारण गर्छ। Audio कक्षाले reload जस्ता विधिहरू प्रदान गर्छ जसले फाइल वा URL बाट अडियो डेटा पुनः लोड गर्न अनुमति दिन्छ, र src_attr, autoplay_attr, र element_id_attr जस्ता विशेषताहरू HTML मा अडियो तत्वको लागि सम्बन्धित विशेषताहरू पुनःप्राप्त गर्न अनुमति दिन्छ।\n",
    "\n",
    "समग्रमा, यी फङ्सनहरू र कक्षाहरू अडियोलाई टेक्स्टमा ट्रान्सक्राइब गर्न, च्याटबोटबाट अडियो प्रतिक्रिया उत्पन्न गर्न, र Jupyter Notebook वातावरणमा अडियो देखाउन र बजाउन प्रयोग गरिन्छ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**अस्वीकरण**:  \nयो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरेर अनुवाद गरिएको हो। हामी शुद्धताको लागि प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटिहरू वा अशुद्धताहरू हुन सक्छ। यसको मूल भाषा मा रहेको मूल दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीको लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याको लागि हामी जिम्मेवार हुने छैनौं।\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T19:55:27+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "ne"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}