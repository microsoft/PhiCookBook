{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime_genai as og\n",
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = og.Model('..\\..\\..\\..\\Models\\Phi-3-mini-4k-instruct-onnx\\directml\\directml-int4-awq-block-128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = og.Tokenizer(model)\n",
    "tokenizer_stream = tokenizer.create_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_options = {\"max_length\": 1024,\"temperature\":0.6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = og.GeneratorParams(model)\n",
    "params.try_use_cuda_graph_with_max_batch_size(1)\n",
    "params.set_search_options(**search_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<|system|>You are a helpful AI assistant.<|end|><|user|>Can you introduce yourself?<|end|><|assistant|>\"\n",
    "input_tokens = tokenizer.encode(prompt)\n",
    "params.input_ids = input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generator = og.Generator(model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Of course! I'm an AI developed by Microsoft, designed to assist and provide information to users like you. My purpose is to help answer your questions, provide guidance, and offer support in various areas such as general knowledge, research, and problem-solving. I'm powered by advanced machine learning algorithms, enabling me to understand and respond to your queries effectively. While I don't have personal experiences or emotions, I'm here to make your interactions as helpful and informative as possible. How can I assist you today?"
     ]
    }
   ],
   "source": [
    "while not generator.is_done():\n",
    "                generator.compute_logits()\n",
    "                generator.generate_next_token()\n",
    "\n",
    "                new_token = generator.get_next_tokens()[0]\n",
    "                print(tokenizer_stream.decode(new_token), end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**면책 조항**:  \n이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서를 해당 언어로 작성된 상태에서 권위 있는 자료로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역을 사용함으로 인해 발생하는 오해나 잘못된 해석에 대해 책임을 지지 않습니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "coopTranslator": {
   "original_hash": "016e5cff42352953a5e2cbb687ea247e",
   "translation_date": "2025-09-12T16:42:22+00:00",
   "source_file": "code/03.Inference/AIPC/AIPC_DirectML_DEMO.ipynb",
   "language_code": "ko"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}