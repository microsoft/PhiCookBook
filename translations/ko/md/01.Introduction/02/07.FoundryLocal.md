<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "52973a5680a65a810aa80b7036afd31f",
  "translation_date": "2025-07-16T19:44:08+00:00",
  "source_file": "md/01.Introduction/02/07.FoundryLocal.md",
  "language_code": "ko"
}
-->
## Foundry Local에서 Phi-Family 모델 시작하기

### Foundry Local 소개

Foundry Local은 강력한 온디바이스 AI 추론 솔루션으로, 엔터프라이즈급 AI 기능을 로컬 하드웨어에서 직접 사용할 수 있게 해줍니다. 이 튜토리얼에서는 Foundry Local과 함께 Phi-Family 모델을 설정하고 사용하는 방법을 안내하며, AI 작업을 완벽하게 제어하면서도 개인정보 보호와 비용 절감을 동시에 실현할 수 있습니다.

Foundry Local은 AI 모델을 로컬 장치에서 실행함으로써 성능, 개인정보 보호, 맞춤화, 비용 측면에서 이점을 제공합니다. 직관적인 CLI, SDK, REST API를 통해 기존 워크플로우와 애플리케이션에 원활하게 통합됩니다.


![arch](../../../../../translated_images/foundry-local-arch.8823e321dd8258d7.ko.png)

### 왜 Foundry Local을 선택해야 할까요?

Foundry Local의 장점을 이해하면 AI 배포 전략을 더 현명하게 결정할 수 있습니다:

- **온디바이스 추론:** 모델을 직접 하드웨어에서 실행하여 비용을 줄이고 모든 데이터를 장치 내에 안전하게 유지합니다.

- **모델 맞춤화:** 사전 설정된 모델을 선택하거나 직접 만든 모델을 사용해 특정 요구사항과 사용 사례에 맞출 수 있습니다.

- **비용 효율성:** 기존 하드웨어를 활용해 반복적인 클라우드 서비스 비용을 없애 AI 접근성을 높입니다.

- **원활한 통합:** SDK, API 엔드포인트, CLI를 통해 애플리케이션과 연결하며, 필요에 따라 Azure AI Foundry로 쉽게 확장할 수 있습니다.

> **시작 안내:** 이 튜토리얼은 CLI와 SDK 인터페이스를 통해 Foundry Local을 사용하는 방법에 중점을 둡니다. 두 가지 방식을 모두 배워서 사용 사례에 가장 적합한 방법을 선택할 수 있습니다.

## 1부: Foundry Local CLI 설정하기

### 1단계: 설치

Foundry Local CLI는 AI 모델을 로컬에서 관리하고 실행하는 관문입니다. 먼저 시스템에 설치해 보겠습니다.

**지원 플랫폼:** Windows 및 macOS

자세한 설치 방법은 [공식 Foundry Local 문서](https://github.com/microsoft/Foundry-Local/blob/main/README.md)를 참고하세요.

### 2단계: 사용 가능한 모델 탐색

Foundry Local CLI를 설치한 후, 사용 가능한 모델을 확인할 수 있습니다. 다음 명령어로 지원되는 모든 모델을 확인하세요:


```bash
foundry model list
```

### 3단계: Phi Family 모델 이해하기

Phi Family는 다양한 사용 사례와 하드웨어 구성에 최적화된 여러 모델을 제공합니다. Foundry Local에서 사용할 수 있는 Phi 모델은 다음과 같습니다:

**사용 가능한 Phi 모델:** 

- **phi-3.5-mini** - 기본 작업에 적합한 소형 모델
- **phi-3-mini-128k** - 긴 대화를 위한 확장 컨텍스트 버전
- **phi-3-mini-4k** - 일반 용도의 표준 컨텍스트 모델
- **phi-4** - 향상된 기능을 갖춘 고급 모델
- **phi-4-mini** - Phi-4의 경량 버전
- **phi-4-mini-reasoning** - 복잡한 추론 작업에 특화된 모델

> **하드웨어 호환성:** 각 모델은 시스템 성능에 따라 CPU, GPU 등 다양한 하드웨어 가속 설정이 가능합니다.

### 4단계: 첫 번째 Phi 모델 실행하기

실습 예제로 `phi-4-mini-reasoning` 모델을 실행해 보겠습니다. 이 모델은 복잡한 문제를 단계별로 해결하는 데 뛰어납니다.


**모델 실행 명령어:**

```bash
foundry model run Phi-4-mini-reasoning-generic-cpu
```

> **첫 실행 시 주의:** 모델을 처음 실행할 때 Foundry Local이 자동으로 로컬 장치에 모델을 다운로드합니다. 네트워크 속도에 따라 다운로드 시간이 달라질 수 있으니 초기 설정 시 잠시 기다려 주세요.

### 5단계: 실제 문제로 모델 테스트하기

이제 고전적인 논리 문제를 통해 모델의 단계별 추론 능력을 테스트해 보겠습니다:

**예제 문제:**

```txt
Please calculate the following step by step: Now there are pheasants and rabbits in the same cage, there are thirty-five heads on top and ninety-four legs on the bottom, how many pheasants and rabbits are there?
```

**예상 동작:** 모델은 꿩이 다리가 2개, 토끼가 다리가 4개라는 사실을 이용해 문제를 논리적인 단계로 나누어 방정식 시스템을 해결해야 합니다.

**결과:**

![cli](../../../../../translated_images/cli.862ec6b55c2b5d91.ko.png)

## 2부: Foundry Local SDK로 애플리케이션 개발하기

### SDK를 사용하는 이유

CLI는 테스트와 빠른 상호작용에 적합하지만, SDK를 사용하면 Foundry Local을 애플리케이션에 프로그래밍 방식으로 통합할 수 있습니다. 이를 통해 다음과 같은 가능성이 열립니다:

- 맞춤형 AI 기반 애플리케이션 개발
- 자동화된 워크플로우 생성
- 기존 시스템에 AI 기능 통합
- 챗봇 및 인터랙티브 도구 개발

### 지원 프로그래밍 언어

Foundry Local은 다양한 개발 환경에 맞춰 여러 프로그래밍 언어용 SDK를 제공합니다:

**📦 사용 가능한 SDK:**

- **C# (.NET):** [SDK 문서 및 예제](https://github.com/microsoft/Foundry-Local/tree/main/sdk/cs)
- **Python:** [SDK 문서 및 예제](https://github.com/microsoft/Foundry-Local/tree/main/sdk/python)
- **JavaScript:** [SDK 문서 및 예제](https://github.com/microsoft/Foundry-Local/tree/main/sdk/js)
- **Rust:** [SDK 문서 및 예제](https://github.com/microsoft/Foundry-Local/tree/main/sdk/rust)

### 다음 단계

1. 개발 환경에 맞는 SDK를 선택하세요  
2. SDK별 문서를 참고해 상세 구현 방법을 익히세요  
3. 간단한 예제부터 시작해 점차 복잡한 애플리케이션을 개발하세요  
4. 각 SDK 저장소에 제공된 샘플 코드를 활용하세요  

## 결론

지금까지 다음 내용을 배웠습니다:  
- ✅ Foundry Local CLI 설치 및 설정  
- ✅ Phi Family 모델 탐색 및 실행  
- ✅ 실제 문제로 모델 테스트  
- ✅ 애플리케이션 개발을 위한 SDK 옵션 이해  

Foundry Local은 AI 기능을 로컬 환경에 직접 도입할 수 있는 강력한 기반을 제공하며, 성능, 개인정보 보호, 비용을 직접 관리할 수 있게 해줍니다. 필요에 따라 클라우드 솔루션으로 확장하는 유연성도 갖추고 있습니다.

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있으나, 자동 번역에는 오류나 부정확한 부분이 있을 수 있음을 유의하시기 바랍니다. 원문은 해당 언어의 원본 문서가 권위 있는 출처로 간주되어야 합니다. 중요한 정보의 경우 전문적인 인간 번역을 권장합니다. 본 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 당사는 책임을 지지 않습니다.