# **Hugging Face에서 Phi Family 사용하기**


[Hugging Face](https://huggingface.co/)는 풍부한 데이터와 오픈 소스 모델 자원을 갖춘 매우 인기 있는 AI 커뮤니티입니다. Microsoft, Meta, Mistral, Apple, Google 등 다양한 제조사들이 Hugging Face를 통해 오픈 소스 LLM과 SLM을 공개하고 있습니다.

Microsoft Phi Family가 Hugging Face에 공개되었습니다. 개발자들은 시나리오와 비즈니스에 맞춰 해당 Phi Family 모델을 다운로드할 수 있습니다. Hugging Face에서 Phi Pytorch 모델을 배포하는 것 외에도, 양자화된 모델을 GGUF와 ONNX 포맷으로 제공하여 최종 사용자에게 선택권을 제공합니다.


## **Hugging Face에서 모델 다운로드하기**

아래 링크에서 Phi Family 모델을 다운로드할 수 있습니다

[Microsoft Models on Hugging Face](https://huggingface.co/microsoft)

-  **Phi-1 / 1.5** https://huggingface.co/collections/microsoft/phi-1-6626e29134744e94e222d572

-  **Phi-3 / 3.5** https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3

-  **Phi-4** https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4

- **Phi-4-reasoning** https://huggingface.co/microsoft/Phi-4-reasoning

- **Phi-4-reasoning Plus** https://huggingface.co/microsoft/Phi-4-reasoning-plus 

- **Phi-4-mini-reasoning** https://huggingface.co/microsoft/Phi-4-mini-reasoning

모델은 ***Hugging Face CLI SDK***를 설치하거나 ***git clone***을 사용하는 등 다양한 방법으로 다운로드할 수 있습니다.

### **Hugging Face CLI로 Phi Family 모델 다운로드하기**

- Hugging Face CLI 설치하기

```bash

pip install -U "huggingface_hub[cli]"

```

- huggingface-cli로 로그인하기

[설정 페이지](https://huggingface.co/settings/tokens)에서 [사용자 액세스 토큰](https://huggingface.co/docs/hub/security-tokens)을 받아 Hugging Face에 로그인하세요


```bash

huggingface-cli login --token $HF_TOKEN --add-to-git-credential

```

- 다운로드하기


모델을 다운로드하여 캐시에 저장할 수 있습니다

```bash

huggingface-cli download microsoft/phi-4

```

특정 위치에 저장할 수도 있습니다


```bash

huggingface-cli download microsoft/phi-4 --local-dir $YOUR_PATH

```


### **git clone으로 Phi Family 모델 다운로드하기**

***git clone*** 명령어를 사용해서도 모델을 다운로드할 수 있습니다

```bash

git lfs install

git clone https://huggingface.co/microsoft/phi-4

```

## **샘플 - Microsoft Phi-4 추론하기**

- **transformers 라이브러리 설치하기**

```bash

pip install transformers -U

```

- **VSCode에서 이 코드 실행하기**

```python

import transformers

pipeline = transformers.pipeline(
    "text-generation",
    model="microsoft/phi-4",
    model_kwargs={"torch_dtype": "auto"},
    device_map="auto",
)

messages = [
    {"role": "user", "content": "I have $20,000 in my savings account, where I receive a 4% profit per year and payments twice a year. Can you please tell me how long it will take for me to become a millionaire? Also, can you please explain the math step by step as if you were explaining it to an uneducated person?"},
]

outputs = pipeline(messages, max_new_tokens=2048)
print(outputs[0]["generated_text"][-1])

```

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있으나, 자동 번역에는 오류나 부정확한 부분이 있을 수 있음을 유의해 주시기 바랍니다. 원문은 해당 언어의 원본 문서가 권위 있는 출처로 간주되어야 합니다. 중요한 정보의 경우 전문적인 인간 번역을 권장합니다. 본 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 당사는 책임을 지지 않습니다.