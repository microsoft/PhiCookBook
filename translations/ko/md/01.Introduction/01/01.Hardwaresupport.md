<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c4afa6ffd13f29eb34e5f204b94310ff",
  "translation_date": "2025-04-04T05:43:16+00:00",
  "source_file": "md\\01.Introduction\\01\\01.Hardwaresupport.md",
  "language_code": "ko"
}
-->
# Phi 하드웨어 지원

Microsoft Phi는 ONNX Runtime에 최적화되어 있으며 Windows DirectML을 지원합니다. 다양한 하드웨어 유형에서 원활하게 작동하며, GPU, CPU, 심지어 모바일 기기에서도 사용할 수 있습니다.

## 디바이스 하드웨어
지원되는 하드웨어는 다음과 같습니다:

- GPU SKU: RTX 4090 (DirectML)
- GPU SKU: 1 A100 80GB (CUDA)
- CPU SKU: Standard F64s v2 (64 vCPUs, 128 GiB 메모리)

## 모바일 SKU

- Android - 삼성 갤럭시 S21
- Apple iPhone 14 이상 A16/A17 프로세서

## Phi 하드웨어 사양

- 최소 요구 구성
- Windows: DirectX 12 지원 GPU 및 최소 4GB의 통합 RAM

CUDA: Compute Capability >= 7.02를 지원하는 NVIDIA GPU

![HardwareSupport](../../../../../translated_images/01.phihardware.925db5699da7752cf486314e6db087580583cfbcd548970f8a257e31a8aa862c.ko.png)

## 여러 GPU에서 onnxruntime 실행하기

현재 제공되는 Phi ONNX 모델은 1 GPU만 지원합니다. Phi 모델에서 다중 GPU를 지원하는 것은 가능하지만, ORT를 2개의 GPU에서 실행한다고 해서 2개의 ORT 인스턴스를 실행하는 것보다 더 높은 처리량을 보장하지는 않습니다. 최신 정보는 [ONNX Runtime](https://onnxruntime.ai/)을 참조하세요.

[Build 2024 GenAI ONNX 팀](https://youtu.be/WLW4SE8M9i8?si=EtG04UwDvcjunyfC)은 Phi 모델에 대해 다중 GPU 대신 다중 인스턴스를 활성화했다고 발표했습니다.

현재로서는 CUDA_VISIBLE_DEVICES 환경 변수를 사용하여 아래와 같이 하나의 onnxruntime 또는 onnxruntime-genai 인스턴스를 실행할 수 있습니다.

```Python
CUDA_VISIBLE_DEVICES=0 python infer.py
CUDA_VISIBLE_DEVICES=1 python infer.py
```

[Azure AI Foundry](https://ai.azure.com)에서 Phi를 더욱 탐구해보세요.

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 노력하고 있으나, 자동 번역에는 오류나 부정확한 부분이 포함될 수 있습니다. 원문이 작성된 원어 문서를 신뢰할 수 있는 권위 있는 자료로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 당사는 책임을 지지 않습니다.