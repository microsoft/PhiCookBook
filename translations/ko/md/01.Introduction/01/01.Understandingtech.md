<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:42:14+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "ko"
}
-->
# 주요 기술

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12 위에 구축된 하드웨어 가속 머신러닝을 위한 저수준 API입니다.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia에서 개발한 병렬 컴퓨팅 플랫폼이자 API 모델로, 그래픽 처리 장치(GPU)를 이용한 범용 처리(GPGPU)를 가능하게 합니다.  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - 다양한 머신러닝 프레임워크 간 상호 운용성을 제공하는 머신러닝 모델 표현을 위한 오픈 포맷입니다.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - 머신러닝 모델을 표현하고 업데이트하는 데 사용되는 포맷으로, 특히 4-8비트 양자화를 통해 CPU에서 효율적으로 실행 가능한 소형 언어 모델에 적합합니다.

## DirectML

DirectML은 하드웨어 가속 머신러닝을 가능하게 하는 저수준 API입니다. DirectX 12 위에 구축되어 GPU 가속을 활용하며, 벤더에 구애받지 않아 다양한 GPU 제조사에서 코드 변경 없이 사용할 수 있습니다. 주로 GPU에서 모델 학습과 추론 작업에 사용됩니다.

하드웨어 지원 측면에서 DirectML은 AMD 통합 및 독립형 GPU, Intel 통합 GPU, NVIDIA 독립형 GPU 등 다양한 GPU와 호환되도록 설계되었습니다. Windows AI 플랫폼의 일부로 Windows 10 및 11에서 지원되며, 모든 Windows 장치에서 모델 학습과 추론이 가능합니다.

DirectML 관련 업데이트 및 기회로는 최대 150개의 ONNX 연산자 지원, ONNX 런타임과 WinML에서의 활용 등이 있으며, 주요 통합 하드웨어 벤더(IHV)들이 다양한 메타커맨드를 구현해 지원하고 있습니다.

## CUDA

CUDA(Compute Unified Device Architecture)는 Nvidia가 개발한 병렬 컴퓨팅 플랫폼이자 API 모델입니다. CUDA를 지원하는 GPU를 이용해 범용 처리를 가능하게 하며, 이를 GPGPU(General-Purpose computing on Graphics Processing Units)라고 합니다. CUDA는 Nvidia GPU 가속의 핵심 기술로, 머신러닝, 과학 계산, 비디오 처리 등 다양한 분야에서 널리 사용됩니다.

CUDA의 하드웨어 지원은 Nvidia GPU에 한정되며, 각 아키텍처는 CUDA 툴킷의 특정 버전을 지원합니다. CUDA 툴킷은 개발자가 CUDA 애플리케이션을 빌드하고 실행하는 데 필요한 라이브러리와 도구를 제공합니다.

## ONNX

ONNX(Open Neural Network Exchange)는 머신러닝 모델을 표현하기 위한 오픈 포맷입니다. 확장 가능한 계산 그래프 모델 정의와 내장 연산자 및 표준 데이터 타입 정의를 제공합니다. ONNX는 개발자가 다양한 ML 프레임워크 간에 모델을 이동할 수 있게 하여 상호 운용성을 높이고 AI 애플리케이션 개발 및 배포를 용이하게 합니다.

Phi3 mini는 서버 플랫폼, Windows, Linux, Mac 데스크톱, 모바일 CPU 등 다양한 장치에서 CPU와 GPU를 이용해 ONNX Runtime으로 실행할 수 있습니다.  
우리가 추가한 최적화 구성은 다음과 같습니다.

- int4 DML용 ONNX 모델: AWQ를 통해 int4로 양자화  
- fp16 CUDA용 ONNX 모델  
- int4 CUDA용 ONNX 모델: RTN을 통해 int4로 양자화  
- int4 CPU 및 모바일용 ONNX 모델: RTN을 통해 int4로 양자화  

## Llama.cpp

Llama.cpp는 C++로 작성된 오픈소스 소프트웨어 라이브러리입니다. Llama를 포함한 다양한 대형 언어 모델(LLM)에 대해 추론을 수행합니다. 범용 텐서 라이브러리인 ggml과 함께 개발되었으며, 원래 Python 구현보다 더 빠른 추론 속도와 낮은 메모리 사용을 목표로 합니다. 하드웨어 최적화, 양자화 지원과 간단한 API 및 예제도 제공합니다. 효율적인 LLM 추론에 관심이 있다면, Phi3가 Llama.cpp를 실행할 수 있으므로 살펴볼 만합니다.

## GGUF

GGUF(Generic Graph Update Format)는 머신러닝 모델을 표현하고 업데이트하는 데 사용되는 포맷입니다. 특히 4-8비트 양자화를 통해 CPU에서 효율적으로 실행 가능한 소형 언어 모델(SLM)에 적합합니다. GGUF는 빠른 프로토타이핑과 엣지 디바이스 또는 CI/CD 파이프라인 같은 배치 작업에서 모델 실행에 유용합니다.

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있으나, 자동 번역에는 오류나 부정확한 부분이 있을 수 있음을 유의하시기 바랍니다. 원문은 해당 언어의 원본 문서가 권위 있는 자료로 간주되어야 합니다. 중요한 정보의 경우 전문적인 인간 번역을 권장합니다. 본 번역 사용으로 인한 오해나 잘못된 해석에 대해 당사는 책임을 지지 않습니다.