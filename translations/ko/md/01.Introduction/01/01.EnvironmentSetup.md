<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-07-16T18:08:06+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "ko"
}
-->
# Phi-3 로컬 시작하기

이 가이드는 Ollama를 사용하여 Phi-3 모델을 로컬 환경에서 실행할 수 있도록 설정하는 방법을 안내합니다. GitHub Codespaces, VS Code Dev Containers 또는 로컬 환경 등 여러 가지 방법으로 모델을 실행할 수 있습니다.

## 환경 설정

### GitHub Codespaces

GitHub Codespaces를 사용하면 이 템플릿을 가상으로 실행할 수 있습니다. 버튼을 클릭하면 브라우저에서 웹 기반 VS Code 인스턴스가 열립니다:

1. 템플릿 열기 (몇 분 정도 걸릴 수 있습니다):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. 터미널 창 열기

### VS Code Dev Containers

⚠️ 이 옵션은 Docker Desktop에 최소 16GB RAM이 할당되어 있어야 작동합니다. RAM이 16GB 미만인 경우 [GitHub Codespaces 옵션](../../../../../md/01.Introduction/01)이나 [로컬 환경 설정](../../../../../md/01.Introduction/01)을 시도해 보세요.

관련 옵션으로 VS Code Dev Containers가 있으며, 이는 [Dev Containers 확장](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)을 사용해 로컬 VS Code에서 프로젝트를 엽니다:

1. Docker Desktop 시작 (설치되어 있지 않다면 설치하세요)
2. 프로젝트 열기:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. 열리는 VS Code 창에서 프로젝트 파일이 나타나면(몇 분 걸릴 수 있음) 터미널 창을 엽니다.
4. [배포 단계](../../../../../md/01.Introduction/01)를 계속 진행하세요.

### 로컬 환경

1. 다음 도구들이 설치되어 있는지 확인하세요:

    * [Ollama](https://ollama.com/)
    * [Python 3.10 이상](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## 모델 테스트

1. Ollama에게 phi3:mini 모델을 다운로드하고 실행하도록 요청하세요:

    ```shell
    ollama run phi3:mini
    ```

    모델 다운로드에는 몇 분이 소요됩니다.

2. 출력에 "success"가 표시되면, 프롬프트에서 해당 모델로 메시지를 보낼 수 있습니다.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. 몇 초 후 모델로부터 응답 스트림이 표시됩니다.

4. 언어 모델에 사용되는 다양한 기법을 배우려면 Python 노트북 [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb)를 열고 각 셀을 실행하세요. 'phi3:mini'가 아닌 다른 모델을 사용했다면 첫 번째 셀에서 `MODEL_NAME`을 변경하세요.

5. Python에서 phi3:mini 모델과 대화하려면 Python 파일 [chat.py](../../../../../code/01.Introduce/chat.py)를 열고 실행하세요. 필요에 따라 파일 상단의 `MODEL_NAME`을 변경할 수 있으며, 시스템 메시지를 수정하거나 few-shot 예제를 추가할 수도 있습니다.

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있으나, 자동 번역에는 오류나 부정확한 부분이 있을 수 있음을 유의해 주시기 바랍니다. 원문은 해당 언어의 원본 문서가 권위 있는 자료로 간주되어야 합니다. 중요한 정보의 경우 전문적인 인간 번역을 권장합니다. 본 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 당사는 책임을 지지 않습니다.