<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-05T16:28:08+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ko"
}
-->
# **Phi 패밀리 양자화**

모델 양자화는 신경망 모델의 파라미터(예: 가중치와 활성화 값)를 큰 값 범위(보통 연속적인 값 범위)에서 더 작은 유한 값 범위로 매핑하는 과정을 말합니다. 이 기술은 모델의 크기와 계산 복잡도를 줄이고, 모바일 기기나 임베디드 시스템과 같은 자원 제한 환경에서 모델의 운영 효율성을 향상시킬 수 있습니다. 모델 양자화는 파라미터의 정밀도를 낮춤으로써 압축을 실현하지만, 동시에 일정 수준의 정밀도 손실을 초래합니다. 따라서 양자화 과정에서는 모델 크기, 계산 복잡도, 정밀도 간의 균형을 맞추는 것이 필요합니다. 일반적인 양자화 방법으로는 고정 소수점 양자화, 부동 소수점 양자화 등이 있으며, 특정 상황과 필요에 따라 적절한 양자화 전략을 선택할 수 있습니다.

우리는 GenAI 모델을 엣지 디바이스에 배포하고, 모바일 장치, AI PC/Copilot+PC, 전통적인 IoT 장치 등 더 많은 기기가 GenAI 시나리오에 진입할 수 있도록 하길 희망합니다. 양자화 모델을 통해 다양한 장치별로 다른 엣지 디바이스에 배포할 수 있으며, 하드웨어 제조사가 제공하는 모델 가속 프레임워크 및 양자화 모델과 결합하여 더 나은 SLM 애플리케이션 시나리오를 구축할 수 있습니다.

양자화 시나리오에서는 INT4, INT8, FP16, FP32 등 다양한 정밀도를 사용합니다. 아래는 자주 사용되는 양자화 정밀도에 대한 설명입니다.

### **INT4**

INT4 양자화는 모델의 가중치와 활성화 값을 4비트 정수로 양자화하는 급진적인 양자화 방식입니다. INT4 양자화는 표현 범위가 작고 정밀도가 낮아 상대적으로 더 큰 정밀도 손실이 발생하는 경향이 있습니다. 그러나 INT8 양자화에 비해 저장 요구량과 계산 복잡도를 더욱 크게 줄일 수 있습니다. 다만 INT4 양자화는 너무 낮은 정확도로 인해 모델 성능 저하가 크게 발생할 수 있어 실제 적용이 드문 편입니다. 또한 모든 하드웨어가 INT4 연산을 지원하지 않으므로, 양자화 방법 선택 시 하드웨어 호환성을 고려해야 합니다.

### **INT8**

INT8 양자화는 모델의 가중치와 활성화를 부동소수점 수에서 8비트 정수로 변환하는 과정입니다. INT8 정수가 표현하는 수치 범위는 더 작고 정밀도도 낮지만, 저장 및 계산 요구를 크게 줄일 수 있습니다. INT8 양자화에서는 모델의 가중치와 활성화 값이 스케일링과 오프셋을 포함하는 양자화 과정을 거쳐 원래 부동소수점 정보를 최대한 보존합니다. 추론 시에는 이 양자화된 값들이 다시 부동소수점으로 비양자화되어 계산되고, 이후 다음 단계에서 다시 INT8로 양자화됩니다. 이 방법은 대부분의 애플리케이션에서 충분한 정확도를 유지하면서도 높은 계산 효율성을 제공합니다.

### **FP16**

FP16 형식은 16비트 부동소수점 수(float16)를 의미하며, 이는 32비트 부동소수점 수(float32)에 비해 메모리 사용량을 절반으로 줄이는 장점이 있어 대규모 딥러닝 애플리케이션에 큰 이점을 제공합니다. FP16 형식을 사용하면 동일한 GPU 메모리 한도 내에서 더 큰 모델을 로드하거나 더 많은 데이터를 처리할 수 있습니다. 최신 GPU 하드웨어가 FP16 연산을 지원함에 따라, FP16 형식 사용 시 계산 속도 향상도 기대할 수 있습니다. 하지만 FP16 형식은 낮은 정밀도라는 고유 단점으로 인해 경우에 따라 수치적 불안정성이나 정밀도 손실이 발생할 수 있습니다.

### **FP32**

FP32 형식은 더 높은 정밀도를 제공하며 넓은 범위의 값을 정확하게 표현할 수 있습니다. 복잡한 수학 연산이 필요하거나 고정밀 결과가 요구되는 시나리오에서 FP32 형식이 선호됩니다. 그러나 높은 정밀도는 더 많은 메모리 사용과 연산 시간 증가를 의미합니다. 대규모 딥러닝 모델, 특히 매우 많은 모델 파라미터와 방대한 데이터가 있을 경우 FP32 형식은 GPU 메모리 부족이나 추론 속도 저하를 초래할 수 있습니다.

모바일 기기나 IoT 기기에서는 Phi-3.x 모델을 INT4로 변환할 수 있으며, AI PC / Copilot PC는 INT8, FP16, FP32와 같은 더 높은 정밀도를 사용할 수 있습니다.

현재 다양한 하드웨어 제조사는 Intel의 OpenVINO, Qualcomm의 QNN, Apple의 MLX, Nvidia의 CUDA 등 다양한 프레임워크를 통해 생성 모델을 지원하며, 이에 모델 양자화를 결합해 로컬 배포를 완성하고 있습니다.

기술 측면에서 양자화 후 지원하는 형식도 PyTorch / TensorFlow 형식, GGUF, ONNX 등으로 다양합니다. 저는 GGUF와 ONNX 간의 형식 비교 및 적용 시나리오를 수행했으며, 모델 프레임워크에서 하드웨어까지 좋은 지원을 받는 ONNX 양자화 형식을 권장합니다. 본 장에서는 GenAI용 ONNX Runtime, OpenVINO, Apple MLX를 중심으로 모델 양자화를 다룰 예정입니다 (더 좋은 방법이 있다면 PR 제출을 통해 알려주셔도 됩니다).

**이 장에서는 다음 내용을 포함합니다**

1. [llama.cpp를 이용한 Phi-3.5 / 4 양자화](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntime용 생성 AI 확장을 이용한 Phi-3.5 / 4 양자화](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO를 이용한 Phi-3.5 / 4 양자화](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX 프레임워크를 이용한 Phi-3.5 / 4 양자화](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 노력하고 있으나, 자동 번역에는 오류나 부정확성이 포함될 수 있음을 양지해 주시기 바랍니다. 원문 문서는 그 언어의 권위 있는 자료로 간주되어야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 본 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해서는 당사가 책임지지 않습니다.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->