<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "418c693c63cc0e817dc560558f730a7a",
  "translation_date": "2025-04-04T06:05:27+00:00",
  "source_file": "md\\01.Introduction\\04\\QuantifyingPhi.md",
  "language_code": "ko"
}
-->
# **Phi 패밀리 정량화**

모델 정량화는 신경망 모델의 매개변수(예: 가중치 및 활성화 값)를 큰 값 범위(일반적으로 연속 값 범위)에서 작은 유한 값 범위로 매핑하는 과정을 말합니다. 이 기술은 모델의 크기와 계산 복잡성을 줄이고 모바일 장치나 임베디드 시스템과 같은 자원 제약 환경에서 모델의 운영 효율성을 높일 수 있습니다. 모델 정량화는 매개변수의 정밀도를 낮춤으로써 압축을 달성하지만, 정밀도의 일정 손실도 초래합니다. 따라서 정량화 과정에서는 모델 크기, 계산 복잡성, 정밀도를 균형 있게 조율해야 합니다. 일반적인 정량화 방법에는 고정 소수점 정량화, 부동 소수점 정량화 등이 있으며, 특정 상황과 요구에 따라 적합한 정량화 전략을 선택할 수 있습니다.

우리는 GenAI 모델을 엣지 디바이스에 배포하여 더 많은 장치가 GenAI 시나리오에 참여할 수 있도록 하고자 합니다. 예를 들어, 모바일 장치, AI PC/코파일럿 PC, 그리고 기존 IoT 장치 등이 포함됩니다. 정량화 모델을 통해 우리는 다양한 장치에 기반하여 이를 엣지 디바이스에 배포할 수 있습니다. 하드웨어 제조사가 제공하는 모델 가속화 프레임워크와 정량화 모델을 결합하여 더 나은 SLM 애플리케이션 시나리오를 구축할 수 있습니다.

정량화 시나리오에서는 INT4, INT8, FP16, FP32와 같은 다양한 정밀도가 있습니다. 아래는 일반적으로 사용되는 정량화 정밀도에 대한 설명입니다.

### **INT4**

INT4 정량화는 모델의 가중치와 활성화 값을 4비트 정수로 정량화하는 급진적인 방법입니다. INT4 정량화는 표현 범위가 작고 정밀도가 낮아지므로 일반적으로 더 큰 정밀도 손실을 초래합니다. 그러나 INT8 정량화와 비교했을 때 INT4 정량화는 모델의 저장 요구 사항과 계산 복잡성을 더욱 줄일 수 있습니다. 하지만 INT4 정량화는 실제 응용에서 비교적 드문 편인데, 정밀도가 너무 낮으면 모델 성능이 크게 저하될 수 있기 때문입니다. 또한 모든 하드웨어가 INT4 작업을 지원하는 것은 아니므로 정량화 방법을 선택할 때 하드웨어 호환성을 고려해야 합니다.

### **INT8**

INT8 정량화는 모델의 가중치와 활성화 값을 부동 소수점 숫자에서 8비트 정수로 변환하는 과정입니다. INT8 정수로 표현되는 숫자 범위는 작고 정밀도가 낮지만, 저장 및 계산 요구 사항을 크게 줄일 수 있습니다. INT8 정량화에서는 모델의 가중치와 활성화 값이 정량화 과정을 거치며, 여기에는 스케일링과 오프셋이 포함되어 원래 부동 소수점 정보를 최대한 보존합니다. 추론 중에는 이러한 정량화된 값이 계산을 위해 부동 소수점 숫자로 디정량화되었다가, 다음 단계에서 다시 INT8로 정량화됩니다. 이 방법은 대부분의 응용에서 충분한 정밀도를 제공하면서 높은 계산 효율을 유지할 수 있습니다.

### **FP16**

FP16 형식, 즉 16비트 부동 소수점(float16)은 32비트 부동 소수점(float32)에 비해 메모리 사용량을 절반으로 줄여주며, 대규모 딥러닝 응용에서 상당한 이점을 제공합니다. FP16 형식을 사용하면 동일한 GPU 메모리 제한 내에서 더 큰 모델을 로드하거나 더 많은 데이터를 처리할 수 있습니다. 현대 GPU 하드웨어가 FP16 작업을 계속 지원함에 따라 FP16 형식을 사용하면 계산 속도 향상도 기대할 수 있습니다. 그러나 FP16 형식은 고유한 단점도 가지고 있는데, 정밀도가 낮아져 일부 경우에 숫자 불안정성이나 정밀도 손실이 발생할 수 있습니다.

### **FP32**

FP32 형식은 높은 정밀도를 제공하며 넓은 범위의 값을 정확하게 표현할 수 있습니다. 복잡한 수학적 연산을 수행하거나 높은 정밀도가 요구되는 시나리오에서는 FP32 형식이 선호됩니다. 하지만 높은 정밀도는 더 많은 메모리 사용과 긴 계산 시간을 의미하기도 합니다. 대규모 딥러닝 모델에서는 특히 모델 매개변수가 많고 데이터량이 방대한 경우 FP32 형식이 GPU 메모리 부족이나 추론 속도 저하를 초래할 수 있습니다.

모바일 장치나 IoT 장치에서는 Phi-3.x 모델을 INT4로 변환할 수 있으며, AI PC/코파일럿 PC에서는 INT8, FP16, FP32와 같은 더 높은 정밀도를 사용할 수 있습니다.

현재, Intel의 OpenVINO, Qualcomm의 QNN, Apple의 MLX, Nvidia의 CUDA와 같은 다양한 하드웨어 제조사가 생성 모델을 지원하는 프레임워크를 제공하며, 모델 정량화를 결합하여 로컬 배포를 완료할 수 있습니다.

기술적으로는 정량화 이후 PyTorch/Tensorflow 형식, GGUF, ONNX와 같은 다양한 형식 지원이 있습니다. 저는 GGUF와 ONNX의 형식 비교 및 응용 시나리오를 수행했습니다. 여기에서는 ONNX 정량화 형식을 추천합니다. 이는 모델 프레임워크에서 하드웨어까지의 지원이 우수합니다. 이 장에서는 GenAI용 ONNX Runtime, OpenVINO, Apple MLX를 사용하여 모델 정량화를 수행하는 데 중점을 둘 것입니다(더 나은 방법이 있다면 PR을 제출하여 알려주세요).

**이 장에서는 다음을 포함합니다**

1. [Phi-3.5 / 4 정량화 - llama.cpp 사용](./UsingLlamacppQuantifyingPhi.md)

2. [Phi-3.5 / 4 정량화 - Generative AI extensions for onnxruntime 사용](./UsingORTGenAIQuantifyingPhi.md)

3. [Phi-3.5 / 4 정량화 - Intel OpenVINO 사용](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Phi-3.5 / 4 정량화 - Apple MLX Framework 사용](./UsingAppleMLXQuantifyingPhi.md)

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 노력하고 있으나, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서를 해당 언어로 작성된 상태에서 권위 있는 자료로 간주해야 합니다. 중요한 정보에 대해서는 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해서는 책임을 지지 않습니다.