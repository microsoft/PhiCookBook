{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Phi 3 Mini 4K Instruct Chatbot နှင့် Whisper\n",
    "\n",
    "### မိတ်ဆက်\n",
    "Interactive Phi 3 Mini 4K Instruct Chatbot သည် Microsoft Phi 3 Mini 4K instruct demo ကို စာသား သို့မဟုတ် အသံထည့်သွင်းမှုများဖြင့် အသုံးပြုသူများ ဆက်သွယ်နိုင်စေရန် ရည်ရွယ်ထားသော ကိရိယာတစ်ခုဖြစ်သည်။ ဤ chatbot ကို ဘာသာပြန်ခြင်း၊ ရာသီဥတု အခြေအနေများ၊ နှင့် အထွေထွေ အချက်အလက် ရှာဖွေခြင်းကဲ့သို့သော အမျိုးမျိုးသော လုပ်ငန်းများအတွက် အသုံးပြုနိုင်သည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "Huggingface Access Token ကိုဖန်တီးပါ\n",
    "\n",
    "Token အသစ်တစ်ခုဖန်တီးပါ  \n",
    "နာမည်အသစ်တစ်ခုပေးပါ  \n",
    "ရေးသားခွင့်များကို ရွေးချယ်ပါ  \n",
    "Token ကိုကူးယူပြီး လုံခြုံတဲ့နေရာမှာ သိမ်းဆည်းထားပါ  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python ကို အသုံးပြုပြီး အဓိကလုပ်ဆောင်ချက်နှစ်ခုကို ပြုလုပ်ထားပါတယ်။ `os` module ကို import လုပ်ခြင်းနှင့် environment variable တစ်ခုကို သတ်မှတ်ခြင်း ဖြစ်ပါတယ်။\n",
    "\n",
    "1. `os` module ကို import လုပ်ခြင်း:\n",
    "   - Python ရဲ့ `os` module က operating system နဲ့ ဆက်သွယ်ဖို့ နည်းလမ်းတစ်ခုကို ပေးပါတယ်။ ဒါက operating system ဆိုင်ရာ အလုပ်များကို ပြုလုပ်နိုင်စေပြီး၊ environment variables ကို access လုပ်ခြင်း၊ ဖိုင်နဲ့ directories တွေကို အလုပ်လုပ်ခြင်း စသည်တို့ကို လုပ်ဆောင်နိုင်ပါတယ်။\n",
    "   - ဒီ code မှာ `os` module ကို `import` statement ကို အသုံးပြုပြီး import လုပ်ထားပါတယ်။ ဒီ statement က `os` module ရဲ့ လုပ်ဆောင်နိုင်မှုတွေကို လက်ရှိ Python script မှာ အသုံးပြုနိုင်အောင် ပြုလုပ်ပေးပါတယ်။\n",
    "\n",
    "2. Environment variable တစ်ခုကို သတ်မှတ်ခြင်း:\n",
    "   - Environment variable ဆိုတာ operating system ပေါ်မှာ အလုပ်လုပ်နေတဲ့ programs တွေက access လုပ်နိုင်တဲ့ တန်ဖိုးတစ်ခုဖြစ်ပါတယ်။ Configuration settings တွေ သို့မဟုတ် အခြားသော အချက်အလက်တွေကို သိမ်းဆည်းထားပြီး၊ အများပြားသော programs တွေက အသုံးပြုနိုင်အောင် ပြုလုပ်ပေးတဲ့ နည်းလမ်းတစ်ခုဖြစ်ပါတယ်။\n",
    "   - ဒီ code မှာ environment variable အသစ်တစ်ခုကို `os.environ` dictionary ကို အသုံးပြုပြီး သတ်မှတ်ထားပါတယ်။ Dictionary ရဲ့ key က `'HF_TOKEN'` ဖြစ်ပြီး၊ value ကို `HUGGINGFACE_TOKEN` variable မှာ assign လုပ်ထားတဲ့ တန်ဖိုးကို အသုံးပြုထားပါတယ်။\n",
    "   - `HUGGINGFACE_TOKEN` variable ကို ဒီ code snippet ရဲ့ အပေါ်ပိုင်းမှာ သတ်မှတ်ထားပြီး၊ `\"hf_**************\"` ဆိုတဲ့ string တန်ဖိုးကို `#@param` syntax ကို အသုံးပြုပြီး assign လုပ်ထားပါတယ်။ ဒီ syntax က Jupyter notebooks မှာ user input နဲ့ parameter configuration ကို notebook interface မှာ တိုက်ရိုက် ပြုလုပ်နိုင်အောင် ပြုလုပ်ပေးတဲ့ နည်းလမ်းတစ်ခုဖြစ်ပါတယ်။\n",
    "   - `'HF_TOKEN'` environment variable ကို သတ်မှတ်ခြင်းအားဖြင့်၊ program ရဲ့ အခြားအစိတ်အပိုင်းတွေ သို့မဟုတ် operating system ပေါ်မှာ အလုပ်လုပ်နေတဲ့ အခြားသော programs တွေက access လုပ်နိုင်ပါတယ်။\n",
    "\n",
    "အကျဉ်းချုပ်အားဖြင့်၊ ဒီ code က `os` module ကို import လုပ်ပြီး `'HF_TOKEN'` ဆိုတဲ့ environment variable ကို `HUGGINGFACE_TOKEN` variable မှာ သတ်မှတ်ထားတဲ့ တန်ဖိုးနဲ့ သတ်မှတ်ပေးထားပါတယ်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ဒီကုဒ်အပိုင်းက clear_output ဆိုတဲ့ function ကို သတ်မှတ်ထားပြီး Jupyter Notebook သို့မဟုတ် IPython ရဲ့ လက်ရှိ cell ရဲ့ output ကို ဖျက်ဖို့ အသုံးပြုပါတယ်။ အခုကုဒ်ကို ခွဲခြမ်းစိတ်ဖြာပြီး function ရဲ့ လုပ်ဆောင်မှုကို နားလည်ကြည့်ရအောင်။\n",
    "\n",
    "clear_output function ဟာ wait ဆိုတဲ့ parameter တစ်ခုကို ယူပါတယ်။ wait ဟာ boolean value ဖြစ်ပြီး အလယ်အလတ်မှာ False အဖြစ် သတ်မှတ်ထားပါတယ်။ ဒီ parameter က function ဟာ ရှိပြီးသား output ကို ဖျက်မလား၊ ဒါမှမဟုတ် အသစ် output ရလာတဲ့အချိန်အထိ စောင့်ပြီးမှ ဖျက်မလားဆိုတာကို သတ်မှတ်ပေးပါတယ်။\n",
    "\n",
    "ဒီ function ကို အသုံးပြုတာက လက်ရှိ cell ရဲ့ output ကို ဖျက်ဖို့ ဖြစ်ပါတယ်။ Jupyter Notebook သို့မဟုတ် IPython မှာ cell တစ်ခု output ထုတ်တဲ့အခါ၊ ဥပမာ print လုပ်တဲ့စာသားတွေ သို့မဟုတ် graphical plots တွေ၊ အဲဒီ output တွေဟာ cell ရဲ့ အောက်မှာ ပြသထားပါတယ်။ clear_output function က အဲဒီ output ကို ဖျက်ဖို့ အဆင်ပြေတဲ့နည်းလမ်းကို ပေးပါတယ်။\n",
    "\n",
    "function ရဲ့ အကောင်အထည်ဖော်မှုကို ကုဒ်အပိုင်းမှာ မပါရှိပါဘူး၊ ellipsis (...) နေရာမှာ placeholder အနေနဲ့ ထားထားပါတယ်။ ဒီ ellipsis က output ကို ဖျက်ဖို့ လိုအပ်တဲ့ အကောင်အထည်ဖော်မှုကို ကိုယ်စားပြုပါတယ်။ function ရဲ့ အကောင်အထည်ဖော်မှုဟာ Jupyter Notebook သို့မဟုတ် IPython API ကို အသုံးပြုပြီး cell ရဲ့ ရှိပြီးသား output ကို ဖယ်ရှားဖို့ ဖြစ်နိုင်ပါတယ်။\n",
    "\n",
    "စုစုပေါင်းအားဖြင့် ဒီ function ဟာ Jupyter Notebook သို့မဟုတ် IPython မှာ လက်ရှိ cell ရဲ့ output ကို ဖျက်ဖို့ အဆင်ပြေတဲ့နည်းလမ်းကို ပေးပါတယ်။ interactive coding session တွေမှာ output ကို စီမံခန့်ခွဲပြီး update လုပ်ဖို့ ပိုမိုလွယ်ကူစေပါတယ်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge TTS ဝန်ဆောင်မှုကို အသုံးပြုပြီး Text-to-Speech (TTS) လုပ်ဆောင်ရန်အတွက် အောက်ပါ function များကို တစ်ခုချင်းစီ ရှင်းပြပါမည်။\n",
    "\n",
    "1. `calculate_rate_string(input_value)`:\n",
    "ဒီ function သည် input value ကို အသုံးပြု၍ TTS voice အတွက် rate string ကိုတွက်ချက်ပေးသည်။ input value သည် စကားပြောအမြန်နှုန်းကို ကိုယ်စားပြုသည်။ 1 သည် သာမန်အမြန်နှုန်းကို ကိုယ်စားပြုသည်။ function သည် input value မှ 1 ကိုနုတ်၍ 100 ဖြင့်မြှောက်ပြီး input value ≥ 1 ဖြစ်/မဖြစ်ပေါ်မူတည်၍ အမှတ်အသားကို သတ်မှတ်သည်။ function သည် \"{sign}{rate}\" format ဖြင့် rate string ကို ပြန်ပေးသည်။\n",
    "\n",
    "2. `make_chunks(input_text, language)`:\n",
    "ဒီ function သည် input text နှင့် language ကို parameter အဖြစ်ယူပြီး ဘာသာစကား-specific စည်းကမ်းများအရ text ကို chunk များအဖြစ် ခွဲပေးသည်။ language သည် \"English\" ဖြစ်ပါက function သည် text ကို period (\".\") တွင် ခွဲပြီး အစ/အဆုံး whitespace ကို ဖယ်ရှားသည်။ ထို့နောက် chunk တစ်ခုစီအတွက် period ကို ထည့်ပြီး filtered chunk များကို ပြန်ပေးသည်။\n",
    "\n",
    "3. `tts_file_name(text)`:\n",
    "ဒီ function သည် input text အပေါ် မူတည်၍ TTS audio file အတွက် file name ကို ဖန်တီးပေးသည်။ text မှ period ကို ဖယ်ရှားပြီး lowercase ပြောင်း၊ whitespace ကို strip လုပ်၊ space များကို underscore ဖြင့် အစားထိုးသည်။ text သည် 25 characters ထက်ပိုလျှင် truncate လုပ်ပြီး text မရှိပါက full text ကို အသုံးပြုသည်။ နောက်ဆုံးတွင် [`uuid`] module ကို အသုံးပြု၍ random string ကို ဖန်တီးပြီး truncated text နှင့် ပေါင်းစပ်ကာ \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\" format ဖြင့် file name ကို ဖန်တီးသည်။\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`:\n",
    "ဒီ function သည် audio file များစွာကို တစ်ခုတည်းသော audio file အဖြစ် ပေါင်းစပ်ပေးသည်။ audio file path များနှင့် output path ကို parameter အဖြစ်ယူသည်။ function သည် `AudioSegment` object ကို initialize လုပ်ပြီး audio file path တစ်ခုချင်းစီကို iterate လုပ်ကာ `AudioSegment.from_file()` method ဖြင့် audio file ကို load လုပ်ပြီး current audio file ကို [`merged_audio`] object ထဲသို့ ထည့်သည်။ နောက်ဆုံးတွင် merged audio ကို output path သို့ MP3 format ဖြင့် export လုပ်သည်။\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`:\n",
    "ဒီ function သည် Edge TTS ဝန်ဆောင်မှုကို အသုံးပြု၍ TTS လုပ်ဆောင်သည်။ text chunk များစာရင်း၊ စကားပြောအမြန်နှုန်း၊ voice name နှင့် save path ကို parameter အဖြစ်ယူသည်။ chunk များအရေအတွက် > 1 ဖြစ်ပါက individual chunk audio file များအတွက် directory ဖန်တီးသည်။ chunk တစ်ခုချင်းစီအတွက် Edge TTS command ကို `calculate_rate_string()` function, voice name နှင့် chunk text ကို အသုံးပြု၍ ဖန်တီးကာ `os.system()` function ဖြင့် execute လုပ်သည်။ command execution အောင်မြင်ပါက audio file path ကို စာရင်းထဲသို့ ထည့်သည်။ chunk များအားလုံးကို process ပြီးနောက် individual audio file များကို `merge_audio_files()` function ဖြင့် ပေါင်းစပ်ကာ merged audio ကို save path သို့ သိမ်းဆည်းသည်။ chunk တစ်ခုသာရှိပါက Edge TTS command ကို တိုက်ရိုက် generate လုပ်ကာ audio ကို save path သို့ သိမ်းဆည်းသည်။ နောက်ဆုံးတွင် ဖန်တီးထားသော audio file path ကို ပြန်ပေးသည်။\n",
    "\n",
    "6. `random_audio_name_generate()`:\n",
    "ဒီ function သည် [`uuid`] module ကို အသုံးပြု၍ random audio file name ကို ဖန်တီးသည်။ random UUID ကို ဖန်တီးပြီး string အဖြစ်ပြောင်းကာ ပထမ 8 characters ကိုယူပြီး \".mp3\" extension ကို ထည့်ကာ random audio file name ကို ပြန်ပေးသည်။\n",
    "\n",
    "7. `talk(input_text)`:\n",
    "ဒီ function သည် TTS လုပ်ဆောင်ရန်အတွက် main entry point ဖြစ်သည်။ input text ကို parameter အဖြစ်ယူသည်။ input text ၏ အရှည်ကို စစ်ဆေးကာ 600 characters ထက်ကြီး/မကြီးကို သတ်မှတ်သည်။ အရှည်နှင့် `translate_text_flag` variable ၏တန်ဖိုးပေါ်မူတည်၍ ဘာသာစကားကို သတ်မှတ်ကာ `make_chunks()` function ကို အသုံးပြု၍ text chunk များစာရင်းကို ဖန်တီးသည်။ audio file အတွက် save path ကို `random_audio_name_generate()` function ဖြင့် ဖန်တီးသည်။ နောက်ဆုံးတွင် `edge_free_tts()` function ကို ခေါ်၍ TTS လုပ်ဆောင်ကာ ဖန်တီးထားသော audio file path ကို ပြန်ပေးသည်။\n",
    "\n",
    "အကျဉ်းချုပ်အားဖြင့် ဒီ function များသည် input text ကို chunk များအဖြစ် ခွဲခြား၊ audio file အတွက် file name ကို ဖန်တီး၊ Edge TTS ဝန်ဆောင်မှုကို အသုံးပြု၍ TTS လုပ်ဆောင်ပြီး individual audio file များကို တစ်ခုတည်းသော audio file အဖြစ် ပေါင်းစပ်ကာ output file ကို ဖန်တီးပေးသည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "အောက်ပါအကြောင်းအရာကို မြန်မာဘာသာဖြင့် ဘာသာပြန်ထားသည် -\n",
    "\n",
    "convert_to_text နှင့် run_text_prompt ဆိုသော function နှစ်ခု၏ အကောင်အထည်ဖော်မှုနှင့် str နှင့် Audio ဆိုသော class နှစ်ခု၏ ကြေညာမှု။\n",
    "\n",
    "convert_to_text function သည် audio_path ကို input အနေနဲ့ယူပြီး whisper_model ဆိုတဲ့ model ကို အသုံးပြုကာ audio ကို text အဖြစ် ပြောင်းလဲရေးဆောင်ရွက်သည်။ function သည် gpu flag ကို True ဖြစ်/မဖြစ် စစ်ဆေးသည်။ gpu flag သည် True ဖြစ်ပါက whisper_model ကို word_timestamps=True, fp16=True, language='English', task='translate' ဆိုသော parameter များနှင့်အတူ အသုံးပြုသည်။ gpu flag သည် False ဖြစ်ပါက fp16=False ဖြင့် whisper_model ကို အသုံးပြုသည်။ transcription ရလဒ်ကို 'scan.txt' ဆိုသော ဖိုင်တွင် သိမ်းဆည်းပြီး text အဖြစ် ပြန်လည်ပေးသည်။\n",
    "\n",
    "run_text_prompt function သည် message နှင့် chat_history ကို input အနေနဲ့ယူသည်။ function သည် phi_demo function ကို အသုံးပြုကာ input message အပေါ်အခြေခံပြီး chatbot response တစ်ခုကို ဖန်တီးသည်။ ဖန်တီးထားသော response ကို talk function သို့ ပေးပို့ကာ response ကို audio ဖိုင်အဖြစ် ပြောင်းလဲပြီး ဖိုင်လမ်းကြောင်းကို ပြန်လည်ပေးသည်။ Audio class ကို အသုံးပြုကာ audio ဖိုင်ကို ပြသပြီး play လုပ်သည်။ audio ကို IPython.display module မှ display function ကို အသုံးပြုကာ ပြသသည်။ Audio object ကို autoplay=True parameter ဖြင့် ဖန်တီးသည်၊ ထို့ကြောင့် audio သည် အလိုအလျောက် play လုပ်သည်။ chat_history ကို input message နှင့် ဖန်တီးထားသော response ဖြင့် update လုပ်ပြီး အလွတ် string နှင့် update လုပ်ထားသော chat_history ကို ပြန်လည်ပေးသည်။\n",
    "\n",
    "str class သည် Python တွင် built-in class တစ်ခုဖြစ်ပြီး character များ၏ အစဉ်အတိုင်းကို ကိုယ်စားပြုသည်။ capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill စသည်တို့ကဲ့သို့သော string များကို ပြုပြင်ဆောင်ရွက်ရန် method များစွာကို ပေးသည်။ method များသည် string များကို ရှာဖွေခြင်း၊ အစားထိုးခြင်း၊ format ပြုလုပ်ခြင်း၊ ပြုပြင်ခြင်း စသည်တို့ကို ဆောင်ရွက်ရန် အခွင့်အရေးပေးသည်။\n",
    "\n",
    "Audio class သည် audio object ကို ကိုယ်စားပြုသော custom class တစ်ခုဖြစ်သည်။ Jupyter Notebook ပတ်ဝန်းကျင်တွင် audio player တစ်ခု ဖန်တီးရန် အသုံးပြုသည်။ class သည် data, filename, url, embed, rate, autoplay, normalize စသည်တို့ကဲ့သို့သော parameter များကို လက်ခံသည်။ data parameter သည် numpy array, sample များ၏ list, filename သို့မဟုတ် URL ကို ကိုယ်စားပြုသော string, သို့မဟုတ် raw PCM data ဖြစ်နိုင်သည်။ filename parameter သည် local ဖိုင်တစ်ခုမှ audio data ကို load လုပ်ရန် အသုံးပြုသည်။ url parameter သည် URL တစ်ခုမှ audio data ကို download လုပ်ရန် အသုံးပြုသည်။ embed parameter သည် audio data ကို data URI ဖြင့် embed လုပ်မည်လား၊ original source မှ reference လုပ်မည်လားကို သတ်မှတ်သည်။ rate parameter သည် audio data ၏ sampling rate ကို သတ်မှတ်သည်။ autoplay parameter သည် audio ကို အလိုအလျောက် play လုပ်မည်မဟုတ်မည်ကို သတ်မှတ်သည်။ normalize parameter သည် audio data ကို အများဆုံး range သို့ rescale လုပ်ရန် သတ်မှတ်သည်။ Audio class သည် reload method ကို ပေးပြီး audio data ကို ဖိုင် သို့မဟုတ် URL မှ ပြန်လည် load လုပ်ရန် အသုံးပြုနိုင်သည်။ src_attr, autoplay_attr, element_id_attr စသည်တို့ကဲ့သို့သော attribute များကို HTML တွင် audio element အတွက် attribute များကို ရယူရန် အသုံးပြုသည်။\n",
    "\n",
    "အကျဉ်းချုပ်အားဖြင့်၊ function နှင့် class များသည် audio ကို text အဖြစ် ပြောင်းလဲခြင်း၊ chatbot response များကို audio အဖြစ် ဖန်တီးခြင်း၊ Jupyter Notebook ပတ်ဝန်းကျင်တွင် audio ကို ပြသပြီး play လုပ်ခြင်းတို့အတွက် အသုံးပြုသည်။\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**အကြောင်းကြားချက်**:  \nဤစာရွက်စာတမ်းကို AI ဘာသာပြန်ဝန်ဆောင်မှု [Co-op Translator](https://github.com/Azure/co-op-translator) ကို အသုံးပြု၍ ဘာသာပြန်ထားပါသည်။ ကျွန်ုပ်တို့သည် တိကျမှုအတွက် ကြိုးစားနေသော်လည်း၊ အလိုအလျောက် ဘာသာပြန်ခြင်းတွင် အမှားများ သို့မဟုတ် မတိကျမှုများ ပါဝင်နိုင်သည်ကို သတိပြုပါ။ မူရင်းစာရွက်စာတမ်းကို ၎င်း၏ မူလဘာသာစကားဖြင့် အာဏာတရားရှိသော အရင်းအမြစ်အဖြစ် သတ်မှတ်သင့်ပါသည်။ အရေးကြီးသော အချက်အလက်များအတွက် လူ့ဘာသာပြန်ပညာရှင်များမှ ပရော်ဖက်ရှင်နယ် ဘာသာပြန်ခြင်းကို အကြံပြုပါသည်။ ဤဘာသာပြန်ကို အသုံးပြုခြင်းမှ ဖြစ်ပေါ်လာသော အလွဲအလွတ်များ သို့မဟုတ် အနားလွဲမှုများအတွက် ကျွန်ုပ်တို့သည် တာဝန်မယူပါ။\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-13T07:06:10+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "my"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}