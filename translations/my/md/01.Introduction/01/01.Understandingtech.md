<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-09T19:41:14+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "my"
}
-->
# အဓိက နည်းပညာများမှာ

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12 အပေါ်မှာ တည်ဆောက်ထားပြီး hardware အားဖြင့် မြန်ဆန်စေသော machine learning အတွက် အနိမ့်အဆင့် API တစ်ခု။
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia က ဖန်တီးထားသော parallel computing ပလက်ဖောင်းနှင့် application programming interface (API) မော်ဒယ်ဖြစ်ပြီး၊ graphics processing units (GPUs) ပေါ်တွင် အထွေထွေ ပရိုဆက်ဆင်းလုပ်ဆောင်နိုင်စေသည်။
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - မတူညီသော ML framework များအကြား အပြန်အလှန်အသုံးပြုနိုင်စေရန် machine learning မော်ဒယ်များကို ဖော်ပြရန် ရည်ရွယ်ထားသော ဖွင့်လှစ်ထားသော ဖော်မတ်။
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - machine learning မော်ဒယ်များကို ဖော်ပြခြင်းနှင့် အပ်ဒိတ်လုပ်ခြင်းအတွက် အသုံးပြုသော ဖော်မတ်ဖြစ်ပြီး၊ 4-8bit quantization ဖြင့် CPU ပေါ်တွင် ထိရောက်စွာ လည်ပတ်နိုင်သော သေးငယ်သော ဘာသာစကား မော်ဒယ်များအတွက် အထူးအသုံးဝင်သည်။

## DirectML

DirectML သည် hardware အားဖြင့် မြန်ဆန်စေသော machine learning အတွက် အနိမ့်အဆင့် API တစ်ခုဖြစ်သည်။ GPU acceleration ကို အသုံးပြုရန် DirectX 12 အပေါ်တွင် တည်ဆောက်ထားပြီး၊ vendor များအလိုက် ကုဒ်ပြောင်းရန် မလိုအပ်ဘဲ မည်သည့် GPU vendor မဆို လည်ပတ်နိုင်သည်။ အဓိကအားဖြင့် GPU များပေါ်တွင် မော်ဒယ်လေ့ကျင့်ခြင်းနှင့် အနုတ်ယူခြင်းလုပ်ငန်းများအတွက် အသုံးပြုသည်။

hardware ပံ့ပိုးမှုအနေဖြင့် DirectML သည် AMD integrated နှင့် discrete GPU များ၊ Intel integrated GPU များ၊ NVIDIA discrete GPU များအပါအဝင် GPU များစွာနှင့် လုပ်ဆောင်နိုင်ရန် ဒီဇိုင်းရေးဆွဲထားသည်။ Windows AI Platform ၏ အစိတ်အပိုင်းတစ်ခုဖြစ်ပြီး Windows 10 နှင့် 11 ပေါ်တွင် ပံ့ပိုးထားသည်၊ ထို့ကြောင့် မည်သည့် Windows စက်ပစ္စည်းမဆို မော်ဒယ်လေ့ကျင့်ခြင်းနှင့် အနုတ်ယူခြင်းလုပ်ငန်းများ ပြုလုပ်နိုင်သည်။

DirectML နှင့် ပတ်သက်၍ ONNX operator များ ၁၅၀ ထိ ပံ့ပိုးခြင်း၊ ONNX runtime နှင့် WinML နှစ်ခုလုံးမှ အသုံးပြုခြင်းကဲ့သို့ အပ်ဒိတ်များနှင့် အခွင့်အလမ်းများ ရှိနေသည်။ အဓိက Integrated Hardware Vendors (IHVs) များက metacommand များကို အမျိုးမျိုး အကောင်အထည်ဖော်ထားသည်။

## CUDA

CUDA သည် Compute Unified Device Architecture ၏ အတိုကောက်ဖြစ်ပြီး Nvidia က ဖန်တီးထားသော parallel computing ပလက်ဖောင်းနှင့် application programming interface (API) မော်ဒယ်ဖြစ်သည်။ CUDA-enabled GPU ကို အသုံးပြု၍ software developer များအနေဖြင့် အထွေထွေ ပရိုဆက်ဆင်းလုပ်ဆောင်နိုင်ပြီး၊ ဤနည်းလမ်းကို GPGPU (General-Purpose computing on Graphics Processing Units) ဟု ခေါ်သည်။ CUDA သည် Nvidia ၏ GPU acceleration အတွက် အဓိက အားဖြည့်ကိရိယာဖြစ်ပြီး machine learning၊ သိပ္ပံတွက်ချက်မှုနှင့် ဗီဒီယို ပြုလုပ်ခြင်းကဲ့သို့သော နယ်ပယ်များတွင် ကျယ်ပြန့်စွာ အသုံးပြုသည်။

CUDA ၏ hardware ပံ့ပိုးမှုမှာ Nvidia ၏ GPU များအတွက်သာ ဖြစ်ပြီး၊ Nvidia ကပိုင်ဆိုင်သော နည်းပညာဖြစ်သည်။ architecture တစ်ခုချင်းစီသည် CUDA toolkit ၏ version များကို ပံ့ပိုးပြီး၊ developer များအတွက် CUDA application များ တည်ဆောက်ရန် လိုအပ်သော library များနှင့် ကိရိယာများကို ပံ့ပိုးပေးသည်။

## ONNX

ONNX (Open Neural Network Exchange) သည် machine learning မော်ဒယ်များကို ဖော်ပြရန် ရည်ရွယ်ထားသော ဖွင့်လှစ်ထားသော ဖော်မတ်ဖြစ်သည်။ တိုးချဲ့နိုင်သော ကွန်ပျူတင်း ဂရပ် မော်ဒယ်ကို သတ်မှတ်ချက်ပေးခြင်း၊ built-in operator များနှင့် စံ data type များကို သတ်မှတ်ချက်ပေးခြင်းတို့ ပါဝင်သည်။ ONNX သည် developer များအနေဖြင့် မတူညီသော ML framework များအကြား မော်ဒယ်များကို လွယ်ကူစွာ ရွှေ့ပြောင်းအသုံးပြုနိုင်စေပြီး AI application များ ဖန်တီးခြင်းနှင့် တပ်ဆင်ခြင်းကို ပိုမိုလွယ်ကူစေသည်။

Phi3 mini သည် ONNX Runtime ဖြင့် CPU နှင့် GPU ပေါ်တွင် server platform များ၊ Windows၊ Linux နှင့် Mac desktop များ၊ mobile CPU များအပါအဝင် စက်ပစ္စည်းအမျိုးမျိုးပေါ်တွင် လည်ပတ်နိုင်သည်။
ကျွန်ုပ်တို့ ထည့်သွင်းထားသော အကောင်းဆုံး ပြင်ဆင်မှုများမှာ

- int4 DML အတွက် ONNX မော်ဒယ်များ: AWQ ဖြင့် int4 သို့ quantize ပြုလုပ်ထားသည်
- fp16 CUDA အတွက် ONNX မော်ဒယ်
- int4 CUDA အတွက် ONNX မော်ဒယ်: RTN ဖြင့် int4 သို့ quantize ပြုလုပ်ထားသည်
- int4 CPU နှင့် Mobile အတွက် ONNX မော်ဒယ်: RTN ဖြင့် int4 သို့ quantize ပြုလုပ်ထားသည်

## Llama.cpp

Llama.cpp သည် C++ ဖြင့် ရေးသားထားသော ဖွင့်လှစ်ထားသော ဆော့ဖ်ဝဲစာကြည့်တိုက်တစ်ခုဖြစ်သည်။ Llama အပါအဝင် အမျိုးမျိုးသော Large Language Models (LLMs) ပေါ်တွင် အနုတ်ယူခြင်းလုပ်ဆောင်သည်။ ggml စာကြည့်တိုက် (general-purpose tensor library) နှင့်အတူ ဖန်တီးထားပြီး၊ llama.cpp သည် မူလ Python အကောင်အထည်ဖော်မှုနှင့် နှိုင်းယှဉ်လျှင် မြန်ဆန်သော အနုတ်ယူခြင်းနှင့် သိုလှောင်မှုအသုံးပြုမှုနည်းပါးစေသည်။ hardware optimization၊ quantization ကို ပံ့ပိုးပြီး၊ ရိုးရှင်းသော API နှင့် ဥပမာများကို ပံ့ပိုးပေးသည်။ ထိရောက်သော LLM အနုတ်ယူခြင်းကို စိတ်ဝင်စားပါက llama.cpp ကို စမ်းသပ်ကြည့်ရန် သင့်တော်ပြီး Phi3 သည် Llama.cpp ကို လည်ပတ်နိုင်သည်။

## GGUF

GGUF (Generic Graph Update Format) သည် machine learning မော်ဒယ်များကို ဖော်ပြခြင်းနှင့် အပ်ဒိတ်လုပ်ခြင်းအတွက် အသုံးပြုသော ဖော်မတ်ဖြစ်သည်။ 4-8bit quantization ဖြင့် CPU ပေါ်တွင် ထိရောက်စွာ လည်ပတ်နိုင်သော သေးငယ်သော ဘာသာစကား မော်ဒယ်များ (SLMs) အတွက် အထူးအသုံးဝင်သည်။ GGUF သည် အမြန် prototype ဖန်တီးခြင်းနှင့် edge device များပေါ်တွင် သို့မဟုတ် CI/CD pipeline ကဲ့သို့သော batch job များတွင် မော်ဒယ်များကို လည်ပတ်ရန် အထူးအသုံးဝင်သည်။

**အကြောင်းကြားချက်**  
ဤစာတမ်းကို AI ဘာသာပြန်ဝန်ဆောင်မှု [Co-op Translator](https://github.com/Azure/co-op-translator) ဖြင့် ဘာသာပြန်ထားပါသည်။ ကျွန်ုပ်တို့သည် တိကျမှန်ကန်မှုအတွက် ကြိုးစားသော်လည်း အလိုအလျောက် ဘာသာပြန်မှုများတွင် အမှားများ သို့မဟုတ် မှားယွင်းချက်များ ပါဝင်နိုင်ကြောင်း သတိပြုပါရန် မေတ္တာရပ်ခံအပ်ပါသည်။ မူရင်းစာတမ်းကို မိမိဘာသာစကားဖြင့်သာ တရားဝင်အချက်အလက်အဖြစ် သတ်မှတ်သင့်ပါသည်။ အရေးကြီးသော အချက်အလက်များအတွက် လူ့ဘာသာပြန်ပညာရှင်မှ ဘာသာပြန်ခြင်းကို အကြံပြုပါသည်။ ဤဘာသာပြန်မှုကို အသုံးပြုရာမှ ဖြစ်ပေါ်လာနိုင်သည့် နားလည်မှုမှားယွင်းမှုများအတွက် ကျွန်ုပ်တို့သည် တာဝန်မယူပါ။