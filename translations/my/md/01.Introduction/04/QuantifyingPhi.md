<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-07-09T19:46:15+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "my"
}
-->
# **Phi မိသားစုကို အရေအတွက်တွက်ခြင်း**

မော်ဒယ် quantization ဆိုသည်မှာ နယူးရယ်နက်ဝတ် မော်ဒယ်အတွင်း ပါရာမီတာများ (အလေးချိန်များနှင့် activation တန်ဖိုးများကဲ့သို့) ကို ကြီးမားသောတန်ဖိုးအကွာအဝေး (ပုံမှန်အားဖြင့် ဆက်လက်တန်ဖိုးအကွာအဝေး) မှ သေးငယ်ပြီး ကန့်သတ်ထားသောတန်ဖိုးအကွာအဝေးသို့ ပြောင်းလဲသတ်မှတ်ခြင်းဖြစ်သည်။ ဤနည်းပညာသည် မော်ဒယ်၏ အရွယ်အစားနှင့် တွက်ချက်မှုရှုပ်ထွေးမှုကို လျော့နည်းစေပြီး မိုဘိုင်းစက်ပစ္စည်းများ သို့မဟုတ် embedded စနစ်များကဲ့သို့ အရင်းအမြစ်ကန့်သတ်ထားသော ပတ်ဝန်းကျင်များတွင် မော်ဒယ်၏ လည်ပတ်မှု ထိရောက်မှုကို တိုးတက်စေသည်။ မော်ဒယ် quantization သည် ပါရာမီတာ precision ကို လျော့နည်းစေခြင်းဖြင့် ဖိအားချုပ်ခြင်းကို ရရှိစေသော်လည်း တိကျမှုအချိုးအစားတစ်ခုခု လျော့နည်းမှုကိုလည်း ဖြစ်ပေါ်စေသည်။ ထို့ကြောင့် quantization လုပ်ငန်းစဉ်တွင် မော်ဒယ်အရွယ်အစား၊ တွက်ချက်မှုရှုပ်ထွေးမှုနှင့် တိကျမှုတို့ကို ညီမျှစွာ ထိန်းညှိရမည်ဖြစ်သည်။ ပုံမှန် quantization နည်းလမ်းများတွင် fixed-point quantization၊ floating-point quantization စသည်တို့ ပါဝင်သည်။ သင့်လိုအပ်ချက်နှင့် အခြေအနေအရ သင့်တော်သော quantization များကို ရွေးချယ်နိုင်ပါသည်။

ကျွန်ုပ်တို့သည် GenAI မော်ဒယ်ကို edge စက်ပစ္စည်းများတွင် တပ်ဆင်ရန်နှင့် မိုဘိုင်းစက်ပစ္စည်းများ၊ AI PC/Copilot+PC နှင့် ရိုးရာ IoT စက်ပစ္စည်းများကဲ့သို့ GenAI ပတ်ဝန်းကျင်များသို့ ပိုမိုစက်ပစ္စည်းများ ဝင်ရောက်နိုင်စေရန် မျှော်လင့်ပါသည်။ quantization မော်ဒယ်မှတဆင့် မတူညီသော စက်ပစ္စည်းအလိုက် မတူညီသော edge စက်ပစ္စည်းများတွင် တပ်ဆင်နိုင်ပါသည်။ hardware ထုတ်လုပ်သူများမှ ပံ့ပိုးပေးသည့် မော်ဒယ်မြန်ဆန်ရေး ဖရိမ်ဝတ်နှင့် quantization မော်ဒယ်တို့နှင့် ပေါင်းစပ်၍ ပိုမိုကောင်းမွန်သော SLM အသုံးပြုမှု ပတ်ဝန်းကျင်များကို တည်ဆောက်နိုင်ပါသည်။

quantization ပတ်ဝန်းကျင်တွင် INT4, INT8, FP16, FP32 ကဲ့သို့ precision များကွဲပြားပါသည်။ အောက်တွင် ပုံမှန်အသုံးပြုသော quantization precision များကို ရှင်းလင်းပြထားပါသည်။

### **INT4**

INT4 quantization သည် မော်ဒယ်၏ အလေးချိန်များနှင့် activation တန်ဖိုးများကို 4-bit အပြည့်အဝ အနိမ့်ဆုံး integer များသို့ quantize လုပ်သည့် အလွန်တင်းကြပ်သော quantization နည်းလမ်းဖြစ်သည်။ INT4 quantization သည် တန်ဖိုးကိုယ်စားပြုမှုအကွာအဝေး သေးငယ်ခြင်းနှင့် တိကျမှုနည်းခြင်းကြောင့် တိကျမှုဆုံးရှုံးမှု ပိုများစေသည်။ သို့သော် INT8 quantization နှင့် နှိုင်းယှဉ်လျှင် INT4 quantization သည် မော်ဒယ်၏ သိမ်းဆည်းမှုလိုအပ်ချက်နှင့် တွက်ချက်မှုရှုပ်ထွေးမှုကို ပိုမိုလျော့နည်းစေနိုင်သည်။ သတိပြုရန်မှာ INT4 quantization သည် လက်တွေ့အသုံးပြုမှုတွင် မကြာခဏ မတွေ့ရပေ၊ အတိကျမှုနည်းလွန်းခြင်းကြောင့် မော်ဒယ်စွမ်းဆောင်ရည်တွင် ထိခိုက်မှုကြီးမားစေနိုင်သည်။ ထို့အပြင် hardware အားလုံးသည် INT4 လုပ်ဆောင်ချက်များကို မပံ့ပိုးနိုင်သဖြင့် quantization နည်းလမ်းရွေးချယ်ရာတွင် hardware ကိုက်ညီမှုကို စဉ်းစားရမည်ဖြစ်သည်။

### **INT8**

INT8 quantization သည် မော်ဒယ်၏ အလေးချိန်များနှင့် activation များကို floating point ကနေ 8-bit integer များသို့ ပြောင်းလဲခြင်းဖြစ်သည်။ INT8 integer များက ကိုယ်စားပြုနိုင်သည့် တန်ဖိုးအကွာအဝေး သေးငယ်ပြီး တိကျမှုနည်းသော်လည်း သိမ်းဆည်းမှုနှင့် တွက်ချက်မှုလိုအပ်ချက်များကို ထိရောက်စွာ လျော့နည်းစေသည်။ INT8 quantization တွင် မော်ဒယ်၏ အလေးချိန်များနှင့် activation တန်ဖိုးများကို scaling နှင့် offset အပါအဝင် quantization လုပ်ငန်းစဉ်ဖြင့် ပြုလုပ်ကာ မူလ floating point အချက်အလက်များကို 최대限度 ထိန်းသိမ်းထားသည်။ inference အချိန်တွင် ဤ quantized တန်ဖိုးများကို floating point သို့ ပြန်လည် dequantize လုပ်ကာ တွက်ချက်ပြီး နောက်တစ်ဆင့်အတွက် INT8 သို့ ပြန် quantize လုပ်သည်။ ဤနည်းလမ်းသည် အများအားဖြင့် လုံလောက်သော တိကျမှုနှင့် မြန်ဆန်သော တွက်ချက်မှု ထိရောက်မှုကို ပေးစွမ်းနိုင်သည်။

### **FP16**

FP16 ပုံစံသည် 16-bit floating point နံပါတ်များ (float16) ဖြစ်ပြီး 32-bit floating point နံပါတ်များ (float32) နှင့် နှိုင်းယှဉ်လျှင် မှတ်ဉာဏ်အသုံးပြုမှုကို တစ်ဝက်လျော့နည်းစေသည်။ ၎င်းသည် ကြီးမားသော deep learning လျှောက်လွှာများတွင် အရေးကြီးသော အားသာချက်များ ရှိသည်။ FP16 ပုံစံသည် တူညီသော GPU မှတ်ဉာဏ်ကန့်သတ်ချက်အတွင်း ပိုမိုကြီးမားသော မော်ဒယ်များကို တင်သွင်းခြင်း သို့မဟုတ် ပိုမိုများသော ဒေတာကို ကိုင်တွယ်နိုင်စေသည်။ ခေတ်မီ GPU hardware များသည် FP16 လုပ်ဆောင်ချက်များကို ဆက်လက်ပံ့ပိုးနေသဖြင့် FP16 ပုံစံအသုံးပြုခြင်းသည် တွက်ချက်မှုအမြန်နှုန်း တိုးတက်မှုကိုလည်း ဖြစ်စေနိုင်သည်။ သို့သော် FP16 ပုံစံတွင် တိကျမှုနည်းခြင်းကဲ့သို့ အားနည်းချက်များရှိပြီး၊ အချို့အခြေအနေများတွင် ဂဏန်းတည်ငြိမ်မှုမရှိခြင်း သို့မဟုတ် တိကျမှုဆုံးရှုံးမှု ဖြစ်ပေါ်စေနိုင်သည်။

### **FP32**

FP32 ပုံစံသည် တိကျမှုမြင့်မားပြီး တန်ဖိုးအမျိုးမျိုးကို တိကျစွာ ကိုယ်စားပြုနိုင်သည်။ ရှုပ်ထွေးသော သင်္ချာဆိုင်ရာ လုပ်ဆောင်ချက်များ ပြုလုပ်ရမည့် သို့မဟုတ် တိကျမှုမြင့်မားသော ရလဒ်များ လိုအပ်သော အခြေအနေများတွင် FP32 ပုံစံကို ဦးစားပေး အသုံးပြုသည်။ သို့သော် တိကျမှုမြင့်မားခြင်းသည် မှတ်ဉာဏ်အသုံးပြုမှုများများနှင့် တွက်ချက်ချိန်ရှည်ခြင်းကိုလည်း ဆိုလိုသည်။ ကြီးမားသော deep learning မော်ဒယ်များတွင်၊ အထူးသဖြင့် မော်ဒယ်ပါရာမီတာများ များပြားပြီး ဒေတာအရေအတွက် ကြီးမားသောအခါ FP32 ပုံစံသည် GPU မှတ်ဉာဏ် မလုံလောက်ခြင်း သို့မဟုတ် inference အမြန်နှုန်း လျော့နည်းခြင်း ဖြစ်စေနိုင်သည်။

မိုဘိုင်းစက်ပစ္စည်းများ သို့မဟုတ် IoT စက်ပစ္စည်းများတွင် Phi-3.x မော်ဒယ်များကို INT4 သို့ ပြောင်းလဲနိုင်ပြီး AI PC / Copilot PC များတွင် INT8, FP16, FP32 ကဲ့သို့ တိကျမှုမြင့်သော ပုံစံများကို အသုံးပြုနိုင်သည်။

လက်ရှိတွင် hardware ထုတ်လုပ်သူ များအလိုက် Intel ၏ OpenVINO, Qualcomm ၏ QNN, Apple ၏ MLX, Nvidia ၏ CUDA စသည်ဖြင့် မတူညီသော ဖရိမ်ဝတ်များရှိပြီး မော်ဒယ် quantization နှင့် ပေါင်းစပ်၍ ဒေသဆိုင်ရာ တပ်ဆင်မှုများ ပြုလုပ်နိုင်ပါသည်။

နည်းပညာအရ quantization ပြုလုပ်ပြီးနောက် PyTorch / Tensorflow ပုံစံများ၊ GGUF နှင့် ONNX ကဲ့သို့ ပုံစံပံ့ပိုးမှု များရှိသည်။ ကျွန်ုပ်သည် GGUF နှင့် ONNX အကြား ပုံစံနှိုင်းယှဉ်ခြင်းနှင့် အသုံးပြုမှု ပတ်ဝန်းကျင်များကို လုပ်ဆောင်ခဲ့ပြီး ONNX quantization ပုံစံကို အကြံပြုလိုပါသည်၊ ၎င်းသည် မော်ဒယ်ဖရိမ်ဝတ်မှ hardware အထိ ကောင်းမွန်စွာ ပံ့ပိုးပေးသည်။ ဤအခန်းတွင် ONNX Runtime for GenAI, OpenVINO နှင့် Apple MLX ကို အသုံးပြု၍ မော်ဒယ် quantization ပြုလုပ်ခြင်းကို အဓိကထား ဆွေးနွေးပါမည် (သင့်တွင် ပိုမိုကောင်းမွန်သော နည်းလမ်းရှိပါက PR တင်ပေးနိုင်ပါသည်)။

**ဤအခန်းတွင် ပါဝင်သော အကြောင်းအရာများ**

1. [llama.cpp ကို အသုံးပြု၍ Phi-3.5 / 4 ကို quantize လုပ်ခြင်း](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntime အတွက် Generative AI extensions ကို အသုံးပြု၍ Phi-3.5 / 4 ကို quantize လုပ်ခြင်း](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO ကို အသုံးပြု၍ Phi-3.5 / 4 ကို quantize လုပ်ခြင်း](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX Framework ကို အသုံးပြု၍ Phi-3.5 / 4 ကို quantize လုပ်ခြင်း](./UsingAppleMLXQuantifyingPhi.md)

**အကြောင်းကြားချက်**  
ဤစာတမ်းကို AI ဘာသာပြန်ဝန်ဆောင်မှု [Co-op Translator](https://github.com/Azure/co-op-translator) ဖြင့် ဘာသာပြန်ထားပါသည်။ ကျွန်ုပ်တို့သည် တိကျမှန်ကန်မှုအတွက် ကြိုးစားသော်လည်း၊ အလိုအလျောက် ဘာသာပြန်ခြင်းများတွင် အမှားများ သို့မဟုတ် မှားယွင်းချက်များ ပါဝင်နိုင်ကြောင်း သတိပြုပါရန် မေတ္တာရပ်ခံအပ်ပါသည်။ မူရင်းစာတမ်းကို မိမိဘာသာစကားဖြင့်သာ တရားဝင်အချက်အလက်အဖြစ် ယူဆသင့်ပါသည်။ အရေးကြီးသော အချက်အလက်များအတွက် လူ့ဘာသာပြန်ပညာရှင်မှ ဘာသာပြန်ခြင်းကို အကြံပြုပါသည်။ ဤဘာသာပြန်ချက်ကို အသုံးပြုရာမှ ဖြစ်ပေါ်လာနိုင်သည့် နားလည်မှုမှားယွင်းမှုများအတွက် ကျွန်ုပ်တို့ တာဝန်မယူပါ။