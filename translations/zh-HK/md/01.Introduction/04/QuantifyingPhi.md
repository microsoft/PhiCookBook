# **量化 Phi 系列**

模型量化是指將神經網絡模型中的參數（如權重和激活值）從較大範圍的數值（通常為連續數值範圍）映射到較小有限範圍的數值的過程。該技術可以減小模型大小和計算複雜度，並提升模型在資源受限環境（如移動設備或嵌入式系統）中的運行效率。模型量化通過降低參數的精度來實現壓縮，但同時也引入一定的精度損失。因此，在量化過程中，需要平衡模型大小、計算複雜度與精度。常見的量化方法包括定點量化、浮點量化等。您可以根據具體場景和需求選擇合適的量化策略。

我們希望將 GenAI 模型部署到邊緣設備，讓更多設備進入 GenAI 場景，例如移動設備、AI PC/Copilot+PC 和傳統物聯網設備。通過量化模型，我們可以根據不同設備將其部署到不同的邊緣設備。結合硬件廠商提供的模型加速框架和量化模型，我們可以打造更好的 SLM 應用場景。

在量化場景中，我們有不同的精度（INT4、INT8、FP16、FP32）。以下是常用量化精度的說明

### **INT4**

INT4 量化是一種激進的量化方法，將模型的權重和激活值量化成 4 位整數。由於表示範圍較小且精度較低，INT4 量化通常導致較大的精度損失。然而，與 INT8 量化相比，INT4 量化可以進一步減少模型的存儲需求和計算複雜度。需要注意的是，INT4 量化在實際應用中較為罕見，因過低的準確率可能嚴重影響模型性能。此外，並非所有硬件都支持 INT4 運算，因此選擇量化方法時需要考慮硬件兼容性。

### **INT8**

INT8 量化是將模型的權重和激活值從浮點數轉換成 8 位整數的過程。雖然 INT8 整數表示的數值範圍較小且精度較低，但能顯著減少存儲和計算需求。在 INT8 量化中，模型的權重和激活值經過量化處理，包括縮放和偏移，以盡可能保留原始浮點信息。在推理過程中，這些量化值會被反量化回浮點數進行計算，然後再量化回 INT8 以進行下一步。此方法在大多數應用中能提供足夠的精度，同時保持高效的計算效率。

### **FP16**

FP16 格式，即 16 位浮點數（float16），相比 32 位浮點數（float32）將內存占用減半，這在大規模深度學習應用中具有顯著優勢。FP16 格式允許在相同 GPU 記憶體限制下載入更大模型或處理更多數據。隨著現代 GPU 硬件持續支持 FP16 運算，使用 FP16 格式還可能提升計算速度。不過，FP16 格式也有其固有缺點，即精度較低，可能在某些情況下導致數值不穩定或精度損失。

### **FP32**

FP32 格式提供更高的精度，能夠準確表示更廣範圍的數值。在需要進行複雜數學運算或要求高精度結果的場景中，通常偏好使用 FP32 格式。但高精度也意味著更多的內存使用和更長的計算時間。對於大規模深度學習模型，特別是當模型參數眾多且數據量龐大時，FP32 格式可能導致 GPU 記憶體不足或推理速度下降。

在移動設備或物聯網設備上，我們可以將 Phi-3.x 模型轉換為 INT4，而 AI PC / Copilot PC 則可使用更高精度如 INT8、FP16、FP32。

目前，不同硬件廠商對生成模型有不同的支援框架，如 Intel 的 OpenVINO、Qualcomm 的 QNN、Apple 的 MLX 以及 Nvidia 的 CUDA 等，結合模型量化來完成本地部署。

在技術層面，我們提供量化後不同格式的支援，如 PyTorch / TensorFlow 格式、GGUF 及 ONNX。我已經做過 GGUF 與 ONNX 之間的格式比較和應用場景介紹。這裡推薦 ONNX 量化格式，其在模型框架到硬件方面都有良好支援。在本章，我們將重點介紹使用 ONNX Runtime for GenAI、OpenVINO 以及 Apple MLX 進行模型量化（如果您有更好的方法，也可以通過提交 PR 給我們）

**本章內容包括**

1. [使用 llama.cpp 對 Phi-3.5 / 4 進行量化](./UsingLlamacppQuantifyingPhi.md)

2. [使用 onnxruntime 的生成 AI 擴展對 Phi-3.5 / 4 進行量化](./UsingORTGenAIQuantifyingPhi.md)

3. [使用 Intel OpenVINO 對 Phi-3.5 / 4 進行量化](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [使用 Apple MLX 框架對 Phi-3.5 / 4 進行量化](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**免责声明**：
本文件係使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 所翻譯。雖然我哋致力追求準確，但請注意自動翻譯可能包含錯誤或不準確之處。原始文件嘅母語版本應視為權威來源。對於重要資訊，建議尋求專業人類翻譯。我哋對因使用本翻譯而引起嘅任何誤解或誤譯概不負責。
<!-- CO-OP TRANSLATOR DISCLAIMER END -->