{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot Interaktif Phi 3 Mini 4K Instruct dengan Whisper\n",
    "\n",
    "### Pengenalan:\n",
    "Chatbot Interaktif Phi 3 Mini 4K Instruct adalah alat yang membolehkan pengguna berinteraksi dengan demo Microsoft Phi 3 Mini 4K Instruct menggunakan input teks atau audio. Chatbot ini boleh digunakan untuk pelbagai tugas, seperti terjemahan, kemas kini cuaca, dan pengumpulan maklumat umum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "Buat Token Akses Huggingface Anda\n",
    "\n",
    "Buat token baharu  \n",
    "Berikan nama baharu  \n",
    "Pilih kebenaran menulis  \n",
    "Salin token dan simpan di tempat yang selamat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kod Python berikut menjalankan dua tugas utama: mengimport modul `os` dan menetapkan pemboleh ubah persekitaran.\n",
    "\n",
    "1. Mengimport modul `os`:\n",
    "   - Modul `os` dalam Python menyediakan cara untuk berinteraksi dengan sistem operasi. Ia membolehkan anda melakukan pelbagai tugas berkaitan sistem operasi, seperti mengakses pemboleh ubah persekitaran, bekerja dengan fail dan direktori, dan sebagainya.\n",
    "   - Dalam kod ini, modul `os` diimport menggunakan pernyataan `import`. Pernyataan ini menjadikan fungsi modul `os` tersedia untuk digunakan dalam skrip Python semasa.\n",
    "\n",
    "2. Menetapkan pemboleh ubah persekitaran:\n",
    "   - Pemboleh ubah persekitaran ialah nilai yang boleh diakses oleh program yang berjalan pada sistem operasi. Ia adalah cara untuk menyimpan tetapan konfigurasi atau maklumat lain yang boleh digunakan oleh pelbagai program.\n",
    "   - Dalam kod ini, pemboleh ubah persekitaran baru ditetapkan menggunakan kamus `os.environ`. Kunci kamus ialah `'HF_TOKEN'`, dan nilainya diberikan daripada pemboleh ubah `HUGGINGFACE_TOKEN`.\n",
    "   - Pemboleh ubah `HUGGINGFACE_TOKEN` ditakrifkan tepat di atas petikan kod ini, dan ia diberikan nilai rentetan `\"hf_**************\"` menggunakan sintaks `#@param`. Sintaks ini sering digunakan dalam Jupyter notebook untuk membolehkan input pengguna dan konfigurasi parameter secara langsung dalam antara muka notebook.\n",
    "   - Dengan menetapkan pemboleh ubah persekitaran `'HF_TOKEN'`, ia boleh diakses oleh bahagian lain program atau program lain yang berjalan pada sistem operasi yang sama.\n",
    "\n",
    "Secara keseluruhan, kod ini mengimport modul `os` dan menetapkan pemboleh ubah persekitaran bernama `'HF_TOKEN'` dengan nilai yang diberikan dalam pemboleh ubah `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Petikan kod ini mentakrifkan fungsi yang dipanggil clear_output yang digunakan untuk membersihkan output sel semasa dalam Jupyter Notebook atau IPython. Mari kita pecahkan kod ini dan fahami fungsinya:\n",
    "\n",
    "Fungsi clear_output mengambil satu parameter yang dipanggil wait, yang merupakan nilai boolean. Secara lalai, wait ditetapkan kepada False. Parameter ini menentukan sama ada fungsi tersebut perlu menunggu sehingga output baru tersedia untuk menggantikan output sedia ada sebelum membersihkannya.\n",
    "\n",
    "Fungsi ini sendiri digunakan untuk membersihkan output sel semasa. Dalam Jupyter Notebook atau IPython, apabila satu sel menghasilkan output, seperti teks yang dicetak atau plot grafik, output tersebut dipaparkan di bawah sel. Fungsi clear_output membolehkan anda membersihkan output tersebut.\n",
    "\n",
    "Pelaksanaan fungsi ini tidak disediakan dalam petikan kod, seperti yang ditunjukkan oleh ellipsis (...). Ellipsis mewakili tempat letak untuk kod sebenar yang melaksanakan pembersihan output. Pelaksanaan fungsi ini mungkin melibatkan interaksi dengan API Jupyter Notebook atau IPython untuk menghapuskan output sedia ada dari sel.\n",
    "\n",
    "Secara keseluruhan, fungsi ini menyediakan cara yang mudah untuk membersihkan output sel semasa dalam Jupyter Notebook atau IPython, menjadikannya lebih mudah untuk mengurus dan mengemas kini output yang dipaparkan semasa sesi pengekodan interaktif.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mari kita melalui pelaksanaan fungsi yang relevan satu persatu untuk melakukan teks-ke-ucapan (TTS) menggunakan perkhidmatan Edge TTS:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Fungsi ini mengambil nilai input dan mengira rentetan kadar untuk suara TTS. Nilai input mewakili kelajuan ucapan yang diinginkan, di mana nilai 1 mewakili kelajuan normal. Fungsi ini mengira rentetan kadar dengan menolak 1 daripada nilai input, menggandakannya dengan 100, dan kemudian menentukan tanda berdasarkan sama ada nilai input lebih besar atau sama dengan 1. Fungsi ini mengembalikan rentetan kadar dalam format \"{sign}{rate}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: Fungsi ini mengambil teks input dan bahasa sebagai parameter. Ia memecahkan teks input kepada bahagian berdasarkan peraturan khusus bahasa. Dalam pelaksanaan ini, jika bahasa adalah \"English\", fungsi ini memecahkan teks pada setiap titik (\".\") dan membuang sebarang ruang kosong di awal atau akhir. Ia kemudian menambahkan titik pada setiap bahagian dan mengembalikan senarai bahagian yang telah ditapis.\n",
    "\n",
    "3. `tts_file_name(text)`: Fungsi ini menjana nama fail untuk fail audio TTS berdasarkan teks input. Ia melakukan beberapa transformasi pada teks: membuang titik di akhir (jika ada), menukar teks kepada huruf kecil, membuang ruang kosong di awal dan akhir, dan menggantikan ruang dengan garis bawah. Ia kemudian memotong teks kepada maksimum 25 aksara (jika lebih panjang) atau menggunakan teks penuh jika ia kosong. Akhirnya, ia menjana rentetan rawak menggunakan modul [`uuid`] dan menggabungkannya dengan teks yang telah dipotong untuk mencipta nama fail dalam format \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Fungsi ini menggabungkan beberapa fail audio menjadi satu fail audio. Ia mengambil senarai laluan fail audio dan laluan output sebagai parameter. Fungsi ini memulakan objek kosong `AudioSegment` yang dipanggil [`merged_audio`]. Ia kemudian mengulangi setiap laluan fail audio, memuatkan fail audio menggunakan kaedah `AudioSegment.from_file()` daripada perpustakaan `pydub`, dan menambahkan fail audio semasa kepada objek [`merged_audio`]. Akhirnya, ia mengeksport audio yang telah digabungkan ke laluan output yang ditentukan dalam format MP3.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: Fungsi ini melakukan operasi TTS menggunakan perkhidmatan Edge TTS. Ia mengambil senarai bahagian teks, kelajuan ucapan, nama suara, dan laluan simpan sebagai parameter. Jika bilangan bahagian lebih daripada 1, fungsi ini mencipta direktori untuk menyimpan fail audio bahagian individu. Ia kemudian mengulangi setiap bahagian, membina arahan Edge TTS menggunakan fungsi `calculate_rate_string()`, nama suara, dan teks bahagian, dan melaksanakan arahan menggunakan fungsi `os.system()`. Jika pelaksanaan arahan berjaya, ia menambahkan laluan fail audio yang dijana ke senarai. Selepas memproses semua bahagian, ia menggabungkan fail audio individu menggunakan fungsi `merge_audio_files()` dan menyimpan audio yang telah digabungkan ke laluan simpan yang ditentukan. Jika hanya ada satu bahagian, ia terus menjana arahan Edge TTS dan menyimpan audio ke laluan simpan. Akhirnya, ia mengembalikan laluan simpan fail audio yang dijana.\n",
    "\n",
    "6. `random_audio_name_generate()`: Fungsi ini menjana nama fail audio rawak menggunakan modul [`uuid`]. Ia menjana UUID rawak, menukarnya kepada rentetan, mengambil 8 aksara pertama, menambahkan sambungan \".mp3\", dan mengembalikan nama fail audio rawak.\n",
    "\n",
    "7. `talk(input_text)`: Fungsi ini adalah titik masuk utama untuk melakukan operasi TTS. Ia mengambil teks input sebagai parameter. Ia mula-mula memeriksa panjang teks input untuk menentukan sama ada ia adalah ayat panjang (lebih besar atau sama dengan 600 aksara). Berdasarkan panjang dan nilai pemboleh ubah `translate_text_flag`, ia menentukan bahasa dan menjana senarai bahagian teks menggunakan fungsi `make_chunks()`. Ia kemudian menjana laluan simpan untuk fail audio menggunakan fungsi `random_audio_name_generate()`. Akhirnya, ia memanggil fungsi `edge_free_tts()` untuk melakukan operasi TTS dan mengembalikan laluan simpan fail audio yang dijana.\n",
    "\n",
    "Secara keseluruhan, fungsi-fungsi ini bekerjasama untuk memecahkan teks input kepada bahagian, menjana nama fail untuk fail audio, melakukan operasi TTS menggunakan perkhidmatan Edge TTS, dan menggabungkan fail audio individu menjadi satu fail audio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pelaksanaan dua fungsi: convert_to_text dan run_text_prompt, serta deklarasi dua kelas: str dan Audio.\n",
    "\n",
    "Fungsi convert_to_text mengambil audio_path sebagai input dan menyalin audio kepada teks menggunakan model yang dipanggil whisper_model. Fungsi ini mula-mula memeriksa sama ada bendera gpu disetkan kepada True. Jika ya, whisper_model digunakan dengan parameter tertentu seperti word_timestamps=True, fp16=True, language='English', dan task='translate'. Jika bendera gpu adalah False, whisper_model digunakan dengan fp16=False. Transkripsi yang dihasilkan kemudian disimpan ke dalam fail bernama 'scan.txt' dan dikembalikan sebagai teks.\n",
    "\n",
    "Fungsi run_text_prompt mengambil mesej dan chat_history sebagai input. Ia menggunakan fungsi phi_demo untuk menghasilkan respons daripada chatbot berdasarkan mesej input. Respons yang dihasilkan kemudian dihantar kepada fungsi talk, yang menukar respons tersebut kepada fail audio dan mengembalikan laluan fail. Kelas Audio digunakan untuk memaparkan dan memainkan fail audio. Audio dipaparkan menggunakan fungsi display dari modul IPython.display, dan objek Audio dicipta dengan parameter autoplay=True, supaya audio mula dimainkan secara automatik. Chat_history dikemas kini dengan mesej input dan respons yang dihasilkan, dan string kosong serta chat_history yang dikemas kini dikembalikan.\n",
    "\n",
    "Kelas str adalah kelas terbina dalam Python yang mewakili urutan aksara. Ia menyediakan pelbagai kaedah untuk memanipulasi dan bekerja dengan string, seperti capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill, dan banyak lagi. Kaedah-kaedah ini membolehkan anda melakukan operasi seperti mencari, mengganti, memformat, dan memanipulasi string.\n",
    "\n",
    "Kelas Audio adalah kelas tersuai yang mewakili objek audio. Ia digunakan untuk mencipta pemain audio dalam persekitaran Jupyter Notebook. Kelas ini menerima pelbagai parameter seperti data, filename, url, embed, rate, autoplay, dan normalize. Parameter data boleh berupa array numpy, senarai sampel, string yang mewakili nama fail atau URL, atau data PCM mentah. Parameter filename digunakan untuk menentukan fail tempatan untuk memuatkan data audio, dan parameter url digunakan untuk menentukan URL untuk memuat turun data audio. Parameter embed menentukan sama ada data audio harus disematkan menggunakan URI data atau dirujuk dari sumber asal. Parameter rate menentukan kadar pensampelan data audio. Parameter autoplay menentukan sama ada audio harus mula dimainkan secara automatik. Parameter normalize menentukan sama ada data audio harus dinormalisasi (diskalakan semula) kepada julat maksimum yang mungkin. Kelas Audio juga menyediakan kaedah seperti reload untuk memuat semula data audio dari fail atau URL, dan atribut seperti src_attr, autoplay_attr, dan element_id_attr untuk mendapatkan atribut yang sepadan untuk elemen audio dalam HTML.\n",
    "\n",
    "Secara keseluruhan, fungsi dan kelas ini digunakan untuk menyalin audio kepada teks, menghasilkan respons audio daripada chatbot, dan memaparkan serta memainkan audio dalam persekitaran Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Penafian**:  \nDokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk memastikan ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat yang kritikal, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T23:19:34+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "ms"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}