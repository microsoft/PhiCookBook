<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-05-09T13:33:20+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ms"
}
-->
# **כימות משפחת Phi**

כימות מודל מתייחס לתהליך של מיפוי הפרמטרים (כגון משקלים וערכי הפעלה) במודל רשת נוירונים מטווח ערכים רחב (בדרך כלל טווח ערכים רציף) לטווח ערכים קטן וסופי. טכנולוגיה זו יכולה להפחית את גודל המודל ואת המורכבות החישובית שלו ולשפר את יעילות ההפעלה של המודל בסביבות עם משאבים מוגבלים כמו מכשירים ניידים או מערכות משובצות. כימות מודל משיג דחיסה על ידי הפחתת הדיוק של הפרמטרים, אך גם גורם לאובדן דיוק מסוים. לכן, בתהליך הכימות יש צורך לאזן בין גודל המודל, המורכבות החישובית והדיוק. שיטות כימות נפוצות כוללות כימות נקודתי קבוע, כימות נקודה צפה ועוד. ניתן לבחור את אסטרטגיית הכימות המתאימה לפי התרחיש והצרכים הספציפיים.

אנו שואפים לפרוס מודל GenAI למכשירי קצה ולאפשר למכשירים רבים יותר להיכנס לתרחישי GenAI, כגון מכשירים ניידים, AI PC/Copilot+PC ומכשירי IoT מסורתיים. באמצעות מודל כימות, ניתן לפרוס אותו למכשירי קצה שונים בהתאם למכשיר. בשילוב עם מסגרת האצת מודל ומודל הכימות המסופקים על ידי יצרני חומרה, נוכל לבנות תרחישי יישום SLM טובים יותר.

בתרחיש הכימות קיימים דיוקים שונים (INT4, INT8, FP16, FP32). להלן הסבר על דיוקי הכימות הנפוצים:

### **INT4**

כימות INT4 הוא שיטת כימות קיצונית שמכמתת את המשקלים וערכי ההפעלה של המודל למספרים שלמים ב-4 ביט. כימות INT4 בדרך כלל גורם לאובדן דיוק גדול יותר עקב טווח הייצוג הקטן יותר והדיוק הנמוך יותר. עם זאת, בהשוואה לכימות INT8, כימות INT4 יכול להפחית עוד יותר את דרישות האחסון והמורכבות החישובית של המודל. יש לציין שכימות INT4 נדיר יחסית ביישומים מעשיים, מכיוון שדיוק נמוך מדי עלול לגרום לירידה משמעותית בביצועי המודל. בנוסף, לא כל החומרה תומכת בפעולות INT4, ולכן יש לשקול תאימות חומרה בבחירת שיטת הכימות.

### **INT8**

כימות INT8 הוא תהליך המרה של משקלים וערכי הפעלה של מודל ממספרים נקודה צפה למספרים שלמים ב-8 ביט. אף שטווח המספרים המיוצגים ב-INT8 קטן ופחות מדויק, הוא יכול להפחית משמעותית את דרישות האחסון והחישוב. בכימות INT8, המשקלים וערכי ההפעלה עוברים תהליך כימות הכולל סקלינג והזזה, כדי לשמר את המידע המקורי בנקודה צפה ככל האפשר. במהלך האינפרנס, ערכים מכוּמנים אלה מפוענחים חזרה למספרים בנקודה צפה לחישוב, ואז מוכתמים מחדש ל-INT8 לשלב הבא. שיטה זו מספקת דיוק מספק ברוב היישומים תוך שמירה על יעילות חישובית גבוהה.

### **FP16**

פורמט FP16, כלומר מספרים בנקודה צפה ב-16 ביט (float16), מפחית את צריכת הזיכרון בחצי לעומת מספרים בנקודה צפה ב-32 ביט (float32), מה שמביא יתרונות משמעותיים ביישומי למידה עמוקה בקנה מידה גדול. פורמט FP16 מאפשר טעינת מודלים גדולים יותר או עיבוד יותר נתונים במסגרת מגבלות הזיכרון של ה-GPU. עם המשך התמיכה של חומרת GPU מודרנית בפעולות FP16, השימוש בפורמט זה עשוי גם לשפר את מהירות החישוב. עם זאת, לפורמט FP16 יש גם חסרונות מובנים, כלומר דיוק נמוך יותר, שעלול לגרום לחוסר יציבות מספרית או אובדן דיוק במקרים מסוימים.

### **FP32**

פורמט FP32 מספק דיוק גבוה יותר ויכול לייצג טווח רחב של ערכים במדויק. בתרחישים שבהם מתבצעות פעולות מתמטיות מורכבות או נדרשים תוצאות מדויקות מאוד, פורמט FP32 הוא המועדף. עם זאת, דיוק גבוה יותר משמעותו גם שימוש בזיכרון רב יותר וזמן חישוב ארוך יותר. עבור מודלים של למידה עמוקה בקנה מידה גדול, במיוחד כאשר יש הרבה פרמטרים וכמות עצומה של נתונים, פורמט FP32 עלול לגרום למחסור בזיכרון GPU או להאטת מהירות האינפרנס.

במכשירים ניידים או מכשירי IoT, ניתן להמיר מודלים של Phi-3.x ל-INT4, בעוד ש-AI PC / Copilot PC יכולים להשתמש בדיוק גבוה יותר כמו INT8, FP16, FP32.

כיום, יצרני חומרה שונים מציעים מסגרות שונות לתמיכה במודלים גנרטיביים, כגון OpenVINO של Intel, QNN של Qualcomm, MLX של Apple ו-CUDA של Nvidia, בשילוב עם כימות מודל להשלמת פריסה מקומית.

בטכנולוגיה, יש לנו תמיכה בפורמטים שונים לאחר הכימות, כגון פורמט PyTorch / Tensorflow, GGUF ו-ONNX. ביצעתי השוואת פורמטים ותרחישי יישום בין GGUF ל-ONNX. כאן אני ממליץ על פורמט הכימות ONNX, שיש לו תמיכה טובה מהמסגרת ועד החומרה. בפרק זה נתמקד ב-ONNX Runtime עבור GenAI, OpenVINO ו-Apple MLX לביצוע כימות מודל (אם יש לכם שיטה טובה יותר, ניתן גם לשלוח לנו PR).

**פרק זה כולל**

1. [כימות Phi-3.5 / 4 באמצעות llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [כימות Phi-3.5 / 4 באמצעות הרחבות Generative AI עבור onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [כימות Phi-3.5 / 4 באמצעות Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [כימות Phi-3.5 / 4 באמצעות Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk ketepatan, sila maklum bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya hendaklah dianggap sebagai sumber yang sahih. Untuk maklumat penting, terjemahan profesional oleh manusia adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.