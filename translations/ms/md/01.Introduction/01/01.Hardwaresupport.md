<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8cdc17ce0f10535da30b53d23fe1a795",
  "translation_date": "2025-07-16T18:26:24+00:00",
  "source_file": "md/01.Introduction/01/01.Hardwaresupport.md",
  "language_code": "ms"
}
-->
# Sokongan Perkakasan Phi

Microsoft Phi telah dioptimumkan untuk ONNX Runtime dan menyokong Windows DirectML. Ia berfungsi dengan baik merentasi pelbagai jenis perkakasan, termasuk GPU, CPU, dan juga peranti mudah alih.

## Perkakasan Peranti  
Secara khusus, perkakasan yang disokong termasuk:

- GPU SKU: RTX 4090 (DirectML)
- GPU SKU: 1 A100 80GB (CUDA)
- CPU SKU: Standard F64s v2 (64 vCPU, 128 GiB memori)

## SKU Mudah Alih

- Android - Samsung Galaxy S21
- Apple iPhone 14 atau lebih tinggi Pemproses A16/A17

## Spesifikasi Perkakasan Phi

- Konfigurasi Minimum Diperlukan.
- Windows: GPU yang menyokong DirectX 12 dan sekurang-kurangnya 4GB RAM gabungan

CUDA: GPU NVIDIA dengan Compute Capability >= 7.02

![HardwareSupport](../../../../../translated_images/01.phihardware.5d51b2377cba18afc6949074542f290c56bb278dac3f4f86302aca6d80fffeb9.ms.png)

## Menjalankan onnxruntime pada pelbagai GPU

Model Phi ONNX yang tersedia sekarang hanya untuk 1 GPU. Sokongan multi-GPU untuk model Phi adalah mungkin, tetapi ORT dengan 2 GPU tidak menjamin ia akan memberikan lebih banyak throughput berbanding 2 instans ort. Sila rujuk [ONNX Runtime](https://onnxruntime.ai/) untuk kemas kini terkini.

Di [Build 2024 the GenAI ONNX Team](https://youtu.be/WLW4SE8M9i8?si=EtG04UwDvcjunyfC) mengumumkan bahawa mereka telah mengaktifkan multi-instans dan bukannya multi-GPU untuk model Phi.

Pada masa ini, ini membolehkan anda menjalankan satu instans onnnxruntime atau onnxruntime-genai dengan pembolehubah persekitaran CUDA_VISIBLE_DEVICES seperti berikut.

```Python
CUDA_VISIBLE_DEVICES=0 python infer.py
CUDA_VISIBLE_DEVICES=1 python infer.py
```

Jangan ragu untuk meneroka Phi dengan lebih lanjut di [Azure AI Foundry](https://ai.azure.com)

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk ketepatan, sila ambil maklum bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang sahih. Untuk maklumat penting, terjemahan profesional oleh manusia adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.