<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:28:24+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "ms"
}
-->
# Key technologies mentioned include

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - a low-level API for hardware-accelerated machine learning built on top of DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - a parallel computing platform and application programming interface (API) model developed by Nvidia, enabling general-purpose processing on graphics processing units (GPUs).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - an open format designed to represent machine learning models that provides interoperability between different ML frameworks.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - a format used for representing and updating machine learning models, particularly useful for smaller language models that can run effectively on CPUs with 4-8bit quantization.

## DirectML

DirectML is a low-level API that enables hardware-accelerated machine learning. It’s built on top of DirectX 12 to utilize GPU acceleration and is vendor-agnostic, meaning it doesn’t require code changes to work across different GPU vendors. It’s primarily used for model training and inferencing workloads on GPUs.

Regarding hardware support, DirectML works with a broad range of GPUs, including AMD integrated and discrete GPUs, Intel integrated GPUs, and NVIDIA discrete GPUs. It’s part of the Windows AI Platform and supported on Windows 10 & 11, allowing model training and inferencing on any Windows device.

There have been updates and new possibilities with DirectML, such as support for up to 150 ONNX operators and usage by both the ONNX runtime and WinML. It’s supported by major Integrated Hardware Vendors (IHVs), each implementing various metacommands.

## CUDA

CUDA, which stands for Compute Unified Device Architecture, is a parallel computing platform and API model developed by Nvidia. It enables developers to use CUDA-enabled GPUs for general-purpose processing—an approach known as GPGPU (General-Purpose computing on Graphics Processing Units). CUDA is a core technology behind Nvidia’s GPU acceleration and is widely used across machine learning, scientific computing, and video processing.

CUDA’s hardware support is exclusive to Nvidia GPUs, as it’s proprietary technology developed by Nvidia. Each GPU architecture supports specific CUDA toolkit versions, which provide the libraries and tools developers need to build and run CUDA applications.

## ONNX

ONNX (Open Neural Network Exchange) is an open format designed to represent machine learning models. It defines an extensible computation graph model, built-in operators, and standard data types. ONNX enables developers to transfer models between different ML frameworks, promoting interoperability and simplifying AI application development and deployment.

Phi3 mini can run with ONNX Runtime on CPU and GPU across various devices, including servers, Windows, Linux and Mac desktops, and mobile CPUs.  
The optimized configurations we have added are:

- ONNX models for int4 DML: Quantized to int4 via AWQ  
- ONNX model for fp16 CUDA  
- ONNX model for int4 CUDA: Quantized to int4 via RTN  
- ONNX model for int4 CPU and Mobile: Quantized to int4 via RTN

## Llama.cpp

Llama.cpp is an open-source C++ library designed for inference on various Large Language Models (LLMs), including Llama. Developed alongside the ggml library (a general-purpose tensor library), llama.cpp aims to offer faster inference and lower memory usage compared to the original Python implementation. It supports hardware optimization, quantization, and provides a simple API with examples. If you’re interested in efficient LLM inference, llama.cpp is worth exploring as Phi3 can run Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) is a format used for representing and updating machine learning models. It’s especially useful for smaller language models (SLMs) that can run efficiently on CPUs using 4-8bit quantization. GGUF is valuable for rapid prototyping and running models on edge devices or in batch jobs such as CI/CD pipelines.

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk ketepatan, sila maklum bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang sahih. Untuk maklumat penting, terjemahan profesional oleh manusia adalah disyorkan. Kami tidak bertanggungjawab terhadap sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.