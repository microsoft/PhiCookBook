<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:46:37+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "ms"
}
-->
# Teknologi utama yang disebutkan termasuk

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - API tahap rendah untuk pembelajaran mesin dipercepat perkakasan yang dibina di atas DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - platform pengkomputeran selari dan model antara muka pengaturcaraan aplikasi (API) yang dibangunkan oleh Nvidia, membolehkan pemprosesan tujuan umum pada unit pemprosesan grafik (GPU).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - format terbuka yang direka untuk mewakili model pembelajaran mesin yang menyediakan interoperabiliti antara rangka kerja ML yang berbeza.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - format yang digunakan untuk mewakili dan mengemas kini model pembelajaran mesin, sangat berguna untuk model bahasa kecil yang boleh dijalankan dengan berkesan pada CPU dengan kuantisasi 4-8bit.

## DirectML

DirectML adalah API tahap rendah yang membolehkan pembelajaran mesin dipercepatkan oleh perkakasan. Ia dibina di atas DirectX 12 untuk menggunakan pecutan GPU dan tidak bergantung pada vendor, bermakna ia tidak memerlukan perubahan kod untuk berfungsi merentas vendor GPU yang berbeza. Ia terutamanya digunakan untuk latihan model dan beban kerja inferens pada GPU.

Dari segi sokongan perkakasan, DirectML direka untuk berfungsi dengan pelbagai jenis GPU, termasuk GPU terintegrasi dan diskret AMD, GPU terintegrasi Intel, dan GPU diskret NVIDIA. Ia adalah sebahagian daripada Platform AI Windows dan disokong pada Windows 10 & 11, membolehkan latihan model dan inferens pada mana-mana peranti Windows.

Terdapat kemas kini dan peluang berkaitan DirectML, seperti menyokong sehingga 150 operator ONNX dan digunakan oleh runtime ONNX serta WinML. Ia disokong oleh vendor perkakasan terintegrasi utama (IHVs), masing-masing melaksanakan pelbagai metaperintah.

## CUDA

CUDA, singkatan bagi Compute Unified Device Architecture, adalah platform pengkomputeran selari dan model antara muka pengaturcaraan aplikasi (API) yang dicipta oleh Nvidia. Ia membolehkan pembangun perisian menggunakan unit pemprosesan grafik (GPU) yang menyokong CUDA untuk pemprosesan tujuan umum â€“ pendekatan yang dikenali sebagai GPGPU (Pengkomputeran Tujuan Umum pada Unit Pemprosesan Grafik). CUDA adalah pemangkin utama pecutan GPU Nvidia dan digunakan secara meluas dalam pelbagai bidang, termasuk pembelajaran mesin, pengkomputeran saintifik, dan pemprosesan video.

Sokongan perkakasan untuk CUDA khusus kepada GPU Nvidia, kerana ia adalah teknologi proprietari yang dibangunkan oleh Nvidia. Setiap seni bina menyokong versi tertentu toolkit CUDA, yang menyediakan perpustakaan dan alat yang diperlukan untuk pembangun membina dan menjalankan aplikasi CUDA.

## ONNX

ONNX (Open Neural Network Exchange) adalah format terbuka yang direka untuk mewakili model pembelajaran mesin. Ia menyediakan definisi model graf pengiraan yang boleh dikembangkan, serta definisi operator terbina dalam dan jenis data standard. ONNX membolehkan pembangun memindahkan model antara rangka kerja ML yang berbeza, membolehkan interoperabiliti dan memudahkan penciptaan serta penyebaran aplikasi AI.

Phi3 mini boleh dijalankan dengan ONNX Runtime pada CPU dan GPU merentas peranti, termasuk platform pelayan, Windows, Linux dan desktop Mac, serta CPU mudah alih.  
Konfigurasi yang telah kami optimakan adalah

- Model ONNX untuk int4 DML: Dikuantisasi ke int4 melalui AWQ  
- Model ONNX untuk fp16 CUDA  
- Model ONNX untuk int4 CUDA: Dikuantisasi ke int4 melalui RTN  
- Model ONNX untuk int4 CPU dan Mudah Alih: Dikuantisasi ke int4 melalui RTN

## Llama.cpp

Llama.cpp adalah perpustakaan perisian sumber terbuka yang ditulis dalam C++. Ia menjalankan inferens pada pelbagai Model Bahasa Besar (LLM), termasuk Llama. Dibangunkan bersama perpustakaan ggml (perpustakaan tensor tujuan umum), llama.cpp bertujuan untuk menyediakan inferens yang lebih pantas dan penggunaan memori yang lebih rendah berbanding pelaksanaan asal dalam Python. Ia menyokong pengoptimuman perkakasan, kuantisasi, dan menawarkan API mudah serta contoh. Jika anda berminat dengan inferens LLM yang cekap, llama.cpp patut diterokai kerana Phi3 boleh menjalankan Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) adalah format yang digunakan untuk mewakili dan mengemas kini model pembelajaran mesin. Ia sangat berguna untuk model bahasa kecil (SLM) yang boleh dijalankan dengan berkesan pada CPU dengan kuantisasi 4-8bit. GGUF bermanfaat untuk prototaip pantas dan menjalankan model pada peranti tepi atau dalam tugasan berkumpulan seperti saluran CI/CD.

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang sahih. Untuk maklumat penting, terjemahan profesional oleh manusia adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.