<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "bc29f7fe7fc16bed6932733eac8c81b8",
  "translation_date": "2025-07-17T04:00:27+00:00",
  "source_file": "md/02.Application/02.Code/Phi3/VSCodeExt/HOL/AIPC/02.PromptflowWithNPU.md",
  "language_code": "ms"
}
-->
# **Makmal 2 - Jalankan Prompt flow dengan Phi-3-mini dalam AIPC**

## **Apa itu Prompt flow**

Prompt flow adalah satu set alat pembangunan yang direka untuk memudahkan kitaran pembangunan aplikasi AI berasaskan LLM dari awal idea, prototaip, ujian, penilaian hingga pengeluaran dan pemantauan. Ia menjadikan kejuruteraan prompt lebih mudah dan membolehkan anda membina aplikasi LLM dengan kualiti pengeluaran.

Dengan prompt flow, anda boleh:

- Membina aliran yang menghubungkan LLM, prompt, kod Python dan alat lain dalam satu aliran kerja yang boleh dijalankan.

- Menyahpepijat dan mengulangi aliran anda, terutamanya interaksi dengan LLM dengan mudah.

- Menilai aliran anda, mengira metrik kualiti dan prestasi dengan set data yang lebih besar.

- Mengintegrasikan ujian dan penilaian ke dalam sistem CI/CD anda untuk memastikan kualiti aliran anda.

- Mengeluarkan aliran anda ke platform perkhidmatan pilihan anda atau mengintegrasikannya ke dalam kod aplikasi dengan mudah.

- (Pilihan tetapi sangat disyorkan) Bekerjasama dengan pasukan anda dengan menggunakan versi awan Prompt flow dalam Azure AI.

## **Apa itu AIPC**

AI PC mempunyai CPU, GPU dan NPU, masing-masing dengan keupayaan pecutan AI khusus. NPU, atau unit pemprosesan neural, adalah pemecut khusus yang mengendalikan tugasan kecerdasan buatan (AI) dan pembelajaran mesin (ML) terus pada PC anda tanpa perlu menghantar data ke awan untuk diproses. GPU dan CPU juga boleh memproses tugasan ini, tetapi NPU sangat cekap untuk pengiraan AI berkuasa rendah. AI PC mewakili perubahan asas dalam cara komputer kita beroperasi. Ia bukan penyelesaian untuk masalah yang tidak pernah wujud sebelum ini. Sebaliknya, ia menjanjikan peningkatan besar untuk penggunaan PC harian.

Jadi bagaimana ia berfungsi? Berbanding AI generatif dan model bahasa besar (LLM) yang dilatih dengan banyak data awam, AI yang berlaku pada PC anda lebih mudah diakses pada hampir semua peringkat. Konsepnya lebih mudah difahami, dan kerana ia dilatih menggunakan data anda sendiri tanpa perlu mengakses awan, manfaatnya lebih segera menarik bagi lebih ramai pengguna.

Dalam jangka masa terdekat, dunia AI PC melibatkan pembantu peribadi dan model AI yang lebih kecil yang berjalan terus pada PC anda, menggunakan data anda untuk menawarkan peningkatan AI yang peribadi, privasi dan lebih selamat untuk perkara yang anda lakukan setiap hari – mencatat minit mesyuarat, mengatur liga bola sepak fantasi, mengautomasikan penambahbaikan untuk penyuntingan foto dan video, atau menyusun jadual perjalanan sempurna untuk perjumpaan keluarga berdasarkan masa ketibaan dan berlepas semua orang.

## **Membina aliran kod generasi pada AIPC**

***Note*** ：Jika anda belum melengkapkan pemasangan persekitaran, sila lawati [Lab 0 -Installations](./01.Installations.md)

1. Buka Sambungan Prompt flow dalam Visual Studio Code dan cipta projek aliran kosong

![create](../../../../../../../../../translated_images/pf_create.bde888dc83502eba082a058175bbf1eee6791219795393a386b06fd3043ec54d.ms.png)

2. Tambah parameter Input dan Output serta Tambah Kod Python sebagai aliran baru

![flow](../../../../../../../../../translated_images/pf_flow.520824c0969f2a94f17e947f86bdc4b4c6c88a2efa394fe3bcfb58c0dbc578a7.ms.png)

Anda boleh rujuk struktur ini (flow.dag.yaml) untuk membina aliran anda

```yaml

inputs:
  question:
    type: string
    default: how to write Bubble Algorithm
outputs:
  answer:
    type: string
    reference: ${Chat_With_Phi3.output}
nodes:
- name: Chat_With_Phi3
  type: python
  source:
    type: code
    path: Chat_With_Phi3.py
  inputs:
    question: ${inputs.question}


```

3. Tambah Kod dalam ***Chat_With_Phi3.py***

```python


from promptflow.core import tool

# import torch
from transformers import AutoTokenizer, pipeline,TextStreamer
import intel_npu_acceleration_library as npu_lib

import warnings

import asyncio
import platform

class Phi3CodeAgent:
    
    model = None
    tokenizer = None
    text_streamer = None
    
    model_id = "microsoft/Phi-3-mini-4k-instruct"

    @staticmethod
    def init_phi3():
        
        if Phi3CodeAgent.model is None or Phi3CodeAgent.tokenizer is None or Phi3CodeAgent.text_streamer is None:
            Phi3CodeAgent.model = npu_lib.NPUModelForCausalLM.from_pretrained(
                                    Phi3CodeAgent.model_id,
                                    torch_dtype="auto",
                                    dtype=npu_lib.int4,
                                    trust_remote_code=True
                                )
            Phi3CodeAgent.tokenizer = AutoTokenizer.from_pretrained(Phi3CodeAgent.model_id)
            Phi3CodeAgent.text_streamer = TextStreamer(Phi3CodeAgent.tokenizer, skip_prompt=True)

    

    @staticmethod
    def chat_with_phi3(prompt):
        
        Phi3CodeAgent.init_phi3()

        messages = "<|system|>You are a AI Python coding assistant. Please help me to generate code in Python.The answer only genertated Python code, but any comments and instructions do not need to be generated<|end|><|user|>" + prompt +"<|end|><|assistant|>"



        generation_args = {
            "max_new_tokens": 1024,
            "return_full_text": False,
            "temperature": 0.3,
            "do_sample": False,
            "streamer": Phi3CodeAgent.text_streamer,
        }

        pipe = pipeline(
            "text-generation",
            model=Phi3CodeAgent.model,
            tokenizer=Phi3CodeAgent.tokenizer,
            # **generation_args
        )

        result = ''

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            response = pipe(messages, **generation_args)
            result =response[0]['generated_text']
            return result


@tool
def my_python_tool(question: str) -> str:
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    return Phi3CodeAgent.chat_with_phi3(question)


```

4. Anda boleh uji aliran dari Debug atau Run untuk periksa sama ada kod generasi berfungsi dengan baik atau tidak

![RUN](../../../../../../../../../translated_images/pf_run.4239e8a0b420a58284edf6ee1471c1697c345670313c8e7beac0edaee15b9a9d.ms.png)

5. Jalankan aliran sebagai API pembangunan dalam terminal

```

pf flow serve --source ./ --port 8080 --host localhost   

```

Anda boleh uji menggunakan Postman / Thunder Client

### **Note**

1. Larian pertama mengambil masa yang lama. Disyorkan untuk muat turun model phi-3 dari Hugging face CLI.

2. Memandangkan kuasa pengkomputeran Intel NPU yang terhad, disyorkan menggunakan Phi-3-mini-4k-instruct

3. Kami menggunakan Pecutan Intel NPU untuk kuantisasi penukaran INT4, tetapi jika anda menjalankan semula perkhidmatan, anda perlu memadamkan folder cache dan nc_workshop.

## **Sumber**

1. Belajar Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Belajar Pecutan Intel NPU [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Kod Contoh, muat turun [Local NPU Agent Sample Code](../../../../../../../../../code/07.Lab/01/AIPC)

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk ketepatan, sila ambil maklum bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang sahih. Untuk maklumat penting, terjemahan profesional oleh manusia adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.