<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "bc29f7fe7fc16bed6932733eac8c81b8",
  "translation_date": "2025-05-09T19:25:17+00:00",
  "source_file": "md/02.Application/02.Code/Phi3/VSCodeExt/HOL/AIPC/02.PromptflowWithNPU.md",
  "language_code": "ms"
}
-->
# **实验2 - 在AIPC上运行Phi-3-mini的Prompt flow**

## **什么是Prompt flow**

Prompt flow是一套开发工具，旨在简化基于大语言模型（LLM）的AI应用从构思、原型设计、测试、评估到生产部署和监控的端到端开发流程。它让prompt工程变得更加简单，使你能够构建具备生产质量的LLM应用。

通过Prompt flow，你可以：

- 创建将LLM、prompt、Python代码及其他工具连接在一起的可执行工作流。

- 轻松调试和迭代你的工作流，尤其是与LLM的交互部分。

- 评估你的工作流，使用更大数据集计算质量和性能指标。

- 将测试和评估集成到CI/CD系统中，确保工作流的质量。

- 轻松将工作流部署到你选择的服务平台，或集成到应用代码库中。

- （可选但强烈推荐）通过Azure AI上的云版本与团队协作。

## **什么是AIPC**

AI PC配备了CPU、GPU和NPU，每种都有特定的AI加速能力。NPU，即神经处理单元，是一种专门的加速器，能在你的PC上直接处理人工智能（AI）和机器学习（ML）任务，而无需将数据发送到云端。GPU和CPU也能处理这些任务，但NPU在低功耗AI计算方面表现尤为出色。AI PC代表了计算机操作方式的根本转变。它不是为了解决之前不存在的问题，而是为日常PC使用带来巨大改进。

那么它是如何工作的呢？相比于基于大量公开数据训练的大型生成式AI模型，运行在你PC上的AI在各个层面都更易于访问。这个概念更容易理解，并且因为它基于你的数据训练，无需访问云端，其优势对更广泛的人群更具吸引力。

在短期内，AI PC将运行个人助理和较小的AI模型，利用你的数据为你日常活动提供个性化、私密且更安全的AI增强——比如记录会议纪要、组织虚拟足球联赛、自动优化照片和视频编辑，或根据每个人的到达和离开时间规划完美的家庭聚会行程。

## **在AIPC上构建生成代码工作流**

***注意*** ：如果你还未完成环境安装，请访问 [Lab 0 - Installations](./01.Installations.md)

1. 在Visual Studio Code中打开Prompt flow扩展，创建一个空的flow项目

![create](../../../../../../../../../translated_images/pf_create.d6172d8277a78a7fa82cd6ff727ed44e037fa78b662f1f62d5963f36d712d229.ms.png)

2. 添加Inputs和Outputs参数，并添加Python代码作为新的flow

![flow](../../../../../../../../../translated_images/pf_flow.d5646a323fb7f444c0b98b4521057a592325c583e7ba18bc31500bc0415e9ef3.ms.png)

你可以参考这个结构（flow.dag.yaml）来构建你的工作流

```yaml

inputs:
  question:
    type: string
    default: how to write Bubble Algorithm
outputs:
  answer:
    type: string
    reference: ${Chat_With_Phi3.output}
nodes:
- name: Chat_With_Phi3
  type: python
  source:
    type: code
    path: Chat_With_Phi3.py
  inputs:
    question: ${inputs.question}


```

3. 在***Chat_With_Phi3.py***中添加代码

```python


from promptflow.core import tool

# import torch
from transformers import AutoTokenizer, pipeline,TextStreamer
import intel_npu_acceleration_library as npu_lib

import warnings

import asyncio
import platform

class Phi3CodeAgent:
    
    model = None
    tokenizer = None
    text_streamer = None
    
    model_id = "microsoft/Phi-3-mini-4k-instruct"

    @staticmethod
    def init_phi3():
        
        if Phi3CodeAgent.model is None or Phi3CodeAgent.tokenizer is None or Phi3CodeAgent.text_streamer is None:
            Phi3CodeAgent.model = npu_lib.NPUModelForCausalLM.from_pretrained(
                                    Phi3CodeAgent.model_id,
                                    torch_dtype="auto",
                                    dtype=npu_lib.int4,
                                    trust_remote_code=True
                                )
            Phi3CodeAgent.tokenizer = AutoTokenizer.from_pretrained(Phi3CodeAgent.model_id)
            Phi3CodeAgent.text_streamer = TextStreamer(Phi3CodeAgent.tokenizer, skip_prompt=True)

    

    @staticmethod
    def chat_with_phi3(prompt):
        
        Phi3CodeAgent.init_phi3()

        messages = "<|system|>You are a AI Python coding assistant. Please help me to generate code in Python.The answer only genertated Python code, but any comments and instructions do not need to be generated<|end|><|user|>" + prompt +"<|end|><|assistant|>"



        generation_args = {
            "max_new_tokens": 1024,
            "return_full_text": False,
            "temperature": 0.3,
            "do_sample": False,
            "streamer": Phi3CodeAgent.text_streamer,
        }

        pipe = pipeline(
            "text-generation",
            model=Phi3CodeAgent.model,
            tokenizer=Phi3CodeAgent.tokenizer,
            # **generation_args
        )

        result = ''

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            response = pipe(messages, **generation_args)
            result =response[0]['generated_text']
            return result


@tool
def my_python_tool(question: str) -> str:
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    return Phi3CodeAgent.chat_with_phi3(question)


```

4. 你可以通过Debug或Run来测试工作流，检查生成代码是否正常

![RUN](../../../../../../../../../translated_images/pf_run.d918637dc00f61e9bdeec37d4cc9646f77d270ac9203bcce13569f3157202b6e.ms.png)

5. 在终端以开发API模式运行工作流

```

pf flow serve --source ./ --port 8080 --host localhost   

```

你可以使用Postman或Thunder Client进行测试

### **注意**

1. 第一次运行会花较长时间，建议通过Hugging face CLI下载phi-3模型。

2. 鉴于Intel NPU的计算能力有限，推荐使用Phi-3-mini-4k-instruct。

3. 我们使用Intel NPU加速进行INT4量化转换，但如果重新运行服务，需要删除cache和nc_workshop文件夹。

## **资源**

1. 学习Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. 学习Intel NPU加速 [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. 示例代码，下载 [Local NPU Agent Sample Code](../../../../../../../../../code/07.Lab/01/AIPC)

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk ketepatan, sila ambil maklum bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang sahih. Untuk maklumat penting, terjemahan profesional oleh manusia adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.