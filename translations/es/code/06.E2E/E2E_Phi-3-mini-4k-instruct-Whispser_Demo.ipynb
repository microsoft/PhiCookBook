{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot interactivo Phi 3 Mini 4K Instruct con Whisper\n",
    "\n",
    "### Introducción:\n",
    "El Chatbot interactivo Phi 3 Mini 4K Instruct es una herramienta que permite a los usuarios interactuar con la demostración de Microsoft Phi 3 Mini 4K Instruct utilizando texto o entrada de audio. El chatbot puede ser utilizado para una variedad de tareas, como traducción, actualizaciones del clima y recopilación de información general.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[Crear tu Token de Acceso de Huggingface](https://huggingface.co/settings/tokens)\n",
    "\n",
    "Crea un nuevo token  \n",
    "Proporciona un nuevo nombre  \n",
    "Selecciona permisos de escritura  \n",
    "Copia el token y guárdalo en un lugar seguro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código en Python realiza dos tareas principales: importar el módulo `os` y establecer una variable de entorno.\n",
    "\n",
    "1. Importar el módulo `os`:\n",
    "   - El módulo `os` en Python proporciona una forma de interactuar con el sistema operativo. Permite realizar diversas tareas relacionadas con el sistema operativo, como acceder a variables de entorno, trabajar con archivos y directorios, etc.\n",
    "   - En este código, el módulo `os` se importa utilizando la instrucción `import`. Esta instrucción hace que la funcionalidad del módulo `os` esté disponible para su uso en el script de Python actual.\n",
    "\n",
    "2. Establecer una variable de entorno:\n",
    "   - Una variable de entorno es un valor que puede ser accedido por programas que se ejecutan en el sistema operativo. Es una forma de almacenar configuraciones o información que puede ser utilizada por múltiples programas.\n",
    "   - En este código, se está estableciendo una nueva variable de entorno utilizando el diccionario `os.environ`. La clave del diccionario es `'HF_TOKEN'`, y el valor se asigna desde la variable `HUGGINGFACE_TOKEN`.\n",
    "   - La variable `HUGGINGFACE_TOKEN` se define justo encima de este fragmento de código y se le asigna un valor de cadena `\"hf_**************\"` utilizando la sintaxis `#@param`. Esta sintaxis se utiliza frecuentemente en Jupyter notebooks para permitir la entrada del usuario y la configuración de parámetros directamente en la interfaz del notebook.\n",
    "   - Al establecer la variable de entorno `'HF_TOKEN'`, esta puede ser accedida por otras partes del programa o por otros programas que se ejecuten en el mismo sistema operativo.\n",
    "\n",
    "En resumen, este código importa el módulo `os` y establece una variable de entorno llamada `'HF_TOKEN'` con el valor proporcionado en la variable `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de código define una función llamada clear_output que se utiliza para borrar la salida de la celda actual en Jupyter Notebook o IPython. Vamos a desglosar el código y entender su funcionalidad:\n",
    "\n",
    "La función clear_output toma un parámetro llamado wait, que es un valor booleano. Por defecto, wait está configurado como False. Este parámetro determina si la función debe esperar hasta que haya nueva salida disponible para reemplazar la salida existente antes de borrarla.\n",
    "\n",
    "La función en sí se utiliza para borrar la salida de la celda actual. En Jupyter Notebook o IPython, cuando una celda produce una salida, como texto impreso o gráficos, esa salida se muestra debajo de la celda. La función clear_output permite borrar esa salida.\n",
    "\n",
    "La implementación de la función no se proporciona en el fragmento de código, como se indica con los puntos suspensivos (...). Los puntos suspensivos representan un marcador de posición para el código real que realiza el borrado de la salida. La implementación de la función puede implicar interactuar con la API de Jupyter Notebook o IPython para eliminar la salida existente de la celda.\n",
    "\n",
    "En general, esta función ofrece una manera conveniente de borrar la salida de la celda actual en Jupyter Notebook o IPython, facilitando la gestión y actualización de la salida mostrada durante sesiones de codificación interactivas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizar texto a voz (TTS) utilizando el servicio Edge TTS. Vamos a revisar las implementaciones de las funciones relevantes una por una:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Esta función toma un valor de entrada y calcula la cadena de velocidad para la voz TTS. El valor de entrada representa la velocidad deseada del habla, donde un valor de 1 representa la velocidad normal. La función calcula la cadena de velocidad restando 1 del valor de entrada, multiplicándolo por 100 y luego determinando el signo según si el valor de entrada es mayor o igual a 1. La función devuelve la cadena de velocidad en el formato \"{signo}{velocidad}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: Esta función toma un texto de entrada y un idioma como parámetros. Divide el texto de entrada en fragmentos según las reglas específicas del idioma. En esta implementación, si el idioma es \"English\", la función divide el texto en cada punto (\".\") y elimina cualquier espacio en blanco al principio o al final. Luego, agrega un punto a cada fragmento y devuelve la lista filtrada de fragmentos.\n",
    "\n",
    "3. `tts_file_name(text)`: Esta función genera un nombre de archivo para el archivo de audio TTS basado en el texto de entrada. Realiza varias transformaciones en el texto: elimina un punto final (si está presente), convierte el texto a minúsculas, elimina espacios en blanco al principio y al final, y reemplaza los espacios con guiones bajos. Luego, trunca el texto a un máximo de 25 caracteres (si es más largo) o utiliza el texto completo si está vacío. Finalmente, genera una cadena aleatoria utilizando el módulo [`uuid`] y la combina con el texto truncado para crear el nombre del archivo en el formato \"/content/edge_tts_voice/{texto_truncado}_{cadena_aleatoria}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Esta función combina varios archivos de audio en un solo archivo de audio. Toma una lista de rutas de archivos de audio y una ruta de salida como parámetros. La función inicializa un objeto vacío `AudioSegment` llamado [`merged_audio`]. Luego, itera a través de cada ruta de archivo de audio, carga el archivo de audio utilizando el método `AudioSegment.from_file()` de la biblioteca `pydub`, y agrega el archivo de audio actual al objeto [`merged_audio`]. Finalmente, exporta el audio combinado a la ruta de salida especificada en formato MP3.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: Esta función realiza la operación TTS utilizando el servicio Edge TTS. Toma una lista de fragmentos de texto, la velocidad del habla, el nombre de la voz y la ruta de guardado como parámetros. Si el número de fragmentos es mayor que 1, la función crea un directorio para almacenar los archivos de audio individuales de los fragmentos. Luego, itera a través de cada fragmento, construye un comando Edge TTS utilizando la función `calculate_rate_string()`, el nombre de la voz y el texto del fragmento, y ejecuta el comando utilizando la función `os.system()`. Si la ejecución del comando es exitosa, agrega la ruta del archivo de audio generado a una lista. Después de procesar todos los fragmentos, combina los archivos de audio individuales utilizando la función `merge_audio_files()` y guarda el audio combinado en la ruta de guardado especificada. Si solo hay un fragmento, genera directamente el comando Edge TTS y guarda el audio en la ruta de guardado. Finalmente, devuelve la ruta de guardado del archivo de audio generado.\n",
    "\n",
    "6. `random_audio_name_generate()`: Esta función genera un nombre de archivo de audio aleatorio utilizando el módulo [`uuid`]. Genera un UUID aleatorio, lo convierte en una cadena, toma los primeros 8 caracteres, agrega la extensión \".mp3\" y devuelve el nombre de archivo de audio aleatorio.\n",
    "\n",
    "7. `talk(input_text)`: Esta función es el punto de entrada principal para realizar la operación TTS. Toma un texto de entrada como parámetro. Primero verifica la longitud del texto de entrada para determinar si es una oración larga (mayor o igual a 600 caracteres). Según la longitud y el valor de la variable `translate_text_flag`, determina el idioma y genera la lista de fragmentos de texto utilizando la función `make_chunks()`. Luego, genera una ruta de guardado para el archivo de audio utilizando la función `random_audio_name_generate()`. Finalmente, llama a la función `edge_free_tts()` para realizar la operación TTS y devuelve la ruta de guardado del archivo de audio generado.\n",
    "\n",
    "En general, estas funciones trabajan juntas para dividir el texto de entrada en fragmentos, generar un nombre de archivo para el archivo de audio, realizar la operación TTS utilizando el servicio Edge TTS y combinar los archivos de audio individuales en un solo archivo de audio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La implementación de dos funciones: convert_to_text y run_text_prompt, así como la declaración de dos clases: str y Audio.\n",
    "\n",
    "La función convert_to_text toma un audio_path como entrada y transcribe el audio a texto utilizando un modelo llamado whisper_model. Primero, la función verifica si la bandera gpu está configurada como True. Si es así, se utiliza el whisper_model con ciertos parámetros como word_timestamps=True, fp16=True, language='English' y task='translate'. Si la bandera gpu es False, el whisper_model se utiliza con fp16=False. La transcripción resultante se guarda en un archivo llamado 'scan.txt' y se devuelve como texto.\n",
    "\n",
    "La función run_text_prompt toma un mensaje y un chat_history como entrada. Utiliza la función phi_demo para generar una respuesta de un chatbot basada en el mensaje de entrada. La respuesta generada se pasa a la función talk, que convierte la respuesta en un archivo de audio y devuelve la ruta del archivo. La clase Audio se utiliza para mostrar y reproducir el archivo de audio. El audio se muestra utilizando la función display del módulo IPython.display, y el objeto Audio se crea con el parámetro autoplay=True, para que el audio comience a reproducirse automáticamente. El chat_history se actualiza con el mensaje de entrada y la respuesta generada, y se devuelven una cadena vacía y el chat_history actualizado.\n",
    "\n",
    "La clase str es una clase incorporada en Python que representa una secuencia de caracteres. Proporciona varios métodos para manipular y trabajar con cadenas, como capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill, y más. Estos métodos te permiten realizar operaciones como buscar, reemplazar, formatear y manipular cadenas.\n",
    "\n",
    "La clase Audio es una clase personalizada que representa un objeto de audio. Se utiliza para crear un reproductor de audio en el entorno de Jupyter Notebook. La clase acepta varios parámetros como data, filename, url, embed, rate, autoplay y normalize. El parámetro data puede ser un array de numpy, una lista de muestras, una cadena que representa un nombre de archivo o URL, o datos PCM sin procesar. El parámetro filename se utiliza para especificar un archivo local desde el cual cargar los datos de audio, y el parámetro url se utiliza para especificar una URL desde la cual descargar los datos de audio. El parámetro embed determina si los datos de audio deben incrustarse utilizando un URI de datos o referenciarse desde la fuente original. El parámetro rate especifica la tasa de muestreo de los datos de audio. El parámetro autoplay determina si el audio debe comenzar a reproducirse automáticamente. El parámetro normalize especifica si los datos de audio deben normalizarse (reescalarse) al rango máximo posible. La clase Audio también proporciona métodos como reload para recargar los datos de audio desde un archivo o URL, y atributos como src_attr, autoplay_attr y element_id_attr para recuperar los atributos correspondientes del elemento de audio en HTML.\n",
    "\n",
    "En general, estas funciones y clases se utilizan para transcribir audio a texto, generar respuestas de audio desde un chatbot y mostrar y reproducir audio en el entorno de Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Descargo de responsabilidad**:  \nEste documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por garantizar la precisión, tenga en cuenta que las traducciones automáticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones erróneas que puedan surgir del uso de esta traducción.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T16:37:35+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "es"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}