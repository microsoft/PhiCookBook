<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "bc29f7fe7fc16bed6932733eac8c81b8",
  "translation_date": "2025-03-27T11:53:16+00:00",
  "source_file": "md\\02.Application\\02.Code\\Phi3\\VSCodeExt\\HOL\\AIPC\\02.PromptflowWithNPU.md",
  "language_code": "es"
}
-->
# **Lab 2 - Ejecutar Prompt flow con Phi-3-mini en AIPC**

## **¿Qué es Prompt flow?**

Prompt flow es un conjunto de herramientas de desarrollo diseñadas para simplificar el ciclo completo de desarrollo de aplicaciones de IA basadas en modelos de lenguaje grande (LLM), desde la ideación, creación de prototipos, pruebas, evaluación hasta el despliegue en producción y monitoreo. Facilita la ingeniería de prompts y te permite construir aplicaciones LLM con calidad de producción.

Con Prompt flow, podrás:

- Crear flujos que conecten LLMs, prompts, código en Python y otras herramientas en un flujo de trabajo ejecutable.

- Depurar e iterar tus flujos, especialmente las interacciones con los LLMs, de manera sencilla.

- Evaluar tus flujos, calcular métricas de calidad y rendimiento con conjuntos de datos más grandes.

- Integrar las pruebas y evaluaciones en tu sistema CI/CD para garantizar la calidad de tu flujo.

- Desplegar tus flujos en la plataforma de servicio que elijas o integrarlos fácilmente en la base de código de tu aplicación.

- (Opcional pero altamente recomendado) Colaborar con tu equipo aprovechando la versión en la nube de Prompt flow en Azure AI.

## **¿Qué es AIPC?**

Un PC de IA (AIPC) tiene una CPU, una GPU y una NPU, cada una con capacidades específicas de aceleración de IA. Una NPU, o unidad de procesamiento neuronal, es un acelerador especializado que maneja tareas de inteligencia artificial (IA) y aprendizaje automático (ML) directamente en tu PC, en lugar de enviar datos a la nube para su procesamiento. La GPU y la CPU también pueden procesar estas cargas de trabajo, pero la NPU es especialmente buena en cálculos de IA de bajo consumo. El AIPC representa un cambio fundamental en la forma en que operan nuestras computadoras. No es una solución para un problema que no existía antes, sino que promete ser una gran mejora para los usos cotidianos de las PC.

¿Cómo funciona? En comparación con la IA generativa y los modelos de lenguaje grande (LLMs) entrenados con grandes cantidades de datos públicos, la IA que se ejecutará en tu PC es más accesible en casi todos los niveles. El concepto es más fácil de entender, y dado que está entrenado con tus datos, sin necesidad de acceder a la nube, los beneficios son más atractivos de manera inmediata para un público más amplio.

En el corto plazo, el mundo del AIPC incluye asistentes personales y modelos de IA más pequeños que se ejecutan directamente en tu PC, utilizando tus datos para ofrecer mejoras de IA personales, privadas y más seguras para las tareas que ya realizas a diario: tomar notas de reuniones, organizar una liga de fútbol fantasy, automatizar mejoras para la edición de fotos y videos, o planificar el itinerario perfecto para una reunión familiar basado en los horarios de llegada y salida de todos.

## **Construyendo flujos de generación de código en AIPC**

***Nota***: Si no has completado la instalación del entorno, por favor visita [Lab 0 - Instalaciones](./01.Installations.md)

1. Abre la extensión de Prompt flow en Visual Studio Code y crea un proyecto de flujo vacío.

![create](../../../../../../../../../translated_images/pf_create.d6172d8277a78a7fa82cd6ff727ed44e037fa78b662f1f62d5963f36d712d229.es.png)

2. Agrega parámetros de entrada y salida, y añade código en Python como un nuevo flujo.

![flow](../../../../../../../../../translated_images/pf_flow.d5646a323fb7f444c0b98b4521057a592325c583e7ba18bc31500bc0415e9ef3.es.png)

Puedes referirte a esta estructura (flow.dag.yaml) para construir tu flujo:

```yaml

inputs:
  question:
    type: string
    default: how to write Bubble Algorithm
outputs:
  answer:
    type: string
    reference: ${Chat_With_Phi3.output}
nodes:
- name: Chat_With_Phi3
  type: python
  source:
    type: code
    path: Chat_With_Phi3.py
  inputs:
    question: ${inputs.question}


```

3. Agrega código en ***Chat_With_Phi3.py***:

```python


from promptflow.core import tool

# import torch
from transformers import AutoTokenizer, pipeline,TextStreamer
import intel_npu_acceleration_library as npu_lib

import warnings

import asyncio
import platform

class Phi3CodeAgent:
    
    model = None
    tokenizer = None
    text_streamer = None
    
    model_id = "microsoft/Phi-3-mini-4k-instruct"

    @staticmethod
    def init_phi3():
        
        if Phi3CodeAgent.model is None or Phi3CodeAgent.tokenizer is None or Phi3CodeAgent.text_streamer is None:
            Phi3CodeAgent.model = npu_lib.NPUModelForCausalLM.from_pretrained(
                                    Phi3CodeAgent.model_id,
                                    torch_dtype="auto",
                                    dtype=npu_lib.int4,
                                    trust_remote_code=True
                                )
            Phi3CodeAgent.tokenizer = AutoTokenizer.from_pretrained(Phi3CodeAgent.model_id)
            Phi3CodeAgent.text_streamer = TextStreamer(Phi3CodeAgent.tokenizer, skip_prompt=True)

    

    @staticmethod
    def chat_with_phi3(prompt):
        
        Phi3CodeAgent.init_phi3()

        messages = "<|system|>You are a AI Python coding assistant. Please help me to generate code in Python.The answer only genertated Python code, but any comments and instructions do not need to be generated<|end|><|user|>" + prompt +"<|end|><|assistant|>"



        generation_args = {
            "max_new_tokens": 1024,
            "return_full_text": False,
            "temperature": 0.3,
            "do_sample": False,
            "streamer": Phi3CodeAgent.text_streamer,
        }

        pipe = pipeline(
            "text-generation",
            model=Phi3CodeAgent.model,
            tokenizer=Phi3CodeAgent.tokenizer,
            # **generation_args
        )

        result = ''

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            response = pipe(messages, **generation_args)
            result =response[0]['generated_text']
            return result


@tool
def my_python_tool(question: str) -> str:
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    return Phi3CodeAgent.chat_with_phi3(question)


```

4. Puedes probar el flujo desde la opción Debug o Run para verificar si el código generado funciona correctamente.

![RUN](../../../../../../../../../translated_images/pf_run.d918637dc00f61e9bdeec37d4cc9646f77d270ac9203bcce13569f3157202b6e.es.png)

5. Ejecuta el flujo como una API de desarrollo en la terminal:

```

pf flow serve --source ./ --port 8080 --host localhost   

```

Puedes probarlo en Postman / Thunder Client.

### **Nota**

1. La primera ejecución lleva mucho tiempo. Se recomienda descargar el modelo phi-3 desde Hugging Face CLI.

2. Considerando la capacidad de cómputo limitada de la NPU de Intel, se recomienda usar Phi-3-mini-4k-instruct.

3. Usamos la aceleración de la NPU de Intel para convertir a INT4, pero si vuelves a ejecutar el servicio, necesitas eliminar las carpetas cache y nc_workshop.

## **Recursos**

1. Aprende sobre Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Aprende sobre la Aceleración de NPU de Intel [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Código de ejemplo, descarga [Código de ejemplo de agente NPU local](../../../../../../../../../code/07.Lab/01/AIPC)

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por garantizar la precisión, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No somos responsables de malentendidos o interpretaciones erróneas que puedan surgir del uso de esta traducción.