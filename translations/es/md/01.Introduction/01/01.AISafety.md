<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "839ccc4b3886ef10cfd4e64977f5792d",
  "translation_date": "2026-01-05T08:15:57+00:00",
  "source_file": "md/01.Introduction/01/01.AISafety.md",
  "language_code": "es"
}
-->
# Seguridad de IA para modelos Phi
La familia de modelos Phi se desarrolló de acuerdo con el [Estándar de IA Responsable de Microsoft](https://www.microsoft.com/ai/principles-and-approach#responsible-ai-standard), que es un conjunto de requisitos a nivel de empresa basado en los siguientes seis principios: responsabilidad, transparencia, equidad, confiabilidad y seguridad, privacidad y seguridad, e inclusividad, que conforman los [principios de IA Responsable de Microsoft](https://www.microsoft.com/ai/responsible-ai).

Al igual que con los modelos Phi anteriores, se adoptó una evaluación multifacética de seguridad y un enfoque de seguridad posterior al entrenamiento, con medidas adicionales para tener en cuenta las capacidades multilingües de esta versión. Nuestro enfoque para el entrenamiento y las evaluaciones de seguridad, incluyendo pruebas en varios idiomas y categorías de riesgo, se describe en el [Documento sobre seguridad posterior al entrenamiento de Phi](https://arxiv.org/abs/2407.13833). Aunque los modelos Phi se benefician de este enfoque, los desarrolladores deben aplicar las mejores prácticas de IA responsable, incluyendo mapear, medir y mitigar los riesgos asociados con su caso de uso específico y el contexto cultural y lingüístico.

## Mejores Prácticas

Como otros modelos, la familia de modelos Phi puede comportarse potencialmente de maneras que no sean justas, confiables o que resulten ofensivas.

Algunos de los comportamientos limitantes de SLM y LLM que debes conocer incluyen:

- **Calidad del Servicio:** Los modelos Phi se entrenan principalmente con texto en inglés. Los idiomas que no sean inglés experimentarán un rendimiento inferior. Las variedades del inglés con menor representación en los datos de entrenamiento podrían tener un rendimiento peor que el inglés americano estándar.
- **Representación de Daños y Perpetuación de Estereotipos:** Estos modelos pueden sobre o sub-representar grupos de personas, borrar la representación de algunos grupos o reforzar estereotipos despectivos o negativos. A pesar de la seguridad posterior al entrenamiento, estas limitaciones pueden estar presentes debido a distintos niveles de representación de los diferentes grupos o la prevalencia de ejemplos de estereotipos negativos en los datos de entrenamiento que reflejan patrones del mundo real y sesgos sociales.
- **Contenido Inapropiado u Ofensivo:** Estos modelos pueden producir otros tipos de contenido inapropiado u ofensivo, lo que podría hacer que su despliegue en contextos sensibles no sea adecuado sin mitigaciones adicionales específicas para el caso de uso.
  Confiabilidad de la Información: Los modelos de lenguaje pueden generar contenido sin sentido o fabricar contenido que podría sonar razonable pero es inexacto o está desactualizado.
- **Ámbito Limitado para Código:** La mayoría de los datos de entrenamiento de Phi-3 se basa en Python y usa paquetes comunes como "typing, math, random, collections, datetime, itertools". Si el modelo genera scripts en Python que utilizan otros paquetes o scripts en otros lenguajes, recomendamos encarecidamente que los usuarios verifiquen manualmente todos los usos de API.

Los desarrolladores deben aplicar las mejores prácticas de IA responsable y son responsables de garantizar que un caso de uso específico cumpla con las leyes y regulaciones pertinentes (por ejemplo, privacidad, comercio, etc.).

## Consideraciones de IA Responsable

Al igual que otros modelos de lenguaje, los modelos de la serie Phi pueden comportarse potencialmente de maneras no justas, poco confiables o ofensivas. Algunos comportamientos limitantes a tener en cuenta incluyen:

**Calidad del Servicio:** Los modelos Phi se entrenan principalmente con texto en inglés. Los idiomas que no sean inglés experimentarán un rendimiento inferior. Las variedades del inglés con menor representación en los datos de entrenamiento podrían tener un rendimiento peor que el inglés americano estándar.

**Representación de Daños y Perpetuación de Estereotipos:** Estos modelos pueden sobre o sub-representar grupos de personas, borrar la representación de algunos grupos o reforzar estereotipos despectivos o negativos. A pesar de la seguridad posterior al entrenamiento, estas limitaciones pueden estar presentes debido a distintos niveles de representación de los diferentes grupos o la prevalencia de ejemplos de estereotipos negativos en los datos de entrenamiento que reflejan patrones del mundo real y sesgos sociales.

**Contenido Inapropiado u Ofensivo:** Estos modelos pueden producir otros tipos de contenido inapropiado u ofensivo, lo que podría hacer que su despliegue en contextos sensibles no sea adecuado sin mitigaciones adicionales específicas para el caso de uso.
Confiabilidad de la Información: Los modelos de lenguaje pueden generar contenido sin sentido o fabricar contenido que podría sonar razonable pero es inexacto o está desactualizado.

**Ámbito Limitado para Código:** La mayoría de los datos de entrenamiento de Phi-3 se basa en Python y usa paquetes comunes como "typing, math, random, collections, datetime, itertools". Si el modelo genera scripts en Python que utilizan otros paquetes o scripts en otros idiomas, recomendamos encarecidamente que los usuarios verifiquen manualmente todos los usos de API.

Los desarrolladores deben aplicar las mejores prácticas de IA responsable y son responsables de garantizar que un caso de uso específico cumpla con las leyes y regulaciones pertinentes (por ejemplo, privacidad, comercio, etc.). Las áreas importantes a considerar incluyen:

**Asignación:** Los modelos pueden no ser adecuados para escenarios que puedan tener un impacto consecuente en el estatus legal o la asignación de recursos u oportunidades de vida (por ejemplo: vivienda, empleo, crédito, etc.) sin evaluaciones adicionales y técnicas de eliminación de sesgos adicionales.

**Escenarios de Alto Riesgo:** Los desarrolladores deben evaluar la idoneidad de usar modelos en escenarios de alto riesgo donde salidas injustas, poco confiables u ofensivas podrían ser extremadamente costosas o causar daño. Esto incluye brindar asesoría en dominios sensibles o expertos donde la precisión y confiabilidad son críticas (por ejemplo: asesoría legal o de salud). Se deben implementar salvaguardas adicionales a nivel de la aplicación según el contexto del despliegue.

**Desinformación:** Los modelos pueden generar información inexacta. Los desarrolladores deben seguir las mejores prácticas de transparencia e informar a los usuarios finales que están interactuando con un sistema de IA. A nivel de aplicación, los desarrolladores pueden construir mecanismos de retroalimentación y pipelines para fundamentar las respuestas en información contextual específica del caso de uso, técnica conocida como Generación Aumentada por Recuperación (RAG).

**Generación de Contenido Perjudicial:** Los desarrolladores deben evaluar las salidas según su contexto y usar clasificadores de seguridad disponibles o soluciones personalizadas apropiadas para su caso de uso.

**Uso Indebido:** Otras formas de uso indebido como fraude, spam o producción de malware pueden ser posibles, y los desarrolladores deben asegurarse de que sus aplicaciones no violen leyes y regulaciones vigentes.

### Ajuste fino y seguridad de contenido de IA

Después de afinar un modelo, recomendamos encarecidamente aprovechar las medidas de [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) para monitorear el contenido generado por los modelos, identificar y bloquear riesgos potenciales, amenazas y problemas de calidad.

![Phi3AISafety](../../../../../translated_images/01.phi3aisafety.c0d7fc42f5a5c405.es.png)

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) soporta contenido tanto de texto como de imagen. Puede desplegarse en la nube, contenedores desconectados y dispositivos edge/incrustados.

## Resumen de Azure AI Content Safety

Azure AI Content Safety no es una solución única para todos los casos; puede personalizarse para alinearse con las políticas específicas de las empresas. Además, sus modelos multilingües le permiten entender múltiples idiomas simultáneamente.

![AIContentSafety](../../../../../translated_images/01.AIcontentsafety.a288819b8ce8da1a.es.png)

- **Azure AI Content Safety**
- **Microsoft Developer**
- **5 videos**

El servicio Azure AI Content Safety detecta contenido dañino generado por usuarios y por IA en aplicaciones y servicios. Incluye API de texto e imagen que permiten detectar material dañino o inapropiado.

[Lista de reproducción de AI Content Safety](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Aviso Legal**:  
Este documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por la precisión, tenga en cuenta que las traducciones automáticas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por un humano. No nos hacemos responsables de malentendidos o interpretaciones erróneas derivadas del uso de esta traducción.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->