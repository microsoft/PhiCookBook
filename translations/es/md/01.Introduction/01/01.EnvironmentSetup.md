<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-03-27T05:39:01+00:00",
  "source_file": "md\\01.Introduction\\01\\01.EnvironmentSetup.md",
  "language_code": "es"
}
-->
# Comienza con Phi-3 de manera local

Esta guía te ayudará a configurar tu entorno local para ejecutar el modelo Phi-3 utilizando Ollama. Puedes ejecutar el modelo de varias maneras, incluyendo GitHub Codespaces, contenedores de desarrollo en VS Code, o tu entorno local.

## Configuración del entorno

### GitHub Codespaces

Puedes ejecutar esta plantilla de forma virtual utilizando GitHub Codespaces. El botón abrirá una instancia de VS Code basada en la web en tu navegador:

1. Abre la plantilla (esto puede tardar varios minutos):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Abre una ventana de terminal.

### Contenedores de desarrollo en VS Code

⚠️ Esta opción solo funcionará si tu Docker Desktop tiene al menos 16 GB de RAM asignados. Si tienes menos de 16 GB de RAM, puedes intentar la opción de [GitHub Codespaces](../../../../../md/01.Introduction/01) o [configurarlo de manera local](../../../../../md/01.Introduction/01).

Otra opción relacionada son los contenedores de desarrollo en VS Code, que abrirán el proyecto en tu VS Code local utilizando la [extensión Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Inicia Docker Desktop (instálalo si no lo tienes ya instalado).
2. Abre el proyecto:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. En la ventana de VS Code que se abre, una vez que los archivos del proyecto aparezcan (esto puede tardar varios minutos), abre una ventana de terminal.
4. Continúa con los [pasos de despliegue](../../../../../md/01.Introduction/01).

### Entorno local

1. Asegúrate de que las siguientes herramientas estén instaladas:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Prueba del modelo

1. Pídele a Ollama que descargue y ejecute el modelo phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Esto tomará unos minutos mientras se descarga el modelo.

2. Una vez que veas "success" en el resultado, puedes enviar un mensaje a ese modelo desde el prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Después de varios segundos, deberías ver una respuesta generada por el modelo.

4. Para aprender sobre las diferentes técnicas utilizadas con modelos de lenguaje, abre el cuaderno de Python [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) y ejecuta cada celda. Si utilizaste un modelo diferente a 'phi3:mini', cambia el `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` al principio del archivo según sea necesario, y también puedes modificar el mensaje del sistema o agregar ejemplos few-shot si lo deseas.

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisión, tenga en cuenta que las traducciones automáticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones erróneas que puedan surgir del uso de esta traducción.