<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:40:05+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "es"
}
-->
# Tecnologías clave mencionadas incluyen

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - una API de bajo nivel para aprendizaje automático acelerado por hardware, construida sobre DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - una plataforma de computación paralela y modelo de interfaz de programación de aplicaciones (API) desarrollado por Nvidia, que permite el procesamiento de propósito general en unidades de procesamiento gráfico (GPUs).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - un formato abierto diseñado para representar modelos de aprendizaje automático que proporciona interoperabilidad entre diferentes frameworks de ML.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - un formato usado para representar y actualizar modelos de aprendizaje automático, especialmente útil para modelos de lenguaje más pequeños que pueden ejecutarse eficazmente en CPUs con cuantización de 4-8 bits.

## DirectML

DirectML es una API de bajo nivel que permite el aprendizaje automático acelerado por hardware. Está construida sobre DirectX 12 para aprovechar la aceleración por GPU y es independiente del fabricante, lo que significa que no requiere cambios en el código para funcionar con diferentes proveedores de GPU. Se utiliza principalmente para cargas de trabajo de entrenamiento e inferencia de modelos en GPUs.

En cuanto al soporte de hardware, DirectML está diseñado para funcionar con una amplia gama de GPUs, incluyendo GPUs integradas y discretas de AMD, GPUs integradas de Intel y GPUs discretas de NVIDIA. Forma parte de la Plataforma de IA de Windows y es compatible con Windows 10 y 11, permitiendo el entrenamiento e inferencia de modelos en cualquier dispositivo Windows.

Ha habido actualizaciones y oportunidades relacionadas con DirectML, como el soporte para hasta 150 operadores ONNX y su uso tanto por el runtime de ONNX como por WinML. Está respaldado por los principales proveedores de hardware integrados (IHVs), cada uno implementando varios metacomandos.

## CUDA

CUDA, que significa Compute Unified Device Architecture, es una plataforma de computación paralela y modelo de interfaz de programación de aplicaciones (API) creado por Nvidia. Permite a los desarrolladores de software usar una unidad de procesamiento gráfico (GPU) compatible con CUDA para procesamiento de propósito general, un enfoque conocido como GPGPU (Computación de propósito general en GPUs). CUDA es un habilitador clave de la aceleración por GPU de Nvidia y se usa ampliamente en diversos campos, incluyendo aprendizaje automático, computación científica y procesamiento de video.

El soporte de hardware para CUDA es específico para las GPUs de Nvidia, ya que es una tecnología propietaria desarrollada por Nvidia. Cada arquitectura soporta versiones específicas del toolkit CUDA, que proporciona las bibliotecas y herramientas necesarias para que los desarrolladores construyan y ejecuten aplicaciones CUDA.

## ONNX

ONNX (Open Neural Network Exchange) es un formato abierto diseñado para representar modelos de aprendizaje automático. Proporciona una definición de un modelo de grafo computacional extensible, así como definiciones de operadores incorporados y tipos de datos estándar. ONNX permite a los desarrolladores mover modelos entre diferentes frameworks de ML, facilitando la interoperabilidad y haciendo más sencillo crear y desplegar aplicaciones de IA.

Phi3 mini puede ejecutarse con ONNX Runtime en CPU y GPU en diferentes dispositivos, incluyendo plataformas servidor, escritorios Windows, Linux y Mac, y CPUs móviles.  
Las configuraciones optimizadas que hemos añadido son:

- Modelos ONNX para int4 DML: Cuantizados a int4 mediante AWQ  
- Modelo ONNX para fp16 CUDA  
- Modelo ONNX para int4 CUDA: Cuantizado a int4 mediante RTN  
- Modelo ONNX para int4 CPU y móvil: Cuantizado a int4 mediante RTN  

## Llama.cpp

Llama.cpp es una biblioteca de software de código abierto escrita en C++. Realiza inferencia en varios modelos de lenguaje grande (LLMs), incluyendo Llama. Desarrollada junto con la biblioteca ggml (una biblioteca tensorial de propósito general), llama.cpp busca ofrecer inferencia más rápida y menor uso de memoria en comparación con la implementación original en Python. Soporta optimización de hardware, cuantización y ofrece una API sencilla y ejemplos. Si te interesa la inferencia eficiente de LLMs, llama.cpp vale la pena explorarlo, ya que Phi3 puede ejecutar Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) es un formato usado para representar y actualizar modelos de aprendizaje automático. Es especialmente útil para modelos de lenguaje más pequeños (SLMs) que pueden ejecutarse eficazmente en CPUs con cuantización de 4-8 bits. GGUF es beneficioso para prototipado rápido y para ejecutar modelos en dispositivos edge o en trabajos por lotes como pipelines de CI/CD.

**Aviso legal**:  
Este documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por la precisión, tenga en cuenta que las traducciones automáticas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para información crítica, se recomienda la traducción profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones erróneas derivadas del uso de esta traducción.