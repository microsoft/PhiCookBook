<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-03-27T06:03:41+00:00",
  "source_file": "md\\01.Introduction\\01\\01.Understandingtech.md",
  "language_code": "es"
}
-->
# Tecnologías clave mencionadas incluyen

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - una API de bajo nivel para aprendizaje automático acelerado por hardware, construida sobre DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - una plataforma de computación paralela y modelo de interfaz de programación de aplicaciones (API) desarrollada por Nvidia, que permite el procesamiento de propósito general en unidades de procesamiento gráfico (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - un formato abierto diseñado para representar modelos de aprendizaje automático que proporciona interoperabilidad entre diferentes frameworks de ML.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - un formato utilizado para representar y actualizar modelos de aprendizaje automático, especialmente útil para modelos de lenguaje más pequeños que pueden ejecutarse eficazmente en CPUs con cuantización de 4-8 bits.

## DirectML

DirectML es una API de bajo nivel que permite el aprendizaje automático acelerado por hardware. Está construida sobre DirectX 12 para aprovechar la aceleración de GPU y es independiente del proveedor, lo que significa que no requiere cambios en el código para funcionar con diferentes fabricantes de GPU. Se utiliza principalmente para tareas de entrenamiento de modelos y inferencia en GPUs.

En cuanto al soporte de hardware, DirectML está diseñado para trabajar con una amplia gama de GPUs, incluidas GPUs integradas y discretas de AMD, GPUs integradas de Intel y GPUs discretas de NVIDIA. Es parte de la Plataforma de Inteligencia Artificial de Windows y es compatible con Windows 10 y 11, permitiendo el entrenamiento e inferencia de modelos en cualquier dispositivo con Windows.

Ha habido actualizaciones y oportunidades relacionadas con DirectML, como el soporte de hasta 150 operadores ONNX y su uso tanto en el runtime de ONNX como en WinML. Cuenta con el respaldo de los principales Proveedores de Hardware Integrado (IHVs), cada uno implementando varios metacomandos.

## CUDA

CUDA, que significa Compute Unified Device Architecture, es una plataforma de computación paralela y modelo de interfaz de programación de aplicaciones (API) creada por Nvidia. Permite a los desarrolladores de software utilizar una unidad de procesamiento gráfico (GPU) habilitada para CUDA para el procesamiento de propósito general, un enfoque denominado GPGPU (Computación de Propósito General en Unidades de Procesamiento Gráfico). CUDA es un habilitador clave de la aceleración de GPU de Nvidia y se utiliza ampliamente en diversos campos, incluyendo aprendizaje automático, computación científica y procesamiento de video.

El soporte de hardware para CUDA es específico de las GPUs de Nvidia, ya que es una tecnología propietaria desarrollada por Nvidia. Cada arquitectura soporta versiones específicas del toolkit CUDA, que proporciona las bibliotecas y herramientas necesarias para que los desarrolladores construyan y ejecuten aplicaciones CUDA.

## ONNX

ONNX (Open Neural Network Exchange) es un formato abierto diseñado para representar modelos de aprendizaje automático. Proporciona una definición de un modelo de gráfico de computación extensible, así como definiciones de operadores integrados y tipos de datos estándar. ONNX permite a los desarrolladores mover modelos entre diferentes frameworks de ML, lo que facilita la interoperabilidad y simplifica la creación y despliegue de aplicaciones de inteligencia artificial.

Phi3 mini puede ejecutarse con ONNX Runtime en CPU y GPU a través de dispositivos, incluyendo plataformas de servidores, escritorios con Windows, Linux y Mac, y CPUs móviles. Las configuraciones optimizadas que hemos agregado son:

- Modelos ONNX para int4 DML: Cuantizados a int4 a través de AWQ
- Modelo ONNX para fp16 CUDA
- Modelo ONNX para int4 CUDA: Cuantizado a int4 a través de RTN
- Modelo ONNX para int4 CPU y móvil: Cuantizado a int4 a través de RTN

## Llama.cpp

Llama.cpp es una biblioteca de software de código abierto escrita en C++. Realiza inferencias en varios Modelos de Lenguaje Grande (LLMs), incluyendo Llama. Desarrollada junto con la biblioteca ggml (una biblioteca de tensores de propósito general), llama.cpp tiene como objetivo proporcionar inferencias más rápidas y un menor uso de memoria en comparación con la implementación original en Python. Soporta optimización de hardware, cuantización y ofrece una API simple y ejemplos. Si estás interesado en una inferencia eficiente de LLM, llama.cpp es una opción que vale la pena explorar, ya que Phi3 puede ejecutar Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) es un formato utilizado para representar y actualizar modelos de aprendizaje automático. Es particularmente útil para modelos de lenguaje más pequeños (SLMs) que pueden ejecutarse eficazmente en CPUs con cuantización de 4-8 bits. GGUF es beneficioso para la creación rápida de prototipos y la ejecución de modelos en dispositivos de borde o en trabajos por lotes como pipelines de CI/CD.

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducción por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por garantizar la precisión, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones erróneas que surjan del uso de esta traducción.