<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "624fe133fba62773979d45f54519f7bb",
  "translation_date": "2025-07-16T18:49:41+00:00",
  "source_file": "md/01.Introduction/02/01.HF.md",
  "language_code": "es"
}
-->
# **Uso de Phi Family en Hugging Face**


[Hugging Face](https://huggingface.co/) es una comunidad de IA muy popular con abundantes datos y recursos de modelos de código abierto. Diferentes fabricantes lanzan modelos LLM y SLM de código abierto a través de Hugging Face, como Microsoft, Meta, Mistral, Apple, Google, etc.

La familia Microsoft Phi ya está disponible en Hugging Face. Los desarrolladores pueden descargar el modelo correspondiente de Phi Family según los escenarios y negocios. Además de desplegar modelos Phi Pytorch en Hugging Face, también hemos lanzado modelos cuantizados, usando formatos GGUF y ONNX para ofrecer opciones a los usuarios finales.


## **Descargar modelos en Hugging Face**

Puedes descargar el modelo de la familia Phi con este enlace

[Modelos de Microsoft en Hugging Face](https://huggingface.co/microsoft)

-  **Phi-1 / 1.5** https://huggingface.co/collections/microsoft/phi-1-6626e29134744e94e222d572

-  **Phi-3 / 3.5** https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3

-  **Phi-4** https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4

- **Phi-4-reasoning** https://huggingface.co/microsoft/Phi-4-reasoning

- **Phi-4-reasoning Plus** https://huggingface.co/microsoft/Phi-4-reasoning-plus 

- **Phi-4-mini-reasoning** https://huggingface.co/microsoft/Phi-4-mini-reasoning

Puedes descargar el modelo de diferentes maneras, como instalando el ***Hugging Face CLI SDK*** o usando ***git clone***.

### **Usar Hugging Face CLI para descargar el modelo Phi Family**

- Instalar Hugging Face CLI

```bash

pip install -U "huggingface_hub[cli]"

```

- Usar huggingface-cli para iniciar sesión

Inicia sesión en Hugging Face con el [User Access Token](https://huggingface.co/docs/hub/security-tokens) desde tu [página de Configuración](https://huggingface.co/settings/tokens)


```bash

huggingface-cli login --token $HF_TOKEN --add-to-git-credential

```

- Descargar


Puedes descargar el modelo y guardarlo en caché

```bash

huggingface-cli download microsoft/phi-4

```

Puedes establecer la ubicación en una ruta personalizada


```bash

huggingface-cli download microsoft/phi-4 --local-dir $YOUR_PATH

```


### **Usar git clone para descargar el modelo Phi Family**

También puedes usar ***git clone*** para descargar el modelo

```bash

git lfs install

git clone https://huggingface.co/microsoft/phi-4

```

## **Ejemplos - Inferencia con Microsoft Phi-4**

- **Instalando la librería transformers**

```bash

pip install transformers -U

```

- **Ejecutando este código en VSCode**

```python

import transformers

pipeline = transformers.pipeline(
    "text-generation",
    model="microsoft/phi-4",
    model_kwargs={"torch_dtype": "auto"},
    device_map="auto",
)

messages = [
    {"role": "user", "content": "I have $20,000 in my savings account, where I receive a 4% profit per year and payments twice a year. Can you please tell me how long it will take for me to become a millionaire? Also, can you please explain the math step by step as if you were explaining it to an uneducated person?"},
]

outputs = pipeline(messages, max_new_tokens=2048)
print(outputs[0]["generated_text"][-1])

```

**Aviso legal**:  
Este documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por la precisión, tenga en cuenta que las traducciones automáticas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones erróneas derivadas del uso de esta traducción.