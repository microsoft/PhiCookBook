<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8cdc17ce0f10535da30b53d23fe1a795",
  "translation_date": "2025-07-16T18:22:50+00:00",
  "source_file": "md/01.Introduction/01/01.Hardwaresupport.md",
  "language_code": "de"
}
-->
# Phi Hardware-Unterstützung

Microsoft Phi wurde für ONNX Runtime optimiert und unterstützt Windows DirectML. Es funktioniert gut auf verschiedenen Hardwaretypen, einschließlich GPUs, CPUs und sogar mobilen Geräten.

## Geräte-Hardware  
Konkret umfasst die unterstützte Hardware:

- GPU SKU: RTX 4090 (DirectML)  
- GPU SKU: 1 A100 80GB (CUDA)  
- CPU SKU: Standard F64s v2 (64 vCPUs, 128 GiB Arbeitsspeicher)  

## Mobile SKU

- Android – Samsung Galaxy S21  
- Apple iPhone 14 oder höher mit A16/A17 Prozessor  

## Phi Hardware-Spezifikation

- Mindestkonfiguration erforderlich.  
- Windows: DirectX 12-fähige GPU und mindestens 4 GB Gesamtspeicher  

CUDA: NVIDIA GPU mit Compute Capability >= 7.02  

![HardwareSupport](../../../../../translated_images/01.phihardware.5d51b2377cba18afc6949074542f290c56bb278dac3f4f86302aca6d80fffeb9.de.png)

## Ausführen von onnxruntime auf mehreren GPUs

Derzeit sind verfügbare Phi ONNX-Modelle nur für 1 GPU ausgelegt. Es ist möglich, Multi-GPU für Phi-Modelle zu unterstützen, aber ORT mit 2 GPUs garantiert nicht, dass es eine höhere Durchsatzrate als 2 Instanzen von ORT liefert. Für die neuesten Informationen siehe bitte [ONNX Runtime](https://onnxruntime.ai/).

Auf der [Build 2024 hat das GenAI ONNX Team](https://youtu.be/WLW4SE8M9i8?si=EtG04UwDvcjunyfC) angekündigt, dass sie Multi-Instance statt Multi-GPU für Phi-Modelle aktiviert haben.

Derzeit ermöglicht dies, eine onnxruntime- oder onnxruntime-genai-Instanz mit der Umgebungsvariable CUDA_VISIBLE_DEVICES wie folgt auszuführen.

```Python
CUDA_VISIBLE_DEVICES=0 python infer.py
CUDA_VISIBLE_DEVICES=1 python infer.py
```

Fühlen Sie sich frei, Phi weiter in [Azure AI Foundry](https://ai.azure.com) zu erkunden.

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner Ursprungssprache ist als maßgebliche Quelle zu betrachten. Für wichtige Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die aus der Nutzung dieser Übersetzung entstehen.