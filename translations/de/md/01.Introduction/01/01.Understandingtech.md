<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-03-27T06:04:49+00:00",
  "source_file": "md\\01.Introduction\\01\\01.Understandingtech.md",
  "language_code": "de"
}
-->
# Wichtige Technologien umfassen

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) – eine Low-Level-API für hardwarebeschleunigtes maschinelles Lernen, die auf DirectX 12 aufbaut.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) – eine Plattform für paralleles Rechnen und ein API-Modell, das von Nvidia entwickelt wurde und allgemeine Rechenprozesse auf Grafikkarten (GPUs) ermöglicht.
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) – ein offenes Format, das entwickelt wurde, um maschinelle Lernmodelle darzustellen und Interoperabilität zwischen verschiedenen ML-Frameworks zu gewährleisten.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) – ein Format zur Darstellung und Aktualisierung von maschinellen Lernmodellen, besonders nützlich für kleinere Sprachmodelle, die effektiv auf CPUs mit 4- bis 8-Bit-Quantisierung laufen können.

## DirectML

DirectML ist eine Low-Level-API, die hardwarebeschleunigtes maschinelles Lernen ermöglicht. Sie basiert auf DirectX 12, um GPU-Beschleunigung zu nutzen, und ist herstellerunabhängig, was bedeutet, dass keine Codeänderungen erforderlich sind, um mit GPUs verschiedener Anbieter zu arbeiten. DirectML wird hauptsächlich für Modelltraining und Inferenzaufgaben auf GPUs verwendet.

Hinsichtlich der Hardwareunterstützung wurde DirectML so konzipiert, dass es mit einer breiten Palette von GPUs funktioniert, einschließlich AMD integrierter und diskreter GPUs, Intel integrierter GPUs und NVIDIA diskreter GPUs. Es ist Teil der Windows AI-Plattform und wird unter Windows 10 und 11 unterstützt, sodass Modelltraining und Inferenz auf jedem Windows-Gerät möglich sind.

Es gab Aktualisierungen und Möglichkeiten im Zusammenhang mit DirectML, wie die Unterstützung von bis zu 150 ONNX-Operatoren und die Nutzung durch ONNX Runtime und WinML. Es wird von großen Hardwareanbietern (IHVs) unterstützt, die jeweils verschiedene Metakommandos implementieren.

## CUDA

CUDA, was für Compute Unified Device Architecture steht, ist eine Plattform für paralleles Rechnen und ein API-Modell, das von Nvidia entwickelt wurde. Es ermöglicht Softwareentwicklern, eine CUDA-fähige GPU für allgemeine Rechenprozesse zu nutzen – ein Ansatz, der als GPGPU (General-Purpose Computing on Graphics Processing Units) bezeichnet wird. CUDA ist ein zentraler Bestandteil der GPU-Beschleunigung von Nvidia und wird in verschiedenen Bereichen eingesetzt, darunter maschinelles Lernen, wissenschaftliches Rechnen und Videobearbeitung.

Die Hardwareunterstützung für CUDA ist spezifisch für die GPUs von Nvidia, da es sich um eine proprietäre Technologie handelt, die von Nvidia entwickelt wurde. Jede Architektur unterstützt spezifische Versionen des CUDA-Toolkits, das die notwendigen Bibliotheken und Werkzeuge für Entwickler bereitstellt, um CUDA-Anwendungen zu erstellen und auszuführen.

## ONNX

ONNX (Open Neural Network Exchange) ist ein offenes Format, das entwickelt wurde, um maschinelle Lernmodelle darzustellen. Es bietet eine Definition eines erweiterbaren Berechnungsgraphenmodells sowie Definitionen von eingebauten Operatoren und Standarddatentypen. ONNX ermöglicht es Entwicklern, Modelle zwischen verschiedenen ML-Frameworks zu übertragen, was die Interoperabilität fördert und die Erstellung und Bereitstellung von KI-Anwendungen erleichtert.

Phi3 Mini kann mit ONNX Runtime auf CPU und GPU über verschiedene Geräte hinweg laufen, einschließlich Serverplattformen, Windows-, Linux- und Mac-Desktops sowie mobilen CPUs. Die von uns optimierten Konfigurationen umfassen:

- ONNX-Modelle für int4 DML: Quantisiert auf int4 mittels AWQ
- ONNX-Modell für fp16 CUDA
- ONNX-Modell für int4 CUDA: Quantisiert auf int4 mittels RTN
- ONNX-Modell für int4 CPU und Mobilgeräte: Quantisiert auf int4 mittels RTN

## Llama.cpp

Llama.cpp ist eine Open-Source-Softwarebibliothek, die in C++ geschrieben wurde. Sie führt Inferenz auf verschiedenen großen Sprachmodellen (LLMs), einschließlich Llama, durch. Entwickelt zusammen mit der ggml-Bibliothek (einer allgemeinen Tensorbibliothek), zielt llama.cpp darauf ab, schnellere Inferenz und einen geringeren Speicherverbrauch im Vergleich zur ursprünglichen Python-Implementierung zu bieten. Es unterstützt Hardwareoptimierung, Quantisierung und bietet eine einfache API sowie Beispiele. Wenn Sie an effizienter LLM-Inferenz interessiert sind, lohnt sich ein Blick auf llama.cpp, da Phi3 Llama.cpp ausführen kann.

## GGUF

GGUF (Generic Graph Update Format) ist ein Format zur Darstellung und Aktualisierung von maschinellen Lernmodellen. Es ist besonders nützlich für kleinere Sprachmodelle (SLMs), die effektiv auf CPUs mit 4- bis 8-Bit-Quantisierung laufen können. GGUF ist vorteilhaft für schnelles Prototyping und das Ausführen von Modellen auf Edge-Geräten oder in Batch-Jobs wie CI/CD-Pipelines.

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die durch die Nutzung dieser Übersetzung entstehen.