<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-07T10:52:32+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "de"
}
-->
# Wichtige erwähnte Technologien umfassen

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) – eine Low-Level-API für hardwarebeschleunigtes maschinelles Lernen, die auf DirectX 12 aufbaut.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) – eine parallele Rechenplattform und API-Modell von Nvidia, das die allgemeine Verarbeitung auf Grafikprozessoren (GPUs) ermöglicht.  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) – ein offenes Format zur Darstellung von Modellen des maschinellen Lernens, das Interoperabilität zwischen verschiedenen ML-Frameworks bietet.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) – ein Format zur Darstellung und Aktualisierung von ML-Modellen, besonders nützlich für kleinere Sprachmodelle, die effektiv mit 4-8bit Quantisierung auf CPUs laufen können.

## DirectML

DirectML ist eine Low-Level-API, die hardwarebeschleunigtes maschinelles Lernen ermöglicht. Sie baut auf DirectX 12 auf, um GPU-Beschleunigung zu nutzen, und ist herstellerunabhängig, was bedeutet, dass keine Codeänderungen nötig sind, um auf verschiedenen GPU-Herstellern zu funktionieren. Hauptsächlich wird sie für das Training und die Inferenz von Modellen auf GPUs verwendet.

Was die Hardwareunterstützung angeht, ist DirectML darauf ausgelegt, mit einer Vielzahl von GPUs zu arbeiten, darunter AMD integrierte und dedizierte GPUs, Intel integrierte GPUs und NVIDIA dedizierte GPUs. Es ist Teil der Windows AI-Plattform und wird unter Windows 10 & 11 unterstützt, wodurch Training und Inferenz auf jedem Windows-Gerät möglich sind.

Es gab Updates und Erweiterungen für DirectML, wie die Unterstützung von bis zu 150 ONNX-Operatoren, und es wird sowohl vom ONNX Runtime als auch von WinML genutzt. Die Technologie wird von großen Integrated Hardware Vendors (IHVs) unterstützt, die verschiedene Metabefehle implementieren.

## CUDA

CUDA, das für Compute Unified Device Architecture steht, ist eine parallele Rechenplattform und ein API-Modell von Nvidia. Es ermöglicht Softwareentwicklern, eine CUDA-fähige GPU für allgemeine Berechnungen zu nutzen – ein Ansatz, der als GPGPU (General-Purpose Computing on Graphics Processing Units) bezeichnet wird. CUDA ist ein zentraler Faktor für die GPU-Beschleunigung von Nvidia und wird in vielen Bereichen eingesetzt, darunter maschinelles Lernen, wissenschaftliches Rechnen und Videobearbeitung.

Die Hardwareunterstützung für CUDA ist auf Nvidia-GPUs beschränkt, da es sich um eine proprietäre Technologie von Nvidia handelt. Jede Architektur unterstützt bestimmte Versionen des CUDA-Toolkits, das die notwendigen Bibliotheken und Werkzeuge für Entwickler bereitstellt, um CUDA-Anwendungen zu erstellen und auszuführen.

## ONNX

ONNX (Open Neural Network Exchange) ist ein offenes Format zur Darstellung von Modellen des maschinellen Lernens. Es definiert ein erweiterbares Rechenmodell in Form eines Graphen sowie eingebaute Operatoren und Standard-Datentypen. ONNX ermöglicht es Entwicklern, Modelle zwischen verschiedenen ML-Frameworks zu übertragen, was Interoperabilität schafft und die Erstellung sowie den Einsatz von KI-Anwendungen erleichtert.

Phi3 mini kann ONNX Runtime auf CPU und GPU über verschiedene Geräte hinweg ausführen, einschließlich Serverplattformen, Windows-, Linux- und Mac-Desktops sowie mobilen CPUs.  
Die von uns hinzugefügten optimierten Konfigurationen sind:

- ONNX-Modelle für int4 DML: Quantisierung auf int4 mittels AWQ  
- ONNX-Modell für fp16 CUDA  
- ONNX-Modell für int4 CUDA: Quantisierung auf int4 mittels RTN  
- ONNX-Modell für int4 CPU und Mobile: Quantisierung auf int4 mittels RTN

## Llama.cpp

Llama.cpp ist eine Open-Source-Softwarebibliothek, geschrieben in C++. Sie führt Inferenz auf verschiedenen Large Language Models (LLMs) durch, darunter Llama. Entwickelt zusammen mit der ggml-Bibliothek (einer universellen Tensorbibliothek), zielt llama.cpp darauf ab, schnellere Inferenz und geringeren Speicherverbrauch im Vergleich zur ursprünglichen Python-Implementierung zu bieten. Es unterstützt Hardwareoptimierung, Quantisierung und bietet eine einfache API sowie Beispiele. Wenn du an effizienter LLM-Inferenz interessiert bist, ist llama.cpp einen Blick wert, da Phi3 Llama.cpp ausführen kann.

## GGUF

GGUF (Generic Graph Update Format) ist ein Format zur Darstellung und Aktualisierung von ML-Modellen. Es ist besonders nützlich für kleinere Sprachmodelle (SLMs), die mit 4-8bit Quantisierung effektiv auf CPUs laufen können. GGUF eignet sich gut für schnelles Prototyping und den Einsatz von Modellen auf Edge-Geräten oder in Batch-Jobs wie CI/CD-Pipelines.

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner Ursprungssprache ist als maßgebliche Quelle zu betrachten. Für wichtige Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die durch die Nutzung dieser Übersetzung entstehen.