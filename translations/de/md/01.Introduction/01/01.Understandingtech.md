<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:40:15+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "de"
}
-->
# Wichtige erwähnte Technologien sind

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) – eine Low-Level-API für hardwarebeschleunigtes maschinelles Lernen, die auf DirectX 12 aufbaut.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) – eine parallele Rechenplattform und API-Modell von Nvidia, das allgemeine Berechnungen auf Grafikprozessoren (GPUs) ermöglicht.  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) – ein offenes Format zur Darstellung von Machine-Learning-Modellen, das Interoperabilität zwischen verschiedenen ML-Frameworks bietet.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) – ein Format zur Darstellung und Aktualisierung von Machine-Learning-Modellen, besonders nützlich für kleinere Sprachmodelle, die mit 4-8bit Quantisierung effektiv auf CPUs laufen können.

## DirectML

DirectML ist eine Low-Level-API, die hardwarebeschleunigtes maschinelles Lernen ermöglicht. Sie basiert auf DirectX 12, um GPU-Beschleunigung zu nutzen, und ist herstellerunabhängig, das heißt, sie erfordert keine Codeänderungen, um auf verschiedenen GPU-Herstellern zu funktionieren. Hauptsächlich wird sie für Modelltraining und Inferenz-Workloads auf GPUs verwendet.

Was die Hardwareunterstützung betrifft, so ist DirectML darauf ausgelegt, mit einer breiten Palette von GPUs zu arbeiten, darunter AMD integrierte und dedizierte GPUs, Intel integrierte GPUs sowie NVIDIA dedizierte GPUs. Es ist Teil der Windows AI Platform und wird auf Windows 10 & 11 unterstützt, was Modelltraining und Inferenz auf jedem Windows-Gerät ermöglicht.

Es gab Updates und neue Möglichkeiten im Zusammenhang mit DirectML, wie die Unterstützung von bis zu 150 ONNX-Operatoren und die Nutzung sowohl durch die ONNX Runtime als auch WinML. Es wird von großen Integrated Hardware Vendors (IHVs) unterstützt, die verschiedene Metabefehle implementieren.

## CUDA

CUDA, was für Compute Unified Device Architecture steht, ist eine parallele Rechenplattform und API-Modell, das von Nvidia entwickelt wurde. Es ermöglicht Softwareentwicklern, eine CUDA-fähige GPU für allgemeine Berechnungen zu nutzen – ein Ansatz, der als GPGPU (General-Purpose computing on Graphics Processing Units) bezeichnet wird. CUDA ist ein zentraler Baustein für Nvidias GPU-Beschleunigung und wird in vielen Bereichen eingesetzt, darunter maschinelles Lernen, wissenschaftliches Rechnen und Videobearbeitung.

Die Hardwareunterstützung für CUDA ist auf Nvidias GPUs beschränkt, da es sich um eine proprietäre Technologie von Nvidia handelt. Jede Architektur unterstützt bestimmte Versionen des CUDA-Toolkits, das die notwendigen Bibliotheken und Werkzeuge bereitstellt, damit Entwickler CUDA-Anwendungen erstellen und ausführen können.

## ONNX

ONNX (Open Neural Network Exchange) ist ein offenes Format zur Darstellung von Machine-Learning-Modellen. Es definiert ein erweiterbares Rechengraphmodell sowie eingebaute Operatoren und Standard-Datentypen. ONNX ermöglicht es Entwicklern, Modelle zwischen verschiedenen ML-Frameworks zu übertragen, was Interoperabilität schafft und die Erstellung sowie den Einsatz von KI-Anwendungen erleichtert.

Phi3 mini kann mit ONNX Runtime auf CPU und GPU über verschiedene Geräte hinweg laufen, einschließlich Serverplattformen, Windows-, Linux- und Mac-Desktops sowie mobilen CPUs.  
Die von uns hinzugefügten optimierten Konfigurationen sind:

- ONNX-Modelle für int4 DML: Quantisiert auf int4 via AWQ  
- ONNX-Modell für fp16 CUDA  
- ONNX-Modell für int4 CUDA: Quantisiert auf int4 via RTN  
- ONNX-Modell für int4 CPU und Mobile: Quantisiert auf int4 via RTN

## Llama.cpp

Llama.cpp ist eine Open-Source-Softwarebibliothek, geschrieben in C++. Sie führt Inferenz auf verschiedenen Large Language Models (LLMs) durch, darunter Llama. Entwickelt zusammen mit der ggml-Bibliothek (einer universellen Tensor-Bibliothek), zielt llama.cpp darauf ab, schnellere Inferenz und geringeren Speicherverbrauch im Vergleich zur ursprünglichen Python-Implementierung zu bieten. Es unterstützt Hardware-Optimierung, Quantisierung und bietet eine einfache API sowie Beispiele. Wenn du an effizienter LLM-Inferenz interessiert bist, lohnt sich ein Blick auf llama.cpp, da Phi3 Llama.cpp ausführen kann.

## GGUF

GGUF (Generic Graph Update Format) ist ein Format zur Darstellung und Aktualisierung von Machine-Learning-Modellen. Es ist besonders nützlich für kleinere Sprachmodelle (SLMs), die mit 4-8bit Quantisierung effektiv auf CPUs laufen können. GGUF eignet sich gut für schnelles Prototyping und den Einsatz von Modellen auf Edge-Geräten oder in Batch-Jobs wie CI/CD-Pipelines.

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner Ursprungssprache ist als maßgebliche Quelle zu betrachten. Für wichtige Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die aus der Nutzung dieser Übersetzung entstehen.