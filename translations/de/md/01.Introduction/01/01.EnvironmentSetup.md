<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-03-27T05:40:02+00:00",
  "source_file": "md\\01.Introduction\\01\\01.EnvironmentSetup.md",
  "language_code": "de"
}
-->
# Erste Schritte mit Phi-3 lokal

Diese Anleitung hilft dir, deine lokale Umgebung einzurichten, um das Phi-3-Modell mit Ollama auszuführen. Du kannst das Modell auf verschiedene Arten verwenden, einschließlich GitHub Codespaces, VS Code Dev Containers oder deiner lokalen Umgebung.

## Einrichtung der Umgebung

### GitHub Codespaces

Du kannst diese Vorlage virtuell mit GitHub Codespaces ausführen. Der Button öffnet eine webbasierte VS Code-Instanz in deinem Browser:

1. Öffne die Vorlage (dies kann einige Minuten dauern):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Öffne ein Terminalfenster.

### VS Code Dev Containers

⚠️ Diese Option funktioniert nur, wenn deinem Docker Desktop mindestens 16 GB RAM zugewiesen sind. Wenn du weniger als 16 GB RAM hast, kannst du die [GitHub Codespaces-Option](../../../../../md/01.Introduction/01) ausprobieren oder [die lokale Einrichtung](../../../../../md/01.Introduction/01) verwenden.

Eine verwandte Option sind VS Code Dev Containers, die das Projekt in deinem lokalen VS Code mit der [Dev Containers-Erweiterung](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) öffnen:

1. Starte Docker Desktop (installiere es, falls noch nicht vorhanden).
2. Öffne das Projekt:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Im geöffneten VS Code-Fenster, sobald die Projektdateien angezeigt werden (dies kann einige Minuten dauern), öffne ein Terminalfenster.
4. Fahre mit den [Bereitstellungsschritten](../../../../../md/01.Introduction/01) fort.

### Lokale Umgebung

1. Stelle sicher, dass die folgenden Tools installiert sind:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Teste das Modell

1. Fordere Ollama auf, das phi3:mini-Modell herunterzuladen und auszuführen:

    ```shell
    ollama run phi3:mini
    ```

    Das Herunterladen des Modells wird einige Minuten dauern.

2. Sobald du "success" in der Ausgabe siehst, kannst du eine Nachricht an das Modell senden.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Nach einigen Sekunden solltest du eine Antwort vom Modell erhalten.

4. Um mehr über verschiedene Techniken im Umgang mit Sprachmodellen zu erfahren, öffne das Python-Notebook [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) und führe jede Zelle aus. Falls du ein anderes Modell als 'phi3:mini' verwendet hast, ändere den `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` am Anfang der Datei entsprechend. Du kannst auch die Systemnachricht anpassen oder Beispiele für Few-Shot-Learning hinzufügen, falls gewünscht.

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.