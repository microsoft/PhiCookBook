<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-07-16T18:06:32+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "de"
}
-->
# Erste Schritte mit Phi-3 lokal

Diese Anleitung hilft dir dabei, deine lokale Umgebung einzurichten, um das Phi-3 Modell mit Ollama auszuführen. Du kannst das Modell auf verschiedene Arten betreiben, zum Beispiel mit GitHub Codespaces, VS Code Dev Containers oder direkt in deiner lokalen Umgebung.

## Umgebung einrichten

### GitHub Codespaces

Du kannst diese Vorlage virtuell mit GitHub Codespaces ausführen. Der Button öffnet eine webbasierte VS Code-Instanz in deinem Browser:

1. Öffne die Vorlage (das kann einige Minuten dauern):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Öffne ein Terminalfenster

### VS Code Dev Containers

⚠️ Diese Option funktioniert nur, wenn Docker Desktop mindestens 16 GB RAM zugewiesen ist. Wenn du weniger als 16 GB RAM hast, kannst du die [GitHub Codespaces Option](../../../../../md/01.Introduction/01) ausprobieren oder [die lokale Einrichtung](../../../../../md/01.Introduction/01) verwenden.

Eine verwandte Möglichkeit sind VS Code Dev Containers, die das Projekt in deinem lokalen VS Code mit der [Dev Containers Erweiterung](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) öffnen:

1. Starte Docker Desktop (installiere es, falls noch nicht geschehen)
2. Öffne das Projekt:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Im sich öffnenden VS Code Fenster, sobald die Projektdateien geladen sind (das kann einige Minuten dauern), öffne ein Terminalfenster.
4. Fahre mit den [Deployment-Schritten](../../../../../md/01.Introduction/01) fort

### Lokale Umgebung

1. Stelle sicher, dass folgende Tools installiert sind:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Modell testen

1. Bitte Ollama, das phi3:mini Modell herunterzuladen und auszuführen:

    ```shell
    ollama run phi3:mini
    ```

    Das Herunterladen des Modells dauert einige Minuten.

2. Sobald du „success“ in der Ausgabe siehst, kannst du dem Modell eine Nachricht über die Eingabeaufforderung senden.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Nach einigen Sekunden solltest du einen Antwort-Stream vom Modell erhalten.

4. Um verschiedene Techniken mit Sprachmodellen kennenzulernen, öffne das Python-Notebook [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) und führe jede Zelle aus. Wenn du ein anderes Modell als 'phi3:mini' verwendet hast, ändere den `MODEL_NAME` in der ersten Zelle.

5. Um ein Gespräch mit dem phi3:mini Modell aus Python zu führen, öffne die Python-Datei [chat.py](../../../../../code/01.Introduce/chat.py) und führe sie aus. Du kannst den `MODEL_NAME` oben in der Datei bei Bedarf ändern und auch die Systemnachricht anpassen oder Few-Shot-Beispiele hinzufügen.

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner Ursprungssprache ist als maßgebliche Quelle zu betrachten. Für wichtige Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die aus der Nutzung dieser Übersetzung entstehen.