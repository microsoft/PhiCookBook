<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-07T10:52:03+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "de"
}
-->
# Erste Schritte mit Phi-3 lokal

Diese Anleitung hilft dir, deine lokale Umgebung einzurichten, um das Phi-3-Modell mit Ollama auszuführen. Du kannst das Modell auf verschiedene Weise betreiben, unter anderem mit GitHub Codespaces, VS Code Dev Containers oder deiner lokalen Umgebung.

## Einrichtung der Umgebung

### GitHub Codespaces

Du kannst diese Vorlage virtuell mit GitHub Codespaces ausführen. Der Button öffnet eine webbasierte VS Code-Instanz in deinem Browser:

1. Öffne die Vorlage (das kann einige Minuten dauern):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Öffne ein Terminalfenster

### VS Code Dev Containers

⚠️ Diese Option funktioniert nur, wenn deinem Docker Desktop mindestens 16 GB RAM zugewiesen sind. Falls du weniger als 16 GB RAM hast, kannst du die [GitHub Codespaces Option](../../../../../md/01.Introduction/01) ausprobieren oder [die lokale Einrichtung](../../../../../md/01.Introduction/01) vornehmen.

Eine verwandte Möglichkeit sind VS Code Dev Containers, die das Projekt in deinem lokalen VS Code mit der [Dev Containers Erweiterung](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) öffnen:

1. Starte Docker Desktop (installiere es, falls noch nicht geschehen)
2. Öffne das Projekt:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Im sich öffnenden VS Code Fenster, sobald die Projektdateien geladen sind (das kann einige Minuten dauern), öffne ein Terminalfenster.
4. Fahre mit den [Deployment-Schritten](../../../../../md/01.Introduction/01) fort

### Lokale Umgebung

1. Stelle sicher, dass folgende Tools installiert sind:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Teste das Modell

1. Bitte Ollama, das phi3:mini Modell herunterzuladen und auszuführen:

    ```shell
    ollama run phi3:mini
    ```

    Das Herunterladen des Modells kann einige Minuten dauern.

2. Sobald du „success“ in der Ausgabe siehst, kannst du dem Modell eine Nachricht über die Eingabeaufforderung senden.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Nach einigen Sekunden solltest du einen Antwortstream vom Modell erhalten.

4. Um mehr über verschiedene Techniken im Umgang mit Sprachmodellen zu erfahren, öffne das Python-Notebook [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) und führe jede Zelle aus. Falls du ein anderes Modell als 'phi3:mini' verwendet hast, ändere den `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` ganz oben in der Datei entsprechend. Du kannst auch die Systemnachricht anpassen oder bei Bedarf Few-Shot-Beispiele hinzufügen.

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache gilt als maßgebliche Quelle. Für wichtige Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die aus der Verwendung dieser Übersetzung entstehen.