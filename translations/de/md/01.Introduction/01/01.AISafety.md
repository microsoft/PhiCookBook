<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "839ccc4b3886ef10cfd4e64977f5792d",
  "translation_date": "2026-01-05T08:21:55+00:00",
  "source_file": "md/01.Introduction/01/01.AISafety.md",
  "language_code": "de"
}
-->
# KI-Sicherheit für Phi-Modelle
Die Phi-Modellfamilie wurde gemäß dem [Microsoft Responsible AI Standard](https://www.microsoft.com/ai/principles-and-approach#responsible-ai-standard) entwickelt, einem unternehmensweiten Satz von Anforderungen, der auf den folgenden sechs Prinzipien basiert: Verantwortlichkeit, Transparenz, Fairness, Zuverlässigkeit und Sicherheit, Datenschutz und Sicherheit sowie Inklusivität, die die [Responsible AI-Prinzipien von Microsoft](https://www.microsoft.com/ai/responsible-ai) bilden.

Wie bei den vorherigen Phi-Modellen wurde ein vielschichtiges Sicherheitsbewertungs- und Sicherheitsnachtrainingsverfahren angewendet, wobei zusätzliche Maßnahmen zur Berücksichtigung der mehrsprachigen Fähigkeiten dieser Veröffentlichung getroffen wurden. Unser Ansatz für Sicherheitstraining und -bewertungen, einschließlich Tests in mehreren Sprachen und Risikokategorien, ist im [Phi Safety Post-Training Paper](https://arxiv.org/abs/2407.13833) beschrieben. Während die Phi-Modelle von diesem Ansatz profitieren, sollten Entwickler verantwortungsbewusste KI-Best-Practices anwenden, einschließlich der Abbildung, Messung und Minderung von Risiken, die mit ihrem spezifischen Anwendungsfall sowie kulturellem und sprachlichem Kontext verbunden sind.

## Best Practices

Wie andere Modelle können auch die Phi-Modelle sich potenziell unfair, unzuverlässig oder anstößig verhalten.

Einige der einschränkenden Verhaltensweisen von SLM und LLM, auf die Sie achten sollten, sind:

- **Qualität des Dienstes:** Die Phi-Modelle werden hauptsächlich mit englischem Text trainiert. Sprachen außer Englisch werden eine schlechtere Leistung aufweisen. Englischsprachige Varianten, die in den Trainingsdaten weniger vertreten sind, könnten eine schlechtere Leistung als Standardamerikanisches Englisch aufweisen.
- **Darstellung von Schäden & Fortführung von Stereotypen:** Diese Modelle können Gruppen von Menschen über- oder unterrepräsentieren, die Repräsentation einiger Gruppen auslöschen oder erniedrigende bzw. negative Stereotype verstärken. Trotz Sicherheitsnachtrainings können diese Einschränkungen weiterhin bestehen, da unterschiedliche Repräsentationsgrade verschiedener Gruppen oder die Häufigkeit von Beispielen negativer Stereotype in den Trainingsdaten, die reale Muster und gesellschaftliche Vorurteile widerspiegeln, vorliegen.
- **Unangemessene oder anstößige Inhalte:** Diese Modelle können andere Arten unangemessener oder anstößiger Inhalte erzeugen, was den Einsatz in sensiblen Kontexten ohne zusätzliche, auf den Anwendungsfall zugeschnittene Maßnahmen unangemessen machen kann.
- **Informationszuverlässigkeit:** Sprachmodelle können unsinnige Inhalte generieren oder Inhalte erfinden, die vernünftig klingen, aber ungenau oder veraltet sind.
- **Begrenzter Umfang für Code:** Der Großteil der Phi-3 Trainingsdaten basiert auf Python und verwendet gängige Pakete wie „typing, math, random, collections, datetime, itertools“. Wenn das Modell Python-Skripte generiert, die andere Pakete nutzen, oder Skripte in anderen Sprachen erstellt, empfehlen wir dringend, alle API-Verwendungen manuell zu überprüfen.

Entwickler sollten verantwortungsbewusste KI-Best-Practices anwenden und sind dafür verantwortlich sicherzustellen, dass ein spezifischer Anwendungsfall die geltenden Gesetze und Vorschriften (z. B. Datenschutz, Handel usw.) einhält.

## Überlegungen zur verantwortungsvollen KI

Wie andere Sprachmodelle können auch die Phi-Modelle sich potenziell unfair, unzuverlässig oder anstößig verhalten. Einige einschränkende Verhaltensweisen, die zu beachten sind, umfassen:

**Qualität des Dienstes:** Die Phi-Modelle werden hauptsächlich mit englischem Text trainiert. Sprachen außer Englisch werden eine schlechtere Leistung aufweisen. Englischsprachige Varianten, die in den Trainingsdaten weniger vertreten sind, könnten eine schlechtere Leistung als Standardamerikanisches Englisch zeigen.

**Darstellung von Schäden & Fortführung von Stereotypen:** Diese Modelle können Gruppen von Menschen über- oder unterrepräsentieren, die Repräsentation einiger Gruppen auslöschen oder erniedrigende bzw. negative Stereotype verstärken. Trotz Sicherheitsnachtrainings können diese Einschränkungen weiterhin bestehen, da unterschiedliche Repräsentationsgrade verschiedener Gruppen oder die Häufigkeit negativer Stereotypen in den Trainingsdaten, die reale Muster und gesellschaftliche Vorurteile widerspiegeln, vorliegen.

**Unangemessene oder anstößige Inhalte:** Diese Modelle können andere Arten unangemessener oder anstößiger Inhalte erzeugen, was den Einsatz in sensiblen Kontexten ohne zusätzliche, fallspezifische Maßnahmen unangemessen machen kann.
Informationszuverlässigkeit: Sprachmodelle können unsinnige Inhalte generieren oder Inhalte erfinden, die vernünftig klingen, aber ungenau oder veraltet sind.

**Begrenzter Umfang für Code:** Der Großteil der Phi-3 Trainingsdaten basiert auf Python und verwendet gängige Pakete wie „typing, math, random, collections, datetime, itertools“. Wenn das Modell Python-Skripte erzeugt, die andere Pakete nutzen, oder Skripte in anderen Sprachen, empfehlen wir dringend, alle API-Verwendungen manuell zu überprüfen.

Entwickler sollten verantwortungsbewusste KI-Best-Practices anwenden und sind dafür verantwortlich sicherzustellen, dass ein spezifischer Anwendungsfall die geltenden Gesetze und Vorschriften (z. B. Datenschutz, Handel usw.) einhält. Wichtige Bereiche für Überlegungen umfassen:

**Zuweisung:** Modelle sind möglicherweise nicht geeignet für Szenarien, die eine weitreichende Auswirkung auf den Rechtsstatus oder die Zuweisung von Ressourcen oder Lebenschancen (z. B. Wohnen, Beschäftigung, Kredit usw.) haben, ohne weitere Bewertungen und zusätzliche Entbiasing-Techniken.

**Hochrisikoszenarien:** Entwickler sollten die Eignung der Verwendung von Modellen in Hochrisikoszenarien bewerten, in denen unfaire, unzuverlässige oder anstößige Ausgaben sehr kostspielig sein oder Schaden verursachen können. Dies umfasst die Beratung in sensiblen oder fachlichen Domänen, in denen Genauigkeit und Zuverlässigkeit entscheidend sind (z. B. rechtliche oder gesundheitliche Beratung). Auf Anwendungsebene sollten entsprechend dem Einsatzkontext zusätzliche Schutzmaßnahmen implementiert werden.

**Fehlinformationen:** Modelle können ungenaue Informationen erzeugen. Entwickler sollten Transparenz-Best-Practices befolgen und Endbenutzer darüber informieren, dass sie mit einem KI-System interagieren. Auf Anwendungsebene können Entwickler Feedback-Mechanismen und Pipelines entwickeln, um Antworten mit anwendungsspezifischen, kontextbezogenen Informationen zu untermauern – eine Technik, die als Retrieval Augmented Generation (RAG) bekannt ist.

**Erzeugung schädlicher Inhalte:** Entwickler sollten Ausgaben im Kontext bewerten und verfügbare Sicherheitsklassifikatoren oder individuelle Lösungen verwenden, die für ihren Anwendungsfall geeignet sind.

**Missbrauch:** Andere Formen des Missbrauchs wie Betrug, Spam oder Malware-Produktion sind möglich, und Entwickler sollten sicherstellen, dass ihre Anwendungen keine geltenden Gesetze und Vorschriften verletzen.

### Feinabstimmung und KI-Inhaltssicherheit

Nach der Feinabstimmung eines Modells empfehlen wir dringend, Maßnahmen von [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) zu nutzen, um die vom Modell generierten Inhalte zu überwachen, potenzielle Risiken, Bedrohungen und Qualitätsprobleme zu identifizieren und zu blockieren.

![Phi3AISafety](../../../../../translated_images/de/01.phi3aisafety.c0d7fc42f5a5c405.png)

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) unterstützt sowohl Text- als auch Bildinhalte. Es kann in der Cloud, in isolierten Containern und auf Edge-/Embedded-Geräten eingesetzt werden.

## Überblick über Azure AI Content Safety

Azure AI Content Safety ist keine Einheitslösung; sie kann an die spezifischen Richtlinien von Unternehmen angepasst werden. Zusätzlich ermöglichen die mehrsprachigen Modelle das gleichzeitige Verstehen mehrerer Sprachen.

![AIContentSafety](../../../../../translated_images/de/01.AIcontentsafety.a288819b8ce8da1a.png)

- **Azure AI Content Safety**
- **Microsoft Developer**
- **5 Videos**

Der Azure AI Content Safety-Dienst erkennt schädliche, von Benutzern und KI generierte Inhalte in Anwendungen und Diensten. Er umfasst Text- und Bild-APIs, mit denen schädliches oder unangemessenes Material erkannt werden kann.

[AI Content Safety Playlist](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner Ursprungssprache ist als maßgebliche Quelle anzusehen. Für wichtige Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die durch die Nutzung dieser Übersetzung entstehen.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->