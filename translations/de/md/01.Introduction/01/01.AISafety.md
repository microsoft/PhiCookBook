<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c8273672cc57df2be675407a1383aaf0",
  "translation_date": "2025-05-07T10:49:52+00:00",
  "source_file": "md/01.Introduction/01/01.AISafety.md",
  "language_code": "de"
}
-->
# KI-Sicherheit für Phi-Modelle  
Die Phi-Modellreihe wurde gemäß dem [Microsoft Responsible AI Standard](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFl) entwickelt, einem unternehmensweiten Anforderungskatalog, der auf den sechs Prinzipien Verantwortlichkeit, Transparenz, Fairness, Zuverlässigkeit und Sicherheit, Datenschutz und Sicherheit sowie Inklusivität basiert und die [Microsofts Responsible AI Prinzipien](https://www.microsoft.com/ai/responsible-ai) bildet.  

Wie bei den vorherigen Phi-Modellen wurde ein vielschichtiger Ansatz zur Sicherheitsbewertung und Sicherheitsnachschulung gewählt, wobei zusätzliche Maßnahmen ergriffen wurden, um den mehrsprachigen Fähigkeiten dieser Version gerecht zu werden. Unser Ansatz zur Sicherheitsschulung und -bewertung, einschließlich Tests in mehreren Sprachen und Risikokategorien, ist im [Phi Safety Post-Training Paper](https://arxiv.org/abs/2407.13833) beschrieben. Obwohl die Phi-Modelle von diesem Ansatz profitieren, sollten Entwickler verantwortungsbewusste KI-Best Practices anwenden, einschließlich der Identifikation, Messung und Minderung von Risiken, die mit ihrem spezifischen Anwendungsfall sowie kulturellem und sprachlichem Kontext verbunden sind.  

## Best Practices  

Wie andere Modelle können auch die Phi-Modelle potenziell Verhaltensweisen zeigen, die unfair, unzuverlässig oder anstößig sind.  

Einige der einschränkenden Verhaltensweisen von SLM und LLM, auf die Sie achten sollten, sind:  

- **Qualität des Dienstes:** Die Phi-Modelle wurden hauptsächlich mit englischem Text trainiert. Andere Sprachen als Englisch werden eine schlechtere Leistung zeigen. Englischsprachige Varianten mit geringerer Repräsentation im Trainingsdatensatz könnten eine schlechtere Leistung als das amerikanische Standardenglisch aufweisen.  
- **Darstellung von Schäden & Verstärkung von Stereotypen:** Diese Modelle können Gruppen von Menschen über- oder unterrepräsentieren, die Darstellung einiger Gruppen auslöschen oder erniedrigende bzw. negative Stereotypen verstärken. Trotz der Sicherheitsnachschulung können diese Einschränkungen weiterhin bestehen, da unterschiedliche Repräsentationsgrade verschiedener Gruppen oder die Häufigkeit negativer Stereotypen im Trainingsmaterial reale Muster und gesellschaftliche Vorurteile widerspiegeln.  
- **Unangemessene oder anstößige Inhalte:** Diese Modelle können andere Arten unangemessener oder anstößiger Inhalte erzeugen, was den Einsatz in sensiblen Kontexten ohne zusätzliche, an den Anwendungsfall angepasste Schutzmaßnahmen unangebracht machen kann.  
- **Informationszuverlässigkeit:** Sprachmodelle können unsinnige Inhalte generieren oder Inhalte erfinden, die zwar plausibel klingen, aber ungenau oder veraltet sind.  
- **Begrenzter Umfang für Code:** Der Großteil der Phi-3-Trainingsdaten basiert auf Python und verwendet gängige Pakete wie "typing, math, random, collections, datetime, itertools". Falls das Modell Python-Skripte generiert, die andere Pakete oder Skripte in anderen Sprachen nutzen, empfehlen wir dringend, alle API-Nutzungen manuell zu überprüfen.  

Entwickler sollten verantwortungsbewusste KI-Best Practices anwenden und sind dafür verantwortlich sicherzustellen, dass ein spezifischer Anwendungsfall den geltenden Gesetzen und Vorschriften (z. B. Datenschutz, Handel usw.) entspricht.  

## Überlegungen zur verantwortungsvollen KI  

Wie andere Sprachmodelle können auch die Phi-Modelle potenziell Verhaltensweisen zeigen, die unfair, unzuverlässig oder anstößig sind. Einige der einschränkenden Verhaltensweisen, auf die geachtet werden sollte, sind:  

**Qualität des Dienstes:** Die Phi-Modelle wurden hauptsächlich mit englischem Text trainiert. Andere Sprachen als Englisch werden eine schlechtere Leistung zeigen. Englischsprachige Varianten mit geringerer Repräsentation im Trainingsdatensatz könnten eine schlechtere Leistung als das amerikanische Standardenglisch aufweisen.  

**Darstellung von Schäden & Verstärkung von Stereotypen:** Diese Modelle können Gruppen von Menschen über- oder unterrepräsentieren, die Darstellung einiger Gruppen auslöschen oder erniedrigende bzw. negative Stereotypen verstärken. Trotz der Sicherheitsnachschulung können diese Einschränkungen weiterhin bestehen, da unterschiedliche Repräsentationsgrade verschiedener Gruppen oder die Häufigkeit negativer Stereotypen im Trainingsmaterial reale Muster und gesellschaftliche Vorurteile widerspiegeln.  

**Unangemessene oder anstößige Inhalte:** Diese Modelle können andere Arten unangemessener oder anstößiger Inhalte erzeugen, was den Einsatz in sensiblen Kontexten ohne zusätzliche, an den Anwendungsfall angepasste Schutzmaßnahmen unangebracht machen kann.  
Informationszuverlässigkeit: Sprachmodelle können unsinnige Inhalte generieren oder Inhalte erfinden, die zwar plausibel klingen, aber ungenau oder veraltet sind.  

**Begrenzter Umfang für Code:** Der Großteil der Phi-3-Trainingsdaten basiert auf Python und verwendet gängige Pakete wie "typing, math, random, collections, datetime, itertools". Falls das Modell Python-Skripte generiert, die andere Pakete oder Skripte in anderen Sprachen nutzen, empfehlen wir dringend, alle API-Nutzungen manuell zu überprüfen.  

Entwickler sollten verantwortungsbewusste KI-Best Practices anwenden und sind dafür verantwortlich sicherzustellen, dass ein spezifischer Anwendungsfall den geltenden Gesetzen und Vorschriften (z. B. Datenschutz, Handel usw.) entspricht. Wichtige Bereiche, die berücksichtigt werden sollten, sind:  

**Zuweisung:** Modelle sind möglicherweise nicht geeignet für Szenarien, die erhebliche Auswirkungen auf den rechtlichen Status oder die Zuteilung von Ressourcen oder Lebenschancen haben könnten (z. B. Wohnen, Beschäftigung, Kreditvergabe usw.), ohne weitere Bewertungen und zusätzliche Entzerrungstechniken.  

**Hochrisikoszenarien:** Entwickler sollten die Eignung der Modelle für den Einsatz in Hochrisikoszenarien bewerten, in denen unfaire, unzuverlässige oder anstößige Ausgaben sehr kostspielig sein oder Schaden verursachen könnten. Dies umfasst die Beratung in sensiblen oder fachlichen Bereichen, in denen Genauigkeit und Zuverlässigkeit entscheidend sind (z. B. Rechts- oder Gesundheitsberatung). Zusätzliche Schutzmaßnahmen sollten auf Anwendungsebene entsprechend dem Einsatzkontext implementiert werden.  

**Fehlinformationen:** Modelle können ungenaue Informationen erzeugen. Entwickler sollten Transparenz-Best Practices befolgen und Endnutzer darüber informieren, dass sie mit einem KI-System interagieren. Auf Anwendungsebene können Entwickler Feedbackmechanismen und Pipelines implementieren, um Antworten auf anwendungsspezifische, kontextuelle Informationen zu stützen, eine Technik, die als Retrieval Augmented Generation (RAG) bekannt ist.  

**Erzeugung schädlicher Inhalte:** Entwickler sollten die Ausgaben im Kontext bewerten und verfügbare Sicherheitsklassifikatoren oder maßgeschneiderte Lösungen einsetzen, die für ihren Anwendungsfall geeignet sind.  

**Missbrauch:** Andere Formen des Missbrauchs wie Betrug, Spam oder Malware-Erstellung sind möglich, und Entwickler sollten sicherstellen, dass ihre Anwendungen keine geltenden Gesetze und Vorschriften verletzen.  

### Feinabstimmung und KI-Inhaltssicherheit  

Nach der Feinabstimmung eines Modells empfehlen wir dringend, [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) Maßnahmen zu nutzen, um die von den Modellen erzeugten Inhalte zu überwachen, potenzielle Risiken, Bedrohungen und Qualitätsprobleme zu erkennen und zu blockieren.  

![Phi3AISafety](../../../../../translated_images/01.phi3aisafety.c0d7fc42f5a5c40507c5e8be556615b8377a63b8764865d057d4faac3757a478.de.png)  

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) unterstützt sowohl Text- als auch Bildinhalte. Es kann in der Cloud, in isolierten Containern sowie auf Edge- oder eingebetteten Geräten eingesetzt werden.  

## Überblick über Azure AI Content Safety  

Azure AI Content Safety ist keine Lösung, die für alle passt; sie kann an die spezifischen Richtlinien von Unternehmen angepasst werden. Zudem ermöglichen die mehrsprachigen Modelle, mehrere Sprachen gleichzeitig zu verstehen.  

![AIContentSafety](../../../../../translated_images/01.AIcontentsafety.a288819b8ce8da1a56cf708aff010a541799d002ae7ae84bb819b19ab8950591.de.png)  

- **Azure AI Content Safety**  
- **Microsoft Developer**  
- **5 Videos**  

Der Azure AI Content Safety-Dienst erkennt schädliche, von Nutzern generierte und KI-generierte Inhalte in Anwendungen und Diensten. Er umfasst Text- und Bild-APIs, mit denen schädliches oder unangemessenes Material erkannt werden kann.  

[AI Content Safety Playlist](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner Ursprungssprache gilt als maßgebliche Quelle. Für wichtige Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die aus der Verwendung dieser Übersetzung entstehen.