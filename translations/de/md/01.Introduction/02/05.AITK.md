<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4951d458c0b60c02cd1e751b40903877",
  "translation_date": "2025-07-16T19:20:47+00:00",
  "source_file": "md/01.Introduction/02/05.AITK.md",
  "language_code": "de"
}
-->
# Phi Family in AITK

[AI Toolkit für VS Code](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio) erleichtert die Entwicklung generativer KI-Anwendungen, indem es modernste KI-Entwicklungstools und Modelle aus dem Azure AI Foundry Catalog sowie anderen Katalogen wie Hugging Face zusammenbringt. Du kannst den KI-Modellkatalog durchsuchen, der von GitHub Models und Azure AI Foundry Model Catalogs unterstützt wird, Modelle lokal oder remote herunterladen, feinabstimmen, testen und in deiner Anwendung verwenden.

Die AI Toolkit Preview läuft lokal. Lokale Inferenz oder Feinabstimmung hängt vom ausgewählten Modell ab; eventuell benötigst du eine GPU wie eine NVIDIA CUDA GPU. Du kannst GitHub Models auch direkt mit AITK ausführen.

## Erste Schritte

[Erfahre mehr darüber, wie du das Windows-Subsystem für Linux installierst](https://learn.microsoft.com/windows/wsl/install?WT.mc_id=aiml-137032-kinfeylo)

und [wie du die Standard-Distribution änderst](https://learn.microsoft.com/windows/wsl/install#change-the-default-linux-distribution-installed).

[AI Toolkit GitHub Repository](https://github.com/microsoft/vscode-ai-toolkit/)

- Windows, Linux, macOS
  
- Für das Feinabstimmen unter Windows und Linux benötigst du eine Nvidia GPU. Außerdem erfordert **Windows** das Windows-Subsystem für Linux mit einer Ubuntu-Distribution 18.04 oder höher. [Erfahre mehr zur Installation des Windows-Subsystems für Linux](https://learn.microsoft.com/windows/wsl/install) und [wie du die Standard-Distribution änderst](https://learn.microsoft.com/windows/wsl/install#change-the-default-linux-distribution-installed).

### AI Toolkit installieren

Das AI Toolkit wird als [Visual Studio Code Erweiterung](https://code.visualstudio.com/docs/setup/additional-components#_vs-code-extensions) ausgeliefert, daher musst du zuerst [VS Code](https://code.visualstudio.com/docs/setup/windows?WT.mc_id=aiml-137032-kinfeylo) installieren und das AI Toolkit aus dem [VS Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio) herunterladen.
Das [AI Toolkit ist im Visual Studio Marketplace verfügbar](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio) und kann wie jede andere VS Code Erweiterung installiert werden.

Wenn du mit der Installation von VS Code Erweiterungen nicht vertraut bist, folge diesen Schritten:

### Anmelden

1. Wähle in der Aktivitätsleiste von VS Code **Extensions** aus
1. Gib in der Suchleiste "AI Toolkit" ein
1. Wähle "AI Toolkit for Visual Studio code" aus
1. Klicke auf **Install**

Jetzt bist du bereit, die Erweiterung zu nutzen!

Du wirst aufgefordert, dich bei GitHub anzumelden. Klicke dazu bitte auf "Allow", um fortzufahren. Du wirst zur GitHub-Anmeldeseite weitergeleitet.

Bitte melde dich an und folge den Anweisungen. Nach erfolgreichem Abschluss wirst du zurück zu VS Code geleitet.

Sobald die Erweiterung installiert ist, erscheint das AI Toolkit-Symbol in deiner Aktivitätsleiste.

Lass uns die verfügbaren Funktionen erkunden!

### Verfügbare Aktionen

Die Hauptseitenleiste des AI Toolkit ist in folgende Bereiche gegliedert:

- **Models**
- **Resources**
- **Playground**  
- **Fine-tuning**
- **Evaluation**

Diese findest du im Bereich Resources. Um zu starten, wähle **Model Catalog** aus.

### Ein Modell aus dem Katalog herunterladen

Wenn du das AI Toolkit über die VS Code Seitenleiste startest, kannst du aus folgenden Optionen wählen:

![AI toolkit model catalog](../../../../../translated_images/de/AItoolkitmodel_catalog.7a7be6a7d8468d31.png)

- Finde ein unterstütztes Modell im **Model Catalog** und lade es lokal herunter
- Teste die Modellinferenz im **Model Playground**
- Feineinstellen des Modells lokal oder remote im **Model Fine-tuning**
- Bereitstellen feinabgestimmter Modelle in der Cloud über die Befehls-Palette des AI Toolkit
- Modelle bewerten

> [!NOTE]
>
> **GPU vs CPU**
>
> Du wirst feststellen, dass die Modellkarten die Modellgröße, die Plattform und den Beschleunigertyp (CPU, GPU) anzeigen. Für optimale Leistung auf **Windows-Geräten mit mindestens einer GPU** solltest du Modellversionen wählen, die nur auf Windows abzielen.
>
> So stellst du sicher, dass du ein Modell hast, das für den DirectML-Beschleuniger optimiert ist.
>
> Die Modellnamen haben folgendes Format:
>
> - `{model_name}-{accelerator}-{quantization}-{format}`.
>
>Um zu prüfen, ob dein Windows-Gerät eine GPU hat, öffne den **Task-Manager** und wähle den Reiter **Leistung**. Falls GPUs vorhanden sind, werden sie unter Namen wie "GPU 0" oder "GPU 1" aufgeführt.

### Modell im Playground ausführen

Nachdem alle Parameter gesetzt sind, klicke auf **Generate Project**.

Sobald dein Modell heruntergeladen wurde, wähle auf der Modellkarte im Katalog **Load in Playground**:

- Starte den Modell-Download
- Installiere alle Voraussetzungen und Abhängigkeiten
- Erstelle einen VS Code Arbeitsbereich

![Load model in playground](../../../../../translated_images/de/AItoolkitload_model_into_playground.dcef5355b1653b52.png)

### REST API in deiner Anwendung verwenden

Das AI Toolkit bietet einen lokalen REST API Webserver **auf Port 5272**, der das [OpenAI Chat Completions Format](https://platform.openai.com/docs/api-reference/chat/create) verwendet.

So kannst du deine Anwendung lokal testen, ohne auf einen Cloud-KI-Modellservice angewiesen zu sein. Zum Beispiel zeigt die folgende JSON-Datei, wie der Body der Anfrage konfiguriert wird:

```json
{
    "model": "Phi-4",
    "messages": [
        {
            "role": "user",
            "content": "what is the golden ratio?"
        }
    ],
    "temperature": 0.7,
    "top_p": 1,
    "top_k": 10,
    "max_tokens": 100,
    "stream": true
}
```

Du kannst die REST API mit (zum Beispiel) [Postman](https://www.postman.com/) oder dem CURL (Client URL) Tool testen:

```bash
curl -vX POST http://127.0.0.1:5272/v1/chat/completions -H 'Content-Type: application/json' -d @body.json
```

### Verwendung der OpenAI Client-Bibliothek für Python

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://127.0.0.1:5272/v1/", 
    api_key="x" # required for the API but not used
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "what is the golden ratio?",
        }
    ],
    model="Phi-4",
)

print(chat_completion.choices[0].message.content)
```

### Verwendung der Azure OpenAI Client-Bibliothek für .NET

Füge die [Azure OpenAI Client-Bibliothek für .NET](https://www.nuget.org/packages/Azure.AI.OpenAI/) deinem Projekt über NuGet hinzu:

```bash
dotnet add {project_name} package Azure.AI.OpenAI --version 1.0.0-beta.17
```

Füge deinem Projekt eine C#-Datei namens **OverridePolicy.cs** hinzu und füge folgenden Code ein:

```csharp
// OverridePolicy.cs
using Azure.Core.Pipeline;
using Azure.Core;

internal partial class OverrideRequestUriPolicy(Uri overrideUri)
    : HttpPipelineSynchronousPolicy
{
    private readonly Uri _overrideUri = overrideUri;

    public override void OnSendingRequest(HttpMessage message)
    {
        message.Request.Uri.Reset(_overrideUri);
    }
}
```

Füge anschließend den folgenden Code in deine **Program.cs** Datei ein:

```csharp
// Program.cs
using Azure.AI.OpenAI;

Uri localhostUri = new("http://localhost:5272/v1/chat/completions");

OpenAIClientOptions clientOptions = new();
clientOptions.AddPolicy(
    new OverrideRequestUriPolicy(localhostUri),
    Azure.Core.HttpPipelinePosition.BeforeTransport);
OpenAIClient client = new(openAIApiKey: "unused", clientOptions);

ChatCompletionsOptions options = new()
{
    DeploymentName = "Phi-4",
    Messages =
    {
        new ChatRequestSystemMessage("You are a helpful assistant. Be brief and succinct."),
        new ChatRequestUserMessage("What is the golden ratio?"),
    }
};

StreamingResponse<StreamingChatCompletionsUpdate> streamingChatResponse
    = await client.GetChatCompletionsStreamingAsync(options);

await foreach (StreamingChatCompletionsUpdate chatChunk in streamingChatResponse)
{
    Console.Write(chatChunk.ContentUpdate);
}
```


## Feinabstimmung mit AI Toolkit

- Einstieg mit Modellsuche und Playground.
- Modell-Feinabstimmung und Inferenz mit lokalen Rechenressourcen.
- Remote-Feinabstimmung und Inferenz mit Azure-Ressourcen

[Feinabstimmung mit AI Toolkit](../../03.FineTuning/Finetuning_VSCodeaitoolkit.md)

## AI Toolkit Q&A Ressourcen

Bitte besuche unsere [Q&A-Seite](https://github.com/microsoft/vscode-ai-toolkit/blob/main/archive/QA.md) für häufige Probleme und Lösungen.

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner Ursprungssprache ist als maßgebliche Quelle zu betrachten. Für wichtige Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die aus der Nutzung dieser Übersetzung entstehen.