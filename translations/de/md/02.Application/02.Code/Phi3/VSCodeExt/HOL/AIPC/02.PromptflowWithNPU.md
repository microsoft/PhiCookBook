<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "bc29f7fe7fc16bed6932733eac8c81b8",
  "translation_date": "2025-07-17T03:54:05+00:00",
  "source_file": "md/02.Application/02.Code/Phi3/VSCodeExt/HOL/AIPC/02.PromptflowWithNPU.md",
  "language_code": "de"
}
-->
# **Lab 2 - Ausführen von Prompt flow mit Phi-3-mini in AIPC**

## **Was ist Prompt flow**

Prompt flow ist eine Sammlung von Entwicklungstools, die den gesamten Entwicklungszyklus von LLM-basierten KI-Anwendungen vereinfachen – von der Ideenfindung, Prototypenerstellung, dem Testen und der Bewertung bis hin zur Produktion, Bereitstellung und Überwachung. Es macht Prompt Engineering deutlich einfacher und ermöglicht es dir, LLM-Anwendungen in Produktionsqualität zu erstellen.

Mit Prompt flow kannst du:

- Flows erstellen, die LLMs, Prompts, Python-Code und andere Tools in einem ausführbaren Workflow verbinden.

- Deine Flows debuggen und iterieren, insbesondere die Interaktion mit LLMs, ganz einfach.

- Deine Flows bewerten und Qualitäts- sowie Leistungskennzahlen mit größeren Datensätzen berechnen.

- Tests und Bewertungen in dein CI/CD-System integrieren, um die Qualität deines Flows sicherzustellen.

- Deine Flows auf der von dir gewählten Serving-Plattform bereitstellen oder einfach in den Code deiner App integrieren.

- (Optional, aber sehr empfohlen) Mit deinem Team zusammenarbeiten, indem du die Cloud-Version von Prompt flow in Azure AI nutzt.

## **Was ist AIPC**

Ein AI PC verfügt über eine CPU, eine GPU und eine NPU, die jeweils spezielle KI-Beschleunigungsfunktionen bieten. Eine NPU, oder Neural Processing Unit, ist ein spezialisierter Beschleuniger, der KI- und ML-Aufgaben direkt auf deinem PC ausführt, anstatt Daten zur Verarbeitung in die Cloud zu senden. GPU und CPU können diese Aufgaben ebenfalls übernehmen, aber die NPU ist besonders gut für energieeffiziente KI-Berechnungen geeignet. Der AI PC stellt einen grundlegenden Wandel in der Funktionsweise unserer Computer dar. Er ist keine Lösung für ein zuvor nicht existentes Problem, sondern verspricht eine enorme Verbesserung für den Alltag am PC.

Wie funktioniert das? Im Vergleich zu generativer KI und den riesigen großen Sprachmodellen (LLMs), die mit Unmengen öffentlicher Daten trainiert wurden, ist die KI, die auf deinem PC läuft, auf fast jeder Ebene zugänglicher. Das Konzept ist leichter verständlich, und da es mit deinen eigenen Daten trainiert wird, ohne Cloud-Zugriff zu benötigen, sind die Vorteile für eine breitere Nutzergruppe sofort spürbar.

Kurzfristig umfasst die Welt des AI PC persönliche Assistenten und kleinere KI-Modelle, die direkt auf deinem PC laufen und deine Daten nutzen, um persönliche, private und sicherere KI-Verbesserungen für alltägliche Aufgaben zu bieten – wie das Anfertigen von Sitzungsprotokollen, die Organisation einer Fantasy-Football-Liga, die Automatisierung von Verbesserungen bei Foto- und Videobearbeitung oder die Planung der perfekten Reiseroute für ein Familientreffen basierend auf den Ankunfts- und Abfahrtszeiten aller Teilnehmer.

## **Erstellen von Generierungscode-Flows auf AIPC**

***Hinweis*** ：Falls du die Umgebung noch nicht eingerichtet hast, besuche bitte [Lab 0 - Installationen](./01.Installations.md)

1. Öffne die Prompt flow Extension in Visual Studio Code und erstelle ein leeres Flow-Projekt

![create](../../../../../../../../../translated_images/pf_create.bde888dc83502eba082a058175bbf1eee6791219795393a386b06fd3043ec54d.de.png)

2. Füge Eingabe- und Ausgabeparameter hinzu und füge Python-Code als neuen Flow hinzu

![flow](../../../../../../../../../translated_images/pf_flow.520824c0969f2a94f17e947f86bdc4b4c6c88a2efa394fe3bcfb58c0dbc578a7.de.png)

Du kannst dich an dieser Struktur (flow.dag.yaml) orientieren, um deinen Flow zu erstellen

```yaml

inputs:
  question:
    type: string
    default: how to write Bubble Algorithm
outputs:
  answer:
    type: string
    reference: ${Chat_With_Phi3.output}
nodes:
- name: Chat_With_Phi3
  type: python
  source:
    type: code
    path: Chat_With_Phi3.py
  inputs:
    question: ${inputs.question}


```

3. Füge Code in ***Chat_With_Phi3.py*** ein

```python


from promptflow.core import tool

# import torch
from transformers import AutoTokenizer, pipeline,TextStreamer
import intel_npu_acceleration_library as npu_lib

import warnings

import asyncio
import platform

class Phi3CodeAgent:
    
    model = None
    tokenizer = None
    text_streamer = None
    
    model_id = "microsoft/Phi-3-mini-4k-instruct"

    @staticmethod
    def init_phi3():
        
        if Phi3CodeAgent.model is None or Phi3CodeAgent.tokenizer is None or Phi3CodeAgent.text_streamer is None:
            Phi3CodeAgent.model = npu_lib.NPUModelForCausalLM.from_pretrained(
                                    Phi3CodeAgent.model_id,
                                    torch_dtype="auto",
                                    dtype=npu_lib.int4,
                                    trust_remote_code=True
                                )
            Phi3CodeAgent.tokenizer = AutoTokenizer.from_pretrained(Phi3CodeAgent.model_id)
            Phi3CodeAgent.text_streamer = TextStreamer(Phi3CodeAgent.tokenizer, skip_prompt=True)

    

    @staticmethod
    def chat_with_phi3(prompt):
        
        Phi3CodeAgent.init_phi3()

        messages = "<|system|>You are a AI Python coding assistant. Please help me to generate code in Python.The answer only genertated Python code, but any comments and instructions do not need to be generated<|end|><|user|>" + prompt +"<|end|><|assistant|>"



        generation_args = {
            "max_new_tokens": 1024,
            "return_full_text": False,
            "temperature": 0.3,
            "do_sample": False,
            "streamer": Phi3CodeAgent.text_streamer,
        }

        pipe = pipeline(
            "text-generation",
            model=Phi3CodeAgent.model,
            tokenizer=Phi3CodeAgent.tokenizer,
            # **generation_args
        )

        result = ''

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            response = pipe(messages, **generation_args)
            result =response[0]['generated_text']
            return result


@tool
def my_python_tool(question: str) -> str:
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    return Phi3CodeAgent.chat_with_phi3(question)


```

4. Du kannst den Flow über Debug oder Run testen, um zu prüfen, ob der Generierungscode funktioniert

![RUN](../../../../../../../../../translated_images/pf_run.4239e8a0b420a58284edf6ee1471c1697c345670313c8e7beac0edaee15b9a9d.de.png)

5. Starte den Flow als Entwicklungs-API im Terminal

```

pf flow serve --source ./ --port 8080 --host localhost   

```

Du kannst ihn in Postman / Thunder Client testen

### **Hinweis**

1. Der erste Lauf dauert lange. Es wird empfohlen, das phi-3 Modell über die Hugging face CLI herunterzuladen.

2. Aufgrund der begrenzten Rechenleistung der Intel NPU wird empfohlen, Phi-3-mini-4k-instruct zu verwenden.

3. Wir nutzen die Intel NPU-Beschleunigung zur Quantisierung in INT4, aber wenn du den Service neu startest, musst du die Cache- und nc_workshop-Ordner löschen.

## **Ressourcen**

1. Lerne Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Lerne Intel NPU Acceleration [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Beispielcode, Download [Local NPU Agent Sample Code](../../../../../../../../../code/07.Lab/01/AIPC)

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir auf Genauigkeit achten, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner Ursprungssprache ist als maßgebliche Quelle zu betrachten. Für wichtige Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die aus der Nutzung dieser Übersetzung entstehen.