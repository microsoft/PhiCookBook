<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3dbbf568625b1ee04b354c2dc81d3248",
  "translation_date": "2025-07-17T04:22:32+00:00",
  "source_file": "md/02.Application/02.Code/Phi3/VSCodeExt/HOL/Apple/02.PromptflowWithMLX.md",
  "language_code": "de"
}
-->
# **Lab 2 - Ausführen von Prompt flow mit Phi-3-mini in AIPC**

## **Was ist Prompt flow**

Prompt flow ist eine Sammlung von Entwicklungstools, die den gesamten Entwicklungszyklus von LLM-basierten KI-Anwendungen vereinfachen – von der Ideenfindung, Prototypenerstellung, dem Testen und der Bewertung bis hin zur Produktion, Bereitstellung und Überwachung. Es macht Prompt Engineering deutlich einfacher und ermöglicht es dir, LLM-Anwendungen in Produktionsqualität zu erstellen.

Mit Prompt flow kannst du:

- Flows erstellen, die LLMs, Prompts, Python-Code und andere Tools in einem ausführbaren Workflow verbinden.

- Deine Flows debuggen und iterieren, insbesondere die Interaktion mit LLMs, ganz einfach.

- Deine Flows bewerten und Qualitäts- sowie Leistungskennzahlen mit größeren Datensätzen berechnen.

- Tests und Bewertungen in dein CI/CD-System integrieren, um die Qualität deines Flows sicherzustellen.

- Deine Flows auf der von dir gewählten Serving-Plattform bereitstellen oder einfach in den Code deiner App integrieren.

- (Optional, aber sehr empfohlen) Mit deinem Team zusammenarbeiten, indem du die Cloud-Version von Prompt flow in Azure AI nutzt.

## **Erstellen von Generierungscode-Flows auf Apple Silicon**

***Hinweis***: Falls du die Umgebung noch nicht eingerichtet hast, besuche bitte [Lab 0 - Installationen](./01.Installations.md)

1. Öffne die Prompt flow Extension in Visual Studio Code und erstelle ein leeres Flow-Projekt

![create](../../../../../../../../../translated_images/pf_create.bde888dc83502eba082a058175bbf1eee6791219795393a386b06fd3043ec54d.de.png)

2. Füge Eingabe- und Ausgabeparameter hinzu und füge Python-Code als neuen Flow hinzu

![flow](../../../../../../../../../translated_images/pf_flow.520824c0969f2a94f17e947f86bdc4b4c6c88a2efa394fe3bcfb58c0dbc578a7.de.png)

Du kannst dich an dieser Struktur (flow.dag.yaml) orientieren, um deinen Flow zu erstellen

```yaml

inputs:
  prompt:
    type: string
    default: Write python code for Fibonacci serie. Please use markdown as output
outputs:
  result:
    type: string
    reference: ${gen_code_by_phi3.output}
nodes:
- name: gen_code_by_phi3
  type: python
  source:
    type: code
    path: gen_code_by_phi3.py
  inputs:
    prompt: ${inputs.prompt}


```

3. Quantifiziere phi-3-mini

Wir möchten SLM besser auf lokalen Geräten ausführen. Im Allgemeinen quantifizieren wir das Modell (INT4, FP16, FP32)

```bash

python -m mlx_lm.convert --hf-path microsoft/Phi-3-mini-4k-instruct

```

**Hinweis:** Der Standardordner ist mlx_model

4. Füge Code in ***Chat_With_Phi3.py*** ein

```python


from promptflow import tool

from mlx_lm import load, generate


# The inputs section will change based on the arguments of the tool function, after you save the code
# Adding type to arguments and return value will help the system show the types properly
# Please update the function name/signature per need
@tool
def my_python_tool(prompt: str) -> str:

    model_id = './mlx_model_phi3_mini'

    model, tokenizer = load(model_id)

    # <|user|>\nWrite python code for Fibonacci serie. Please use markdown as output<|end|>\n<|assistant|>

    response = generate(model, tokenizer, prompt="<|user|>\n" + prompt  + "<|end|>\n<|assistant|>", max_tokens=2048, verbose=True)

    return response


```

4. Du kannst den Flow über Debug oder Run testen, um zu prüfen, ob der Generierungscode funktioniert

![RUN](../../../../../../../../../translated_images/pf_run.4239e8a0b420a58284edf6ee1471c1697c345670313c8e7beac0edaee15b9a9d.de.png)

5. Führe den Flow als Entwicklungs-API im Terminal aus

```

pf flow serve --source ./ --port 8080 --host localhost   

```

Du kannst es in Postman / Thunder Client testen

### **Hinweis**

1. Der erste Lauf dauert lange. Es wird empfohlen, das phi-3 Modell über die Hugging face CLI herunterzuladen.

2. Aufgrund der begrenzten Rechenleistung des Intel NPU wird empfohlen, Phi-3-mini-4k-instruct zu verwenden.

3. Wir nutzen die Intel NPU Beschleunigung zur Quantisierung in INT4, aber wenn du den Service neu startest, musst du die Cache- und nc_workshop-Ordner löschen.

## **Ressourcen**

1. Lerne Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Lerne Intel NPU Acceleration [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Beispielcode, Download [Local NPU Agent Sample Code](../../../../../../../../../code/07.Lab/01/AIPC/local-npu-agent)

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner Ursprungssprache ist als maßgebliche Quelle zu betrachten. Für wichtige Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die aus der Nutzung dieser Übersetzung entstehen.