<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3dbbf568625b1ee04b354c2dc81d3248",
  "translation_date": "2025-03-27T12:18:53+00:00",
  "source_file": "md\\02.Application\\02.Code\\Phi3\\VSCodeExt\\HOL\\Apple\\02.PromptflowWithMLX.md",
  "language_code": "de"
}
-->
# **Lab 2 - Prompt Flow mit Phi-3-mini in AIPC ausführen**

## **Was ist Prompt Flow**

Prompt Flow ist eine Suite von Entwicklungstools, die darauf abzielt, den gesamten Entwicklungszyklus von KI-Anwendungen auf Basis von LLMs zu optimieren – von der Ideenfindung, dem Prototyping, Testen, Evaluieren bis hin zur Produktionsbereitstellung und Überwachung. Es erleichtert das Prompt Engineering erheblich und ermöglicht es Ihnen, LLM-Anwendungen in Produktionsqualität zu erstellen.

Mit Prompt Flow können Sie:

- Flows erstellen, die LLMs, Prompts, Python-Code und andere Tools in einem ausführbaren Workflow miteinander verbinden.

- Ihre Flows debuggen und iterieren, insbesondere die Interaktion mit LLMs, auf einfache Weise.

- Ihre Flows bewerten und Qualitäts- sowie Leistungskennzahlen mit größeren Datensätzen berechnen.

- Tests und Bewertungen in Ihr CI/CD-System integrieren, um die Qualität Ihres Flows sicherzustellen.

- Ihre Flows auf die von Ihnen gewählte Plattform bereitstellen oder problemlos in den Code Ihrer App integrieren.

- (Optional, aber sehr empfehlenswert) Mit Ihrem Team zusammenarbeiten, indem Sie die Cloud-Version von Prompt Flow in Azure AI nutzen.



## **Generierung von Code-Flows auf Apple Silicon**

***Hinweis***: Wenn Sie die Umgebungsinstallation noch nicht abgeschlossen haben, besuchen Sie bitte [Lab 0 - Installationen](./01.Installations.md)

1. Öffnen Sie die Prompt Flow-Erweiterung in Visual Studio Code und erstellen Sie ein leeres Flow-Projekt.

![create](../../../../../../../../../translated_images/pf_create.d6172d8277a78a7fa82cd6ff727ed44e037fa78b662f1f62d5963f36d712d229.de.png)

2. Fügen Sie Eingabe- und Ausgabeparameter hinzu und fügen Sie Python-Code als neuen Flow hinzu.

![flow](../../../../../../../../../translated_images/pf_flow.d5646a323fb7f444c0b98b4521057a592325c583e7ba18bc31500bc0415e9ef3.de.png)

Sie können sich an dieser Struktur (flow.dag.yaml) orientieren, um Ihren Flow zu erstellen:

```yaml

inputs:
  prompt:
    type: string
    default: Write python code for Fibonacci serie. Please use markdown as output
outputs:
  result:
    type: string
    reference: ${gen_code_by_phi3.output}
nodes:
- name: gen_code_by_phi3
  type: python
  source:
    type: code
    path: gen_code_by_phi3.py
  inputs:
    prompt: ${inputs.prompt}


```

3. Phi-3-mini quantifizieren

Wir möchten SLM besser auf lokalen Geräten ausführen. In der Regel quantifizieren wir das Modell (INT4, FP16, FP32).

```bash

python -m mlx_lm.convert --hf-path microsoft/Phi-3-mini-4k-instruct

```

**Hinweis:** Der Standardordner ist mlx_model.

4. Code in ***Chat_With_Phi3.py*** hinzufügen.

```python


from promptflow import tool

from mlx_lm import load, generate


# The inputs section will change based on the arguments of the tool function, after you save the code
# Adding type to arguments and return value will help the system show the types properly
# Please update the function name/signature per need
@tool
def my_python_tool(prompt: str) -> str:

    model_id = './mlx_model_phi3_mini'

    model, tokenizer = load(model_id)

    # <|user|>\nWrite python code for Fibonacci serie. Please use markdown as output<|end|>\n<|assistant|>

    response = generate(model, tokenizer, prompt="<|user|>\n" + prompt  + "<|end|>\n<|assistant|>", max_tokens=2048, verbose=True)

    return response


```

4. Sie können den Flow über Debug oder Run testen, um zu überprüfen, ob der generierte Code funktioniert.

![RUN](../../../../../../../../../translated_images/pf_run.d918637dc00f61e9bdeec37d4cc9646f77d270ac9203bcce13569f3157202b6e.de.png)

5. Flow als Entwicklungs-API im Terminal ausführen.

```

pf flow serve --source ./ --port 8080 --host localhost   

```

Sie können es in Postman / Thunder Client testen.


### **Hinweise**

1. Der erste Lauf dauert lange. Es wird empfohlen, das Phi-3-Modell über die Hugging Face CLI herunterzuladen.

2. Angesichts der begrenzten Rechenleistung des Intel NPU wird empfohlen, Phi-3-mini-4k-instruct zu verwenden.

3. Wir nutzen die Intel NPU-Beschleunigung, um die INT4-Konvertierung zu quantifizieren. Wenn Sie den Dienst jedoch erneut ausführen, müssen Sie den Cache und die nc_workshop-Ordner löschen.



## **Ressourcen**

1. Erfahren Sie mehr über Prompt Flow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Erfahren Sie mehr über Intel NPU-Beschleunigung [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Beispielcode herunterladen [Local NPU Agent Sample Code](../../../../../../../../../code/07.Lab/01/AIPC/local-npu-agent)

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner Ausgangssprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir haften nicht für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.