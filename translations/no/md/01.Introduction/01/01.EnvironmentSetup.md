<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-07-16T18:10:29+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "no"
}
-->
# Kom i gang med Phi-3 lokalt

Denne guiden hjelper deg med å sette opp ditt lokale miljø for å kjøre Phi-3-modellen ved hjelp av Ollama. Du kan kjøre modellen på flere måter, blant annet ved å bruke GitHub Codespaces, VS Code Dev Containers eller ditt lokale miljø.

## Miljøoppsett

### GitHub Codespaces

Du kan kjøre denne malen virtuelt ved å bruke GitHub Codespaces. Knappen åpner en nettbasert VS Code-instans i nettleseren din:

1. Åpne malen (dette kan ta noen minutter):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Åpne et terminalvindu

### VS Code Dev Containers

⚠️ Dette alternativet fungerer bare hvis Docker Desktop har minst 16 GB RAM tilgjengelig. Hvis du har mindre enn 16 GB RAM, kan du prøve [GitHub Codespaces-alternativet](../../../../../md/01.Introduction/01) eller [sette det opp lokalt](../../../../../md/01.Introduction/01).

Et relatert alternativ er VS Code Dev Containers, som åpner prosjektet i din lokale VS Code ved hjelp av [Dev Containers-utvidelsen](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Start Docker Desktop (installer det hvis det ikke allerede er installert)
2. Åpne prosjektet:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. I VS Code-vinduet som åpnes, når prosjektfilene vises (dette kan ta noen minutter), åpner du et terminalvindu.
4. Fortsett med [distribusjonstrinnene](../../../../../md/01.Introduction/01)

### Lokalt miljø

1. Sørg for at følgende verktøy er installert:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Test modellen

1. Be Ollama laste ned og kjøre phi3:mini-modellen:

    ```shell
    ollama run phi3:mini
    ```

    Det vil ta noen minutter å laste ned modellen.

2. Når du ser "success" i output, kan du sende en melding til modellen fra prompten.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Etter noen sekunder bør du se en respons strømme inn fra modellen.

4. For å lære om ulike teknikker brukt med språkmodeller, åpne Python-notatboken [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) og kjør hver celle. Hvis du brukte en annen modell enn 'phi3:mini', endre `MODEL_NAME` i den første cellen.

5. For å ha en samtale med phi3:mini-modellen fra Python, åpne Python-filen [chat.py](../../../../../code/01.Introduce/chat.py) og kjør den. Du kan endre `MODEL_NAME` øverst i filen etter behov, og du kan også justere systemmeldingen eller legge til få-skudd-eksempler hvis ønskelig.

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter nøyaktighet, vennligst vær oppmerksom på at automatiske oversettelser kan inneholde feil eller unøyaktigheter. Det opprinnelige dokumentet på originalspråket skal anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.