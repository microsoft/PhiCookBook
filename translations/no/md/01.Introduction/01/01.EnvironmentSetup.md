<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-09T07:10:59+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "no"
}
-->
# Kom i gang med Phi-3 lokalt

Denne veiledningen hjelper deg med å sette opp ditt lokale miljø for å kjøre Phi-3-modellen med Ollama. Du kan kjøre modellen på flere måter, blant annet ved å bruke GitHub Codespaces, VS Code Dev Containers eller ditt lokale miljø.

## Miljøoppsett

### GitHub Codespaces

Du kan kjøre denne malen virtuelt ved å bruke GitHub Codespaces. Knappen åpner en nettbasert VS Code-instans i nettleseren din:

1. Åpne malen (dette kan ta noen minutter):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Åpne et terminalvindu

### VS Code Dev Containers

⚠️ Denne metoden fungerer kun hvis Docker Desktop har minst 16 GB RAM tilgjengelig. Hvis du har mindre enn 16 GB RAM, kan du prøve [GitHub Codespaces-alternativet](../../../../../md/01.Introduction/01) eller [sette det opp lokalt](../../../../../md/01.Introduction/01).

Et nærliggende alternativ er VS Code Dev Containers, som åpner prosjektet i din lokale VS Code ved hjelp av [Dev Containers-utvidelsen](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Start Docker Desktop (installer det hvis det ikke allerede er installert)
2. Åpne prosjektet:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. I VS Code-vinduet som åpnes, når prosjektfilene vises (dette kan ta noen minutter), åpner du et terminalvindu.
4. Fortsett med [utrullingsstegene](../../../../../md/01.Introduction/01)

### Lokalt miljø

1. Sørg for at følgende verktøy er installert:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Test modellen

1. Be Ollama laste ned og kjøre phi3:mini-modellen:

    ```shell
    ollama run phi3:mini
    ```

    Dette vil ta noen minutter for å laste ned modellen.

2. Når du ser "success" i output, kan du sende en melding til modellen via prompten.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Etter noen sekunder bør du se en respons strømme inn fra modellen.

4. For å lære om forskjellige teknikker brukt med språkmodeller, åpne Python-notatboken [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) og kjør hver celle. Hvis du har brukt en annen modell enn 'phi3:mini', endre `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` øverst i filen etter behov, og du kan også justere systemmeldingen eller legge til få-skudd-eksempler om ønskelig.

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter nøyaktighet, vennligst vær oppmerksom på at automatiserte oversettelser kan inneholde feil eller unøyaktigheter. Det opprinnelige dokumentet på originalspråket bør betraktes som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.