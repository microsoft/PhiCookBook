<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:24:45+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "no"
}
-->
# Nøkkelteknologier som nevnes inkluderer

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - et lavnivå-API for maskinlæring med maskinvareakselerasjon bygget på DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - en plattform for parallellberegning og et API-modell utviklet av Nvidia, som muliggjør generell prosessering på grafikkprosessorer (GPUer).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - et åpent format designet for å representere maskinlæringsmodeller som gir interoperabilitet mellom ulike ML-rammeverk.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - et format brukt til å representere og oppdatere maskinlæringsmodeller, spesielt nyttig for mindre språkmodeller som kan kjøre effektivt på CPUer med 4-8bit kvantisering.

## DirectML

DirectML er et lavnivå-API som muliggjør maskinlæring med maskinvareakselerasjon. Det er bygget på DirectX 12 for å utnytte GPU-akselerasjon og er leverandøruavhengig, noe som betyr at det ikke krever kodeendringer for å fungere på tvers av ulike GPU-leverandører. Det brukes hovedsakelig til modelltrening og inferensarbeidsbelastninger på GPUer.

Når det gjelder maskinvarestøtte, er DirectML designet for å fungere med et bredt spekter av GPUer, inkludert AMD integrerte og diskrete GPUer, Intel integrerte GPUer og NVIDIA diskrete GPUer. Det er en del av Windows AI Platform og støttes på Windows 10 og 11, noe som muliggjør modelltrening og inferens på alle Windows-enheter.

Det har vært oppdateringer og muligheter knyttet til DirectML, som støtte for opptil 150 ONNX-operatører og bruk av både ONNX runtime og WinML. Det støttes av store Integrated Hardware Vendors (IHVs), som hver implementerer ulike metakommandoer.

## CUDA

CUDA, som står for Compute Unified Device Architecture, er en plattform for parallellberegning og et API-modell utviklet av Nvidia. Det lar programvareutviklere bruke en CUDA-aktivert grafikkprosessor (GPU) til generell prosessering – en tilnærming kalt GPGPU (General-Purpose computing on Graphics Processing Units). CUDA er en nøkkelteknologi for Nvidias GPU-akselerasjon og brukes mye innen ulike felt, inkludert maskinlæring, vitenskapelig beregning og videobehandling.

Maskinvarestøtten for CUDA er spesifikk for Nvidias GPUer, da det er en proprietær teknologi utviklet av Nvidia. Hver arkitektur støtter spesifikke versjoner av CUDA-verktøykassen, som gir nødvendige biblioteker og verktøy for utviklere til å bygge og kjøre CUDA-applikasjoner.

## ONNX

ONNX (Open Neural Network Exchange) er et åpent format designet for å representere maskinlæringsmodeller. Det gir en definisjon av en utvidbar beregningsgrafmodell, samt definisjoner av innebygde operatører og standard datatyper. ONNX gjør det mulig for utviklere å flytte modeller mellom ulike ML-rammeverk, noe som muliggjør interoperabilitet og gjør det enklere å lage og distribuere AI-applikasjoner.

Phi3 mini kan kjøre med ONNX Runtime på CPU og GPU på tvers av enheter, inkludert serverplattformer, Windows, Linux og Mac desktop, samt mobile CPUer.  
De optimaliserte konfigurasjonene vi har lagt til er

- ONNX-modeller for int4 DML: Kvantisert til int4 via AWQ  
- ONNX-modell for fp16 CUDA  
- ONNX-modell for int4 CUDA: Kvantisert til int4 via RTN  
- ONNX-modell for int4 CPU og Mobile: Kvantisert til int4 via RTN  

## Llama.cpp

Llama.cpp er et åpen kildekode programvarebibliotek skrevet i C++. Det utfører inferens på ulike store språkmodeller (LLMs), inkludert Llama. Utviklet sammen med ggml-biblioteket (et generelt tensorbibliotek), har llama.cpp som mål å tilby raskere inferens og lavere minnebruk sammenlignet med den opprinnelige Python-implementeringen. Det støtter maskinvareoptimalisering, kvantisering, og tilbyr et enkelt API og eksempler. Hvis du er interessert i effektiv LLM-inferens, er llama.cpp verdt å utforske siden Phi3 kan kjøre Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) er et format brukt til å representere og oppdatere maskinlæringsmodeller. Det er spesielt nyttig for mindre språkmodeller (SLMs) som kan kjøre effektivt på CPUer med 4-8bit kvantisering. GGUF er fordelaktig for rask prototyping og kjøring av modeller på edge-enheter eller i batch-jobber som CI/CD-pipelines.

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter nøyaktighet, vennligst vær oppmerksom på at automatiserte oversettelser kan inneholde feil eller unøyaktigheter. Det originale dokumentet på dets opprinnelige språk bør betraktes som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.