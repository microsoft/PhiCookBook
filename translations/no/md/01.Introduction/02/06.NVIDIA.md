<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7b08e277df2a9307f861ae54bc30c772",
  "translation_date": "2025-07-16T19:37:33+00:00",
  "source_file": "md/01.Introduction/02/06.NVIDIA.md",
  "language_code": "no"
}
-->
## Phi-familien i NVIDIA NIM

NVIDIA NIM er et sett med brukervennlige mikrotjenester designet for å akselerere utrullingen av generative AI-modeller på tvers av skyen, datasenteret og arbeidsstasjoner. NIM-er kategoriseres etter modelfamilie og per modell. For eksempel bringer NVIDIA NIM for store språkmodeller (LLMs) kraften til toppmoderne LLM-er til bedriftsapplikasjoner, og tilbyr enestående evner innen naturlig språkbehandling og forståelse.

NIM gjør det enkelt for IT- og DevOps-team å selvhoste store språkmodeller (LLMs) i sine egne administrerte miljøer, samtidig som utviklere får tilgang til bransjestandard-APIer som lar dem bygge kraftige copiloter, chatboter og AI-assistenter som kan transformere virksomheten deres. Ved å utnytte NVIDIAs banebrytende GPU-akselerasjon og skalerbar utrulling, tilbyr NIM den raskeste veien til inferens med enestående ytelse.

Du kan bruke NVIDIA NIM til å kjøre inferens på Phi-familie-modeller

![nim](../../../../../translated_images/Phi-NIM.09bebb743387ee4a5028d7d4f8fed55e619711b26c8937526b43a2af980f7dcf.no.png)

### **Eksempler - Phi-3-Vision i NVIDIA NIM**

Tenk deg at du har et bilde (`demo.png`) og ønsker å generere Python-kode som behandler dette bildet og lagrer en ny versjon av det (`phi-3-vision.jpg`).

Koden over automatiserer denne prosessen ved å:

1. Sette opp miljøet og nødvendige konfigurasjoner.
2. Lage en prompt som instruerer modellen til å generere den nødvendige Python-koden.
3. Sende prompten til modellen og samle inn den genererte koden.
4. Ekstrahere og kjøre den genererte koden.
5. Vise originalbildet og det behandlede bildet.

Denne tilnærmingen utnytter kraften i AI for å automatisere bildebehandlingsoppgaver, noe som gjør det enklere og raskere å nå målene dine.

[Sample Code Solution](../../../../../code/06.E2E/E2E_Nvidia_NIM_Phi3_Vision.ipynb)

La oss gå gjennom hva hele koden gjør, steg for steg:

1. **Installer nødvendig pakke**:
    ```python
    !pip install langchain_nvidia_ai_endpoints -U
    ```
    Denne kommandoen installerer `langchain_nvidia_ai_endpoints`-pakken, og sikrer at det er den nyeste versjonen.

2. **Importer nødvendige moduler**:
    ```python
    from langchain_nvidia_ai_endpoints import ChatNVIDIA
    import getpass
    import os
    import base64
    ```
    Disse importene henter inn nødvendige moduler for å samhandle med NVIDIA AI-endepunktene, håndtere passord sikkert, samhandle med operativsystemet, og kode/dekode data i base64-format.

3. **Sett opp API-nøkkel**:
    ```python
    if not os.getenv("NVIDIA_API_KEY"):
        os.environ["NVIDIA_API_KEY"] = getpass.getpass("Enter your NVIDIA API key: ")
    ```
    Denne koden sjekker om miljøvariabelen `NVIDIA_API_KEY` er satt. Hvis ikke, ber den brukeren om å skrive inn API-nøkkelen på en sikker måte.

4. **Definer modell og bildefilbane**:
    ```python
    model = 'microsoft/phi-3-vision-128k-instruct'
    chat = ChatNVIDIA(model=model)
    img_path = './imgs/demo.png'
    ```
    Dette setter hvilken modell som skal brukes, oppretter en instans av `ChatNVIDIA` med den angitte modellen, og definerer banen til bildefilen.

5. **Lag tekstprompt**:
    ```python
    text = "Please create Python code for image, and use plt to save the new picture under imgs/ and name it phi-3-vision.jpg."
    ```
    Dette definerer en tekstprompt som instruerer modellen til å generere Python-kode for å behandle et bilde.

6. **Kod bildet i base64**:
    ```python
    with open(img_path, "rb") as f:
        image_b64 = base64.b64encode(f.read()).decode()
    image = f'<img src="data:image/png;base64,{image_b64}" />'
    ```
    Denne koden leser bildefilen, koder den i base64, og lager en HTML-bildetag med den kodede dataen.

7. **Kombiner tekst og bilde til en prompt**:
    ```python
    prompt = f"{text} {image}"
    ```
    Dette kombinerer tekstprompten og HTML-bildetagen til en enkelt streng.

8. **Generer kode med ChatNVIDIA**:
    ```python
    code = ""
    for chunk in chat.stream(prompt):
        print(chunk.content, end="")
        code += chunk.content
    ```
    Denne koden sender prompten til `ChatNVIDIA`-modellen og samler inn den genererte koden i biter, skriver ut og legger til hver bit i `code`-strengen.

9. **Ekstraher Python-kode fra generert innhold**:
    ```python
    begin = code.index('```python') + 9
    code = code[begin:]
    end = code.index('```')
    code = code[:end]
    ```
    Dette trekker ut den faktiske Python-koden fra det genererte innholdet ved å fjerne markdown-formateringen.

10. **Kjør den genererte koden**:
    ```python
    import subprocess
    result = subprocess.run(["python", "-c", code], capture_output=True)
    ```
    Dette kjører den ekstraherte Python-koden som en subprocess og fanger opp outputen.

11. **Vis bilder**:
    ```python
    from IPython.display import Image, display
    display(Image(filename='./imgs/phi-3-vision.jpg'))
    display(Image(filename='./imgs/demo.png'))
    ```
    Disse linjene viser bildene ved hjelp av `IPython.display`-modulen.

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter nøyaktighet, vennligst vær oppmerksom på at automatiske oversettelser kan inneholde feil eller unøyaktigheter. Det opprinnelige dokumentet på originalspråket skal anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.