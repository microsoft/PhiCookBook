<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3dbbf568625b1ee04b354c2dc81d3248",
  "translation_date": "2025-07-17T04:26:52+00:00",
  "source_file": "md/02.Application/02.Code/Phi3/VSCodeExt/HOL/Apple/02.PromptflowWithMLX.md",
  "language_code": "no"
}
-->
# **Lab 2 - Kjør Prompt flow med Phi-3-mini i AIPC**

## **Hva er Prompt flow**

Prompt flow er en samling utviklingsverktøy designet for å effektivisere hele utviklingssyklusen for LLM-baserte AI-applikasjoner, fra idéutvikling, prototyping, testing, evaluering til produksjonssetting og overvåking. Det gjør prompt engineering mye enklere og lar deg bygge LLM-apper med produksjonskvalitet.

Med prompt flow kan du:

- Lage flows som kobler sammen LLM-er, prompts, Python-kode og andre verktøy i en kjørbar arbeidsflyt.

- Feilsøke og iterere på flowene dine, spesielt interaksjonen med LLM-er, på en enkel måte.

- Evaluere flowene dine, beregne kvalitets- og ytelsesmetrikker med større datasett.

- Integrere testing og evaluering i CI/CD-systemet ditt for å sikre kvaliteten på flowen.

- Distribuere flowene dine til den serveringsplattformen du velger, eller enkelt integrere dem i appens kodebase.

- (Valgfritt, men sterkt anbefalt) Samarbeide med teamet ditt ved å bruke skyversjonen av Prompt flow i Azure AI.



## **Bygge generasjonskode-flows på Apple Silicon**

***Note*** ：Hvis du ikke har fullført miljøinstallasjonen, vennligst besøk [Lab 0 -Installations](./01.Installations.md)

1. Åpne Prompt flow-utvidelsen i Visual Studio Code og opprett et tomt flow-prosjekt

![create](../../../../../../../../../translated_images/no/pf_create.bde888dc83502eba.png)

2. Legg til Inputs og Outputs parametere og legg til Python-kode som ny flow

![flow](../../../../../../../../../translated_images/no/pf_flow.520824c0969f2a94.png)


Du kan bruke denne strukturen (flow.dag.yaml) som referanse for å bygge flowen din

```yaml

inputs:
  prompt:
    type: string
    default: Write python code for Fibonacci serie. Please use markdown as output
outputs:
  result:
    type: string
    reference: ${gen_code_by_phi3.output}
nodes:
- name: gen_code_by_phi3
  type: python
  source:
    type: code
    path: gen_code_by_phi3.py
  inputs:
    prompt: ${inputs.prompt}


```

3. Kvantifiser phi-3-mini

Vi ønsker å kjøre SLM bedre på lokale enheter. Generelt kvantifiserer vi modellen (INT4, FP16, FP32)


```bash

python -m mlx_lm.convert --hf-path microsoft/Phi-3-mini-4k-instruct

```

**Note:** standardmappe er mlx_model 

4. Legg til kode i ***Chat_With_Phi3.py***


```python


from promptflow import tool

from mlx_lm import load, generate


# The inputs section will change based on the arguments of the tool function, after you save the code
# Adding type to arguments and return value will help the system show the types properly
# Please update the function name/signature per need
@tool
def my_python_tool(prompt: str) -> str:

    model_id = './mlx_model_phi3_mini'

    model, tokenizer = load(model_id)

    # <|user|>\nWrite python code for Fibonacci serie. Please use markdown as output<|end|>\n<|assistant|>

    response = generate(model, tokenizer, prompt="<|user|>\n" + prompt  + "<|end|>\n<|assistant|>", max_tokens=2048, verbose=True)

    return response


```

4. Du kan teste flowen fra Debug eller Run for å sjekke om generasjonskoden fungerer som den skal

![RUN](../../../../../../../../../translated_images/no/pf_run.4239e8a0b420a582.png)

5. Kjør flow som utviklings-API i terminalen

```

pf flow serve --source ./ --port 8080 --host localhost   

```

Du kan teste det i Postman / Thunder Client


### **Note**

1. Første kjøring tar lang tid. Det anbefales å laste ned phi-3-modellen via Hugging face CLI.

2. Med tanke på den begrensede regnekraften til Intel NPU, anbefales det å bruke Phi-3-mini-4k-instruct

3. Vi bruker Intel NPU-akselerasjon for å kvantifisere til INT4, men hvis du kjører tjenesten på nytt, må du slette cache- og nc_workshop-mappene.



## **Ressurser**

1. Lær Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Lær Intel NPU Acceleration [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Eksempelkode, last ned [Local NPU Agent Sample Code](../../../../../../../../../code/07.Lab/01/AIPC/local-npu-agent)

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter nøyaktighet, vennligst vær oppmerksom på at automatiske oversettelser kan inneholde feil eller unøyaktigheter. Det opprinnelige dokumentet på originalspråket skal anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.