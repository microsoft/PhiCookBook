{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaktiv Phi 3 Mini 4K Instruksjonschatbot med Whisper\n",
    "\n",
    "### Introduksjon:\n",
    "Den interaktive Phi 3 Mini 4K instruksjonschatboten er et verktøy som lar brukere samhandle med Microsoft Phi 3 Mini 4K instruksjonsdemo ved hjelp av tekst- eller lydinnspill. Chatboten kan brukes til en rekke oppgaver, som oversettelse, værmeldinger og generell informasjonsinnhenting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[Opprett din Huggingface Access Token](https://huggingface.co/settings/tokens)\n",
    "\n",
    "Opprett en ny token  \n",
    "Gi den et nytt navn  \n",
    "Velg skrive-tillatelser  \n",
    "Kopier token og lagre den på et trygt sted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den følgende Python-koden utfører to hovedoppgaver: importering av `os`-modulen og setting av en miljøvariabel.\n",
    "\n",
    "1. Importering av `os`-modulen:\n",
    "   - `os`-modulen i Python gir en måte å interagere med operativsystemet på. Den lar deg utføre ulike oppgaver relatert til operativsystemet, som å få tilgang til miljøvariabler, jobbe med filer og kataloger, osv.\n",
    "   - I denne koden blir `os`-modulen importert ved hjelp av `import`-setningen. Denne setningen gjør funksjonaliteten til `os`-modulen tilgjengelig for bruk i det gjeldende Python-skriptet.\n",
    "\n",
    "2. Setting av en miljøvariabel:\n",
    "   - En miljøvariabel er en verdi som kan nås av programmer som kjører på operativsystemet. Det er en måte å lagre konfigurasjonsinnstillinger eller annen informasjon som kan brukes av flere programmer.\n",
    "   - I denne koden blir en ny miljøvariabel satt ved hjelp av `os.environ`-ordboken. Nøkkelen i ordboken er `'HF_TOKEN'`, og verdien blir tildelt fra variabelen `HUGGINGFACE_TOKEN`.\n",
    "   - Variabelen `HUGGINGFACE_TOKEN` er definert rett over denne kodebiten, og den blir tildelt en strengverdi `\"hf_**************\"` ved hjelp av `#@param`-syntaksen. Denne syntaksen brukes ofte i Jupyter-notebooks for å tillate brukerinput og parameterkonfigurasjon direkte i notebook-grensesnittet.\n",
    "   - Ved å sette miljøvariabelen `'HF_TOKEN'` kan den nås av andre deler av programmet eller andre programmer som kjører på det samme operativsystemet.\n",
    "\n",
    "Oppsummert importerer denne koden `os`-modulen og setter en miljøvariabel med navnet `'HF_TOKEN'` med verdien som er angitt i variabelen `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denne kodebiten definerer en funksjon kalt clear_output som brukes til å fjerne utdataene fra den nåværende cellen i Jupyter Notebook eller IPython. La oss bryte ned koden og forstå dens funksjonalitet:\n",
    "\n",
    "Funksjonen clear_output tar én parameter kalt wait, som er en boolsk verdi. Som standard er wait satt til False. Denne parameteren avgjør om funksjonen skal vente til nye utdata er tilgjengelige for å erstatte de eksisterende utdataene før de fjernes.\n",
    "\n",
    "Selve funksjonen brukes til å fjerne utdataene fra den nåværende cellen. I Jupyter Notebook eller IPython, når en celle produserer utdata, som for eksempel tekst som er skrevet ut eller grafiske diagrammer, vises disse utdataene under cellen. Funksjonen clear_output lar deg fjerne disse utdataene.\n",
    "\n",
    "Implementeringen av funksjonen er ikke gitt i kodebiten, som indikert av ellipsen (...). Ellipsen representerer en plassholder for den faktiske koden som utfører fjerningen av utdataene. Implementeringen av funksjonen kan innebære interaksjon med Jupyter Notebook- eller IPython-APIet for å fjerne de eksisterende utdataene fra cellen.\n",
    "\n",
    "Alt i alt gir denne funksjonen en praktisk måte å fjerne utdataene fra den nåværende cellen i Jupyter Notebook eller IPython, noe som gjør det enklere å administrere og oppdatere de viste utdataene under interaktive kodingsøkter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utfør tekst-til-tale (TTS) ved hjelp av Edge TTS-tjenesten. La oss gå gjennom de relevante funksjonsimplementasjonene én etter én:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Denne funksjonen tar en inputverdi og beregner hastighetsstrengen for TTS-stemmen. Inputverdien representerer ønsket talehastighet, der en verdi på 1 representerer normal hastighet. Funksjonen beregner hastighetsstrengen ved å trekke 1 fra inputverdien, multiplisere den med 100, og deretter bestemme fortegnet basert på om inputverdien er større enn eller lik 1. Funksjonen returnerer hastighetsstrengen i formatet \"{sign}{rate}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: Denne funksjonen tar en inputtekst og et språk som parametere. Den deler opp inputteksten i deler basert på språkspesifikke regler. I denne implementasjonen, hvis språket er \"English\", deler funksjonen teksten ved hvert punktum (\".\") og fjerner eventuelle innledende eller avsluttende mellomrom. Den legger deretter til et punktum på slutten av hver del og returnerer den filtrerte listen med deler.\n",
    "\n",
    "3. `tts_file_name(text)`: Denne funksjonen genererer et filnavn for TTS-lydfilen basert på inputteksten. Den utfører flere transformasjoner på teksten: fjerner et avsluttende punktum (hvis til stede), konverterer teksten til små bokstaver, fjerner innledende og avsluttende mellomrom, og erstatter mellomrom med understreker. Deretter trunkerer den teksten til maksimalt 25 tegn (hvis lengre) eller bruker hele teksten hvis den er tom. Til slutt genererer den en tilfeldig streng ved hjelp av [`uuid`]-modulen og kombinerer den med den trunkerte teksten for å lage filnavnet i formatet \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Denne funksjonen slår sammen flere lydfiler til én enkelt lydfil. Den tar en liste med lydfilbaner og en utbanesti som parametere. Funksjonen initialiserer et tomt `AudioSegment`-objekt kalt [`merged_audio`]. Deretter itererer den gjennom hver lydfilbane, laster lydfilen ved hjelp av `AudioSegment.from_file()`-metoden fra `pydub`-biblioteket, og legger til den aktuelle lydfilen i [`merged_audio`]-objektet. Til slutt eksporterer den den sammenslåtte lyden til den spesifiserte utbanen i MP3-format.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: Denne funksjonen utfører TTS-operasjonen ved hjelp av Edge TTS-tjenesten. Den tar en liste med tekstdeler, talehastigheten, stemmens navn og lagringsbanen som parametere. Hvis antallet deler er større enn 1, oppretter funksjonen en katalog for lagring av de individuelle lydfilene for hver del. Deretter itererer den gjennom hver del, konstruerer en Edge TTS-kommando ved hjelp av `calculate_rate_string()`-funksjonen, stemmens navn og tekstdelen, og utfører kommandoen ved hjelp av `os.system()`-funksjonen. Hvis kommandoutførelsen er vellykket, legger den til banen til den genererte lydfilen i en liste. Etter å ha behandlet alle delene, slår den sammen de individuelle lydfilene ved hjelp av `merge_audio_files()`-funksjonen og lagrer den sammenslåtte lyden til den spesifiserte lagringsbanen. Hvis det bare er én del, genererer den direkte Edge TTS-kommandoen og lagrer lyden til lagringsbanen. Til slutt returnerer den lagringsbanen til den genererte lydfilen.\n",
    "\n",
    "6. `random_audio_name_generate()`: Denne funksjonen genererer et tilfeldig lydfilnavn ved hjelp av [`uuid`]-modulen. Den genererer en tilfeldig UUID, konverterer den til en streng, tar de første 8 tegnene, legger til \".mp3\"-utvidelsen, og returnerer det tilfeldige lydfilnavnet.\n",
    "\n",
    "7. `talk(input_text)`: Denne funksjonen er hovedinngangspunktet for å utføre TTS-operasjonen. Den tar en inputtekst som parameter. Den sjekker først lengden på inputteksten for å avgjøre om det er en lang setning (større enn eller lik 600 tegn). Basert på lengden og verdien av variabelen `translate_text_flag`, bestemmer den språket og genererer listen med tekstdeler ved hjelp av `make_chunks()`-funksjonen. Deretter genererer den en lagringsbane for lydfilen ved hjelp av `random_audio_name_generate()`-funksjonen. Til slutt kaller den `edge_free_tts()`-funksjonen for å utføre TTS-operasjonen og returnerer lagringsbanen til den genererte lydfilen.\n",
    "\n",
    "Samlet sett arbeider disse funksjonene sammen for å dele opp inputteksten i deler, generere et filnavn for lydfilen, utføre TTS-operasjonen ved hjelp av Edge TTS-tjenesten, og slå sammen de individuelle lydfilene til én enkelt lydfil.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementeringen av to funksjoner: convert_to_text og run_text_prompt, samt deklarasjonen av to klasser: str og Audio.\n",
    "\n",
    "Funksjonen convert_to_text tar en audio_path som input og transkriberer lyd til tekst ved hjelp av en modell kalt whisper_model. Funksjonen sjekker først om gpu-flagget er satt til True. Hvis det er tilfelle, brukes whisper_model med visse parametere som word_timestamps=True, fp16=True, language='English', og task='translate'. Hvis gpu-flagget er False, brukes whisper_model med fp16=False. Den resulterende transkripsjonen lagres deretter i en fil kalt 'scan.txt' og returneres som tekst.\n",
    "\n",
    "Funksjonen run_text_prompt tar en melding og en chat_history som input. Den bruker phi_demo-funksjonen til å generere et svar fra en chatbot basert på inputmeldingen. Det genererte svaret sendes deretter til talk-funksjonen, som konverterer svaret til en lydfil og returnerer filbanen. Audio-klassen brukes til å vise og spille av lydfilen. Lyden vises ved hjelp av display-funksjonen fra IPython.display-modulen, og Audio-objektet opprettes med autoplay=True-parameteren, slik at lyden starter automatisk. Chat_history oppdateres med inputmeldingen og det genererte svaret, og en tom streng samt den oppdaterte chat_history returneres.\n",
    "\n",
    "Str-klassen er en innebygd klasse i Python som representerer en sekvens av tegn. Den tilbyr ulike metoder for å manipulere og arbeide med strenger, som capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill, og flere. Disse metodene lar deg utføre operasjoner som søk, erstatning, formatering og manipulering av strenger.\n",
    "\n",
    "Audio-klassen er en tilpasset klasse som representerer et lydobjekt. Den brukes til å lage en lydspiller i Jupyter Notebook-miljøet. Klassen aksepterer ulike parametere som data, filename, url, embed, rate, autoplay og normalize. Data-parameteren kan være en numpy-array, en liste med prøver, en streng som representerer et filnavn eller URL, eller rå PCM-data. Filename-parameteren brukes til å spesifisere en lokal fil for å laste inn lyddata fra, og url-parameteren brukes til å spesifisere en URL for å laste ned lyddata fra. Embed-parameteren avgjør om lyddataene skal integreres ved hjelp av en data-URI eller refereres fra den opprinnelige kilden. Rate-parameteren spesifiserer samplingsfrekvensen for lyddataene. Autoplay-parameteren avgjør om lyden skal starte automatisk. Normalize-parameteren spesifiserer om lyddataene skal normaliseres (skaleres) til det maksimale mulige området. Audio-klassen tilbyr også metoder som reload for å laste inn lyddata på nytt fra fil eller URL, og attributter som src_attr, autoplay_attr og element_id_attr for å hente de tilsvarende attributtene for lydelementet i HTML.\n",
    "\n",
    "Samlet sett brukes disse funksjonene og klassene til å transkribere lyd til tekst, generere lydsvar fra en chatbot, og vise og spille av lyd i Jupyter Notebook-miljøet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfraskrivelse**:  \nDette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter nøyaktighet, vær oppmerksom på at automatiserte oversettelser kan inneholde feil eller unøyaktigheter. Det originale dokumentet på sitt opprinnelige språk bør anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T23:16:02+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "no"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}