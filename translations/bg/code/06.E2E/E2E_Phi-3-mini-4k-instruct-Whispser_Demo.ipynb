{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Интерактивен Phi 3 Mini 4K Instruct Chatbot с Whisper\n",
    "\n",
    "### Въведение:\n",
    "Интерактивният Phi 3 Mini 4K Instruct Chatbot е инструмент, който позволява на потребителите да взаимодействат с демонстрацията на Microsoft Phi 3 Mini 4K instruct чрез текстов или аудио вход. Чатботът може да се използва за различни задачи, като превод, актуализации за времето и събиране на обща информация.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "Създайте вашия Huggingface Access Token\n",
    "\n",
    "Създайте нов токен  \n",
    "Дайте ново име  \n",
    "Изберете права за запис  \n",
    "Копирайте токена и го запазете на сигурно място\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следният Python код изпълнява две основни задачи: импортиране на модула `os` и задаване на променлива на средата.\n",
    "\n",
    "1. Импортиране на модула `os`:\n",
    "   - Модулът `os` в Python предоставя начин за взаимодействие с операционната система. Той позволява изпълнение на различни задачи, свързани с операционната система, като достъп до променливи на средата, работа с файлове и директории и други.\n",
    "   - В този код модулът `os` се импортира чрез командата `import`. Тази команда прави функционалността на модула `os` достъпна за използване в текущия Python скрипт.\n",
    "\n",
    "2. Задаване на променлива на средата:\n",
    "   - Променлива на средата е стойност, която може да бъде достъпна от програми, работещи на операционната система. Това е начин за съхранение на настройки за конфигурация или друга информация, която може да бъде използвана от множество програми.\n",
    "   - В този код се задава нова променлива на средата чрез речника `os.environ`. Ключът на речника е `'HF_TOKEN'`, а стойността се задава от променливата `HUGGINGFACE_TOKEN`.\n",
    "   - Променливата `HUGGINGFACE_TOKEN` е дефинирана точно над този кодов фрагмент и ѝ е присвоена текстова стойност `\"hf_**************\"` чрез синтаксиса `#@param`. Този синтаксис често се използва в Jupyter notebooks, за да позволи въвеждане от потребителя и конфигуриране на параметри директно в интерфейса на notebook-а.\n",
    "   - Чрез задаването на променливата на средата `'HF_TOKEN'`, тя може да бъде достъпна от други части на програмата или от други програми, работещи на същата операционна система.\n",
    "\n",
    "Като цяло, този код импортира модула `os` и задава променлива на средата с име `'HF_TOKEN'` със стойността, предоставена в променливата `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Този кодов фрагмент дефинира функция, наречена clear_output, която се използва за изчистване на изхода на текущата клетка в Jupyter Notebook или IPython. Нека разгледаме кода и разберем неговата функционалност:\n",
    "\n",
    "Функцията clear_output приема един параметър, наречен wait, който е булева стойност. По подразбиране wait е зададен на False. Този параметър определя дали функцията трябва да изчака, докато новият изход стане достъпен, за да замени съществуващия изход, преди да го изчисти.\n",
    "\n",
    "Самата функция се използва за изчистване на изхода на текущата клетка. В Jupyter Notebook или IPython, когато клетка генерира изход, като например отпечатан текст или графични диаграми, този изход се показва под клетката. Функцията clear_output позволява да изчистите този изход.\n",
    "\n",
    "Реализацията на функцията не е предоставена в кодовия фрагмент, както е указано чрез многоточието (...). Многоточието представлява заместител за действителния код, който извършва изчистването на изхода. Реализацията на функцията може да включва взаимодействие с API на Jupyter Notebook или IPython, за да се премахне съществуващият изход от клетката.\n",
    "\n",
    "Като цяло, тази функция предоставя удобен начин за изчистване на изхода на текущата клетка в Jupyter Notebook или IPython, което улеснява управлението и актуализирането на показания изход по време на интерактивни сесии за програмиране.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извършване на текст-към-говор (TTS) с помощта на Edge TTS услугата. Нека разгледаме съответните реализации на функциите една по една:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Тази функция приема входна стойност и изчислява скоростта на гласа за TTS. Входната стойност представлява желаната скорост на речта, където стойност 1 означава нормална скорост. Функцията изчислява скоростта, като изважда 1 от входната стойност, умножава я по 100 и определя знака в зависимост от това дали входната стойност е по-голяма или равна на 1. Функцията връща скоростта във формат \"{sign}{rate}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: Тази функция приема текст и език като параметри. Тя разделя текста на части според езиково специфични правила. В тази имплементация, ако езикът е \"English\", функцията разделя текста при всяка точка (\".\") и премахва водещите и завършващите интервали. След това добавя точка към всяка част и връща филтрирания списък с части.\n",
    "\n",
    "3. `tts_file_name(text)`: Тази функция генерира име за аудио файла на TTS въз основа на входния текст. Тя извършва няколко трансформации върху текста: премахва завършваща точка (ако има), преобразува текста в малки букви, премахва водещите и завършващите интервали и заменя интервалите с долни черти. След това съкращава текста до максимум 25 символа (ако е по-дълъг) или използва целия текст, ако е празен. Накрая генерира случайна низ с помощта на модула [`uuid`] и го комбинира със съкратения текст, за да създаде име на файла във формат \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Тази функция обединява множество аудио файлове в един. Тя приема списък с пътища към аудио файлове и изходен път като параметри. Функцията инициализира празен обект `AudioSegment`, наречен [`merged_audio`]. След това преминава през всеки път към аудио файл, зарежда аудио файла с помощта на метода `AudioSegment.from_file()` от библиотеката `pydub` и добавя текущия аудио файл към обекта [`merged_audio`]. Накрая експортира обединеното аудио към посочения изходен път в MP3 формат.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: Тази функция извършва TTS операция с помощта на Edge TTS услугата. Тя приема списък с текстови части, скорост на речта, име на гласа и път за запазване като параметри. Ако броят на частите е повече от 1, функцията създава директория за съхранение на индивидуалните аудио файлове. След това преминава през всяка част, конструира Edge TTS команда с помощта на функцията `calculate_rate_string()`, името на гласа и текста на частта и изпълнява командата с помощта на функцията `os.system()`. Ако изпълнението на командата е успешно, добавя пътя на генерирания аудио файл към списък. След обработката на всички части, обединява индивидуалните аудио файлове с помощта на функцията `merge_audio_files()` и запазва обединеното аудио към посочения път за запазване. Ако има само една част, директно генерира Edge TTS команда и запазва аудиото към пътя за запазване. Накрая връща пътя за запазване на генерирания аудио файл.\n",
    "\n",
    "6. `random_audio_name_generate()`: Тази функция генерира случайно име за аудио файл с помощта на модула [`uuid`]. Тя генерира случайно UUID, преобразува го в низ, взема първите 8 символа, добавя разширението \".mp3\" и връща случайното име на аудио файла.\n",
    "\n",
    "7. `talk(input_text)`: Тази функция е основната входна точка за извършване на TTS операция. Тя приема входен текст като параметър. Първо проверява дължината на входния текст, за да определи дали е дълго изречение (по-голямо или равно на 600 символа). Въз основа на дължината и стойността на променливата `translate_text_flag`, определя езика и генерира списък с текстови части с помощта на функцията `make_chunks()`. След това генерира път за запазване на аудио файла с помощта на функцията `random_audio_name_generate()`. Накрая извиква функцията `edge_free_tts()` за извършване на TTS операцията и връща пътя за запазване на генерирания аудио файл.\n",
    "\n",
    "Като цяло, тези функции работят заедно, за да разделят входния текст на части, генерират име за аудио файла, извършват TTS операция с помощта на Edge TTS услугата и обединяват индивидуалните аудио файлове в един аудио файл.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация на две функции: convert_to_text и run_text_prompt, както и декларация на два класа: str и Audio.\n",
    "\n",
    "Функцията convert_to_text приема audio_path като вход и транскрибира аудиото в текст, използвайки модел, наречен whisper_model. Функцията първо проверява дали флагът gpu е зададен на True. Ако е така, whisper_model се използва с определени параметри като word_timestamps=True, fp16=True, language='English' и task='translate'. Ако флагът gpu е False, whisper_model се използва с fp16=False. Получената транскрипция след това се записва във файл с име 'scan.txt' и се връща като текст.\n",
    "\n",
    "Функцията run_text_prompt приема съобщение и chat_history като вход. Тя използва функцията phi_demo, за да генерира отговор от чатбот въз основа на входното съобщение. Генерираният отговор след това се предава на функцията talk, която конвертира отговора в аудио файл и връща пътя до файла. Класът Audio се използва за показване и възпроизвеждане на аудио файла. Аудиото се показва чрез функцията display от модула IPython.display, а обектът Audio се създава с параметъра autoplay=True, така че аудиото да започне да се възпроизвежда автоматично. chat_history се актуализира с входното съобщение и генерирания отговор, след което се връщат празен низ и актуализираният chat_history.\n",
    "\n",
    "Класът str е вграден клас в Python, който представлява последователност от символи. Той предоставя различни методи за манипулиране и работа със стрингове, като например capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill и други. Тези методи позволяват извършване на операции като търсене, заместване, форматиране и манипулиране на стрингове.\n",
    "\n",
    "Класът Audio е персонализиран клас, който представлява аудио обект. Той се използва за създаване на аудио плейър в средата на Jupyter Notebook. Класът приема различни параметри като data, filename, url, embed, rate, autoplay и normalize. Параметърът data може да бъде numpy масив, списък от семпли, низ, представляващ име на файл или URL, или сурови PCM данни. Параметърът filename се използва за посочване на локален файл, от който да се заредят аудио данните, а параметърът url се използва за посочване на URL, от който да се изтеглят аудио данните. Параметърът embed определя дали аудио данните трябва да бъдат вградени чрез data URI или да се реферират от оригиналния източник. Параметърът rate указва честотата на семплиране на аудио данните. Параметърът autoplay определя дали аудиото трябва да започне да се възпроизвежда автоматично. Параметърът normalize указва дали аудио данните трябва да бъдат нормализирани (мащабирани) до максималния възможен обхват. Класът Audio също предоставя методи като reload за презареждане на аудио данните от файл или URL, както и атрибути като src_attr, autoplay_attr и element_id_attr за извличане на съответните атрибути за аудио елемента в HTML.\n",
    "\n",
    "Като цяло, тези функции и класове се използват за транскрибиране на аудио в текст, генериране на аудио отговори от чатбот и показване и възпроизвеждане на аудио в средата на Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Отказ от отговорност**:  \nТози документ е преведен с помощта на AI услуга за превод [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматичните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия изходен език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален превод от човек. Ние не носим отговорност за каквито и да е недоразумения или погрешни интерпретации, произтичащи от използването на този превод.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-13T07:02:51+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "bg"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}