<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "006e8cf75211d3297f24e1b22e38955f",
  "translation_date": "2025-05-09T18:34:30+00:00",
  "source_file": "md/02.Application/01.TextAndChat/Phi3/E2E_Phi-3-mini_with_whisper.md",
  "language_code": "bg"
}
-->
# Interactive Phi 3 Mini 4K Instruct Chatbot с Whisper

## Преглед

Interactive Phi 3 Mini 4K Instruct Chatbot е инструмент, който позволява на потребителите да взаимодействат с Microsoft Phi 3 Mini 4K instruct демото чрез текстов или аудио вход. Чатботът може да се използва за различни задачи, като превод, актуализации за времето и събиране на обща информация.

### Първи стъпки

За да използвате този чатбот, просто следвайте тези инструкции:

1. Отворете нов [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb)
2. В главния прозорец на ноутбука ще видите интерфейс на чат с текстово поле и бутон „Send“.
3. За да използвате текстовия чатбот, просто напишете съобщението си в текстовото поле и натиснете бутона „Send“. Чатботът ще отговори с аудио файл, който може да се пусне директно в ноутбука.

**Note**: Този инструмент изисква GPU и достъп до моделите Microsoft Phi-3 и OpenAI Whisper, които се използват за разпознаване на реч и превод.

### Изисквания за GPU

За да стартирате това демо, ви е необходима 12GB видео памет.

Изискванията за памет при стартиране на **Microsoft-Phi-3-Mini-4K instruct** демото на GPU зависят от няколко фактора, като размер на входните данни (аудио или текст), езикът за превод, скоростта на модела и наличната памет на GPU.

Обикновено моделът Whisper е проектиран да работи на GPU. Препоръчителното минимално количество GPU памет за работа с Whisper е 8 GB, но той може да използва и по-големи количества памет при нужда.

Важно е да се има предвид, че работа с голям обем данни или много заявки към модела може да изисква повече GPU памет и/или да доведе до проблеми с производителността. Препоръчва се да тествате своя случай на употреба с различни конфигурации и да наблюдавате използването на паметта, за да определите оптималните настройки за вашите нужди.

## E2E пример за Interactive Phi 3 Mini 4K Instruct Chatbot с Whisper

Jupyter ноутбукът с заглавие [Interactive Phi 3 Mini 4K Instruct Chatbot с Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) показва как да използвате Microsoft Phi 3 Mini 4K instruct демото за генериране на текст от аудио или писмен текстов вход. В ноутбука са дефинирани няколко функции:

1. `tts_file_name(text)`: Тази функция генерира име на файл въз основа на входния текст за запазване на генерирания аудио файл.
1. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: Тази функция използва Edge TTS API за генериране на аудио файл от списък с парчета входен текст. Входните параметри са списъкът с парчета, скорост на речта, име на гласа и път за записване на аудио файла.
1. `talk(input_text)`: Тази функция генерира аудио файл чрез Edge TTS API и го записва с произволно име във /content/audio директорията. Входният параметър е текстът, който трябва да се преобразува в реч.
1. `run_text_prompt(message, chat_history)`: Тази функция използва Microsoft Phi 3 Mini 4K instruct демото за генериране на аудио файл от входящо съобщение и го добавя към историята на чата.
1. `run_audio_prompt(audio, chat_history)`: Тази функция преобразува аудио файл в текст чрез Whisper модел API и предава резултата на функцията `run_text_prompt()`.
1. Кодът стартира Gradio приложение, което позволява на потребителите да взаимодействат с Phi 3 Mini 4K instruct демото, като въвеждат съобщения или качват аудио файлове. Резултатът се показва като текстово съобщение в приложението.

## Отстраняване на проблеми

Инсталиране на Cuda GPU драйвери

1. Уверете се, че вашите Linux приложения са актуални

    ```bash
    sudo apt update
    ```

1. Инсталирайте Cuda драйвери

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. Регистрирайте местоположението на cuda драйвера

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. Проверка на размера на Nvidia GPU паметта (Изисква се 12GB GPU памет)

    ```bash
    nvidia-smi
    ```

1. Изчистване на кеша: Ако използвате PyTorch, можете да извикате torch.cuda.empty_cache(), за да освободите всички неизползвани кеширани памети, така че те да могат да бъдат използвани от други GPU приложения

    ```python
    torch.cuda.empty_cache() 
    ```

1. Проверка на Nvidia Cuda

    ```bash
    nvcc --version
    ```

1. Изпълнете следните стъпки, за да създадете Hugging Face токен.

    - Отидете на [Hugging Face Token Settings page](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo).
    - Изберете **New token**.
    - Въведете име на проекта, който искате да използвате.
    - Изберете **Type** като **Write**.

> **Note**
>
> Ако получите следната грешка:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> За да я разрешите, въведете следната команда в терминала си.
>
> ```bash
> sudo ldconfig
> ```

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI преводаческа услуга [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля имайте предвид, че автоматизираните преводи могат да съдържат грешки или неточности. Оригиналният документ на неговия оригинален език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да е недоразумения или неправилни тълкувания, произтичащи от използването на този превод.