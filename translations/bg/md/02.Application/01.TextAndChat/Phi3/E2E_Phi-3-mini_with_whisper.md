<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "006e8cf75211d3297f24e1b22e38955f",
  "translation_date": "2025-07-17T02:23:06+00:00",
  "source_file": "md/02.Application/01.TextAndChat/Phi3/E2E_Phi-3-mini_with_whisper.md",
  "language_code": "bg"
}
-->
# Interactive Phi 3 Mini 4K Instruct Chatbot с Whisper

## Преглед

Interactive Phi 3 Mini 4K Instruct Chatbot е инструмент, който позволява на потребителите да взаимодействат с демонстрацията Microsoft Phi 3 Mini 4K instruct чрез текстов или аудио вход. Чатботът може да се използва за различни задачи, като превод, актуализации за времето и общо събиране на информация.

### Започване

За да използвате този чатбот, просто следвайте тези инструкции:

1. Отворете нов [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb)
2. В главния прозорец на тетрадката ще видите интерфейс за чат с текстово поле за въвеждане и бутон „Send“.
3. За да използвате текстовия чатбот, просто напишете съобщението си в текстовото поле и натиснете бутона „Send“. Чатботът ще отговори с аудио файл, който може да се пусне директно в тетрадката.

**Note**: Този инструмент изисква GPU и достъп до моделите Microsoft Phi-3 и OpenAI Whisper, които се използват за разпознаване на реч и превод.

### Изисквания за GPU

За да стартирате тази демонстрация, ви трябват 12 GB GPU памет.

Изискванията за памет при стартиране на демонстрацията **Microsoft-Phi-3-Mini-4K instruct** на GPU зависят от няколко фактора, като размер на входните данни (аудио или текст), езика за превод, скоростта на модела и наличната памет на GPU.

Обикновено моделът Whisper е проектиран да работи на GPU. Препоръчителното минимално количество GPU памет за работа с Whisper е 8 GB, но може да поддържа и по-големи обеми памет при нужда.

Важно е да се отбележи, че обработката на голям обем данни или голям брой заявки към модела може да изисква повече GPU памет и/или да доведе до проблеми с производителността. Препоръчително е да тествате вашия случай с различни конфигурации и да следите използването на паметта, за да определите оптималните настройки за вашите конкретни нужди.

## E2E пример за Interactive Phi 3 Mini 4K Instruct Chatbot с Whisper

Jupyter тетрадката със заглавие [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) показва как да използвате Microsoft Phi 3 Mini 4K instruct Demo за генериране на текст от аудио или писмен вход. Тетрадката дефинира няколко функции:

1. `tts_file_name(text)`: Тази функция генерира име на файл въз основа на входния текст за запазване на генерирания аудио файл.
1. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: Тази функция използва Edge TTS API за генериране на аудио файл от списък с части от входния текст. Входните параметри са списъкът с части, скоростта на речта, името на гласа и пътят за запазване на генерирания аудио файл.
1. `talk(input_text)`: Тази функция генерира аудио файл, използвайки Edge TTS API и го записва с произволно име в директорията /content/audio. Входният параметър е текстът, който трябва да се преобразува в реч.
1. `run_text_prompt(message, chat_history)`: Тази функция използва Microsoft Phi 3 Mini 4K instruct demo за генериране на аудио файл от входно съобщение и го добавя към историята на чата.
1. `run_audio_prompt(audio, chat_history)`: Тази функция преобразува аудио файл в текст, използвайки Whisper модел API, и го подава на функцията `run_text_prompt()`.
1. Кодът стартира Gradio приложение, което позволява на потребителите да взаимодействат с Phi 3 Mini 4K instruct demo чрез писане на съобщения или качване на аудио файлове. Резултатът се показва като текстово съобщение в приложението.

## Отстраняване на проблеми

Инсталиране на Cuda GPU драйвери

1. Уверете се, че вашата Linux система е актуална

    ```bash
    sudo apt update
    ```

1. Инсталирайте Cuda драйвери

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. Регистрирайте местоположението на cuda драйвера

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. Проверка на размера на Nvidia GPU паметта (Изисква се 12GB GPU памет)

    ```bash
    nvidia-smi
    ```

1. Изчистване на кеша: Ако използвате PyTorch, можете да извикате torch.cuda.empty_cache(), за да освободите всички неизползвани кеширани памети, така че да могат да се използват от други GPU приложения

    ```python
    torch.cuda.empty_cache() 
    ```

1. Проверка на Nvidia Cuda

    ```bash
    nvcc --version
    ```

1. Изпълнете следните стъпки, за да създадете Hugging Face токен.

    - Отидете на [Hugging Face Token Settings page](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo).
    - Изберете **New token**.
    - Въведете име на проекта, което искате да използвате.
    - Изберете **Type** като **Write**.

> **Note**
>
> Ако получите следната грешка:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> За да я разрешите, въведете следната команда в терминала си.
>
> ```bash
> sudo ldconfig
> ```

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI преводаческа услуга [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи могат да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да е недоразумения или неправилни тълкувания, произтичащи от използването на този превод.