<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7f72d7981ed3640865700f51ae407da4",
  "translation_date": "2026-01-14T16:04:55+00:00",
  "source_file": "md/02.Application/01.TextAndChat/Phi3/E2E_Phi-3-mini_with_whisper.md",
  "language_code": "bg"
}
-->
# Интерактивен Phi 3 Mini 4K Инструктивен Чатбот с Whisper

## Преглед

Интерактивният Phi 3 Mini 4K Инструктивен Чатбот е инструмент, който позволява на потребителите да взаимодействат с демонстрацията Microsoft Phi 3 Mini 4K instruct чрез текстов или аудио вход. Чатботът може да се използва за различни задачи, като превод, прогноза за времето и събиране на обща информация.

### Започване

За да използвате този чатбот, просто следвайте тези инструкции:

1. Отворете нов [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb)
2. В основния прозорец на тетрадката ще видите интерфейс с чатбокс с текстово поле за въвеждане и бутон „Изпрати“.
3. За да използвате текстовия чатбот, просто напишете вашето съобщение в текстовото поле и кликнете бутона „Изпрати“. Чатботът ще отговори с аудио файл, който може да бъде възпроизведен директно в тетрадката.

**Забележка**: Този инструмент изисква GPU и достъп до моделите Microsoft Phi-3 и OpenAI Whisper, които се използват за разпознаване на реч и превод.

### Изисквания за GPU

За да стартирате тази демонстрация ви е нужна 12GB GPU памет.

Изискванията за памет при стартирането на **Microsoft-Phi-3-Mini-4K instruct** демонстрацията на GPU ще зависят от няколко фактора, като размер на входните данни (аудио или текст), езика, използван за превод, скоростта на модела и наличната памет на GPU.

Общо взето, моделът Whisper е предназначен да работи на GPU. Препоръчителното минимално количество GPU памет за работа с Whisper е 8 GB, но може да поеме и по-големи обеми памет при нужда.

Важното е, че обработката на големи количества данни или голям обем заявки към модела може да изисква повече GPU памет и/или да причини проблеми с производителността. Препоръчва се да тествате своя случай с различни настройки и да наблюдавате използването на паметта, за да определите оптималните параметри за вашите конкретни нужди.

## E2E пример за Интерактивен Phi 3 Mini 4K Инструктивен Чатбот с Whisper

Jupyter тетрадката със заглавие [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) показва как да използвате Microsoft Phi 3 Mini 4K instruct Demo за генериране на текст от аудио или написан текст. Тетрадката дефинира няколко функции:

1. `tts_file_name(text)`: Тази функция генерира име на файл на база входния текст за запазване на генерирания аудио файл.
1. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: Тази функция използва Edge TTS API за генериране на аудио файл от списък с части от входен текст. Входните параметри са списъкът с части, скорост на речта, име на гласа и път за запазване на аудио файла.
1. `talk(input_text)`: Тази функция генерира аудио файл чрез Edge TTS API и го записва с произволно име в директория /content/audio. Входният параметър е текстът, който да се преобразува в реч.
1. `run_text_prompt(message, chat_history)`: Тази функция използва Microsoft Phi 3 Mini 4K instruct демонстрацията, за да генерира аудио файл от входно съобщение и го добавя към историята на чата.
1. `run_audio_prompt(audio, chat_history)`: Тази функция преобразува аудио файл в текст с помощта на Whisper модел API и го подава към функцията `run_text_prompt()`.
1. Кодът стартира Gradio приложение, което позволява на потребителите да взаимодействат с Phi 3 Mini 4K instruct демото като въвеждат съобщения или качват аудио файлове. Изходът се показва като текстово съобщение в приложението.

## Отстраняване на проблеми

Инсталиране на Cuda GPU драйвери

1. Уверете се, че вашето Linux приложение е актуализирано

    ```bash
    sudo apt update
    ```

1. Инсталирайте Cuda драйвери

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. Регистрирайте местоположението на cuda драйвера

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. Проверка на размера на Nvidia GPU паметта (Изисква се 12GB GPU памет)

    ```bash
    nvidia-smi
    ```

1. Изчистване на кеша: Ако използвате PyTorch, можете да извикате torch.cuda.empty_cache(), за да освободите цялата неизползвана кеширана памет, така че да може да се използва от други GPU приложения

    ```python
    torch.cuda.empty_cache() 
    ```

1. Проверка на Nvidia Cuda

    ```bash
    nvcc --version
    ```

1. Изпълнете следните действия, за да създадете Hugging Face токен.

    - Отидете на [страницата за настройка на токен на Hugging Face](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo).
    - Изберете **New token**.
    - Въведете проектно **Име**, което искате да използвате.
    - Изберете **Type** да бъде **Write**.

> [!NOTE]
>
> Ако получите следната грешка:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> За да го разрешите, напишете следната команда във вашия терминал.
>
> ```bash
> sudo ldconfig
> ```

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Отказ от отговорност**:  
Този документ е преведен с помощта на AI преводаческа услуга [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматичните преводи могат да съдържат грешки или неточности. Оригиналният документ на неговия език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да е недоразумения или неправилни тълкувания, произтичащи от използването на този превод.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->