<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3dbbf568625b1ee04b354c2dc81d3248",
  "translation_date": "2025-05-09T19:41:48+00:00",
  "source_file": "md/02.Application/02.Code/Phi3/VSCodeExt/HOL/Apple/02.PromptflowWithMLX.md",
  "language_code": "bg"
}
-->
# **Лаборатория 2 - Стартиране на Prompt flow с Phi-3-mini в AIPC**

## **Какво е Prompt flow**

Prompt flow е набор от инструменти за разработка, предназначени да улеснят целия цикъл на разработка на AI приложения, базирани на LLM, от идеен замисъл, прототипиране, тестване, оценка до внедряване в продукция и мониторинг. Той прави инженерството на prompt-ове много по-лесно и ви позволява да създавате LLM приложения с качество за продукция.

С prompt flow ще можете да:

- Създавате потоци, които свързват LLM, prompt-ове, Python код и други инструменти в изпълним работен процес.

- Отстранявате грешки и итеративно подобрявате вашите потоци, особено взаимодействието с LLM, лесно.

- Оценявате потоците си, изчислявате метрики за качество и производителност с по-големи набори от данни.

- Интегрирате тестването и оценката във вашата CI/CD система, за да гарантирате качеството на потока.

- Внедрявате потоците си на избраната платформа за обслужване или ги интегрирате лесно в кода на вашето приложение.

- (По избор, но силно препоръчително) Сътрудничите с вашия екип, използвайки облачната версия на Prompt flow в Azure AI.

## **Създаване на генерационни кодови потоци на Apple Silicon**

***Note*** ：Ако все още не сте завършили инсталацията на средата, моля посетете [Lab 0 -Installations](./01.Installations.md)

1. Отворете Prompt flow разширението във Visual Studio Code и създайте празен проект за поток

![create](../../../../../../../../../translated_images/pf_create.d6172d8277a78a7fa82cd6ff727ed44e037fa78b662f1f62d5963f36d712d229.bg.png)

2. Добавете входни и изходни параметри и добавете Python код като нов поток

![flow](../../../../../../../../../translated_images/pf_flow.d5646a323fb7f444c0b98b4521057a592325c583e7ba18bc31500bc0415e9ef3.bg.png)

Можете да използвате тази структура (flow.dag.yaml), за да конструирате вашия поток

```yaml

inputs:
  prompt:
    type: string
    default: Write python code for Fibonacci serie. Please use markdown as output
outputs:
  result:
    type: string
    reference: ${gen_code_by_phi3.output}
nodes:
- name: gen_code_by_phi3
  type: python
  source:
    type: code
    path: gen_code_by_phi3.py
  inputs:
    prompt: ${inputs.prompt}


```

3. Квантизиране на phi-3-mini

Целим по-добро изпълнение на SLM на локални устройства. Обикновено квантизираме модела (INT4, FP16, FP32)

```bash

python -m mlx_lm.convert --hf-path microsoft/Phi-3-mini-4k-instruct

```

**Note:** по подразбиране папката е mlx_model 

4. Добавете код в ***Chat_With_Phi3.py***

```python


from promptflow import tool

from mlx_lm import load, generate


# The inputs section will change based on the arguments of the tool function, after you save the code
# Adding type to arguments and return value will help the system show the types properly
# Please update the function name/signature per need
@tool
def my_python_tool(prompt: str) -> str:

    model_id = './mlx_model_phi3_mini'

    model, tokenizer = load(model_id)

    # <|user|>\nWrite python code for Fibonacci serie. Please use markdown as output<|end|>\n<|assistant|>

    response = generate(model, tokenizer, prompt="<|user|>\n" + prompt  + "<|end|>\n<|assistant|>", max_tokens=2048, verbose=True)

    return response


```

4. Можете да тествате потока чрез Debug или Run, за да проверите дали генерационният код работи правилно

![RUN](../../../../../../../../../translated_images/pf_run.d918637dc00f61e9bdeec37d4cc9646f77d270ac9203bcce13569f3157202b6e.bg.png)

5. Стартирайте потока като API за разработка в терминала

```

pf flow serve --source ./ --port 8080 --host localhost   

```

Можете да го тествате в Postman / Thunder Client

### **Note**

1. Първото стартиране отнема повече време. Препоръчително е да изтеглите phi-3 модела чрез Hugging face CLI.

2. Поради ограничената изчислителна мощност на Intel NPU, препоръчително е да използвате Phi-3-mini-4k-instruct.

3. Използваме Intel NPU ускорение за квантизиране в INT4, но ако рестартирате услугата, трябва да изтриете кеша и папките nc_workshop.

## **Ресурси**

1. Научете повече за Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Научете повече за Intel NPU Acceleration [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Примерен код, изтеглете [Local NPU Agent Sample Code](../../../../../../../../../code/07.Lab/01/AIPC/local-npu-agent)

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI преводаческа услуга [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи могат да съдържат грешки или неточности. Оригиналният документ на неговия първичен език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да е недоразумения или неправилни тълкувания, произтичащи от използването на този превод.