<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3dbbf568625b1ee04b354c2dc81d3248",
  "translation_date": "2025-07-17T04:29:06+00:00",
  "source_file": "md/02.Application/02.Code/Phi3/VSCodeExt/HOL/Apple/02.PromptflowWithMLX.md",
  "language_code": "bg"
}
-->
# **Лаборатория 2 - Стартиране на Prompt flow с Phi-3-mini в AIPC**

## **Какво е Prompt flow**

Prompt flow е набор от инструменти за разработка, създадени да улеснят целия цикъл на разработка на AI приложения, базирани на LLM, от идеята, прототипирането, тестването и оценката до внедряването в продукция и мониторинга. Той прави prompt инженерството много по-лесно и ви позволява да създавате LLM приложения с качество за продукция.

С prompt flow ще можете да:

- Създавате потоци, които свързват LLM, prompts, Python код и други инструменти в изпълним работен процес.

- Отстранявате грешки и итеративно подобрявате потоците си, особено взаимодействието с LLM, лесно.

- Оценявате потоците си, изчислявате метрики за качество и производителност с по-големи набори от данни.

- Интегрирате тестването и оценката в CI/CD системата си, за да гарантирате качеството на потока.

- Внедрявате потоците си на избраната от вас платформа за обслужване или ги интегрирате лесно в кода на вашето приложение.

- (По избор, но силно препоръчително) Сътрудничите с екипа си, използвайки облачната версия на Prompt flow в Azure AI.

## **Създаване на генерационни кодови потоци на Apple Silicon**

***Note*** ：Ако все още не сте завършили инсталацията на средата, моля посетете [Lab 0 -Installations](./01.Installations.md)

1. Отворете Prompt flow разширението във Visual Studio Code и създайте празен проект за поток

![create](../../../../../../../../../translated_images/pf_create.bde888dc83502eba082a058175bbf1eee6791219795393a386b06fd3043ec54d.bg.png)

2. Добавете входни и изходни параметри и добавете Python код като нов поток

![flow](../../../../../../../../../translated_images/pf_flow.520824c0969f2a94f17e947f86bdc4b4c6c88a2efa394fe3bcfb58c0dbc578a7.bg.png)

Можете да се ориентирате по тази структура (flow.dag.yaml), за да изградите своя поток

```yaml

inputs:
  prompt:
    type: string
    default: Write python code for Fibonacci serie. Please use markdown as output
outputs:
  result:
    type: string
    reference: ${gen_code_by_phi3.output}
nodes:
- name: gen_code_by_phi3
  type: python
  source:
    type: code
    path: gen_code_by_phi3.py
  inputs:
    prompt: ${inputs.prompt}


```

3. Квантифициране на phi-3-mini

Целим по-добро изпълнение на SLM на локални устройства. Обикновено квантифицираме модела (INT4, FP16, FP32)

```bash

python -m mlx_lm.convert --hf-path microsoft/Phi-3-mini-4k-instruct

```

**Note:** по подразбиране папката е mlx_model

4. Добавете код в ***Chat_With_Phi3.py***

```python


from promptflow import tool

from mlx_lm import load, generate


# The inputs section will change based on the arguments of the tool function, after you save the code
# Adding type to arguments and return value will help the system show the types properly
# Please update the function name/signature per need
@tool
def my_python_tool(prompt: str) -> str:

    model_id = './mlx_model_phi3_mini'

    model, tokenizer = load(model_id)

    # <|user|>\nWrite python code for Fibonacci serie. Please use markdown as output<|end|>\n<|assistant|>

    response = generate(model, tokenizer, prompt="<|user|>\n" + prompt  + "<|end|>\n<|assistant|>", max_tokens=2048, verbose=True)

    return response


```

4. Можете да тествате потока чрез Debug или Run, за да проверите дали генерирането на кода работи правилно

![RUN](../../../../../../../../../translated_images/pf_run.4239e8a0b420a58284edf6ee1471c1697c345670313c8e7beac0edaee15b9a9d.bg.png)

5. Стартирайте потока като development API в терминала

```

pf flow serve --source ./ --port 8080 --host localhost   

```

Можете да го тествате в Postman / Thunder Client

### **Note**

1. Първото стартиране отнема много време. Препоръчително е да изтеглите phi-3 модела чрез Hugging face CLI.

2. Поради ограничените изчислителни възможности на Intel NPU, препоръчително е да използвате Phi-3-mini-4k-instruct.

3. Използваме Intel NPU Acceleration за квантифициране в INT4, но ако рестартирате услугата, трябва да изтриете кеша и папките nc_workshop.

## **Ресурси**

1. Научете Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Научете Intel NPU Acceleration [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Примерен код, изтеглете [Local NPU Agent Sample Code](../../../../../../../../../code/07.Lab/01/AIPC/local-npu-agent)

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI преводаческа услуга [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи могат да съдържат грешки или неточности. Оригиналният документ на неговия език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да е недоразумения или неправилни тълкувания, произтичащи от използването на този превод.