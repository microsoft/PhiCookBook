<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f1ff728038c4f554b660a36b76cbdd6e",
  "translation_date": "2025-05-09T12:33:12+00:00",
  "source_file": "md/01.Introduction/03/overview.md",
  "language_code": "bg"
}
-->
В контекста на Phi-3-mini, инференцията се отнася до процеса на използване на модела за правене на прогнози или генериране на резултати въз основа на входни данни. Позволете ми да ви предоставя повече информация за Phi-3-mini и неговите възможности за инференция.

Phi-3-mini е част от серията модели Phi-3, пуснати от Microsoft. Тези модели са създадени да преосмислят възможностите на Малките Езикови Модели (SLMs).

Ето някои ключови моменти за Phi-3-mini и неговите възможности за инференция:

## **Обзор на Phi-3-mini:**
- Phi-3-mini има параметър с размер 3.8 милиарда.
- Моделът може да работи не само на традиционни компютърни устройства, но и на крайни устройства като мобилни телефони и IoT устройства.
- Пускането на Phi-3-mini дава възможност на отделни лица и предприятия да използват SLM модели на различни хардуерни платформи, особено в среди с ограничени ресурси.
- Поддържа различни формати на модели, включително традиционния PyTorch формат, квантованата версия на gguf формата и квантованата версия базирана на ONNX.

## **Достъп до Phi-3-mini:**
За да получите достъп до Phi-3-mini, можете да използвате [Semantic Kernel](https://github.com/microsoft/SemanticKernelCookBook?WT.mc_id=aiml-138114-kinfeylo) в Copilot приложение. Semantic Kernel обикновено е съвместим с Azure OpenAI Service, отворени модели в Hugging Face и локални модели.
Също така можете да използвате [Ollama](https://ollama.com) или [LlamaEdge](https://llamaedge.com) за извикване на квантовани модели. Ollama позволява на индивидуални потребители да използват различни квантовани модели, докато LlamaEdge осигурява кросплатформена достъпност за GGUF модели.

## **Квантовани модели:**
Много потребители предпочитат да използват квантовани модели за локална инференция. Например, можете директно да стартирате Ollama run Phi-3 или да го конфигурирате офлайн чрез Modelfile. Modelfile посочва пътя към GGUF файла и формата на подканата.

## **Възможности за генеративен AI:**
Комбинирането на SLM модели като Phi-3-mini отваря нови възможности за генеративен AI. Инференцията е само първата стъпка; тези модели могат да се използват за различни задачи в среди с ограничени ресурси, забавяне и разходи.

## **Отключване на генеративен AI с Phi-3-mini: Ръководство за инференция и внедряване**  
Научете как да използвате Semantic Kernel, Ollama/LlamaEdge и ONNX Runtime за достъп и инференция на Phi-3-mini модели и разгледайте възможностите на генеративния AI в различни приложни сценарии.

**Характеристики**  
Инференция на phi3-mini модел в:

- [Semantic Kernel](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/semantickernel?WT.mc_id=aiml-138114-kinfeylo)
- [Ollama](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ollama?WT.mc_id=aiml-138114-kinfeylo)
- [LlamaEdge WASM](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm?WT.mc_id=aiml-138114-kinfeylo)
- [ONNX Runtime](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/onnx?WT.mc_id=aiml-138114-kinfeylo)
- [iOS](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ios?WT.mc_id=aiml-138114-kinfeylo)

В обобщение, Phi-3-mini дава възможност на разработчиците да изследват различни формати на модели и да използват генеративен AI в различни приложни сценарии.

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI преводаческа услуга [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи могат да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да е недоразумения или неправилни тълкувания, възникнали от използването на този превод.