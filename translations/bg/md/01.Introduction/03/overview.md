<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f1ff728038c4f554b660a36b76cbdd6e",
  "translation_date": "2025-07-16T21:13:07+00:00",
  "source_file": "md/01.Introduction/03/overview.md",
  "language_code": "bg"
}
-->
В контекста на Phi-3-mini, инференцията се отнася до процеса на използване на модела за правене на прогнози или генериране на изходи въз основа на входни данни. Нека ви предоставя повече информация за Phi-3-mini и неговите възможности за инференция.

Phi-3-mini е част от серията модели Phi-3, пуснати от Microsoft. Тези модели са създадени, за да преосмислят възможностите на Малките Езикови Модели (SLMs).

Ето някои ключови моменти за Phi-3-mini и неговите възможности за инференция:

## **Преглед на Phi-3-mini:**
- Phi-3-mini има размер на параметрите от 3.8 милиарда.
- Може да работи не само на традиционни компютърни устройства, но и на крайни устройства като мобилни телефони и IoT устройства.
- Пускането на Phi-3-mini дава възможност на отделни потребители и предприятия да внедряват SLMs на различни хардуерни устройства, особено в среди с ограничени ресурси.
- Поддържа различни формати на модели, включително традиционния PyTorch формат, квантизираната версия на gguf формата и ONNX-базирана квантизирана версия.

## **Достъп до Phi-3-mini:**
За да получите достъп до Phi-3-mini, можете да използвате [Semantic Kernel](https://github.com/microsoft/SemanticKernelCookBook?WT.mc_id=aiml-138114-kinfeylo) в Copilot приложение. Semantic Kernel е съвместим с Azure OpenAI Service, отворени модели в Hugging Face и локални модели.
Можете също да използвате [Ollama](https://ollama.com) или [LlamaEdge](https://llamaedge.com) за извикване на квантизирани модели. Ollama позволява на отделни потребители да извикват различни квантизирани модели, докато LlamaEdge осигурява кросплатформена достъпност за GGUF модели.

## **Квантизирани модели:**
Много потребители предпочитат да използват квантизирани модели за локална инференция. Например, можете директно да стартирате Ollama run Phi-3 или да го конфигурирате офлайн чрез Modelfile. Modelfile указва пътя до GGUF файла и формата на подканата.

## **Възможности на генеративния AI:**
Комбинирането на SLMs като Phi-3-mini отваря нови възможности за генеративен AI. Инференцията е само първата стъпка; тези модели могат да се използват за различни задачи в среди с ограничени ресурси, изискващи ниска латентност и контролирани разходи.

## **Отключване на генеративния AI с Phi-3-mini: Ръководство за инференция и внедряване**  
Научете как да използвате Semantic Kernel, Ollama/LlamaEdge и ONNX Runtime за достъп и инференция на Phi-3-mini модели и разгледайте възможностите на генеративния AI в различни приложни сценарии.

**Характеристики**  
Инференция на phi3-mini модел в:

- [Semantic Kernel](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/semantickernel?WT.mc_id=aiml-138114-kinfeylo)
- [Ollama](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ollama?WT.mc_id=aiml-138114-kinfeylo)
- [LlamaEdge WASM](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm?WT.mc_id=aiml-138114-kinfeylo)
- [ONNX Runtime](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/onnx?WT.mc_id=aiml-138114-kinfeylo)
- [iOS](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ios?WT.mc_id=aiml-138114-kinfeylo)

В обобщение, Phi-3-mini позволява на разработчиците да изследват различни формати на модели и да използват генеративен AI в разнообразни приложни сценарии.

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI преводаческа услуга [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи могат да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да е недоразумения или неправилни тълкувания, произтичащи от използването на този превод.