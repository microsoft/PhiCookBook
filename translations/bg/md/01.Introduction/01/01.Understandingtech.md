<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:32:23+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "bg"
}
-->
# Основни технологии, споменати в текста

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - ниско ниво API за хардуерно ускорено машинно обучение, базиран на DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - платформа за паралелни изчисления и API модел, разработен от Nvidia, който позволява общо предназначена обработка на графични процесори (GPU).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - отворен формат, създаден за представяне на модели за машинно обучение, който осигурява съвместимост между различни ML рамки.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - формат за представяне и обновяване на модели за машинно обучение, особено полезен за по-малки езикови модели, които могат ефективно да работят на CPU с 4-8 битова квантизация.

## DirectML

DirectML е ниско ниво API, което позволява хардуерно ускорено машинно обучение. Изградено е върху DirectX 12, за да използва ускорението на GPU и е независим от производителя, което означава, че не изисква промени в кода, за да работи с различни GPU производители. Основно се използва за обучение и изпълнение на модели върху GPU.

Що се отнася до хардуерната поддръжка, DirectML е проектирано да работи с широк спектър от GPU-та, включително AMD интегрирани и дискретни GPU-та, Intel интегрирани GPU-та и NVIDIA дискретни GPU-та. Тази технология е част от Windows AI платформата и се поддържа на Windows 10 и 11, позволявайки обучение и изпълнение на модели на всяко Windows устройство.

Има обновления и възможности, свързани с DirectML, като поддръжка на до 150 ONNX оператора и използване както от ONNX runtime, така и от WinML. Технологията се поддържа от големи интегрирани хардуерни производители (IHVs), всеки от които имплементира различни метакоманди.

## CUDA

CUDA, което означава Compute Unified Device Architecture, е платформа за паралелни изчисления и API модел, създаден от Nvidia. Тя позволява на софтуерните разработчици да използват графичен процесор с CUDA поддръжка за общо предназначена обработка – подход, наречен GPGPU (General-Purpose computing on Graphics Processing Units). CUDA е ключов фактор за ускорението на Nvidia GPU-та и се използва широко в различни области, включително машинно обучение, научни изчисления и видеообработка.

Хардуерната поддръжка за CUDA е специфична за GPU-тата на Nvidia, тъй като това е патентована технология, разработена от Nvidia. Всяка архитектура поддържа конкретни версии на CUDA toolkit, който осигурява необходимите библиотеки и инструменти за разработчиците да изграждат и изпълняват CUDA приложения.

## ONNX

ONNX (Open Neural Network Exchange) е отворен формат, създаден за представяне на модели за машинно обучение. Той предоставя дефиниция на разширяем модел на изчислителен граф, както и дефиниции на вградени оператори и стандартни типове данни. ONNX позволява на разработчиците да преместват модели между различни ML рамки, осигурявайки съвместимост и улеснявайки създаването и внедряването на AI приложения.

Phi3 mini може да работи с ONNX Runtime на CPU и GPU на различни устройства, включително сървърни платформи, Windows, Linux и Mac настолни компютри, както и мобилни CPU.  
Оптимизираните конфигурации, които добавихме, са:

- ONNX модели за int4 DML: квантизирани до int4 чрез AWQ  
- ONNX модел за fp16 CUDA  
- ONNX модел за int4 CUDA: квантизиран до int4 чрез RTN  
- ONNX модел за int4 CPU и Mobile: квантизиран до int4 чрез RTN

## Llama.cpp

Llama.cpp е отворена софтуерна библиотека, написана на C++. Тя изпълнява инференция върху различни големи езикови модели (LLMs), включително Llama. Разработена заедно с ggml библиотеката (универсална библиотека за тензори), llama.cpp цели да осигури по-бърза инференция и по-ниска консумация на памет в сравнение с оригиналната Python имплементация. Поддържа хардуерна оптимизация, квантизация и предлага прост API и примери3. Ако се интересувате от ефективна инференция на LLM, llama.cpp си заслужава да бъде разгледана, тъй като Phi3 може да работи с Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) е формат, използван за представяне и обновяване на модели за машинно обучение. Особено полезен е за по-малки езикови модели (SLMs), които могат ефективно да работят на CPU с 4-8 битова квантизация. GGUF е подходящ за бързо прототипиране и изпълнение на модели на гранични устройства или в пакетни задачи като CI/CD процеси.

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI преводаческа услуга [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи могат да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да е недоразумения или неправилни тълкувания, произтичащи от използването на този превод.