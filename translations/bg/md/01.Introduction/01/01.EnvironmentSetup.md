<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-07-16T18:12:32+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "bg"
}
-->
# Започнете с Phi-3 локално

Това ръководство ще ви помогне да настроите локалната си среда за работа с модела Phi-3 чрез Ollama. Можете да стартирате модела по няколко различни начина, включително чрез GitHub Codespaces, VS Code Dev Containers или директно на вашия компютър.

## Настройка на средата

### GitHub Codespaces

Можете да стартирате този шаблон виртуално, като използвате GitHub Codespaces. Бутонът ще отвори уеб-базирана инстанция на VS Code в браузъра ви:

1. Отворете шаблона (може да отнеме няколко минути):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Отворете терминал

### VS Code Dev Containers

⚠️ Тази опция ще работи само ако Docker Desktop има поне 16 GB RAM. Ако имате по-малко от 16 GB RAM, можете да опитате [GitHub Codespaces](../../../../../md/01.Introduction/01) или да [настроите локално](../../../../../md/01.Introduction/01).

Свързана опция е VS Code Dev Containers, която ще отвори проекта във вашия локален VS Code с помощта на [Dev Containers разширението](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Стартирайте Docker Desktop (инсталирайте го, ако още не е инсталиран)
2. Отворете проекта:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. В прозореца на VS Code, който се отвори, след като файловете на проекта се заредят (може да отнеме няколко минути), отворете терминал.
4. Продължете с [стъпките за разгръщане](../../../../../md/01.Introduction/01)

### Локална среда

1. Уверете се, че следните инструменти са инсталирани:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Тествайте модела

1. Помолете Ollama да изтегли и стартира модела phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Изтеглянето на модела ще отнеме няколко минути.

2. След като видите "success" в изхода, можете да изпратите съобщение към модела от конзолата.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. След няколко секунди трябва да видите отговор от модела.

4. За да научите за различните техники, използвани с езикови модели, отворете Python тетрадката [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) и изпълнете всяка клетка. Ако използвате модел, различен от 'phi3:mini', променете `MODEL_NAME` в първата клетка.

5. За да проведете разговор с модела phi3:mini от Python, отворете Python файла [chat.py](../../../../../code/01.Introduce/chat.py) и го стартирайте. Можете да промените `MODEL_NAME` в началото на файла според нуждите, както и да модифицирате системното съобщение или да добавите няколко примера, ако желаете.

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI преводаческа услуга [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи могат да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да е недоразумения или неправилни тълкувания, произтичащи от използването на този превод.