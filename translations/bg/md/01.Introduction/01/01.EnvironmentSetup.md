<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-09T07:18:53+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "bg"
}
-->
# Започнете с Phi-3 локално

Това ръководство ще ви помогне да настроите локалната си среда за работа с модела Phi-3 чрез Ollama. Можете да стартирате модела по няколко различни начина, включително чрез GitHub Codespaces, VS Code Dev Containers или локалната си среда.

## Настройка на средата

### GitHub Codespaces

Можете да стартирате този шаблон виртуално, като използвате GitHub Codespaces. Бутонът ще отвори уеб базирана версия на VS Code във вашия браузър:

1. Отворете шаблона (може да отнеме няколко минути):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Отворете терминален прозорец

### VS Code Dev Containers

⚠️ Тази опция работи само ако Docker Desktop има поне 16 GB RAM. Ако имате по-малко от 16 GB RAM, можете да опитате [GitHub Codespaces](../../../../../md/01.Introduction/01) или да [настроите локално](../../../../../md/01.Introduction/01).

Свързана опция е VS Code Dev Containers, която ще отвори проекта във вашия локален VS Code с помощта на [Dev Containers разширението](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Стартирайте Docker Desktop (инсталирайте го, ако още не е инсталиран)
2. Отворете проекта:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. В отворения прозорец на VS Code, след като файловете на проекта се заредят (може да отнеме няколко минути), отворете терминален прозорец.
4. Продължете със [стъпките за разгръщане](../../../../../md/01.Introduction/01)

### Локална среда

1. Уверете се, че следните инструменти са инсталирани:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Тествайте модела

1. Помолете Ollama да изтегли и стартира модела phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Това ще отнеме няколко минути за изтегляне на модела.

2. След като видите "success" в изхода, можете да изпратите съобщение към модела от конзолата.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. След няколко секунди трябва да видите поток от отговори от модела.

4. За да научите повече за различни техники с езикови модели, отворете Python тетрадката [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) и изпълнете всяка клетка. Ако използвате модел различен от 'phi3:mini', променете `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` в началото на файла според нуждите, и също така можете да модифицирате системното съобщение или да добавите примери с няколко изстрела, ако желаете.

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI преводаческа услуга [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи могат да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да е недоразумения или неправилни тълкувания, произтичащи от използването на този превод.