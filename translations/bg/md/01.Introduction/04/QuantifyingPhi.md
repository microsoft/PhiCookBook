<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-05-09T13:38:22+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "bg"
}
-->
# **Квантоване на Phi фамилията**

Квантоването на модел означава процеса на преобразуване на параметрите (като тегла и стойности на активация) в невронен модел от голям диапазон стойности (обикновено непрекъснат) към по-малък краен набор от стойности. Тази технология може да намали размера и изчислителната сложност на модела и да подобри ефективността му при работа в среди с ограничени ресурси, като мобилни устройства или вградени системи. Квантоването постига компресия чрез намаляване на точността на параметрите, но също така води до определена загуба на точност. Затова в процеса на квантоване е необходимо да се намери баланс между размера на модела, изчислителната сложност и точността. Често използваните методи за квантоване включват фиксирана точка, плаваща запетая и други. Можете да изберете подходящата стратегия според конкретния случай и нужди.

Целим да внедрим GenAI моделите на крайни устройства и да позволим повече устройства да навлязат в GenAI сценарии, като мобилни устройства, AI PC/Copilot+PC и традиционни IoT устройства. Чрез квантования модел можем да го разположим на различни крайни устройства в зависимост от хардуера. В комбинация с рамката за ускорение на модели и квантования модел, предоставени от производителите на хардуер, можем да създадем по-добри SLM приложения.

В сценария на квантоване имаме различни прецизности (INT4, INT8, FP16, FP32). По-долу е обяснението на най-често използваните квантовани прецизности.

### **INT4**

INT4 квантоването е радикален метод, който преобразува теглата и стойностите на активация в 4-битови цели числа. Обикновено INT4 води до по-голяма загуба на точност заради по-малкия диапазон и по-ниската прецизност. Въпреки това, в сравнение с INT8, INT4 значително намалява нуждата от съхранение и изчислителната сложност на модела. Трябва да се има предвид, че INT4 се използва рядко в практиката, тъй като прекалено ниската точност може да доведе до сериозно влошаване на представянето на модела. Освен това не всички хардуери поддържат INT4 операции, затова при избора на метод трябва да се вземе предвид съвместимостта.

### **INT8**

INT8 квантоването преобразува теглата и активациите на модела от числа с плаваща запетая към 8-битови цели числа. Въпреки че числовият диапазон на INT8 е по-малък и по-малко точен, това значително намалява изискванията за съхранение и изчисления. При INT8 теглата и стойностите на активация преминават през процес на квантоване, включващ мащабиране и изместване, за да се запази максимално информацията от оригиналните числа с плаваща запетая. По време на изпълнение тези квантовани стойности се деквантоват обратно в числа с плаваща запетая за изчисления и след това отново се квантоват в INT8 за следващата стъпка. Този метод осигурява достатъчна точност в повечето приложения, като същевременно поддържа висока изчислителна ефективност.

### **FP16**

FP16 форматът, т.е. 16-битови числа с плаваща запетая (float16), намалява използваната памет наполовина спрямо 32-битовите числа с плаваща запетая (float32), което е голямо предимство при мащабни дълбоки невронни мрежи. FP16 позволява зареждане на по-големи модели или обработка на повече данни в рамките на същите ограничения на GPU паметта. Съвременните GPU хардуери все повече поддържат FP16 операции, което може да доведе и до ускорение на изчисленията. Въпреки това FP16 има и своите недостатъци – по-ниска точност, която в някои случаи може да причини числови нестабилности или загуба на точност.

### **FP32**

FP32 форматът осигурява по-висока точност и може точно да представя широк диапазон от стойности. В случаи, когато се извършват сложни математически операции или се изискват резултати с висока точност, FP32 е предпочитан. Високата точност обаче означава и по-голямо използване на памет и по-дълго време за изчисления. При големи модели с много параметри и огромни обеми данни FP32 може да доведе до недостиг на GPU памет или забавяне на изпълнението.

На мобилни устройства или IoT устройства можем да конвертираме Phi-3.x модели в INT4, докато AI PC / Copilot PC могат да използват по-висока прецизност като INT8, FP16, FP32.

В момента различните производители на хардуер имат различни рамки за поддръжка на генеративни модели, като Intel OpenVINO, Qualcomm QNN, Apple MLX и Nvidia CUDA, които в комбинация с квантоването позволяват локално внедряване.

От технологична гледна точка имаме различна поддръжка на формати след квантоване, като PyTorch / Tensorflow, GGUF и ONNX. Направих сравнение на формати и приложения между GGUF и ONNX. Тук препоръчвам ONNX квантования формат, който има добра поддръжка както от рамките за модели, така и от хардуера. В тази глава ще се фокусираме върху ONNX Runtime за GenAI, OpenVINO и Apple MLX за извършване на квантоване на моделите (ако имате по-добър метод, можете да го споделите чрез PR).

**Тази глава включва**

1. [Квантоване на Phi-3.5 / 4 с llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Квантоване на Phi-3.5 / 4 с разширения за Generative AI за onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Квантоване на Phi-3.5 / 4 с Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Квантоване на Phi-3.5 / 4 с Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI преводаческа услуга [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи могат да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да е недоразумения или неправилни тълкувания, произтичащи от използването на този превод.