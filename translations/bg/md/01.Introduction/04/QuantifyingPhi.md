<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-05T10:01:32+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "bg"
}
-->
# **Количествено изразяване на семейството Phi**

Квантизацията на модел означава процеса на преобразуване на параметрите (като тегла и стойности на активация) в невронен модел от голям диапазон стойности (обикновено непрекъснат диапазон) към по-малък краен диапазон стойности. Тази технология може да намали размера и изчислителната сложност на модела и да подобри ефективността на работа на модела в среди с ограничени ресурси, като мобилни устройства или вградени системи. Квантизацията на модел постига компресия чрез намаляване на прецизността на параметрите, но също така води до определена загуба на точност. Поради това в процеса на квантизация е необходимо да се балансират размерът на модела, изчислителната сложност и точността. Обичайни методи за квантизация включват фиксирана точка, плаваща запетая и др. Можете да изберете подходящата стратегия за квантизация според конкретния сценарий и нужди.

Надяваме се да внедрим модел GenAI в крайни устройства и да позволим повече устройства да влязат в сценарии с GenAI, като мобилни устройства, AI PC/Copilot+PC и традиционни IoT устройства. Чрез квантизирания модел можем да го разположим в различни крайни устройства според различните устройства. В комбинация с рамката за ускоряване на модели и квантизирания модел, предоставени от хардуерни производители, можем да изградим по-добри сценарии на приложение за SLM.

В контекста на квантизацията имаме различна прецизност (INT4, INT8, FP16, FP32). По-долу е обяснение на често използваните прецизни квантизации.

### **INT4**

INT4 квантизацията е радикален метод за квантизация, който квантизира теглата и стойностите на активация на модела в 4-битови цели числа. INT4 квантизацията обикновено води до по-голяма загуба на точност поради по-малкия диапазон на представяне и по-ниската прецизност. Въпреки това, в сравнение с INT8 квантизацията, INT4 може допълнително да намали изискванията за съхранение и изчислителната сложност на модела. Трябва да се отбележи, че INT4 квантизацията е относително рядка в практиките, тъй като твърде ниската точност може да причини значително влошаване на производителността на модела. Освен това не целият хардуер поддържа INT4 операции, така че съвместимостта с хардуера трябва да се има предвид при избора на метод за квантизация.

### **INT8**

INT8 квантизацията е процесът на преобразуване на теглата и активациите на модела от числа с плаваща запетая в 8-битови цели числа. Въпреки че числовият диапазон, представен от INT8 цели числа, е по-малък и по-малко прецизен, това може значително да намали изискванията за съхранение и изчисления. В INT8 квантизация, теглата и стойностите на активация на модела преминават през процес на квантизация, включващ мащабиране и отместване, за да се запази оригиналната информация с плаваща запетая възможно най-добре. По време на извеждането тези квантизирани стойности ще бъдат деквантизирани обратно в числа с плаваща запетая за изчисление, а след това отново квантизирани в INT8 за следващата стъпка. Този метод може да осигури достатъчна точност в повечето приложения, като същевременно поддържа висока изчислителна ефективност.

### **FP16**

Форматът FP16, тоест 16-битови числа с плаваща запетая (float16), намалява обема на паметта наполовина в сравнение с 32-битови числа с плаваща запетая (float32), което има значителни предимства в големи дълбоки учебни приложения. Форматът FP16 позволява зареждане на по-големи модели или обработка на повече данни в рамките на същите ограничения на GPU паметта. Тъй като съвременният GPU хардуер продължава да поддържа FP16 операции, използването на формат FP16 може също да доведе до подобрения в скоростта на изчисление. Въпреки това, форматът FP16 има и своите вградени недостатъци, а именно по-ниска прецизност, което може да доведе до числова нестабилност или загуба на точност в някои случаи.

### **FP32**

Форматът FP32 осигурява по-висока прецизност и може точно да представя широк диапазон от стойности. В сценарии, където се извършват сложни математически операции или са необходими резултати с висока точност, форматът FP32 е предпочитан. Въпреки това, високата точност също означава повече използвана памет и по-дълго време за изчисление. За големи дълбоки учебни модели, особено когато има много параметри на модела и огромно количество данни, форматът FP32 може да доведе до недостатъчна памет на GPU или понижаване на скоростта на извеждане.

На мобилни устройства или IoT устройства можем да конвертираме модели Phi-3.x в INT4, докато AI PC / Copilot PC могат да използват по-висока прецизност като INT8, FP16, FP32.

В момента различни хардуерни производители имат различни рамки за поддръжка на генеративни модели, като Intel OpenVINO, Qualcomm QNN, Apple MLX и Nvidia CUDA и т.н., в комбинация с квантизация на модели за завършване на локално разполагане.

От технологична гледна точка, след квантизация имаме различна поддръжка на формати, като PyTorch / TensorFlow формат, GGUF и ONNX. Направил съм сравнение на формати и сценарии на приложение между GGUF и ONNX. Тук препоръчвам формата за квантизация ONNX, който има добра поддръжка от рамката на модела до хардуера. В тази глава ще се фокусираме върху ONNX Runtime за GenAI, OpenVINO и Apple MLX за извършване на квантизация на модели (ако имате по-добър начин, можете да ни го предоставите чрез подаване на PR).

**Тази глава включва**

1. [Квантизация на Phi-3.5 / 4 с помощта на llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Квантизация на Phi-3.5 / 4 с помощта на разширения за генеративен AI за onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Квантизация на Phi-3.5 / 4 с помощта на Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Квантизация на Phi-3.5 / 4 с помощта на Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Отказ от отговорност**:
Този документ е преведен с помощта на AI преводаческа услуга [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи могат да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да е недоразумения или неправилни тълкувания, произтичащи от използването на този превод.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->