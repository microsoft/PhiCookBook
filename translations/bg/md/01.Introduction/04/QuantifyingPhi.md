<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-07-16T21:50:30+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "bg"
}
-->
# **Квантизиране на фамилията Phi**

Квантизирането на модел означава процеса на преобразуване на параметрите (като тегла и стойности на активация) в невронен модел от голям диапазон стойности (обикновено непрекъснат) към по-малък краен диапазон. Тази технология може да намали размера и изчислителната сложност на модела и да подобри ефективността му при работа в среди с ограничени ресурси, като мобилни устройства или вградени системи. Квантизирането постига компресия чрез намаляване на точността на параметрите, но също така въвежда известна загуба на прецизност. Затова в процеса на квантизиране е необходимо да се балансират размерът на модела, изчислителната сложност и точността. Често използвани методи за квантизиране са фиксирана точка, плаваща точка и други. Можете да изберете подходящата стратегия според конкретния сценарий и нужди.

Ние се стремим да внедрим GenAI моделите на крайни устройства и да позволим повече устройства да навлязат в GenAI сценарии, като мобилни устройства, AI PC/Copilot+PC и традиционни IoT устройства. Чрез квантизиран модел можем да го разположим на различни крайни устройства според техните характеристики. В комбинация с рамките за ускорение на модели и квантизираните модели, предоставени от хардуерните производители, можем да изградим по-добри SLM приложения.

В сценария на квантизиране имаме различни точности (INT4, INT8, FP16, FP32). По-долу е обяснение на най-често използваните точности при квантизиране.

### **INT4**

INT4 квантизирането е радикален метод, който преобразува теглата и стойностите на активация в 4-битови цели числа. INT4 обикновено води до по-голяма загуба на точност заради по-малкия диапазон и по-ниската прецизност. Въпреки това, в сравнение с INT8, INT4 може допълнително да намали изискванията за съхранение и изчислителната сложност на модела. Трябва да се отбележи, че INT4 квантизирането е сравнително рядко в практиката, тъй като твърде ниската точност може да доведе до значително влошаване на представянето на модела. Освен това не всички хардуери поддържат INT4 операции, затова при избора на метод за квантизиране трябва да се има предвид съвместимостта с хардуера.

### **INT8**

INT8 квантизирането представлява преобразуване на теглата и активациите на модела от числа с плаваща запетая към 8-битови цели числа. Въпреки че числовият диапазон на INT8 е по-малък и по-малко точен, това значително намалява изискванията за съхранение и изчисления. При INT8 квантизирането теглата и стойностите на активация преминават през процес на квантизация, включващ скалиране и изместване, за да се запази максимално информацията от оригиналните числа с плаваща запетая. По време на инференс тези квантизирани стойности се деквантизират обратно в числа с плаваща запетая за изчисления, след което отново се квантизират в INT8 за следващата стъпка. Този метод осигурява достатъчна точност в повечето приложения, като същевременно поддържа висока изчислителна ефективност.

### **FP16**

FP16 форматът, т.е. 16-битови числа с плаваща запетая (float16), намалява използваната памет наполовина в сравнение с 32-битовите числа с плаваща запетая (float32), което е голямо предимство при мащабни дълбоки невронни мрежи. FP16 позволява зареждането на по-големи модели или обработката на повече данни в рамките на същите ограничения на GPU паметта. Тъй като съвременният GPU хардуер все повече поддържа FP16 операции, използването на този формат може да доведе и до ускорение на изчисленията. Въпреки това FP16 има и своите недостатъци – по-ниска точност, която в някои случаи може да доведе до числова нестабилност или загуба на прецизност.

### **FP32**

FP32 форматът осигурява по-висока точност и може точно да представи широк диапазон от стойности. В ситуации, където се извършват сложни математически операции или се изискват резултати с висока точност, FP32 е предпочитан. Високата точност обаче означава и по-голяма консумация на памет и по-дълго време за изчисления. При големи дълбоки модели, особено с много параметри и огромно количество данни, FP32 може да доведе до недостиг на GPU памет или забавяне на инференса.

На мобилни устройства или IoT устройства можем да конвертираме Phi-3.x модели в INT4, докато AI PC / Copilot PC могат да използват по-висока точност като INT8, FP16 или FP32.

В момента различните хардуерни производители предлагат различни рамки за поддръжка на генеративни модели, като Intel OpenVINO, Qualcomm QNN, Apple MLX и Nvidia CUDA, които в комбинация с квантизирането на модели позволяват локално разполагане.

От технологична гледна точка имаме различна поддръжка на формати след квантизирането, като PyTorch / Tensorflow формат, GGUF и ONNX. Направил съм сравнение на формати и приложни сценарии между GGUF и ONNX. Тук препоръчвам ONNX формата за квантизация, който има добра поддръжка от рамките за модели до хардуера. В тази глава ще се фокусираме върху ONNX Runtime за GenAI, OpenVINO и Apple MLX за извършване на квантизация на модели (ако имате по-добър метод, можете да ни го предложите чрез PR).

**Тази глава включва**

1. [Квантизиране на Phi-3.5 / 4 с помощта на llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Квантизиране на Phi-3.5 / 4 с разширения за генеративен AI за onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Квантизиране на Phi-3.5 / 4 с Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Квантизиране на Phi-3.5 / 4 с Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI преводаческа услуга [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи могат да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да е недоразумения или неправилни тълкувания, произтичащи от използването на този превод.