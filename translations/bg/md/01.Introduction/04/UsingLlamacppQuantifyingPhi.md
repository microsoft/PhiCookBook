<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "462bddc47427d8785f3c9fd817b346fe",
  "translation_date": "2025-07-16T22:12:17+00:00",
  "source_file": "md/01.Introduction/04/UsingLlamacppQuantifyingPhi.md",
  "language_code": "bg"
}
-->
# **Квантизиране на Phi Family с помощта на llama.cpp**

## **Какво е llama.cpp**

llama.cpp е софтуерна библиотека с отворен код, основно написана на C++, която извършва инференция върху различни големи езикови модели (LLM), като Llama. Основната ѝ цел е да осигури съвременно представяне при инференция на LLM на широка гама хардуер с минимална настройка. Освен това, за тази библиотека има налични Python обвивки, които предлагат високоефективен API за допълване на текст и уеб сървър, съвместим с OpenAI.

Основната цел на llama.cpp е да позволи инференция на LLM с минимална настройка и съвременно представяне на разнообразен хардуер – локално и в облака.

- Чиста C/C++ имплементация без зависимости
- Apple silicon е приоритет – оптимизиран чрез ARM NEON, Accelerate и Metal фреймуъркове
- Поддръжка на AVX, AVX2 и AVX512 за x86 архитектури
- Квантизация с 1.5-бит, 2-бит, 3-бит, 4-бит, 5-бит, 6-бит и 8-бит цели числа за по-бърза инференция и намалена употреба на памет
- Персонализирани CUDA ядра за изпълнение на LLM на NVIDIA GPU (поддръжка на AMD GPU чрез HIP)
- Поддръжка на Vulkan и SYCL бекенд
- Хибридна инференция CPU+GPU за частично ускоряване на модели, по-големи от общия капацитет на VRAM

## **Квантизиране на Phi-3.5 с llama.cpp**

Моделът Phi-3.5-Instruct може да бъде квантизиран с помощта на llama.cpp, но Phi-3.5-Vision и Phi-3.5-MoE все още не се поддържат. Форматът, конвертиран от llama.cpp, е gguf, който е и най-широко използваният формат за квантизация.

Има голям брой квантизирани модели в GGUF формат в Hugging Face. AI Foundry, Ollama и LlamaEdge разчитат на llama.cpp, затова GGUF моделите също често се използват.

### **Какво е GGUF**

GGUF е бинарен формат, оптимизиран за бързо зареждане и записване на модели, което го прави много ефективен за инференция. GGUF е създаден за използване с GGML и други изпълнители. GGUF е разработен от @ggerganov, който е и създател на llama.cpp – популярна C/C++ рамка за инференция на LLM. Модели, първоначално разработени във фреймуъркове като PyTorch, могат да бъдат конвертирани в GGUF формат за използване с тези двигатели.

### **ONNX срещу GGUF**

ONNX е традиционен формат за машинно и дълбоко обучение, който е добре поддържан в различни AI фреймуъркове и има добри приложения в edge устройства. От друга страна, GGUF е базиран на llama.cpp и може да се каже, че е създаден в ерата на GenAI. Двата формата имат сходни приложения. Ако търсите по-добра производителност в вградени хардуерни и приложни слоеве, ONNX може да е вашият избор. Ако използвате производни фреймуъркове и технологии на llama.cpp, тогава GGUF може да е по-подходящ.

### **Квантизиране на Phi-3.5-Instruct с llama.cpp**

**1. Конфигуриране на средата**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. Квантизация**

Използване на llama.cpp за конвертиране на Phi-3.5-Instruct в FP16 GGUF


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Квантизиране на Phi-3.5 до INT4


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. Тестване**

Инсталиране на llama-cpp-python


```bash

pip install llama-cpp-python -U

```

***Забележка*** 

Ако използвате Apple Silicon, моля инсталирайте llama-cpp-python по следния начин


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

Тестване 


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **Ресурси**

1. Научете повече за llama.cpp [https://github.com/ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)
2. Научете повече за onnxruntime [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)
3. Научете повече за GGUF [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**Отказ от отговорност**:  
Този документ е преведен с помощта на AI преводаческа услуга [Co-op Translator](https://github.com/Azure/co-op-translator). Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи могат да съдържат грешки или неточности. Оригиналният документ на неговия роден език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Ние не носим отговорност за каквито и да е недоразумения или неправилни тълкувания, произтичащи от използването на този превод.