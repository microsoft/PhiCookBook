{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaktív Phi 3 Mini 4K Instruct Chatbot a Whisperrel\n",
    "\n",
    "### Bevezetés:\n",
    "Az Interaktív Phi 3 Mini 4K Instruct Chatbot egy eszköz, amely lehetővé teszi a felhasználók számára, hogy szöveges vagy hangbemenet segítségével kapcsolatba lépjenek a Microsoft Phi 3 Mini 4K instruct demóval. A chatbot különféle feladatokra használható, például fordításra, időjárás-frissítésekre és általános információgyűjtésre.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "Hozd létre a Huggingface hozzáférési tokenedet: [Create your Huggingface Access Token](https://huggingface.co/settings/tokens)\n",
    "\n",
    "Hozz létre egy új tokent  \n",
    "Adj meg egy új nevet  \n",
    "Válaszd ki az írási jogosultságokat  \n",
    "Másold ki a tokent, és mentsd el biztonságos helyre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A következő Python kód két fő feladatot lát el: az `os` modul importálása és egy környezeti változó beállítása.\n",
    "\n",
    "1. Az `os` modul importálása:\n",
    "   - Az `os` modul a Pythonban lehetőséget nyújt az operációs rendszerrel való interakcióra. Segítségével különféle operációs rendszerhez kapcsolódó feladatokat végezhetünk, például környezeti változók elérése, fájlokkal és könyvtárakkal való munka stb.\n",
    "   - Ebben a kódban az `os` modult az `import` utasítással importáljuk. Ez az utasítás elérhetővé teszi az `os` modul funkcionalitását az aktuális Python szkriptben.\n",
    "\n",
    "2. Környezeti változó beállítása:\n",
    "   - A környezeti változó egy olyan érték, amelyet az operációs rendszeren futó programok elérhetnek. Ez egy módja annak, hogy konfigurációs beállításokat vagy más információkat tároljunk, amelyeket több program is használhat.\n",
    "   - Ebben a kódban egy új környezeti változó kerül beállításra az `os.environ` szótár segítségével. A szótár kulcsa `'HF_TOKEN'`, az értéket pedig a `HUGGINGFACE_TOKEN` változóból kapja.\n",
    "   - A `HUGGINGFACE_TOKEN` változó közvetlenül a kódrészlet felett van definiálva, és egy sztring értéket kap, `\"hf_**************\"` formában, a `#@param` szintaxis használatával. Ez a szintaxis gyakran Jupyter notebookokban használatos, hogy lehetővé tegye a felhasználói bemenetet és a paraméterek konfigurálását közvetlenül a notebook felületén.\n",
    "   - A `'HF_TOKEN'` környezeti változó beállításával az érték elérhetővé válik a program más részei vagy az operációs rendszeren futó más programok számára.\n",
    "\n",
    "Összességében ez a kód importálja az `os` modult, és beállít egy `'HF_TOKEN'` nevű környezeti változót a `HUGGINGFACE_TOKEN` változóban megadott értékkel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ez a kódrészlet egy clear_output nevű függvényt definiál, amelyet arra használnak, hogy törölje az aktuális cella kimenetét Jupyter Notebookban vagy IPythonban. Nézzük meg részletesebben a kódot és értsük meg a működését:\n",
    "\n",
    "A clear_output függvény egy paramétert fogad, amelynek neve wait, és ez egy logikai érték. Alapértelmezés szerint a wait értéke False. Ez a paraméter határozza meg, hogy a függvény várjon-e addig, amíg új kimenet elérhető, mielőtt törölné a meglévő kimenetet.\n",
    "\n",
    "Maga a függvény arra szolgál, hogy törölje az aktuális cella kimenetét. Jupyter Notebookban vagy IPythonban, amikor egy cella kimenetet generál, például nyomtatott szöveget vagy grafikus ábrákat, az a cella alatt jelenik meg. A clear_output függvény lehetővé teszi, hogy ezt a kimenetet töröljük.\n",
    "\n",
    "A függvény implementációja nincs megadva a kódrészletben, amit az ellipszis (...) jelez. Az ellipszis egy helyőrző, amely az aktuális kódot jelöli, amely a kimenet törlését végzi. A függvény implementációja valószínűleg a Jupyter Notebook vagy IPython API-val való interakciót foglalja magában, hogy eltávolítsa az aktuális cella meglévő kimenetét.\n",
    "\n",
    "Összességében ez a függvény egy kényelmes módot biztosít az aktuális cella kimenetének törlésére Jupyter Notebookban vagy IPythonban, megkönnyítve a megjelenített kimenet kezelését és frissítését interaktív kódolási munkamenetek során.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hajtsuk végre szövegfelolvasást (TTS) az Edge TTS szolgáltatás segítségével. Nézzük meg a releváns függvények implementációit egyenként:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Ez a függvény egy bemeneti értéket kap, és kiszámítja a TTS hang sebességét reprezentáló karakterláncot. A bemeneti érték a beszéd kívánt sebességét jelöli, ahol az 1 normál sebességet jelent. A függvény úgy számítja ki a sebesség karakterláncot, hogy kivon 1-et a bemeneti értékből, megszorozza 100-zal, majd meghatározza az előjelet annak alapján, hogy a bemeneti érték nagyobb vagy egyenlő-e 1-nél. A függvény a sebesség karakterláncot a következő formátumban adja vissza: \"{sign}{rate}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: Ez a függvény egy bemeneti szöveget és egy nyelvet kap paraméterként. A szöveget nyelvspecifikus szabályok alapján darabolja fel. Ebben az implementációban, ha a nyelv \"English\", a függvény minden pontnál (\".\") felosztja a szöveget, eltávolítva az elején vagy végén lévő szóközöket. Ezután minden darabhoz hozzáad egy pontot, és visszaadja a szűrt darabok listáját.\n",
    "\n",
    "3. `tts_file_name(text)`: Ez a függvény fájlnevet generál a TTS hangfájlhoz a bemeneti szöveg alapján. Több átalakítást végez a szövegen: eltávolítja a végén lévő pontot (ha van), kisbetűssé alakítja, eltávolítja az elején és végén lévő szóközöket, és helyettesíti a szóközöket aláhúzásokkal. Ezután a szöveget legfeljebb 25 karakterre rövidíti (ha hosszabb), vagy az egész szöveget használja, ha üres. Végül a [`uuid`] modul segítségével véletlenszerű karakterláncot generál, és kombinálja a rövidített szöveggel, hogy létrehozza a fájlnevet a következő formátumban: \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Ez a függvény több hangfájlt egyetlen hangfájlba egyesít. Egy hangfájlok útvonalait tartalmazó listát és egy kimeneti útvonalat kap paraméterként. A függvény inicializál egy üres [`AudioSegment`] objektumot, amelyet [`merged_audio`]-nak nevez. Ezután végigmegy minden hangfájl útvonalán, betölti a hangfájlt a `AudioSegment.from_file()` metódus segítségével a `pydub` könyvtárból, és hozzáadja az aktuális hangfájlt a [`merged_audio`] objektumhoz. Végül exportálja az egyesített hangot a megadott kimeneti útvonalra MP3 formátumban.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: Ez a függvény végrehajtja a TTS műveletet az Edge TTS szolgáltatás segítségével. Egy szövegdarabok listáját, a beszéd sebességét, a hang nevét és a mentési útvonalat kapja paraméterként. Ha a darabok száma nagyobb, mint 1, a függvény létrehoz egy könyvtárat az egyes darabok hangfájljainak tárolására. Ezután végigmegy minden darabon, összeállít egy Edge TTS parancsot a `calculate_rate_string()` függvény, a hang neve és a darab szövege segítségével, majd végrehajtja a parancsot az `os.system()` függvény segítségével. Ha a parancs végrehajtása sikeres, hozzáadja a generált hangfájl útvonalát egy listához. Miután feldolgozta az összes darabot, egyesíti az egyes hangfájlokat a `merge_audio_files()` függvény segítségével, és elmenti az egyesített hangot a megadott mentési útvonalra. Ha csak egy darab van, közvetlenül generálja az Edge TTS parancsot, és elmenti a hangot a mentési útvonalra. Végül visszaadja a generált hangfájl mentési útvonalát.\n",
    "\n",
    "6. `random_audio_name_generate()`: Ez a függvény véletlenszerű hangfájlnév generálására szolgál a [`uuid`] modul segítségével. Véletlenszerű UUID-t generál, karakterlánccá alakítja, az első 8 karaktert veszi, hozzáadja a \".mp3\" kiterjesztést, és visszaadja a véletlenszerű hangfájlnév.\n",
    "\n",
    "7. `talk(input_text)`: Ez a függvény a fő belépési pont a TTS művelet végrehajtásához. Egy bemeneti szöveget kap paraméterként. Először ellenőrzi a bemeneti szöveg hosszát, hogy meghatározza, hosszú mondatról van-e szó (600 karakternél hosszabb vagy egyenlő). A hossz és a `translate_text_flag` változó értéke alapján meghatározza a nyelvet, és létrehozza a szövegdarabok listáját a `make_chunks()` függvény segítségével. Ezután generál egy mentési útvonalat a hangfájlhoz a `random_audio_name_generate()` függvény segítségével. Végül meghívja az `edge_free_tts()` függvényt a TTS művelet végrehajtásához, és visszaadja a generált hangfájl mentési útvonalát.\n",
    "\n",
    "Összességében ezek a függvények együttműködnek, hogy feldarabolják a bemeneti szöveget, fájlnevet generáljanak a hangfájlhoz, végrehajtsák a TTS műveletet az Edge TTS szolgáltatás segítségével, és egyesítsék az egyes hangfájlokat egyetlen hangfájlba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A két függvény, a convert_to_text és a run_text_prompt megvalósítása, valamint két osztály, a str és az Audio deklarációja.\n",
    "\n",
    "A convert_to_text függvény egy audio_path bemenetet kap, és egy whisper_model nevű modell segítségével átalakítja az audiót szöveggé. A függvény először ellenőrzi, hogy a gpu jelző True értékre van-e állítva. Ha igen, akkor a whisper_model bizonyos paraméterekkel kerül használatra, mint például word_timestamps=True, fp16=True, language='English', és task='translate'. Ha a gpu jelző False, akkor a whisper_model fp16=False paraméterrel kerül használatra. Az eredményül kapott átiratot egy 'scan.txt' nevű fájlba menti, és szövegként visszaadja.\n",
    "\n",
    "A run_text_prompt függvény egy message és egy chat_history bemenetet kap. A phi_demo függvényt használja arra, hogy egy chatbot válaszát generálja az adott üzenet alapján. A generált választ továbbadja a talk függvénynek, amely az üzenetet audiófájllá alakítja, és visszaadja az audiófájl elérési útját. Az Audio osztályt használják az audiófájl megjelenítésére és lejátszására. Az audiót az IPython.display modul display függvényével jelenítik meg, és az Audio objektum autoplay=True paraméterrel kerül létrehozásra, így az audió automatikusan elindul. A chat_history frissül az üzenettel és a generált válasszal, majd egy üres string és a frissített chat_history kerül visszaadásra.\n",
    "\n",
    "A str osztály egy beépített Python osztály, amely karakterek sorozatát reprezentálja. Számos metódust biztosít a szövegek manipulálására és kezelésére, mint például capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill, és még sok más. Ezek a metódusok lehetővé teszik olyan műveletek végrehajtását, mint keresés, csere, formázás és szövegek manipulálása.\n",
    "\n",
    "Az Audio osztály egy egyedi osztály, amely egy audió objektumot reprezentál. A Jupyter Notebook környezetben audiólejátszó létrehozására használják. Az osztály különböző paramétereket fogad el, mint például data, filename, url, embed, rate, autoplay, és normalize. A data paraméter lehet numpy tömb, minták listája, fájlnév vagy URL-t reprezentáló string, vagy nyers PCM adat. A filename paraméter egy helyi fájlt ad meg, amelyből az audióadatokat be kell tölteni, míg az url paraméter egy URL-t ad meg, ahonnan az audióadatokat le kell tölteni. Az embed paraméter meghatározza, hogy az audióadatokat beágyazott adat URI-ként vagy az eredeti forrásból kell-e hivatkozni. A rate paraméter az audióadatok mintavételi sebességét adja meg. Az autoplay paraméter meghatározza, hogy az audió automatikusan elinduljon-e. A normalize paraméter azt határozza meg, hogy az audióadatokat normalizálni (átméretezni) kell-e a maximális lehetséges tartományra. Az Audio osztály olyan metódusokat is biztosít, mint a reload, amely újratölti az audióadatokat fájlból vagy URL-ből, valamint olyan attribútumokat, mint a src_attr, autoplay_attr, és element_id_attr, amelyek az audió HTML elem megfelelő attribútumait adják vissza.\n",
    "\n",
    "Összességében ezek a függvények és osztályok arra szolgálnak, hogy audiót szöveggé alakítsanak, chatbot válaszokat generáljanak audió formájában, és audiót jelenítsenek meg és játsszanak le a Jupyter Notebook környezetben.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Felelősség kizárása**:  \nEz a dokumentum az AI fordítási szolgáltatás [Co-op Translator](https://github.com/Azure/co-op-translator) segítségével lett lefordítva. Bár törekszünk a pontosságra, kérjük, vegye figyelembe, hogy az automatikus fordítások hibákat vagy pontatlanságokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelvén tekintendő hiteles forrásnak. Kritikus információk esetén javasolt professzionális emberi fordítást igénybe venni. Nem vállalunk felelősséget semmilyen félreértésért vagy téves értelmezésért, amely a fordítás használatából eredhet.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-13T07:00:18+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "hu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}