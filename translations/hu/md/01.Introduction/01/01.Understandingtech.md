<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:47:12+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "hu"
}
-->
# A kulcsfontosságú technológiák között szerepelnek

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) – egy alacsony szintű API hardveresen gyorsított gépi tanuláshoz, amely a DirectX 12-re épül.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) – egy párhuzamos számítási platform és alkalmazásprogramozási felület (API) modell, amelyet az Nvidia fejlesztett ki, lehetővé téve az általános célú feldolgozást grafikus feldolgozó egységeken (GPU-kon).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) – egy nyílt formátum, amely gépi tanulási modellek ábrázolására szolgál, és biztosítja az interoperabilitást különböző ML keretrendszerek között.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) – egy formátum gépi tanulási modellek ábrázolására és frissítésére, különösen hasznos kisebb nyelvi modellek esetén, amelyek hatékonyan futtathatók CPU-kon 4-8 bites kvantálással.

## DirectML

A DirectML egy alacsony szintű API, amely hardveresen gyorsított gépi tanulást tesz lehetővé. A DirectX 12-re épül, hogy kihasználja a GPU gyorsítást, és gyártófüggetlen, vagyis nem igényel kódmódosítást a különböző GPU-gyártók között. Elsősorban modellek tanítására és inferenciára használják GPU-kon.

A hardvertámogatás tekintetében a DirectML széles körű GPU-kat támogat, beleértve az AMD integrált és diszkrét GPU-it, az Intel integrált GPU-it, valamint az NVIDIA diszkrét GPU-it. A Windows AI Platform része, és támogatott Windows 10 és 11 rendszereken, lehetővé téve a modellek tanítását és inferenciáját bármely Windows eszközön.

Frissítések és lehetőségek is érkeztek a DirectML-hez, például akár 150 ONNX operátor támogatása, valamint az ONNX runtime és a WinML általi használat. Jelentős integrált hardvergyártók (IHV-k) támogatják, akik különféle metaparancsokat valósítanak meg.

## CUDA

A CUDA, vagyis Compute Unified Device Architecture, egy párhuzamos számítási platform és alkalmazásprogramozási felület (API) modell, amelyet az Nvidia fejlesztett ki. Lehetővé teszi a fejlesztők számára, hogy CUDA-kompatibilis grafikus feldolgozó egységeket (GPU-kat) használjanak általános célú számításokra – ezt nevezik GPGPU-nak (General-Purpose computing on Graphics Processing Units). A CUDA kulcsfontosságú az Nvidia GPU gyorsításában, és széles körben alkalmazzák gépi tanulásban, tudományos számításokban és videófeldolgozásban.

A CUDA hardvertámogatása kizárólag Nvidia GPU-kra korlátozódik, mivel ez egy Nvidia által fejlesztett szabadalmaztatott technológia. Minden architektúra specifikus CUDA toolkit verziókat támogat, amelyek a szükséges könyvtárakat és eszközöket biztosítják a fejlesztők számára a CUDA alkalmazások építéséhez és futtatásához.

## ONNX

Az ONNX (Open Neural Network Exchange) egy nyílt formátum, amely gépi tanulási modellek ábrázolására szolgál. Lehetővé teszi egy bővíthető számítási gráf modell definiálását, valamint beépített operátorok és szabványos adattípusok meghatározását. Az ONNX segítségével a fejlesztők könnyedén mozgathatják modelljeiket különböző ML keretrendszerek között, elősegítve az interoperabilitást és megkönnyítve az AI alkalmazások létrehozását és telepítését.

A Phi3 mini képes futni ONNX Runtime-tal CPU-n és GPU-n különböző eszközökön, beleértve szerverplatformokat, Windows, Linux és Mac asztali gépeket, valamint mobil CPU-kat.
Az általunk hozzáadott optimalizált konfigurációk:

- ONNX modellek int4 DML-hez: int4-re kvantált AWQ segítségével
- ONNX modell fp16 CUDA-hoz
- ONNX modell int4 CUDA-hoz: int4-re kvantált RTN segítségével
- ONNX modell int4 CPU-hoz és mobilhoz: int4-re kvantált RTN segítségével

## Llama.cpp

A Llama.cpp egy nyílt forráskódú szoftverkönyvtár, amely C++ nyelven íródott. Különböző nagy nyelvi modelleken (LLM-ek) végez inferenciát, beleértve a Llama modellt is. A ggml könyvtárral (általános célú tenzor könyvtár) együtt fejlesztették, és célja, hogy gyorsabb inferenciát és alacsonyabb memóriahasználatot biztosítson az eredeti Python implementációhoz képest. Támogatja a hardveres optimalizációt, kvantálást, valamint egyszerű API-t és példákat kínál. Ha hatékony LLM inferenciára van szükséged, érdemes megismerkedni a llama.cpp-vel, mivel a Phi3 képes futtatni azt.

## GGUF

A GGUF (Generic Graph Update Format) egy formátum gépi tanulási modellek ábrázolására és frissítésére. Különösen hasznos kisebb nyelvi modellek (SLM-ek) esetén, amelyek hatékonyan futtathatók CPU-kon 4-8 bites kvantálással. A GGUF előnyös gyors prototípus-készítéshez, valamint modellek futtatásához élő eszközökön vagy kötegelt feladatokban, például CI/CD pipeline-okban.

**Jogi nyilatkozat**:  
Ez a dokumentum az AI fordító szolgáltatás, a [Co-op Translator](https://github.com/Azure/co-op-translator) segítségével készült. Bár a pontosságra törekszünk, kérjük, vegye figyelembe, hogy az automatikus fordítások hibákat vagy pontatlanságokat tartalmazhatnak. Az eredeti dokumentum az anyanyelvén tekintendő hiteles forrásnak. Kritikus információk esetén professzionális emberi fordítást javaslunk. Nem vállalunk felelősséget a fordítás használatából eredő félreértésekért vagy téves értelmezésekért.