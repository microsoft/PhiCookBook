<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:29:48+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "hu"
}
-->
# A kulcsfontosságú technológiák között szerepelnek

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) – egy alacsony szintű API hardveres gyorsítású gépi tanuláshoz, amely a DirectX 12-re épül.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) – egy párhuzamos számítási platform és alkalmazásprogramozási felület (API) modell, amelyet az Nvidia fejlesztett ki, lehetővé téve az általános célú feldolgozást grafikus feldolgozó egységeken (GPU-kon).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) – egy nyílt formátum gépi tanulási modellek ábrázolására, amely biztosítja az interoperabilitást különböző ML keretrendszerek között.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) – egy formátum gépi tanulási modellek ábrázolására és frissítésére, különösen hasznos kisebb nyelvi modellek esetén, amelyek hatékonyan futtathatók CPU-kon 4-8 bites kvantálással.

## DirectML

A DirectML egy alacsony szintű API, amely hardveres gyorsítású gépi tanulást tesz lehetővé. A DirectX 12-re épül, hogy kihasználja a GPU gyorsítást, és gyártófüggetlen, azaz nem igényel kódmódosítást különböző GPU gyártók között. Elsősorban modelltréning és inferencia munkafolyamatokhoz használják GPU-kon.

A hardvertámogatás szempontjából a DirectML széles körű GPU-kat támogat, beleértve az AMD integrált és dedikált GPU-it, az Intel integrált GPU-it, valamint az NVIDIA dedikált GPU-it. A Windows AI Platform része, és Windows 10 & 11 rendszereken támogatott, lehetővé téve modelltréninget és inferenciát bármely Windows eszközön.

A DirectML-hez kapcsolódóan voltak frissítések és lehetőségek, például akár 150 ONNX operátor támogatása, valamint az ONNX runtime és a WinML használata. Jelentős integrált hardvergyártók (IHV-k) támogatják, akik különféle metaparancsokat valósítanak meg.

## CUDA

A CUDA, azaz Compute Unified Device Architecture, egy párhuzamos számítási platform és API modell, amelyet az Nvidia fejlesztett ki. Lehetővé teszi a fejlesztők számára, hogy CUDA-kompatibilis GPU-kat használjanak általános célú feldolgozásra – ezt GPGPU-nak (General-Purpose computing on Graphics Processing Units) nevezik. A CUDA kulcsfontosságú az Nvidia GPU gyorsításában, és széles körben alkalmazzák gépi tanulásban, tudományos számításokban és videófeldolgozásban.

A CUDA hardvertámogatása kizárólag Nvidia GPU-kra korlátozódik, mivel ez egy Nvidia által fejlesztett, szabadalmaztatott technológia. Minden architektúra meghatározott CUDA toolkit verziókat támogat, amelyek a szükséges könyvtárakat és eszközöket biztosítják a fejlesztők számára a CUDA alkalmazások építéséhez és futtatásához.

## ONNX

Az ONNX (Open Neural Network Exchange) egy nyílt formátum gépi tanulási modellek ábrázolására. Meghatároz egy bővíthető számítási gráf modellt, valamint beépített operátorok és szabványos adattípusok definícióit. Az ONNX lehetővé teszi a fejlesztők számára, hogy modelleket mozgassanak különböző ML keretrendszerek között, elősegítve az interoperabilitást, és megkönnyítve az AI alkalmazások létrehozását és telepítését.

A Phi3 mini képes ONNX Runtime-tal futni CPU-n és GPU-n különböző eszközökön, beleértve szerverplatformokat, Windows, Linux és Mac asztali gépeket, valamint mobil CPU-kat.
Az általunk hozzáadott optimalizált konfigurációk:

- ONNX modellek int4 DML-hez: AWQ-val int4-re kvantált
- ONNX modell fp16 CUDA-hoz
- ONNX modell int4 CUDA-hoz: RTN-nel int4-re kvantált
- ONNX modell int4 CPU-hoz és mobilhoz: RTN-nel int4-re kvantált

## Llama.cpp

A Llama.cpp egy nyílt forráskódú szoftverkönyvtár, amely C++ nyelven íródott. Különböző nagy nyelvi modelleken (LLM-ek) végez inferenciát, beleértve a Llama modellt is. A ggml könyvtárral (általános célú tensor könyvtár) együtt fejlesztették, és célja, hogy gyorsabb inferenciát és kisebb memóriahasználatot biztosítson az eredeti Python implementációhoz képest. Támogatja a hardveres optimalizációt, kvantálást, és egyszerű API-t valamint példákat kínál. Ha hatékony LLM inferencia érdekel, érdemes megismerkedni a llama.cpp-vel, mivel a Phi3 képes futtatni azt.

## GGUF

A GGUF (Generic Graph Update Format) egy formátum gépi tanulási modellek ábrázolására és frissítésére. Különösen hasznos kisebb nyelvi modellek (SLM-ek) esetén, amelyek hatékonyan futtathatók CPU-kon 4-8 bites kvantálással. A GGUF előnyös gyors prototípus készítéshez, valamint modellek futtatásához élő eszközökön vagy batch feladatokban, például CI/CD folyamatokban.

**Jogi nyilatkozat**:  
Ez a dokumentum az AI fordító szolgáltatás [Co-op Translator](https://github.com/Azure/co-op-translator) segítségével készült. Bár a pontosságra törekszünk, kérjük, vegye figyelembe, hogy az automatikus fordítások hibákat vagy pontatlanságokat tartalmazhatnak. Az eredeti dokumentum az anyanyelvén tekintendő hiteles forrásnak. Fontos információk esetén szakmai, emberi fordítást javaslunk. Nem vállalunk felelősséget az ebből eredő félreértésekért vagy félreértelmezésekért.