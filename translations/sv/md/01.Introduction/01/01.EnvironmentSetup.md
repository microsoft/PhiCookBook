<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-09T07:09:55+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "sv"
}
-->
# Kom igång med Phi-3 lokalt

Den här guiden hjälper dig att ställa in din lokala miljö för att köra Phi-3-modellen med Ollama. Du kan köra modellen på flera olika sätt, inklusive med GitHub Codespaces, VS Code Dev Containers eller din lokala miljö.

## Miljöinställning

### GitHub Codespaces

Du kan köra den här mallen virtuellt genom att använda GitHub Codespaces. Knappen öppnar en webbaserad VS Code-instans i din webbläsare:

1. Öppna mallen (det kan ta några minuter):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Öppna ett terminalfönster

### VS Code Dev Containers

⚠️ Det här alternativet fungerar bara om din Docker Desktop har tilldelats minst 16 GB RAM. Om du har mindre än 16 GB RAM kan du prova [GitHub Codespaces-alternativet](../../../../../md/01.Introduction/01) eller [ställa in det lokalt](../../../../../md/01.Introduction/01).

Ett relaterat alternativ är VS Code Dev Containers, som öppnar projektet i din lokala VS Code med hjälp av [Dev Containers-tillägget](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Starta Docker Desktop (installera det om det inte redan är installerat)
2. Öppna projektet:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. I det VS Code-fönster som öppnas, när projektfilerna visas (det kan ta några minuter), öppna ett terminalfönster.
4. Fortsätt med [distributionsstegen](../../../../../md/01.Introduction/01)

### Lokal miljö

1. Se till att följande verktyg är installerade:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Testa modellen

1. Be Ollama att ladda ner och köra phi3:mini-modellen:

    ```shell
    ollama run phi3:mini
    ```

    Det tar några minuter att ladda ner modellen.

2. När du ser "success" i utdata kan du skicka ett meddelande till modellen från prompten.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Efter några sekunder bör du se ett svar strömma in från modellen.

4. För att lära dig om olika tekniker som används med språkmodeller, öppna Python-notebooken [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) och kör varje cell. Om du använde en annan modell än 'phi3:mini', ändra `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` högst upp i filen vid behov, och du kan också justera systemmeddelandet eller lägga till few-shot-exempel om du vill.

**Ansvarsfriskrivning**:  
Detta dokument har översatts med hjälp av AI-översättningstjänsten [Co-op Translator](https://github.com/Azure/co-op-translator). Även om vi strävar efter noggrannhet, var vänlig observera att automatiska översättningar kan innehålla fel eller brister. Det ursprungliga dokumentet på dess modersmål bör betraktas som den auktoritativa källan. För kritisk information rekommenderas professionell mänsklig översättning. Vi ansvarar inte för eventuella missförstånd eller feltolkningar som uppstår vid användning av denna översättning.