# Kom igång med Phi-3 lokalt

Den här guiden hjälper dig att ställa in din lokala miljö för att köra Phi-3-modellen med Ollama. Du kan köra modellen på flera olika sätt, bland annat med GitHub Codespaces, VS Code Dev Containers eller i din lokala miljö.

## Miljöinställning

### GitHub Codespaces

Du kan köra den här mallen virtuellt genom att använda GitHub Codespaces. Knappen öppnar en webbaserad VS Code-instans i din webbläsare:

1. Öppna mallen (det kan ta några minuter):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Öppna ett terminalfönster

### VS Code Dev Containers

⚠️ Det här alternativet fungerar bara om din Docker Desktop har tilldelats minst 16 GB RAM. Om du har mindre än 16 GB RAM kan du prova [GitHub Codespaces-alternativet](../../../../../md/01.Introduction/01) eller [ställa in det lokalt](../../../../../md/01.Introduction/01).

Ett relaterat alternativ är VS Code Dev Containers, som öppnar projektet i din lokala VS Code med hjälp av [Dev Containers-tillägget](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Starta Docker Desktop (installera det om det inte redan är installerat)
2. Öppna projektet:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. I VS Code-fönstret som öppnas, när projektfilerna visas (det kan ta några minuter), öppna ett terminalfönster.
4. Fortsätt med [distributionsstegen](../../../../../md/01.Introduction/01)

### Lokal miljö

1. Se till att följande verktyg är installerade:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Testa modellen

1. Be Ollama att ladda ner och köra phi3:mini-modellen:

    ```shell
    ollama run phi3:mini
    ```

    Det tar några minuter att ladda ner modellen.

2. När du ser "success" i utdata kan du skicka ett meddelande till modellen från prompten.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Efter några sekunder bör du se ett svar strömma in från modellen.

4. För att lära dig om olika tekniker som används med språkmodeller, öppna Python-notebooken [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) och kör varje cell. Om du använde en annan modell än 'phi3:mini', ändra `MODEL_NAME` i den första cellen.

5. För att ha en konversation med phi3:mini-modellen från Python, öppna Python-filen [chat.py](../../../../../code/01.Introduce/chat.py) och kör den. Du kan ändra `MODEL_NAME` högst upp i filen vid behov, och du kan också modifiera systemmeddelandet eller lägga till few-shot-exempel om du vill.

**Ansvarsfriskrivning**:  
Detta dokument har översatts med hjälp av AI-översättningstjänsten [Co-op Translator](https://github.com/Azure/co-op-translator). Även om vi strävar efter noggrannhet, vänligen observera att automatiska översättningar kan innehålla fel eller brister. Det ursprungliga dokumentet på dess modersmål bör betraktas som den auktoritativa källan. För kritisk information rekommenderas professionell mänsklig översättning. Vi ansvarar inte för några missförstånd eller feltolkningar som uppstår vid användning av denna översättning.