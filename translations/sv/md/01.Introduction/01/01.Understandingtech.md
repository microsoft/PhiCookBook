<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:23:48+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "sv"
}
-->
# Nyckelteknologier som nämns inkluderar

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - ett låg-nivå API för hårdvaruaccelererad maskininlärning byggt ovanpå DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - en plattform för parallellberäkning och ett API utvecklat av Nvidia som möjliggör allmän beräkning på grafikkort (GPU:er).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - ett öppet format designat för att representera maskininlärningsmodeller och möjliggöra interoperabilitet mellan olika ML-ramverk.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - ett format som används för att representera och uppdatera maskininlärningsmodeller, särskilt användbart för mindre språkmodeller som kan köras effektivt på CPU:er med 4-8bit kvantisering.

## DirectML

DirectML är ett låg-nivå API som möjliggör hårdvaruaccelererad maskininlärning. Det är byggt ovanpå DirectX 12 för att utnyttja GPU-acceleration och är leverantörsoberoende, vilket innebär att det inte kräver kodändringar för att fungera med olika GPU-leverantörer. Det används främst för modellträning och inferensarbetsbelastningar på GPU:er.

När det gäller hårdvarustöd är DirectML utformat för att fungera med ett brett spektrum av GPU:er, inklusive AMD integrerade och diskreta GPU:er, Intel integrerade GPU:er och NVIDIA diskreta GPU:er. Det är en del av Windows AI Platform och stöds på Windows 10 & 11, vilket möjliggör modellträning och inferens på vilken Windows-enhet som helst.

Det har gjorts uppdateringar och möjligheter relaterade till DirectML, såsom stöd för upp till 150 ONNX-operatörer och används både av ONNX runtime och WinML. Det backas upp av stora Integrated Hardware Vendors (IHVs), som implementerar olika metakommandon.

## CUDA

CUDA, som står för Compute Unified Device Architecture, är en plattform för parallellberäkning och ett API utvecklat av Nvidia. Det gör det möjligt för mjukvaruutvecklare att använda en CUDA-aktiverad grafikkrets (GPU) för allmän beräkning – en metod som kallas GPGPU (General-Purpose computing on Graphics Processing Units). CUDA är en viktig drivkraft för Nvidias GPU-acceleration och används i stor utsträckning inom områden som maskininlärning, vetenskapliga beräkningar och videobehandling.

Hårdvarustödet för CUDA är specifikt för Nvidias GPU:er eftersom det är en proprietär teknik utvecklad av Nvidia. Varje arkitektur stöder specifika versioner av CUDA toolkit, som tillhandahåller de nödvändiga biblioteken och verktygen för utvecklare att bygga och köra CUDA-applikationer.

## ONNX

ONNX (Open Neural Network Exchange) är ett öppet format designat för att representera maskininlärningsmodeller. Det tillhandahåller en definition av en utbyggbar beräkningsgrafmodell samt definitioner av inbyggda operatörer och standarddatatyper. ONNX gör det möjligt för utvecklare att flytta modeller mellan olika ML-ramverk, vilket underlättar interoperabilitet och gör det enklare att skapa och distribuera AI-applikationer.

Phi3 mini kan köras med ONNX Runtime på CPU och GPU över olika enheter, inklusive serverplattformar, Windows, Linux och Mac-datorer samt mobila CPU:er.  
De optimerade konfigurationer vi har lagt till är

- ONNX-modeller för int4 DML: Kvantiserade till int4 via AWQ  
- ONNX-modell för fp16 CUDA  
- ONNX-modell för int4 CUDA: Kvantiserade till int4 via RTN  
- ONNX-modell för int4 CPU och Mobile: Kvantiserade till int4 via RTN

## Llama.cpp

Llama.cpp är ett open-source mjukvarubibliotek skrivet i C++. Det utför inferens på olika stora språkmodeller (LLMs), inklusive Llama. Utvecklat tillsammans med ggml-biblioteket (ett allmänt tensorbibliotek), syftar llama.cpp till att erbjuda snabbare inferens och lägre minnesanvändning jämfört med den ursprungliga Python-implementeringen. Det stödjer hårdvaruoptimering, kvantisering och erbjuder ett enkelt API samt exempel3. Om du är intresserad av effektiv inferens för LLM är llama.cpp värt att utforska eftersom Phi3 kan köra Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) är ett format som används för att representera och uppdatera maskininlärningsmodeller. Det är särskilt användbart för mindre språkmodeller (SLMs) som kan köras effektivt på CPU:er med 4-8bit kvantisering. GGUF är fördelaktigt för snabb prototypframtagning och för att köra modeller på edge-enheter eller i batchjobb som CI/CD-pipelines.

**Ansvarsfriskrivning**:  
Detta dokument har översatts med hjälp av AI-översättningstjänsten [Co-op Translator](https://github.com/Azure/co-op-translator). Även om vi strävar efter noggrannhet, vänligen var medveten om att automatiska översättningar kan innehålla fel eller brister. Det ursprungliga dokumentet på dess modersmål bör betraktas som den auktoritativa källan. För kritisk information rekommenderas professionell mänsklig översättning. Vi ansvarar inte för eventuella missförstånd eller feltolkningar som uppstår vid användning av denna översättning.