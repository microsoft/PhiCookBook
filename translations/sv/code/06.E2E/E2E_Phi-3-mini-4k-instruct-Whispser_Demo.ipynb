{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaktiv Phi 3 Mini 4K Instruktionschatbot med Whisper\n",
    "\n",
    "### Introduktion:\n",
    "Den interaktiva Phi 3 Mini 4K Instruktionschatbot är ett verktyg som låter användare interagera med Microsoft Phi 3 Mini 4K instruktionsdemo via text- eller ljudinmatning. Chatboten kan användas för en mängd olika uppgifter, såsom översättning, väderuppdateringar och allmän informationsinsamling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "Skapa din Huggingface Access Token\n",
    "\n",
    "Skapa en ny token  \n",
    "Ange ett nytt namn  \n",
    "Välj skrivbehörigheter  \n",
    "Kopiera token och spara den på en säker plats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Följande Python-kod utför två huvudsakliga uppgifter: importera modulen `os` och sätta en miljövariabel.\n",
    "\n",
    "1. Importera modulen `os`:\n",
    "   - Modulen `os` i Python ger ett sätt att interagera med operativsystemet. Den gör det möjligt att utföra olika operativsystemrelaterade uppgifter, såsom att komma åt miljövariabler, arbeta med filer och kataloger, etc.\n",
    "   - I denna kod importeras modulen `os` med hjälp av `import`-satsen. Denna sats gör funktionaliteten i modulen `os` tillgänglig för användning i det aktuella Python-skriptet.\n",
    "\n",
    "2. Sätta en miljövariabel:\n",
    "   - En miljövariabel är ett värde som kan nås av program som körs på operativsystemet. Det är ett sätt att lagra konfigurationsinställningar eller annan information som kan användas av flera program.\n",
    "   - I denna kod sätts en ny miljövariabel med hjälp av ordboken `os.environ`. Nyckeln i ordboken är `'HF_TOKEN'`, och värdet tilldelas från variabeln `HUGGINGFACE_TOKEN`.\n",
    "   - Variabeln `HUGGINGFACE_TOKEN` definieras precis ovanför denna kodsnutt och tilldelas ett strängvärde `\"hf_**************\"` med hjälp av syntaxen `#@param`. Denna syntax används ofta i Jupyter-notebooks för att tillåta användarinmatning och parameterkonfiguration direkt i notebook-gränssnittet.\n",
    "   - Genom att sätta miljövariabeln `'HF_TOKEN'` kan den nås av andra delar av programmet eller andra program som körs på samma operativsystem.\n",
    "\n",
    "Sammanfattningsvis importerar denna kod modulen `os` och sätter en miljövariabel med namnet `'HF_TOKEN'` med det värde som tillhandahålls i variabeln `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den här kodsnutten definierar en funktion som heter clear_output, som används för att rensa utdata från den aktuella cellen i Jupyter Notebook eller IPython. Låt oss gå igenom koden och förstå dess funktionalitet:\n",
    "\n",
    "Funktionen clear_output tar en parameter som heter wait, vilket är en boolesk variabel. Som standard är wait satt till False. Denna parameter avgör om funktionen ska vänta tills ny utdata är tillgänglig för att ersätta den befintliga utdatan innan den rensas.\n",
    "\n",
    "Själva funktionen används för att rensa utdata från den aktuella cellen. I Jupyter Notebook eller IPython, när en cell genererar utdata, såsom utskriven text eller grafiska diagram, visas den utdatan under cellen. Funktionen clear_output gör det möjligt att rensa bort den utdatan.\n",
    "\n",
    "Implementeringen av funktionen tillhandahålls inte i kodsnutten, vilket indikeras av ellipsen (...). Ellipsen representerar en platshållare för den faktiska koden som utför rensningen av utdatan. Implementeringen av funktionen kan innebära interaktion med Jupyter Notebook- eller IPython-API:et för att ta bort den befintliga utdatan från cellen.\n",
    "\n",
    "Sammanfattningsvis erbjuder denna funktion ett praktiskt sätt att rensa utdata från den aktuella cellen i Jupyter Notebook eller IPython, vilket gör det enklare att hantera och uppdatera den visade utdatan under interaktiva kodningssessioner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utför text-till-tal (TTS) med Edge TTS-tjänsten. Låt oss gå igenom relevanta funktionsimplementeringar en i taget:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Den här funktionen tar ett inmatningsvärde och beräknar hastighetssträngen för TTS-rösten. Inmatningsvärdet representerar den önskade talhastigheten, där ett värde på 1 motsvarar normal hastighet. Funktionen beräknar hastighetssträngen genom att subtrahera 1 från inmatningsvärdet, multiplicera det med 100 och sedan bestämma tecknet baserat på om inmatningsvärdet är större än eller lika med 1. Funktionen returnerar hastighetssträngen i formatet \"{sign}{rate}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: Den här funktionen tar en inmatningstext och ett språk som parametrar. Den delar upp inmatningstexten i delar baserat på språkets specifika regler. I denna implementation, om språket är \"English\", delar funktionen texten vid varje punkt (\".\") och tar bort eventuella inledande eller avslutande mellanslag. Den lägger sedan till en punkt till varje del och returnerar den filtrerade listan av delar.\n",
    "\n",
    "3. `tts_file_name(text)`: Den här funktionen genererar ett filnamn för TTS-ljudfilen baserat på inmatningstexten. Den utför flera transformationer på texten: tar bort en avslutande punkt (om den finns), konverterar texten till små bokstäver, tar bort inledande och avslutande mellanslag och ersätter mellanslag med understreck. Den trunkerar sedan texten till maximalt 25 tecken (om den är längre) eller använder hela texten om den är tom. Slutligen genererar den en slumpmässig sträng med hjälp av [`uuid`] och kombinerar den med den trunkerade texten för att skapa filnamnet i formatet \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Den här funktionen slår samman flera ljudfiler till en enda ljudfil. Den tar en lista med ljudfilsvägar och en utmatningsväg som parametrar. Funktionen initierar ett tomt `AudioSegment`-objekt kallat [`merged_audio`]. Den itererar sedan genom varje ljudfilsväg, laddar ljudfilen med metoden `AudioSegment.from_file()` från biblioteket `pydub` och lägger till den aktuella ljudfilen till [`merged_audio`]-objektet. Slutligen exporterar den det sammanslagna ljudet till den angivna utmatningsvägen i MP3-format.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: Den här funktionen utför TTS-operationen med Edge TTS-tjänsten. Den tar en lista med textdelar, talhastigheten, röstnamnet och sparvägen som parametrar. Om antalet delar är större än 1 skapar funktionen en katalog för att lagra de individuella ljudfilerna för varje del. Den itererar sedan genom varje del, konstruerar ett Edge TTS-kommando med hjälp av funktionen `calculate_rate_string()`, röstnamnet och texten för delen, och kör kommandot med funktionen `os.system()`. Om kommandot körs framgångsrikt lägger den till sökvägen för den genererade ljudfilen till en lista. Efter att ha bearbetat alla delar slår den samman de individuella ljudfilerna med funktionen `merge_audio_files()` och sparar det sammanslagna ljudet till den angivna sparvägen. Om det bara finns en del genererar den direkt Edge TTS-kommandot och sparar ljudet till sparvägen. Slutligen returnerar den sparvägen för den genererade ljudfilen.\n",
    "\n",
    "6. `random_audio_name_generate()`: Den här funktionen genererar ett slumpmässigt ljudfilnamn med hjälp av [`uuid`]-modulen. Den genererar ett slumpmässigt UUID, konverterar det till en sträng, tar de första 8 tecknen, lägger till \".mp3\"-tillägget och returnerar det slumpmässiga ljudfilnamnet.\n",
    "\n",
    "7. `talk(input_text)`: Den här funktionen är huvudfunktionen för att utföra TTS-operationen. Den tar en inmatningstext som parameter. Den kontrollerar först längden på inmatningstexten för att avgöra om det är en lång mening (större än eller lika med 600 tecken). Baserat på längden och värdet på variabeln `translate_text_flag` avgör den språket och genererar listan med textdelar med funktionen `make_chunks()`. Den genererar sedan en sparväg för ljudfilen med funktionen `random_audio_name_generate()`. Slutligen anropar den funktionen `edge_free_tts()` för att utföra TTS-operationen och returnerar sparvägen för den genererade ljudfilen.\n",
    "\n",
    "Sammanfattningsvis arbetar dessa funktioner tillsammans för att dela upp inmatningstexten i delar, generera ett filnamn för ljudfilen, utföra TTS-operationen med Edge TTS-tjänsten och slå samman de individuella ljudfilerna till en enda ljudfil.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementeringen av två funktioner: convert_to_text och run_text_prompt, samt deklarationen av två klasser: str och Audio.\n",
    "\n",
    "Funktionen convert_to_text tar en audio_path som indata och transkriberar ljudet till text med hjälp av en modell som heter whisper_model. Funktionen kontrollerar först om gpu-flaggan är satt till True. Om den är det används whisper_model med vissa parametrar som word_timestamps=True, fp16=True, language='English' och task='translate'. Om gpu-flaggan är False används whisper_model med fp16=False. Den resulterande transkriptionen sparas sedan i en fil som heter 'scan.txt' och returneras som text.\n",
    "\n",
    "Funktionen run_text_prompt tar ett meddelande och en chat_history som indata. Den använder funktionen phi_demo för att generera ett svar från en chatbot baserat på det givna meddelandet. Det genererade svaret skickas sedan till funktionen talk, som konverterar svaret till en ljudfil och returnerar filens sökväg. Klassen Audio används för att visa och spela upp ljudfilen. Ljudet visas med hjälp av display-funktionen från modulen IPython.display, och Audio-objektet skapas med parametern autoplay=True, så att ljudet börjar spela automatiskt. Chat_history uppdateras med det givna meddelandet och det genererade svaret, och en tom sträng samt den uppdaterade chat_history returneras.\n",
    "\n",
    "Klassen str är en inbyggd klass i Python som representerar en sekvens av tecken. Den tillhandahåller olika metoder för att manipulera och arbeta med strängar, såsom capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill och fler. Dessa metoder gör det möjligt att utföra operationer som att söka, ersätta, formatera och manipulera strängar.\n",
    "\n",
    "Klassen Audio är en anpassad klass som representerar ett ljudobjekt. Den används för att skapa en ljudspelare i Jupyter Notebook-miljön. Klassen accepterar olika parametrar såsom data, filename, url, embed, rate, autoplay och normalize. Parametern data kan vara en numpy-array, en lista med samples, en sträng som representerar ett filnamn eller URL, eller rå PCM-data. Parametern filename används för att specificera en lokal fil att ladda ljuddata från, och parametern url används för att specificera en URL att ladda ner ljuddata från. Parametern embed avgör om ljuddata ska bäddas in med en data-URI eller refereras från den ursprungliga källan. Parametern rate specificerar samplingsfrekvensen för ljuddata. Parametern autoplay avgör om ljudet ska börja spela automatiskt. Parametern normalize specificerar om ljuddata ska normaliseras (omskalas) till det maximala möjliga intervallet. Klassen Audio tillhandahåller också metoder som reload för att ladda om ljuddata från fil eller URL, och attribut som src_attr, autoplay_attr och element_id_attr för att hämta motsvarande attribut för ljudelementet i HTML.\n",
    "\n",
    "Sammanfattningsvis används dessa funktioner och klasser för att transkribera ljud till text, generera ljudsvar från en chatbot och visa samt spela upp ljud i Jupyter Notebook-miljön.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfriskrivning**:  \nDetta dokument har översatts med hjälp av AI-översättningstjänsten [Co-op Translator](https://github.com/Azure/co-op-translator). Även om vi strävar efter noggrannhet, bör det noteras att automatiserade översättningar kan innehålla fel eller brister. Det ursprungliga dokumentet på dess originalspråk bör betraktas som den auktoritativa källan. För kritisk information rekommenderas professionell mänsklig översättning. Vi ansvarar inte för eventuella missförstånd eller feltolkningar som kan uppstå vid användning av denna översättning.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T23:14:50+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "sv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}