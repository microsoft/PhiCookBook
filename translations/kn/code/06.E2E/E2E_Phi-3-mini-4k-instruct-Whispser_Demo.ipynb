{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ಇಂಟರಾಕ್ಟಿವ್ Phi 3 Mini 4K Instruct ಚಾಟ್‌ಬಾಟ್ Whisper ಜೊತೆಗೆ\n",
    "\n",
    "### ಪರಿಚಯ:\n",
    "ಇಂಟರಾಕ್ಟಿವ್ Phi 3 Mini 4K Instruct ಚಾಟ್‌ಬಾಟ್ ಒಂದು ಸಾಧನವಾಗಿದೆ, ಇದು ಬಳಕೆದಾರರಿಗೆ ಪಠ್ಯ ಅಥವಾ ಧ್ವನಿ ಇನ್‌ಪುಟ್ ಬಳಸಿ Microsoft Phi 3 Mini 4K instruct ಡೆಮೊದೊಂದಿಗೆ ಸಂವಹನ ಮಾಡಲು ಅನುಮತಿಸುತ್ತದೆ. ಈ ಚಾಟ್‌ಬಾಟ್ ಅನ್ನು ಅನೇಕ ಕಾರ್ಯಗಳಿಗಾಗಿ ಬಳಸಬಹುದು, ಉದಾಹರಣೆಗೆ ಅನುವಾದ, ಹವಾಮಾನ ನವೀಕರಣಗಳು ಮತ್ತು ಸಾಮಾನ್ಯ ಮಾಹಿತಿಯ ಸಂಗ್ರಹಣೆ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[ನಿಮ್ಮ Huggingface ಪ್ರವೇಶ ಟೋಕನ್ ರಚಿಸಿ](https://huggingface.co/settings/tokens)\n",
    "\n",
    "ಹೊಸ ಟೋಕನ್ ರಚಿಸಿ \n",
    "ಹೊಸ ಹೆಸರನ್ನು ನೀಡಿ \n",
    "ಬರೆಯುವ ಅನುಮತಿಗಳನ್ನು ಆಯ್ಕೆಮಾಡಿ\n",
    "ಟೋಕನ್ ಅನ್ನು ನಕಲಿಸಿ ಮತ್ತು ಅದನ್ನು ಸುರಕ್ಷಿತ ಸ್ಥಳದಲ್ಲಿ ಉಳಿಸಿ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python and it performs two main tasks: importing the `os` module and setting an environment variable.\n",
    "\n",
    "1. Importing the `os` module:\n",
    "   - Python ನಲ್ಲಿ `os` ಮೋಡ್ಯೂಲ್ ಆಪರೇಟಿಂಗ್ ಸಿಸ್ಟಂ ಜೊತೆ ಸಂವಹನ ಮಾಡಲು ಒಂದು ವಿಧಾನವನ್ನು ಒದಗಿಸುತ್ತದೆ. ಇದು ಪರಿಸರ ಚರಗಳನ್ನು ಪ್ರವೇಶಿಸುವುದು, ಕಡತಗಳು ಮತ್ತು ಡೈರೆಕ್ಟರಿಗಳೊಡನೆ ಕೆಲಸ ಮಾಡುವುದು ಮುಂತಾದ ವಿವಿಧ ಆಪರೇಟಿಂಗ್ ಸಿಸ್ಟಂ ಸಂಬಂಧಿ ಕಾರ್ಯಗಳನ್ನು ಮಾಡಲು ಅನುಮತಿಸುತ್ತದೆ, ಇತ್ಯಾದಿ.\n",
    "   - ಈ ಕೋಡ್‌ನಲ್ಲಿ `import` ಹೇಳಿಕೆಯ ಮೂಲಕ `os` ಮೋಡ್ಯೂಲ್ ಅನ್ನು ಆಮದು ಮಾಡಲಾಗಿದೆ. ಈ ಹೇಳಿಕೆ `os` ಮೋಡ್ಯೂಲ್‌ನ ಕಾರ್ಯಕ್ಷಮತೆಯನ್ನು ಪ್ರಸ್ತುತ Python ಸ್ಕ್ರಿಪ್ಟ್‌ನಲ್ಲಿ ಬಳಸಲು ಲಭ್ಯವಾಗಿಸುತ್ತದೆ.\n",
    "\n",
    "2. Setting an environment variable:\n",
    "   - ಪರಿಸರ ಚರವು ಆಪರೇಟಿಂಗ್ ಸಿಸ್ಟಂ‌ನಲ್ಲಿ ಓಡುತ್ತಿರುವ ಪ್ರೋಗ್ರಾಮ್‌ಗಳು ಪ್ರವೇಶಿಸಬಹುದಾದ ಒಂದು ಮೌಲ್ಯ. ಇದು ಬಹುಪ್ರೋಗ್ರಾಮ್‌ಗಳು ಬಳಸಬಹುದಾದ ಕಾನ್ಫಿಗರ್ ಸೆಟ್ಟಿಂಗಳು ಅಥವಾ ಇತರೆ ಮಾಹಿತಿಯನ್ನು ಸಂಗ್ರಹಿಸುವ ರೀತಿ.\n",
    "   - ಈ ಕೋಡ್‌ನಲ್ಲಿ `os.environ` ಡಿಕ್ಷನರಿ ಬಳಸುವ ಮೂಲಕ ಹೊಸ ಪರಿಸರ ಚರವನ್ನು ಸೆಟ್ ಮಾಡಲಾಗಿದೆ. ಡಿಕ್ಷನರಿಯ ಕೀ `'HF_TOKEN'` ಆಗಿದ್ದು, ಮೌಲ್ಯವು `HUGGINGFACE_TOKEN` ಚರದಿಂದ ನಿಯೋಜಿಸಲಾಗಿದೆ.\n",
    "   - `HUGGINGFACE_TOKEN` ಚರವು ಈ ಕೋಡ್ ಸ્નಿಪೆಟ್‌ನ 바로 ಮೇಲ್ಭಾಗದಲ್ಲಿ ತಿದ್ದಲಾಗಿದೆ, ಮತ್ತು ಅದಕ್ಕೆ `#@param` ಸಿಂಟ್ಯಾಕ್ಸ್ ಬಳಸಿ `\"hf_**************\"` ಎಂಬ ಸ್ಟ್ರಿಂಗ್ ಮೌಲ್ಯ ನಿಯೋಜಿಸಲಾಗಿದೆ. ಈ ಸಿಂಟ್ಯಾಕ್ಸ್ ಸಾಮಾನ್ಯವಾಗಿ Jupyter ನೋಟ್ಬುಕ್‌ಗಳಲ್ಲಿ ಬಳಕೆದಾರ ಇನ್‌ಪುಟ್ ಮತ್ತು ಪರಾಮೀಟರ್ ಸಂರಚನೆಯನ್ನು ನೇರವಾಗಿ ನೋಟ್‌ಬುಕ್ ಇಂಟರ್‌ಫೇಸ್ನಲ್ಲಿ ಅನುಮತಿಸಲು ಬಳಸಲಾಗುತ್ತದೆ.\n",
    "   - `'HF_TOKEN'` ಪರಿಸರ ಚರವನ್ನು ಸೆಟ್ ಮಾಡುವ ಮೂಲಕ, ಅದನ್ನು ಪ್ರೋಗ್ರಾಮ್‌ನ ಇತರ ಭಾಗಗಳು ಅಥವಾ ಅದೇ ಆಪರೇಟಿಂಗ್ ಸಿಸ್ಟಮ್‌ನಲ್ಲಿ ಓಡುತ್ತಿರುವ ಇತರೆ ಪ್ರೋಗ್ರಾಮ್‌ಗಳು ಪ್ರವೇಶಿಸಬಹುದು.\n",
    "\n",
    "Overall, this code imports the `os` module and sets an environment variable named `'HF_TOKEN'` with the value provided in the `HUGGINGFACE_TOKEN` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ಈ ಕೋಡ್ ಉದಾಹರಣೆ clear_output ಹೆಸರಿನ ಫಂಕ್ಷನ್ ಅನ್ನು ವ್ಯಾಖ್ಯಾನಿಸುತ್ತದೆ, ಇದು Jupyter Notebook ಅಥವಾ IPython ನಲ್ಲಿ ಪ್ರಸ್ತುತ ಸೆಲ್‌ನ ಫಲಿತಾಂಶವನ್ನು ತೆರವು ಮಾಡಲು ಬಳಸಲಾಗುತ್ತದೆ. ಕೋಡ್ ಅನ್ನು ವಿಭಜಿಸಿ ಅದರ ಕಾರ್ಯಕ್ಷಮತೆಯನ್ನು ಅರ್ಥಮಾಡಿಕೊಳ್ಳೋಣ:\n",
    "\n",
    "clear_output ಫಂಕ್ಷನ್ ಒಂದು ಪ್ಯಾರಾಮೀಟರ್ wait ಅನ್ನು ತೆಗೆದುಕೊಳ್ಳುತ್ತದೆ, ಇದು ಬೂಲಿಯನ್ ಮೌಲ್ಯವಾಗಿದೆ. ಡೀಫಾಲ್ಟ್ ಮೂಲಕ, wait ಗಾಗಿ ಮೊದಲು False ಆಗಿ ಸೆಟ್ ಮಾಡಲಾಗಿದೆ. ಈ ಪ್ಯಾರಾಮೀಟರ್ ಹೊಸ ಫಲಿತಾಂಶ ಲಭ್ಯವಾಗುವವರೆಗೆ ಇರುವ ಫಲಿತಾಂಶವನ್ನು ಬದಲಾಯಿಸಲು ಮುಂಚಿತವಾಗಿ ಕಾಯಬೇಕೇ ಎಂದು ನಿರ್ಧರಿಸುತ್ತದೆ.\n",
    "\n",
    "ಫಂಕ್ಷನ್ ತಾನೇ ಪ್ರಸ್ತುತ ಸೆಲ್‌ನ ಫಲಿತಾಂಶವನ್ನು ತೆರವು ಮಾಡಲು ಬಳಸಲಾಗುತ್ತದೆ. Jupyter Notebook ಅಥವಾ IPython ನಲ್ಲಿ, ಒಂದು ಸೆಲ್ ಮುದ್ರಿತ ಪಠ್ಯ ಅಥವಾ ಗ್ರಾಫಿಕಲ್ ಪ್ಲಾಟ್ಸ್ പോലಿನ ಫಲಿತಾಂಶವನ್ನು ಉತ್ಪಾದಿಸಿದಾಗ, ಆ ಫಲಿತಾಂಶ ಸೆಲ್ ಕೆಳಗೆ ಪ್ರದರ್ಶಿಸಲಾಗುತ್ತದೆ. clear_output ಫಂಕ್ಷನ್ ಆ ಫಲಿತಾಂಶವನ್ನು ತೆರವುಗೊಳಿಸಲು ಅನುಮತಿಸುತ್ತದೆ.\n",
    "\n",
    "ಫಂಕ್ಷನ್‌ನ ಜಾರಿಗೆ ಸಂಬಂಧಿಸಿದ ಕೋಡ್ ಸ್ನಿಪೆಟ್‌ನಲ್ಲಿ ನೀಡಲ್ಪಟ್ಟಿಲ್ಲ, ಇದನ್ನು ಇಲಿಪ್ಸಿಸ್ (...) ಸೂಚಿಸುತ್ತದೆ. ಈ ಇಲಿಪ್ಸಿಸ್ ಫಲಿತಾಂಶವನ್ನು ತೆರವು ಮಾಡುವ ನಿಜವಾದ ಕೋಡ್‌ನ ಪ್ಲೇಸ್ಹೋಲ್ಡರ್ ಅನ್ನು ಪ್ರತಿನಿಧಿಸುತ್ತದೆ. ಫಂಕ್ಷನ್‌ನ ಜಾರಿಗೆ Jupyter Notebook ಅಥವಾ IPython API ಜೊತೆ ಸಂವಹನ ಮಾಡುವುದು ಮತ್ತು ಸೆಲ್‌ನಿಂದ ಇರುವ ಫಲಿತಾಂಶವನ್ನು ತೆಗೆದುಹಾಕುವುದು ಒಳಗೊಂಡಿರಬಹುದು.\n",
    "\n",
    "ಒಟ್ಟಾರೆ, ಈ ಫಂಕ್ಷನ್ Jupyter Notebook ಅಥವಾ IPython ನಲ್ಲಿ ಪ್ರಸ್ತುತ ಸೆಲ್‌ನ ಫಲಿತಾಂಶವನ್ನು ತೆರವುಗೊಳಿಸುವ ಸುಲಭ ವಿಧಾನವನ್ನು ಒದಗಿಸುತ್ತದೆ, ಇಂಟರಾಕ್ಟಿವ್ ಕೋಡಿಂಗ್ ಸೆಷನ್‌ಗಳ ಸಮಯದಲ್ಲಿ ಪ್ರದರ್ಶಿತ ಫಲಿತಾಂಶವನ್ನು ನಿರ್ವಹಿಸಲು ಮತ್ತು ನವೀಕರಿಸಲು ಇದು ಸುಗಮವಾಗಿಸುತ್ತದೆ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform text-to-speech (TTS) using the Edge TTS service. Let's go through the relevant function implementations one by one:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: ಈ ಫಂಕ್ಷನ್ ಒಂದು ಇನ್ಪುಟ್ ಮೌಲ್ಯವನ್ನು ತೆಗೆದುಕೊಳ್ಳುತ್ತದು ಮತ್ತು TTS ವಾಯ್ಸ್‌ಗಾಗಿ ರೇಟ್ ಸ್ಟ್ರಿಂಗ್ ಅನ್ನು ಲೆಕ್ಕ ಹಾಕುತ್ತದೆ. ಇನ್ಪುಟ್ ಮೌಲ್ಯವು ಮಾತನಾಡುವ ವೇಗದ ಇಚ್ಛಿತ ಮೌಲ್ಯವನ್ನು ಸೂಚಿಸುತ್ತದೆ, ಇಲ್ಲಿ 1 ಎನ್ನುವುದು ಸಾಮಾನ್ಯ ವೇಗವನ್ನು ಸೂಚಿಸುತ್ತದೆ. ಫಂಕ್ಷನ್ ರೇಟ್ ಸ್ಟ್ರಿಂಗ್ ಅನ್ನು ಲೆಕ್ಕಹಾಕುವಲ್ಲಿ ಮೊದಲಿಗೆ ಇನ್ಪುಟ್ ಮೌಲ್ಯದಿಂದ 1 ಅನ್ನು ಕಡಿಮೆ ಮಾಡುತ್ತದೆ, ನಂತರ ಅದನ್ನು 100ರಿಂದ ಗುಣಿಸಲು, ಮತ್ತು ಇನ್ಪುಟ್ ಮೌಲ್ಯವು 1ಕ್ಕಿಂತ ದೊಡ್ಡದಾಗಿದೆಯೋ ಅಥವಾ ಸಮನಾಗಿದೆಯೋ ಎಂಬ ಆಧಾರದ ಮೇಲೆ ದೃಷ್ಟಿ ಚಿಹ್ನೆ (sign) ನಿರ್ಧರಿಸುತ್ತದೆ. ಫังก್ಷನ್ \"{sign}{rate}\" ಎಂಬ ಫಾರ್ಮ್ಯಾಟ್‌ನಲ್ಲಿ ರೇಟ್ ಸ್ಟ್ರಿಂಗ್ ಅನ್ನು ಮರಳಿಸುತ್ತದೆ.\n",
    "\n",
    "2.`make_chunks(input_text, language)`: ಈ ಫಂಕ್ಷನ್ ಒಂದು ಇನ್ಪುಟ್ ಪಠ್ಯ ಮತ್ತು ಒಂದು ಭಾಷೆಯನ್ನು ಪರಿಮಾಣಗಳಾಗಿ ತೆಗೆದುಕೊಳ್ಳುತ್ತದೆ. ಇದು ಭಾಷಾ-ನಿರ್ದಿಷ್ಟ ನಿಯಮಗಳನ್ನು ಆಧರಿಸಿ ಪಠ್ಯವನ್ನು ತುಂಡುಗಳಾಗಿ ವಿಭಜಿಸುತ್ತದೆ. ಈ ಅನುಷ್ಠಾನದಲ್ಲಿ, ಭಾಷೆ \"English\" ಆಗಿದ್ದರೆ, ಫಂಕ್ಷನ್ ಪಠ್ಯವನ್ನು ಪ್ರತಿಯೊಬ್ಬ ಪೀರಿಯಾಡ್ (\".\") ನಲ್ಲಿ ವಿಭಜಿಸುತ್ತದೆ ಮತ್ತು ಯಾವುದೇ ಮುಂಭಾಗದ ಅಥವಾ ಕೊನೆಯಲ್ಲಿ ಇರುವ ಖಾಲಿ ಸ್ಥಳಗಳನ್ನು ತೆಗೆದುಹಾಕುತ್ತದೆ. ನಂತರ ಅದು ಪ್ರತಿಯೊಂದು ತುಂಡಿಗೆ ಒಂದು ಪೀರಿಯಾಡ್ ಸೇರಿಸುತ್ತದೆ ಮತ್ತು ಫಿಲ್ಟರ್ ಮಾಡಿದ ತುಂಡುಗಳ ಪಟ್ಟಿಯನ್ನು ಮರಳಿಸುತ್ತದೆ.\n",
    "\n",
    "3. `tts_file_name(text)`: ಈ ಫಂಕ್ಷನ್ ಇನ್ಪುಟ್ ಪಠ್ಯದ ಆಧಾರದ ಮೇಲೆ TTS ಆಡಿಯೋ ಫೈಲ್‌ಗಾಗಿ ಫೈಲ್ ಹೆಸರು ರಚಿಸುತ್ತದೆ. ಇದು ಪಠ್ಯದ ಮೇಲೆ ಹಲವಾರು ಪರಿವರ್ತನೆಗಳನ್ನು ಮಾಡುತ್ತದೆ: ಕೊನೆಯಲ್ಲಿ ಇರುವ ಪೀರಿಯಾಡ್ ಅನ್ನು ತೆಗೆದುಹಾಕುವುದು (ಇದಿದ್ದರೆ), ಪಠ್ಯವನ್ನು lowercase ಗೆ ಪರಿವರ್ತಿಸುವುದು, ಮುಂಭಾಗ ಮತ್ತು ಕೊನೆಯ ಖಾಲಿ ಸ್ಥಳಗಳನ್ನು ತೆರವುಗೊಳಿಸುವುದು, ಮತ್ತು ಖಾಲಿ ಸ್ಥಳಗಳನ್ನು ಅಂಡರ್ಸ್ಕೋರ್‌ಗಳಾಗಿ ಬದಲಿಸುವುದು. ನಂತರ ಅದು ಪಠ್ಯವನ್ನು ಗರಿಷ್ಠ 25 ಅಕ್ಷರಗಳಷ್ಟು ಕತ್ತರಿಸುತ್ತದೆ (ಇದು ದೀರ್ಘವಾಗಿದ್ದರೆ) ಅಥವಾ ಪಠ್ಯ ಖಾಲಿ ಇದ್ದಲ್ಲಿ ಪೂರ್ಣ ಪಠ್ಯವನ್ನು ಬಳಸುತ್ತದೆ. ಕೊನೆಗೆ, ಇದು [`uuid`] ಮೋಡ್ಯೂಲ್ ಬಳಸಿಕೊಂಡು ಒಂದು ಯಾದೃಚ್ಛಿಕ ಸ್ಟ್ರಿಂಗ್ ಅನ್ನು ರಚಿಸಿ, ಅದನ್ನು ಕತ್ತರಿಸಿದ ಪಠ್ಯದೊಂದಿಗೆ ಸಂಯೋಜಿಸಿ \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\" ಎಂಬ ಫಾರ್ಮ್ಯಾಟ್‌ನಲ್ಲಿ ಫೈಲ್ ಹೆಸರನ್ನು ತಯಾರಿಸುತ್ತದೆ.\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: ಈ ಫಂಕ್ಷನ್ ಹಲವು ಆಡಿಯೋ ಫೈಲ್‌ಗಳನ್ನು ಒಂದೇ ಒಂದು ಆಡಿಯೋ ಫೈಲ್ ಆಗಿ ವಿಲೀನಗೊಳಿಸುತ್ತದೆ. ಇದು ಆಡಿಯೋ ಫೈಲ್ ಮಾರ್ಗಗಳ ಪಟ್ಟಿಯನ್ನು ಮತ್ತು ಒಂದು output_path ಅನ್ನು ಪರಿಮಾಣಗಳಾಗಿ ತೆಗೆದುಕೊಳ್ಳುತ್ತದೆ. ಫಂಕ್ಷನ್ ಮೊದಲಿಗೆ ಖಾಲಿ `AudioSegment` ವಸ್ತುವನ್ನು [`merged_audio`] ಎಂಬ ಹೆಸರಿನಲ್ಲಿ ಆರಂಭಿಸುತ್ತದೆ. ನಂತರ ಇದು ಪ್ರತಿ ಆಡಿಯೋ ಫೈಲ್ ಮಾರ್ಗವನ್ನು ಕ್ರಮವಾಗಿ ಪರಿಗಣಿಸುತ್ತದೆ, `pydub` ಲೈಬ್ರರಿಯ `AudioSegment.from_file()` ಮೆತೋಡ್ ಬಳಸಿ ಆಡಿಯೋ ಫೈಲ್ ಅನ್ನು ಲೋಡ್ ಮಾಡುತ್ತದೆ, ಮತ್ತು ಪ್ರಸ್ತುತ ಆಡಿಯೋ ಫೈಲ್ ಅನ್ನು [`merged_audio`] ವಸ್ತುವಿಗೆ ಜೋಡಿಸುತ್ತದೆ. ಕೊನೆಗೆ, ಇದು ವಿಲೀನಗೊಳ್ಳಿದ್ದ ಆಡಿಯೋವನ್ನು ಸೂಚಿಸಿದ output_path ಗೆ MP3 ಫಾರ್ಮ್ಯಾಟ್‌ನಲ್ಲಿ ಎಕ್ಸ್‌ಪೋರ್ಟ್ ಮಾಡುತ್ತದೆ.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path): This function performs the TTS operation using the Edge TTS service. It takes a list of text chunks, the speed of the speech, the voice name, and the save path as parameters. If the number of chunks is greater than 1, the function creates a directory for storing the individual chunk audio files. It then iterates through each chunk, constructs an Edge TTS command using the `calculate_rate_string()' function, the voice name, and the chunk text, and executes the command using the `os.system()` function. If the command execution is successful, it appends the path of the generated audio file to a list. After processing all the chunks, it merges the individual audio files using the `merge_audio_files()` function and saves the merged audio to the specified save path. If there is only one chunk, it directly generates the Edge TTS command and saves the audio to the save path. Finally, it returns the save path of the generated audio file.\n",
    "\n",
    "6. `random_audio_name_generate()`: ಈ ಫಂಕ್ಷನ್ [`uuid`] ಮೋಡ್ಯೂಲ್ ಬಳಸಿ ಯಾದೃಚ್ಛಿಕ ಆಡಿಯೋ ಫೈಲ್ ಹೆಸರು ರಚಿಸುತ್ತದೆ. ಇದು случай ನ್ನು (random UUID) ರಚಿಸಿ ಅದನ್ನು ಸ್ಟ್ರಿಂಗ್‌ಗೆ ಪರಿವರ್ತಿಸಿ, ಮೊದಲ 8 ಅಕ್ಷರಗಳನ್ನು ತೆಗೆದು, \".mp3\" ಎಕ್ಸ್‌ಟೆನ್ಷನ್ ಅನ್ನು ಜೋಡಿಸಿ ಯಾದೃಚ್ಛಿಕ ಆಡಿಯೋ ಫೈಲ್ ಹೆಸರನ್ನು ಮರಳಿಸುತ್ತದೆ.\n",
    "\n",
    "7. `talk(input_text)`: ಈ ಫಂಕ್ಷನ್ TTS ಕಾರ್ಯಕ್ಕಾಗಿ ಮುಖ್ಯ ಎಂಟ್ರಿ ಪಾಯಿಂಟ್ ಆಗಿದೆ. ಇದು ಇನ್ಪುಟ್ ಪಠ್ಯವನ್ನು ಪರಿಮಾಣವಾಗಿ ತೆಗೆದುಕೊಳ್ಳುತ್ತದೆ. ಮೊದಲಿಗೆ ಇದು ಇನ್ಪುಟ್ ಪಠ್ಯದ ಉದ್ದವನ್ನು ಪರಿಶೀಲಿಸಿ ಅದು ದೀರ್ಘ ವಾಕ್ಯವೇ ಎಂದು (600 ಅಕ್ಷರಗಳಿಗಿಂತ ಹೆಚ್ಚು ಅಥವಾ ಸಮ) ನಿರ್ಧರಿಸುತ್ತದೆ. ಉದ್ದ ಮತ್ತು `translate_text_flag` ಚರದ ಮೌಲ್ಯದ ಆಧಾರದ ಮೇಲೆ, ಇದು ಭಾಷೆಯನ್ನು ನಿರ್ಧರಿಸಿ `make_chunks()` ಫಂಕ್ಷನ್ ಬಳಸಿ ಪಠ್ಯದ ತುಂಡುಗಳ ಪಟ್ಟಿಯನ್ನು ರಚಿಸುತ್ತದೆ. ನಂತರ ಇದು `random_audio_name_generate()` ಫಂಕ್ಷನ್ ಬಳಸಿ ಆಡಿಯೋ ಫೈಲ್ ಮತ್ಯಾದಿಯ ಒಂದನ್ನು ತಯಾರಿಸುತ್ತದೆ. ಕೊನೆಗೆ, ಇದು TTS ಕಾರ್ಯವನ್ನು ನಿರ್ವಹಿಸಲು `edge_free_tts()` ಫಂಕ್ಷನ್ ಅನ್ನು ಕರೆದು ಉತ್ಪಾದಿತ ಆಡಿಯೋ ಫೈಲ್‌ನ save path ಅನ್ನು ಮರಳಿಸುತ್ತದೆ.\n",
    "\n",
    "Overall, these functions work together to split the input text into chunks, generate a file name for the audio file, perform the TTS operation using the Edge TTS service, and merge the individual audio files into a single audio file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ಇದು convert_to_text ಮತ್ತು run_text_prompt ಎಂಬ ಎರಡು ಫಂಕ್ಷನ್‌ಗಳ ಅನುಷ್ಠಾನ ಮತ್ತು str ಮತ್ತು Audio ಎಂಬ ಎರಡು ವರ್ಗಗಳ ಘೋಷಣೆಯನ್ನು ಒಳಗೊಂಡಿದೆ.\n",
    "\n",
    "convert_to_text ಫಂಕ್ಷನ್ audio_path ಅನ್ನು ಇನ್‌ಪುಟ್ ಆಗಿ ಸ್ವೀಕರಿಸಿ, whisper_model ಎಂಬ ಮಾದರಿಯನ್ನು ಬಳಸಿ ಆಡಿಯೋವನ್ನು ಪಠ್ಯಕ್ಕೆ ಟ್ರಾನ್ಸ್‌ಕ್ರೈಬ್ ಮಾಡುತ್ತದೆ. ಫಂಕ್ಷನ್ ಮೊದಲಿಗೆ gpu ಫ್ಲಾಗ್ Trueಗೆ ಸೆಟ್ ಆಗಿದೆಯೇ ಎಂದು ಪರಿಶೀಲಿಸುತ್ತದೆ. ಅದು True ಆಗಿದ್ದರೆ, whisper_model ಅನ್ನು word_timestamps=True, fp16=True, language='English', ಮತ್ತು task='translate' ಎಂಬ ಕೆಲ ಪ್ಯಾರಾಮೀಟರ್‌ಗಳೊಂದಿಗೆ ಬಳಸಲಾಗುತ್ತದೆ. gpu ಫ್ಲಾಗ್ False ಆಗಿದ್ದರೆ, whisper_model ಅನ್ನು fp16=False ಹೊಂದುವಂತೆ ಬಳಸಲಾಗುತ್ತದೆ. ಪಡೆಯಲಾದ ಟ್ರಾನ್ಸ್‌ಕ್ರಿಪ್ಷನ್ ನಂತರ 'scan.txt' ಎಂಬ ಫೈಲ್ಗೆ ಉಳಿಸುವ지고 ಪಠ್ಯವಾಗಿ ಮರುಹೊಂದಿಸಲಾಗುತ್ತದೆ.\n",
    "\n",
    "run_text_prompt ಫಂಕ್ಷನ್ message ಮತ್ತು chat_history ಅನ್ನು ಇನ್‌ಪುಟ್ ಆಗಿ принимает (ಸ್ವೀಕರಿಸುತ್ತದೆ). ಇದು phi_demo ಫಂಕ್ಷನ್ ಅನ್ನು ಬಳಸಿಕೊಂಡು ಇನ್‌ಪುಟ್ message ಆಧಾರಿತವಾಗಿ ಚಾಟ್‌ಬಾಟ್‌ನಿಂದ ಪ್ರತಿಕ್ರಿಯೆಯನ್ನು ರಚಿಸುತ್ತದೆ. ರಚಿಸಲಾದ ಪ್ರತಿಕ್ರಿಯೆಯನ್ನು talk ಫಂಕ್ಷನ್‌ಗೆ ನೀಡಲಾಗುತ್ತದೆ, ಅದು ಆ ಪ್ರತಿಕ್ರಿಯೆಯನ್ನು ಆಡಿಯೋ ಫೈಲ್ ಆಗಿ ಪರಿವರ್ತಿಸಿ ಫೈಲ್ ಪಥವನ್ನು ಮರಳಿ ನೀಡುತ್ತದೆ. ಆಡಿಯೋ ಫೈಲ್ ಅನ್ನು ಪ್ರದರ್ಶಿಸಲು ಮತ್ತು ಪ್ಲೇ ಮಾಡಲು Audio ಕ್ಲಾಸ್ ಬಳಸಲಾಗುತ್ತದೆ. ಆಡಿಯೋವನ್ನು IPython.displayモジュールನ display ಫಂಕ್ಷನ್ ಬಳಸಿ ಪ್ರದರ್ಶಿಸಲಾಗುತ್ತದೆ, ಮತ್ತು Audio ಆಬ್ಜೆಕ್ಟ್ ಅನ್ನು autoplay=True ಪ್ಯಾರಾಮೀಟರ್‌ನೊಂದಿಗೆ ರಚಿಸಲಾಗುತ್ತದೆ, ಆದ್ದರಿಂದ ಆಡಿಯೋ ಸ್ವಯಂಚಾಲಿತವಾಗಿ ಆಡಲು ಪ್ರಾರಂಭವಾಗುತ್ತದೆ. chat_history ಇನ್‌ಪುಟ್ message ಮತ್ತು ರಚಿಸಲಾದ ಪ್ರತಿಕ್ರಿಯೆಯಿಂದ ನವೀಕರಿಸಲಾಗುತ್ತದೆ, ಮತ್ತು ಖಾಲಿ ಸ್ಟ್ರಿಂಗ್ ಮತ್ತು ನವೀಕರಿಸಿದ chat_history ಅನ್ನು ರಿಟರ್ನ್ ಮಾಡುತ್ತದೆ.\n",
    "\n",
    "str ಕ್ಲಾಸ್ Python ನಲ್ಲಿನ built-in ಕ್ಲಾಸ್ ಆಗಿದ್ದು, ಅಕ್ಷರಗಳ ಕ್ರಮವನ್ನು (sequence of characters) ಪ್ರತಿನಿಧಿಸುತ್ತದೆ. ಇದು ಸ್ಟ್ರಿಂಗ್‌ಗಳನ್ನು ಸಂಚಾಲಿಸುವುದು ಮತ್ತು ಅವುಗಳೊಂದಿಗೆ ಕೆಲಸ ಮಾಡುವುದಕ್ಕಾಗಿ ಹಲವು ವಿಧಾನಗಳನ್ನು ಒದಗಿಸುತ್ತದೆ, ಉದಾಹರಣೆಗೆ capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill, ಮತ್ತು ಇನ್ನಷ್ಟು. ಈ ವಿಧಾನಗಳು ಹುಡುಕುವುದು, ಬದಲಾಯಿಸುವುದು, ಫಾರ್ಮ್ಯಾಟ್ ಮಾಡುವುದು ಮತ್ತು ಸ್ಟ್ರಿಂಗ್‌ಗಳನ್ನು ಸಂಚಾಲಿಸುವಂತಹ ಕಾರ್ಯಗಳನ್ನು ನಿರ್ವಹಿಸಲು ನಿಮಗೆ ಅನುಕೂಲವಾಗುತ್ತವೆ.\n",
    "\n",
    "Audio ಕ್ಲಾಸ್ ಒಂದು ಕಸ್ಟಮ್ ಕ್ಲಾಸ್ ಆಗಿದ್ದು, ಒಂದು ಆಡಿಯೋ ವಸ್ತುವನ್ನು ಪ್ರತಿನಿಧಿಸುತ್ತದೆ. ಇದು Jupyter Notebook ಪರಿಸರದಲ್ಲಿ ಆಡಿಯೋ ಪ್ಲೇಯರ್ ರಚಿಸಲು ಉಪಯೋಗಿಸಲಾಗುತ್ತದೆ. ಈ ಕ್ಲಾಸ್ data, filename, url, embed, rate, autoplay, ಮತ್ತು normalize ಮುಂತಾದ ವಿವಿಧ ಪ್ಯಾರಾಮೀಟರ್‌ಗಳನ್ನು ಸ್ವೀಕರಿಸುತ್ತದೆ. data ಪ್ಯಾರಾಮೀಟರ್ numpy array ಆಗಿರಬಹುದು, ಸ್ಯಾಂಪಲ್ಸ್‌ನ ಲಿಸ್ಟ್ ಆಗಿರಬಹುದು, ಫೈಲ್‌ನೆ임 ಅಥವಾ URL ಅನ್ನು ಸೂಚಿಸುವ ಸ್ಟ್ರಿಂಗ್ ಆಗಿರಬಹುದು, ಅಥವಾ raw PCM ಡೇಟಾವಾಗಿರಬಹುದು. filename ಪ್ಯಾರಾಮೀಟರ್ ಆಡಿಯೋ ಡೇಟಾವನ್ನು ಲೋಡ್ ಮಾಡಲು ಸ್ಥಳೀಯ ಫೈಲ್ ಅನ್ನು ಸೂಚಿಸಲು ಉಪಯೋಗಿಸಲಾಗುತ್ತದೆ, ಮತ್ತು url ಪ್ಯಾರಾಮೀಟರ್ ಡೇಟಾವನ್ನು ಡೌನ್‌ಲೋಡ್ ಮಾಡಲು URL ಅನ್ನು ಸೂಚಿಸಲು ಉಪಯೋಗಿಸಲಾಗುತ್ತದೆ. embed ಪ್ಯಾರಾಮೀಟರ್ ಆಡಿಯೋ ಡೇಟಾವನ್ನು data URI ಬಳಸಿ ಎम्बೆಡ್ ಮಾಡಬೇಕೇ ಅಥವಾ ಮೂಲ nguồn ಅನ್ನು ಉಲ್ಲೇಖಿಸಬೇಕೇ ಎಂದು ನಿರ್ಧರಿಸುತ್ತದೆ. rate ಪ್ಯಾರಾಮೀಟರ್ ಆಡಿಯೋ ಡೇಟಾದ ಸ್ಯಾಂಪ್ಲಿಂಗ್ ರೇಟ್ ಅನ್ನು ಸೂಚಿಸುತ್ತದೆ. autoplay ಪ್ಯಾರಾಮೀಟರ್ ಆಡಿಯೋ ಸ್ವಯಂಚಾಲಿತವಾಗಿ ಪ್ಲೇ ಆಗಬೇಕೇ ಎಂಬುದನ್ನು ನಿರ್ಧರಿಸುತ್ತದೆ. normalize ಪ್ಯಾರಾಮೀಟರ್ ಆಡಿಯೋ ಡೇಟಾವನ್ನು ಗರಿಷ್ಠ ಸಾಧ್ಯವಿರುವ ಶ್ರೇಣಿಗೆ ಪುನರ್‌ಮಾಪನ (rescale) ಮಾಡುವುದೇ ಎಂಬುದನ್ನು ಸೂಚಿಸುತ್ತದೆ. Audio ಕ್ಲಾಸ್ reload వంటి ವಿಧಾನಗಳನ್ನು ಕೂಡ ಒದಗಿಸುತ್ತದೆ, ಫೈಲ್ ಅಥವಾ URL ನಿಂದ ಆಡಿಯೋ ಡೇಟಾವನ್ನು ಪುನರ್ ಲೋಡ್ ಮಾಡಲು, ಮತ್ತು src_attr, autoplay_attr, ಮತ್ತು element_id_attrಂತಹ ಅಟ್ರಿಬ್ಯೂಟ್‌ಗಳನ್ನು HTMLದಲ್ಲಿನ ಆಡಿಯೋ ಎಲೆಮೆಂಟ್‌ಗೆ ಸಂಬಂಧಿಸಿದ ಅಟ್ರಿಬ್ಯೂಟ್‌ಗಳನ್ನು ಪಡೆದುಕೊಳ್ಳಲು ಒದಗಿಸುತ್ತದೆ.\n",
    "\n",
    "ಒಟ್ಟಾರೆ, ಈ ಫಂಕ್ಷನ್‌ಗಳು ಮತ್ತು ಕ್ಲಾಸ್‌ಗಳು ಆಡಿಯೋವನ್ನು ಪಠ್ಯಕ್ಕೆ ಟ್ರಾನ್ಸ್‌ಕ್ರೈಬ್ ಮಾಡಲು, ಚಾಟ್‌ಬಾಟ್‌ನಿಂದ ಆಡಿಯೋ ಪ್ರತಿಕ್ರಿಯೆಗಳನ್ನು ಸೃಷ್ಟಿಸಲು, ಮತ್ತು Jupyter Notebook ವಾತಾವರಣದಲ್ಲಿ ಆಡಿಯೋವನ್ನು ಪ್ರದರ್ಶಿಸಲು ಮತ್ತು ಪ್ಲೇ ಮಾಡಲು ಬಳಸಲಾಗುತ್ತವೆ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\nಅಸ್ವೀಕರಣ:\nಈ ದಾಖಲೆವನ್ನು AI ಅನುವಾದ ಸೇವೆ Co-op Translator (https://github.com/Azure/co-op-translator) ಬಳಸಿ ಅನುವಾದಿಸಲಾಗಿದೆ. ನಾವು ನಿಖರತೆಗಾಗಿ ಪ್ರಯತ್ನಿಸುತ್ತಿದ್ದರೂ, ಸ್ವಯಂಚಾಲಿತ ಅನುವಾದಗಳಲ್ಲಿ ದೋಷಗಳು ಅಥವಾ ಅನನ್ವಯತೆಗಳು ಇರಬಹುದೆಂದು ದಯವಿಟ್ಟು ಗಮನಿಸಿ. ಮೂಲ ಭಾಷೆಯಲ್ಲಿರುವ ಮೂಲ ದಾಖಲೆನ್ನು ಅಧಿಕೃತ ಮೂಲವೆಂದು ಪರಿಗಣಿಸಬೇಕು. ಮಹತ್ವದ ಮಾಹಿತಿಗಾಗಿ ವೃತ್ತಿಪರ ಮಾನವ ಅನುವಾದವನ್ನು ಶಿಫಾರಸು ಮಾಡಲಾಗುತ್ತದೆ. ಈ ಅನುವಾದದ ಬಳಕೆಯಿಂದ ಉಂಟಾಗುವ ಯಾವುದೇ ತಪ್ಪು ಅರ್ಥಮಾಡಿಕೊಳ್ಳುವಿಕೆಗಳು ಅಥವಾ ದುರವ್ಯಾಖ್ಯಾನಗಳಿಗಾಗಿ ನಾವು ಹೊಣೆಗಾರರಾಗುತ್ತೇವೆಂದಿಲ್ಲ.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-12-22T05:07:39+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "kn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}