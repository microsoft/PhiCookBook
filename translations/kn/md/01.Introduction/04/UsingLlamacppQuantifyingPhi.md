# **llama.cpp ಬಳಸಿ Phi ಕುಟುಂಬವನ್ನು ಕ್ವಾಂಟೈಸ್ ಮಾಡುವುದು**

## **llama.cpp ಎಂದರೆ ಏನು**

llama.cpp ಮುಖ್ಯವಾಗಿ C++ ನಲ್ಲಿ ಬರೆಯಲ್ಪಟ್ಟಿರುವ ಓಪನ್-ಸೋರ್ಸ್ ಸಾಫ್ಟ್‌ವೇರ್ ಲೈಬ್ರರಿಗೆ, ಇದು Llama ಮುಂತಾದ ವಿವಿಧ ದೊಡ್ಡ ಭಾಷಾ ಮಾದರಿಗಳ (LLMs) ಮೇಲೆ ಇನ್ಫರೆನ್ಸ್ ನಡೆಸುತ್ತದೆ. ಇದರ ಮುಖ್ಯ ಉದ್ದೇಶ ಕಡಿಮೆ ಸೆಟ್‌ಅಪ್‌ನೊಂದಿಗೆ ವಿಭಿನ್ನ ಹಾರ್ಡ್‌ವೇರ್‌ಗಳ ಮೇಲೆ ಅತ್ಯಾಧುನಿಕ ಕಾರ್ಯಕ್ಷಮತೆಯೊಂದಿಗೆ LLM ಇನ್ಫರೆನ್ಸ್ ಒದಗಿಸುವುದು. ಹೆಚ್ಚಾಗಿ, ಈ ಲೈಬ್ರರಿ үчүн Python ಬೈಂಡಿಂಗ್‌ಗಳು ಲಭ್ಯವಿದ್ದು, ಅವು ಪಠ್ಯ ಪೂರ್ಣಗೊಳಿಸುವಿಕೆಯುಳ್ಳ ಹೈ-ಲೆವೆಲ್ API ಮತ್ತು OpenAI ಹೊಂದಾಣಿಕೆಯ ವೆಬ್ ಸರ್ವರ್ ಅನ್ನು ಒದಗಿಸುತ್ತವೆ.

llama.cpp ಯ ಮುಖ್ಯ ಉದ್ದೇಶವು ಕಡಿಮೆ ಸೆಟ್‌ಅಪ್‌ನೊಂದಿಗೆ ಮತ್ತು ವಿವಿಧ ರೀತಿಯ ಹಾರ್ಡ್‌ವೇರ್‌ಗಳ ಮೇಲೆ — ಸ್ಥಳೀಯವಾಗಿ ಮತ್ತು ಕ್ಲೌಡ್‌ನಲ್ಲಿ — ಅತ್ಯಾಧುನಿಕ ಕಾರ್ಯಕ್ಷಮತೆಯೊಂದಿಗೆ LLM ಇನ್ಫರೆನ್ಸ್ ಸಕ್ಷಮಗೊಳಿಸುವುದು.

- ಯಾವುದೇ ನಿರ್ಭರತೆಯಿಲ್ಲದ ಸادಾ C/C++ ಅನುಷ್ಠಾನ
- Apple silicon ಪ್ರಥಮ ತರಗತಿಯ ನಾಗರಿಕ — ARM NEON, Accelerate ಮತ್ತು Metal ಫ್ರೇಮ್‌ವರ್ಕ್‌ಗಳ ಮೂಲಕ ಕಂಪ್ಯೂಟ್ ಅವುಗಳಿಗೆ ಸಂಶೋಧನೆ ಮಾಡಲಾಗಿದೆ
- x86 ವಾಸ್ತುಶಿಲ್ಪಗಳಿಗಾಗಿ AVX, AVX2 ಮತ್ತು AVX512 ಬೆಂಬಲ
- ವೇಗದ ಇನ್ಫರೆನ್ಸ್ ಮತ್ತು ಕಡಿಮೆ ಮೆಮೊರಿ ಬಳಕೆಗೆ 1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit ಮತ್ತು 8-bit ಪೂರ್ಣಾಂಕ (integer) ಕ್ವಾಂಟೈಜೆಶನ್
- NVIDIA GPUs ಮೇಲೆ LLMಗಳನ್ನು چلಿಸಲು ಕಸ್ಟಮ್ CUDA ಕಾರ್ನಲ್ಗಳು (AMD GPUs ಗೆ HIP ಮೂಲಕ ಬೆಂಬಲ)
- Vulkan ಮತ್ತು SYCL ಬ್ಯಾಕೆಂಡ್ ಬೆಂಬಲ
- ಒಟ್ಟು VRAM ಸಾಮರ್ಥ್ಯಕ್ಕಿಂತ ದೊಡ್ಡ ಮಾದರಿಗಳನ್ನು ಭಾಗಶಃ ವೇಗಗೊಳಿಸಲು CPU+GPU ಹೈಬ್ರಿಡ್ ಇನ್ಫರೆನ್ಸ್

## **llama.cpp ಬಳಸಿ Phi-3.5 ಅನ್ನು ಕ್ವಾಂಟೈಸ್ ಮಾಡುವುದು**

Phi-3.5-Instruct ಮಾದರಿಯನ್ನು llama.cpp ಬಳಸಿ ಕ್ವಾಂಟೈಸ್ ಮಾಡಬಹುದು, ಆದರೆ Phi-3.5-Vision ಮತ್ತು Phi-3.5-MoE ಇನ್ನೂ ಬೆಂಬಲಿಸಲ್ಪಟ್ಟಿಲ್ಲ. llama.cpp ಮೂಲಕ ಪರಿವರ್ತಿಸಲ್ಪಡುವ ಫಾರ್ಮ್ಯಾಟ್ gguf ಆಗಿದೆ, ಇದು ಹೆಚ್ಚಿನ ಪ್ರಮಾಣದಲ್ಲಿ ಬಳಕೆಯಲ್ಲಿರುವ ಕ್ವಾಂಟೈಜೇಶನ್ ಫಾರ್ಮ್ಯಾಟ್ ಆಗಿದೆ.

Hugging face ನಲ್ಲಿ quantized GGUF ಫಾರ್ಮ್ಯಾಟ್ ಮಾದರಿಗಳ ದೊಡ್ಡ ಸಂಖ್ಯೆಯಿವೆ. AI Foundry, Ollama, ಮತ್ತು LlamaEdgeಗಳು llama.cpp ಮೇಲೆ ಅವಲಂಬಿಸಿರುವುದರಿಂದ GGUF ಮಾದರಿಗಳೂ ಸಹ ಸಾಮಾನ್ಯವಾಗಿ ಬಳಸಲಾಗುತ್ತವೆ.

### **GGUF ಎಂದರೆ ಏನು**

GGUF ಒಂದು ಬೈನರಿ ಫಾರ್ಮ್ಯಾಟ್ ಆಗಿದ್ದು, ಮಾದರಿಗಳನ್ನು ವೇಗವಾಗಿ ಲೋಡ್ ಮತ್ತು சேฟ್ ಮಾಡುವುದಕ್ಕೆ ಸುಧಾರಿತವಾಗಿದೆ, ಇದರಿಂದ ಇನ್ಫರೆನ್ಸ್ ಉದ್ದೇಶಗಳಿಗೆ ಇದು ಅತ್ಯಂತ ಪರಿಣಾಮಕಾರಿಯಾಗಿದೆ. GGUF ಅನ್ನು GGML ಮತ್ತು ಇತರ ಎಕ್ಸಿಕ್ಯೂಟರ್‌ಗಳೊಂದಿಗೆ ಬಳಸಲು ರಚಿಸಲಾಗಿದೆ. GGUF ಅನ್ನು @ggerganov ಅವರು ಅಭಿವೃದ್ಧಿಪಡಿಸಿದ್ದಾರೆ, ಅವರುಲ್ಲದೆ llama.cpp ನ ಅಭಿವೃದ್ಧಿಪಡಣಾ ಕಾರ್ಯದಲ್ಲಿಯೂ ಇದ್ದಾರೆ, ಇದು ಜನಪ್ರಿಯ C/C++ LLM ಇನ್ಫರೆನ್ಸ್ ಫ್ರೇಮ್‌ವರ್ಕ್ ಆಗಿದೆ. ಮೊದಲಿಗೆ PyTorch ಮುಂತಾದ ಫ್ರೇಮ್‌ವರ್ಕ್‌ಗಳಲ್ಲಿ ಅಭಿವೃದ್ಧಿಪಡಿಸಲಾದ ಮಾದರಿಗಳನ್ನು ಆ ಎಂಜಿನ್‌ಗಳೊಂದಿಗೆ ಬಳಸಲು GGUF ಫಾರ್ಮ್ಯಾಟ್‌ಗೆ ಪರಿವರ್ತಿಸಬಹುದು.

### **ONNX vs GGUF**

ONNX ಒಂದು ಶಾಸನಬद्ध ಮಷೀನ್ ಲರ್ನಿಂಗ್/ಡೀಪ್ ಲರ್ನಿಂಗ್ ಫಾರ್ಮ್ಯಾಟ್ ಆಗಿದ್ದು, ಇದು ವಿಭಿನ್ನ AI ಫ್ರೇಮ್‌ವರ್ಕ್‌ಗಳಲ್ಲಿ ಉತ್ತಮ ಬೆಂಬಲ ಹೊಂದಿದ್ದು ಎಡ್ಜ್ ಸಾಧನಗಳಲ್ಲಿ ಉತ್ತಮ ಉಪಯೋಗದ ದೃಶ್ಯಾವಳಿಗಳನ್ನು ಹೊಂದಿದೆ. GGUF ಎಂದರೆ llama.cpp ಮೇಲೆ ಆಧಾರಿತವಾಗಿದೆ ಮತ್ತು GenAI ಯುಗದಲ್ಲಿಯೇ ಉತ್ಪತ್ತಿಗೊಂಡದ್ದಾಗಿ ಹೇಳಬಹುದಾಗಿದೆ. ಎರಡೂ ಸಮಾನ ರೀತಿಯ ಉಪಯೋಗಗಳನ್ನು ಹೊಂದಿವೆ. ಸಂಯೋಜಿತ ಹೆರ್ಬ್ರಡೆಡ್ ಮತ್ತು ಅಪ್ಲಿಕೇಶನ್ ಲೇಯರ್‌ಗಳಲ್ಲಿ ಉತ್ತಮ ಕಾರ್ಯಕ್ಷಮತೆಯನ್ನು ಬಯಸಿದರೆ, ONNX ನೀವು ಆಯ್ಕೆಮಾಡಬಹುದು. ನೀವು llama.cpp ನ ಅವಕೋಶ ಮತ್ತು ತಂತ್ರಜ್ಞಾನವನ್ನು ಬಳಸುತ್ತಿದ್ದರೆ, τότε GGUF ಉತ್ತಮವಾಗಿರಬಹುದು.

### **llama.cpp ಬಳಸಿ Phi-3.5-Instruct ಅನ್ನು ಕ್ವಾಂಟೈಸ್ ಮಾಡುವುದು**

**1. ಪರಿಸರ ಸಂರಚನೆ**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. ಕ್ವಾಂಟೈಜೆಶನ್**

llama.cpp ಬಳಸಿ Phi-3.5-Instruct ಅನ್ನು FP16 GGUF ಗೆ ಪರಿವರ್ತಿಸುವುದು


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Phi-3.5 ಅನ್ನು INT4 ಗೆ ಕ್ವಾಂಟೈಸ್ ಮಾಡುವುದು


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. ಪರೀಕ್ಷೆ**

llama-cpp-python ಅನ್ನು ಸ್ಥಾಪಿಸಿ


```bash

pip install llama-cpp-python -U

```

***ಸೂಚನೆ*** 

If you use Apple Silicon , ದಯವಿಟ್ಟು ಈ ರೀತಿಯಾಗಿ llama-cpp-python ಅನ್ನು ಸ್ಥಾಪಿಸಿ


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

ಪರೀಕ್ಷೆ 


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **ಸಂಪನ್ಮೂಲಗಳು**

1. llama.cpp ಕುರಿತು ಹೆಚ್ಚಿನ ಮಾಹಿತಿಯನ್ನು ತಿಳಿದುಕೊಳ್ಳಿ [https://github.com/ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)
2. onnxruntime ಕುರಿತು ಹೆಚ್ಚಿನ ಮಾಹಿತಿಯನ್ನು ತಿಳಿದುಕೊಳ್ಳಿ [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)
3. GGUF ಕುರಿತು ಹೆಚ್ಚಿನ ಮಾಹಿತಿಯನ್ನು ತಿಳಿದುಕೊಳ್ಳಿ [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**ನಿರಾಕರಣೆ**:
ಈ ದಾಖಲೆ [Co-op Translator](https://github.com/Azure/co-op-translator) ಎಂಬ AI ಅನುವಾದ ಸೇವೆಯನ್ನು ಬಳಸಿ ಅನುವದಿಸಲಾಗಿದೆ. ನಾವು ಶುದ್ದತೆಯನ್ನು ಕಾಯ್ದುಕೊಳ್ಳಲು ಪ್ರಯತ್ನಿಸಿದರೂ ಸಹ, ದಯವಿಟ್ಟು ಗಮನಿಸಿ ಸ್ವಯಂಚಾಲಿತ ಅನುವಾದಗಳಲ್ಲಿ ತಪ್ಪುಗಳು ಅಥವಾ ಅಸೂಕ್ತತೆಗಳು ಇರಬಹುದು. ಮೂಲ ಭಾಷೆಯಲ್ಲಿನ ಮೂಲ ದಾಖಲೆವನ್ನು ಅಧಿಕೃತ ಮೂಲವೆಂದು ಪರಿಗಣಿಸಬೇಕು. ಅತ್ಯಂತ ಪ್ರಮುಖ ಮಾಹಿತಿಗಾಗಿ ವೃತ್ತಿಪರ ಮಾನವ ಅನುವಾದವನ್ನು ಶಿಫಾರಸು ಮಾಡಲಾಗಿದೆ. ಈ ಅನುವಾದದ ಬಳಕೆಯಿಂದ ಉಂಟಾದ ಯಾವುದೇ ತಪ್ಪು ಗ್ರಹಿಕೆಗಳು ಅಥವಾ ತಪ್ಪು ವ್ಯಾಖ್ಯಾನಗಳಿಗೆ ನಾವು ಹೊಣೆಗಾರರಾಗುವುದಿಲ್ಲ.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->