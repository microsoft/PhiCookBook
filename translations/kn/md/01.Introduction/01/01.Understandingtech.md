<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-12-21T23:05:51+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "kn"
}
-->
# ಉಲ್ಲೇಖಿತ ಪ್ರಮುಖ ತಂತ್ರಜ್ಞಾನಗಳು

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12 ಮೇಲ್ಮೈಯಲ್ಲಿ ನಿರ್ಮಿತವಾದ, ಹಾರ್ಡ್‌ವೇರ್‑ವೇಗವರ್ಧಿತ ಯಂತ್ರ ಕಲಿಕಾ ಕಾರ್ಯಗಳಿಗೆ ತಳಮಟ್ಟದ API.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia ಮೂಲಕ ಅಭಿವೃದ್ಧಿಪಡಿಸಿದ ಪ್ಯಾರಲೆಲ್ ಕಂಪ್ಯೂಟಿಂಗ್ ವೇದಿಕೆ ಮತ್ತು ಅಪ್ಲಿಕೇಶನ್ ಪ್ರೋಗ್ರಾಮಿಂಗ್ ಇಂಟರ್‌ಫೇಸ್ (API) ಮಾದರಿ, ಗ್ರಾಫಿಕ್ಸ್ ಪ್ರೊಸೆಸಿಂಗ್ ಯುನಿಟ್‌ಗಳಲ್ಲಿ (GPUs) ಸಾಮಾನ್ಯ ಉದ್ದೇಶದ ಪ್ರોસೆಸಿಂಗ್‌ಗೆ ಅನುಮತಿಸುವುದು.
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - ವಿಭಿನ್ನ ML ಫ್ರೇಮ್ವರ್ಕ್‌ಗಳ ಮಧ್ಯೆ ಇಂಟರೋಪರೇಬಿಲಿಟಿಯನ್ನು ಒದಗಿಸುವ ಹಾಗೂ ಯಂತ್ರ ಕಲಿಕಾ ಮಾದರಿಗಳನ್ನು ಪ್ರತಿನಿಧಿಸಲು ರೂಪುಗೊಂಡಿರುವ ತೆರೆದ ಫಾರ್ಮ್ಯಾಟ್.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - ಯಂತ್ರ ಕಲಿಕಾ ಮಾದರಿಗಳನ್ನು ಪ್ರತಿನಿಧಿಸಲು ಮತ್ತು ಅಪ್ಡೇಟ್ ಮಾಡಲು ಬಳಸುವ ಫಾರ್ಮ್ಯಾಟ್, ವಿಶೇಷವಾಗಿ 4-8bit ಪ್ರಮಾಣೀಕರಣದೊಂದಿಗೆ CPU ಗಳಲ್ಲಿ ಪರಿಣಾಮಕಾರಿಯಾಗಿ ಚಾಲನೆ ಮಾಡಬಹುದಾದ ಸಣ್ಣ ಭಾಷಾ ಮಾದರಿಗಳಿಗಾಗಿ שימוש.

## DirectML

DirectML ಒಂದು ತಳಮಟ್ಟದ API ಆಗಿದ್ದು, ಹಾರ್ಡ್‌ವೇರ್‑ವೇಗವರ್ಧಿತ ಯಂತ್ರ ಕಲಿಕೆಯನ್ನು ಸಾಧ್ಯಮಾಡುತ್ತದೆ. ಇದು GPU ವೇಗವರ್ಧನೆ ಬಳಸಲು DirectX 12 ಮೇಲೆ ನಿರ್ಮಿತವಾಗಿದ್ದು, ವ್ಯಂಡರ್‑ಆಗ್ನೋಸ್ಟಿಕ್ ಆಗಿದೆ — ಅಂದರೆ ವಿಭಿನ್ನ GPU ವENDOR ಗಳಲ್ಲಿ ಕಾರ್ಯನಿರ್ವಹಿಸಲು ಕೋಡ್ ಬದಲಾವಣೆಗಳನ್ನು ಅವಶ್ಯಕತೆಯಾಗಿಸುವುದಿಲ್ಲ. ಇದನ್ನು ಪ್ರಮುಖವಾಗಿ GPU ಗಳ ಮೇಲೆ ಮಾದರಿ ತರಬೇತಿ ಮತ್ತು ಇನ್ಫರೆನ್ಸಿಂಗ್ ಕೆಲಸಗಳಿಗಾಗಿ ಬಳಸಲಾಗುತ್ತದೆ.

ಹಾರ್ಡ್‌ವೇರ್ ಬೆಂಬಲದ ಕುರಿತು, DirectML ಅನ್ನು ವಿಭಿನ್ನ vrsteಗಳ GPU ಗಳೊಂದಿಗೆ ಕೆಲಸ ಮಾಡಲು ವಿನ್ಯಾಸಗೊಳಿಸಲಾಗಿದೆ, ಇದರಲ್ಲಿ AMD ಇಂಟಿಗ್ರೇಟೆಡ್ ಮತ್ತು ಡಿಸ್ಕ್ರೀಟ್ GPU ಗಳು, Intel ಇಂಟಿಗ್ರೇಟೆಡ್ GPU ಗಳು ಮತ್ತು NVIDIA ಡಿಸ್ಕ್ರೀಟ್ GPU ಗಳು ಸೇರಿವೆ. ಇದು Windows AI ಪ್ಲಾಟ್‌ಫಾರ್ಮ್ ಭಾಗವಾಗಿದ್ದು Windows 10 & 11 ಮೇಲೆ ಬೆಂಬಲಿತವಾಗಿದೆ, ಯಾವುದೇ Windows ಸಾಧನದ ಮೇಲೆ ಮಾದರಿ ತರಬೇತಿ ಮತ್ತು ಇನ್ಫರೆನ್ಸಿಂಗ್ ಮಾಡಲು ಅನುಮತಿಸುತ್ತದೆ.

DirectML ಗೆ ಸಂಬಂಧಿಸಿದ ನವೀಕರಣಗಳು ಮತ್ತು ಅವಕಾಶಗಳಿವೆ, ಉದಾಹರಣೆಗೆ 150 ONNX ಆಪರೇಟರ್‌ಗಳ ವರೆಗೆ ಬೆಂಬಲ ನೀಡುವುದು ಮತ್ತು ONNX runtime ಮತ್ತು WinML ಎರಡರಲ್ಲಿಯೂ ಉಪಯೋಗಿಸಲಾಗುತ್ತಿದೆ. ಇದು ಪ್ರಮುಖ ಇಂಟೆಗ್ರೇಟೆಡ್ ಹಾರ್ಡ್‌ವೇರ್ ವೆಂಡರ್‌ಗಳು (IHVs) ದಿಂದ ಬೆಂಬಲಿತವಾಗಿದೆ, ಪ್ರತಿ ವೆಂಡರ್ ವಿವಿಧ ಮೆಟಾ ಕಮಾಂಡ್ಗಳನ್ನು ಅನುಷ್ಟಾನಗೊಳಿಸುತ್ತಾರೆ.

## CUDA

CUDA, ಅಂದರೆ Compute Unified Device Architecture, Nvidia ಮೂಲಕ ರಚಿಸಲಾದ ಪ್ಯಾರಲೆಲ್ ಕಂಪ್ಯೂಟಿಂಗ್ ವೇದಿಕೆ ಮತ್ತು API ಮಾದರಿಯಾಗಿದೆ. ಇದು ಸಾಫ್ಟ್‌ವೇರ್ ಅಭಿವೃದ್ಧಿಪಡಿಸುವವರಿಗೆ CUDA ಸಕ್ರಿಯಗೊಳಿಸಲಾದ ಗ್ರಾಫಿಕ್ಸ್ ಪ್ರೊಸೆಸಿಂಗ್ ಯುನಿಟ್ (GPU) ಅನ್ನು ಸಾಮಾನ್ಯ ಉದ್ದೇಶದ ಪ್ರಾಸೆಸಿಂಗ್‌ಗಾಗಿ ಉಪಯೋಗಿಸಲು ಅನುಮತಿಸುತ್ತದೆ — ಇದನ್ನು GPGPU (General‑Purpose computing on Graphics Processing Units) ಎಂದು ಕರೆಯಲಾಗುತ್ತದೆ. CUDA Nvidia ಯ_GPU ವೇಗವರ್ಧನೆಯ ಪ್ರಮುಖ ಸಿಧ್ದಾಂತವಾಗಿದ್ದು, ಯಂತ್ರ ಕಲಿಕೆ, ವೈಜ್ಞಾನಿಕ ಗಣನೆ ಮತ್ತು ವಿಡಿಯೋ ಪ್ರೊಸೆಸಿಂಗ್ ಇತ್ಯಾದಿ ಕ್ಷೇತ್ರಗಳಲ್ಲಿ ವ್ಯಾಪಕವಾಗಿ ಬಳಕೆಗೊಳ್ಳುತ್ತದೆ.

CUDA ಗೆ ಹಾರ್ಡ್‌ವೇರ್ ಬೆಂಬಲವು Nvidia ಯ GPU ಗಳಿಗೆ ನಿರ್ದಿಷ್ಟವಾಗಿದೆ, ಏಕೆಂದರೆ ಇದು Nvidia ನೇ ತಂತ್ರಜ್ಞಾನವಾಗಿದೆ. ಪ್ರತಿ ಆರ್ಕಿಟೆಕ್ಚರ್‌ವು ಅಭಿವೃದ್ಧಿಪಡಕರಿಗೆ CUDA ಅಪ್ಲಿಕೇಶನ್‌ಗಳನ್ನು ನಿರ್ಮಿಸಲು ಮತ್ತು ಚಲಾಯಿಸಲು ಅಗತ್ಯವಾದ ಗ್ರಂಥಾಲಯಗಳು ಮತ್ತು ಸಾಧನಗಳನ್ನು ಒದಗಿಸುವ CUDA ಟೂಲ್ಕಿಟ್ ನ ನಿರ್ದಿಷ್ಟ ಆವೃತ್ತಿಗಳನ್ನು ಬೆಂಬಲಿಸುತ್ತದೆ.

## ONNX

ONNX (Open Neural Network Exchange) ಒಂದು ತೆರೆದ ಫಾರ್ಮ್ಯಾಟ್ ಆಗಿದ್ದು, ಯಂತ್ರ ಕಲಿಕಾ ಮಾದರಿಗಳನ್ನು ಪ್ರತಿನಿಧಿಸಲು ರೂಪಗೊಳ್ಳಲಾಗಿದೆ. ಇದು ವಿಸ್ತರಿಸಬಹುದಾದ ಗಣನೆ ಗ್ರಾಫ್ ಮಾದರಿಯ ವ್ಯಾಖ್ಯಾನವನ್ನು ಜೊತೆಗೆ ನಿರ್ಮಿತ ಆಪರೇಟರ್‌ಗಳ ವ್ಯಾಖ್ಯಾನಗಳು ಮತ್ತು ಮಾನಕ ಡೇಟಾ ಪ್ರಕಾರಗಳನ್ನು ಸೀಮಿತವಾಗಿ ನಿರ್ದಿಷ್ಟಪಡಿಸುತ್ತದೆ. ONNX ಅಭಿವೃದ್ಧಿಪಡಕರಿಗೆ ವಿಭಿನ್ನ ML ಫ್ರೇಮ್ವರ್ಕ್‌ಗಳ ನಡುವೆ ಮಾದರಿಗಳನ್ನು ಸ್ಥಳಾಂತರಿಸಲು ಅವಕಾಶ ನೀಡುತ್ತದೆ, ಇಂಟರೋಪರೇಬಿಲಿಟಿಯನ್ನು ಸಾಧಿಸಿ AI ಅಪ್ಲಿಕೇಶನ್‌ಗಳನ್ನು ರಚಿಸಲಾಗುವುದು ಮತ್ತು ನಿಯೋಜಿಸುವುದನ್ನು ಸುಲಭಗೊಳಿಸುತ್ತದೆ.

Phi3 mini ONNX Runtime ಮೂಲಕ CPU ಮತ್ತು GPUಗಳಲ್ಲಿ ವಿವಿಧ ಸಾಧನಗಳ ಮೇಲೆ, ಸರ್ವರ್ ವేదికಗಳು, Windows, Linux ಮತ್ತು Mac ಡೆಸ್ಕ್‌ಟಾಪ್‌ಗಳು ಮತ್ತು ಮೊಬೈಲ್ CPU ಗಳ ಸಹಿತ ಚಲಾಯಿಸಬಹುದು.
ನಾವು ಸೇರಿಸಿದ ಆಪ್ಟಿಮೈಜ್ಡ್ ಕಾನ್ಫಿಗರೇಶನ್‌ಗಳು ಇವು:

- ONNX models for int4 DML: Quantized to int4 via AWQ
- ONNX model for fp16 CUDA
- ONNX model for int4 CUDA: Quantized to int4 via RTN
- ONNX model for int4 CPU and Mobile: Quantized to int4 via RTN

## Llama.cpp

Llama.cpp ಒಂದು ಮುಕ್ತಮೂಲ ಸಾಫ್ಟ್‌ವೇರ್ ಗ್ರಂಥಾಲಯವಾಗಿದ್ದು C++ ನಲ್ಲಿ ಬರೆಯಲ್ಪಟ್ಟಿದೆ. ಇದು Llama ಸೇರಿದಂತೆ ವಿಸ್ತೃತವಾದ Large Language Models (LLMs) ಮೇಲೆ ಇನ್ಫರೆನ್ಸ್ ನಡೆಸುತ್ತದೆ. ggml ಗ್ರಂಥಾಲಯದ ಜೊತೆಗೆ ಅಭಿವೃದ್ಧಿಪಡಿಸಿದ (ಸಾಮಾನ್ಯ ಉದ್ದೇಶದ ಟೆನ್ಸರ್ ಗ್ರಂಥಾಲಯ) llama.cpp ಮೂಲ Python ಬಾಹ್ಯ ರೂಪಾಂತರಕ್ಕೆ ತುಲನೆ ಮಾಡಿದರೆ ವೇಗವಾದ ಇನ್ಫರೆನ್ಸ್ ಮತ್ತು ಕಡಿಮೆ ಮೆಮರಿ ಬಳಕೆ ನೀಡಲು ಉದ್ದೇಶಿಸಲಾಗಿದೆ. ಇದು ಹಾರ್ಡ್‌ವೇರ್ ಆಪ್ಟಿಮೈಜೇಶನ್, ಪ್ರಮಾಣೀಕರಣವನ್ನು ಬೆಂಬಲಿಸುತ್ತದೆ ಮತ್ತು ಸರಳ API ಮತ್ತು examples3 ಅನ್ನು ನೀಡುತ್ತದೆ. ಪರಿಣಾಮಕಾರಿ LLM ಇನ್ಫರೆನ್ಸ್ ನಲ್ಲಿ ಆಸಕ್ತಿ ಇದ್ದರೆ, llama.cpp ಅನ್ನು ಪರಿಶೀಲಿಸುವುದು ಉಚಿತ—Phi3 Llama.cpp ಅನ್ನು ಚಾಲನೆ ಮಾಡಬಹುದು.

## GGUF

GGUF (Generic Graph Update Format) ಯಂತ್ರ ಕಲಿಕಾ ಮಾದರಿಗಳನ್ನು ಪ್ರತಿನಿಧಿಸಲು ಮತ್ತು ಅಪ್ಡೇಟ್ ಮಾಡಲು ಬಳಸಲಾಗುವ ಫಾರ್ಮ್ಯಾಟ್ ಆಗಿದೆ. ಇದು ವಿಶೇಷವಾಗಿ ಸಣ್ಣ ಭಾಷಾ ಮಾದರಿಗಳಿಗಾಗಿ ಉಪಯುಕ್ತವಾಗಿದ್ದು, 4-8bit ಪ್ರಮಾಣೀಕರಣದೊಂದಿಗೆ CPU ಗಳ ಮೇಲೆ ಪರಿಣಾಮಕಾರಿಯಾಗಿ ಚಾಲನೆ ಮಾಡಬಹುದು. GGUF ತ್ವರಿತ ಪ್ರೋಟೋಟೈಪಿಂಗ್ ಮತ್ತು ಎಡ್ಜ್ ಸಾಧನಗಳು ಅಥವಾ CI/CD ಪೈಪ್ಲೈನಿನಂತಹ ಬ್ಯಾಚ್ ಕೆಲಸಗಳಲ್ಲಿ ಮಾದರಿಗಳನ್ನು ನಡೆಸಲು ಲಾಭದಾಯಕವಾಗಿದೆ.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
ನಿರಾಕರಣೆ:
ಈ ದಾಖಲೆ AI ಅನುವಾದ ಸೇವೆ [Co-op Translator](https://github.com/Azure/co-op-translator) ಬಳಸಿ ಅನುವಾದಿಸಲಾಗಿದೆ. ನಾವು ನಿಖರತೆಯನ್ನು ಸಾಧಿಸಲು ಪ್ರಯತ್ನಿಸುತ್ತೇವೆ; ಆದಾಗ್ಯೂ, ಸ್ವಯಂಚಾಲಿತ ಅನುವಾದಗಳಲ್ಲಿ ದೋಷಗಳು ಅಥವಾ ಅಸ್ಪಷ್ಟತೆಗಳು ಇರಬಹುದು. ಮೂಲ ಭಾಷೆಯಲ್ಲಿರುವ ಮೂಲ ದಾಖಲೆನ್ನು ಪ್ರಾಧಿಕೃತ ಮೂಲವೆಂದು ಪರಿಗಣಿಸಬೇಕು. ಗಂಭೀರ ಮಾಹಿತಿಗಾಗಿ ವೃತ್ತಿಪರ ಮಾನವ ಅನುವಾದವನ್ನು ಶಿಫಾರಸು ಮಾಡಲಾಗುತ್ತದೆ. ಈ ಅನುವಾದದ ಬಳಕೆಯಿಂದ ಉಂಟಾಗುವ ಯಾವುದೇ ತಪ್ಪಾಗಿ ಅರ್ಥಮಾಡಿಕೊಳ್ಳುವಿಕೆ ಅಥವಾ ದುರುವ್ಯాఖ్యಾತಿಗಾಗಿ ನಾವು ಹೊಣೆಗಾರರಲ್ಲ.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->