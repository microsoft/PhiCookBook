<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:29:17+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "sw"
}
-->
# Teknolojia kuu zilizotajwa ni pamoja na

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - API ya kiwango cha chini kwa ujifunzaji wa mashine unaoendeshwa kwa kasi na vifaa, iliyojengwa juu ya DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - jukwaa la kompyuta sambamba na mfano wa API uliotengenezwa na Nvidia, unaowezesha usindikaji wa madhumuni mbalimbali kwenye vitengo vya usindikaji picha (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - muundo wazi ulioundwa kuwakilisha mifano ya ujifunzaji wa mashine unaotoa ushirikiano kati ya mifumo tofauti ya ML.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - muundo unaotumika kuwakilisha na kusasisha mifano ya ujifunzaji wa mashine, hasa muhimu kwa mifano midogo ya lugha inayoweza kuendeshwa kwa ufanisi kwenye CPU kwa quantization ya 4-8bit.

## DirectML

DirectML ni API ya kiwango cha chini inayowezesha ujifunzaji wa mashine unaoendeshwa kwa kasi na vifaa. Imejengwa juu ya DirectX 12 kutumia kasi ya GPU na haina utegemezi wa muuzaji, ikimaanisha haihitaji mabadiliko ya msimbo ili ifanye kazi kwa wauzaji tofauti wa GPU. Inatumiwa hasa kwa kazi za mafunzo ya modeli na utabiri kwenye GPUs.

Kuhusu msaada wa vifaa, DirectML imeundwa kufanya kazi na aina mbalimbali za GPUs, ikiwa ni pamoja na GPUs za AMD zilizojumuishwa na zisizo za ndani, GPUs za Intel zilizojumuishwa, na GPUs zisizo za ndani za NVIDIA. Ni sehemu ya Windows AI Platform na inaungwa mkono kwenye Windows 10 & 11, ikiruhusu mafunzo ya modeli na utabiri kwenye kifaa chochote cha Windows.

Kumekuwa na masasisho na fursa zinazohusiana na DirectML, kama vile kusaidia hadi waendeshaji 150 wa ONNX na kutumika na ONNX runtime na WinML. Inasaidiwa na Wauzaji Wakuu wa Vifaa Vilivyojumuishwa (IHVs), kila mmoja akitekeleza amri mbalimbali za metacommands.

## CUDA

CUDA, inayomaanisha Compute Unified Device Architecture, ni jukwaa la kompyuta sambamba na mfano wa API uliotengenezwa na Nvidia. Inaruhusu waendelezaji wa programu kutumia GPU iliyo na uwezo wa CUDA kwa usindikaji wa madhumuni mbalimbali â€“ mbinu inayojulikana kama GPGPU (General-Purpose computing on Graphics Processing Units). CUDA ni kiungo muhimu cha kasi ya GPU ya Nvidia na hutumika sana katika nyanja mbalimbali, ikiwa ni pamoja na ujifunzaji wa mashine, hesabu za kisayansi, na usindikaji wa video.

Msaada wa vifaa kwa CUDA ni maalum kwa GPUs za Nvidia, kwani ni teknolojia miliki iliyotengenezwa na Nvidia. Kila usanifu unaunga mkono matoleo maalum ya zana za CUDA, zinazotoa maktaba na zana muhimu kwa waendelezaji kujenga na kuendesha programu za CUDA.

## ONNX

ONNX (Open Neural Network Exchange) ni muundo wazi ulioundwa kuwakilisha mifano ya ujifunzaji wa mashine. Inatoa ufafanuzi wa mfano wa grafu ya hesabu inayoweza kupanuliwa, pamoja na ufafanuzi wa waendeshaji waliomo na aina za kawaida za data. ONNX inaruhusu waendelezaji kuhamisha mifano kati ya mifumo tofauti ya ML, ikiwezesha ushirikiano na kurahisisha uundaji na usambazaji wa programu za AI.

Phi3 mini inaweza kuendesha kwa ONNX Runtime kwenye CPU na GPU katika vifaa mbalimbali, ikiwa ni pamoja na majukwaa ya seva, Windows, Linux na Mac desktops, na CPU za simu za mkononi.
Mipangilio iliyoboreshwa tuliyoongeza ni

- Mifano ya ONNX kwa int4 DML: Imequantize kwa int4 kupitia AWQ
- Mfano wa ONNX kwa fp16 CUDA
- Mfano wa ONNX kwa int4 CUDA: Imequantize kwa int4 kupitia RTN
- Mfano wa ONNX kwa int4 CPU na Mobile: Imequantize kwa int4 kupitia RTN

## Llama.cpp

Llama.cpp ni maktaba ya programu chanzo wazi iliyoandikwa kwa C++. Inafanya utabiri kwenye Mifano Mikubwa ya Lugha (LLMs), ikiwa ni pamoja na Llama. Imetengenezwa sambamba na maktaba ya ggml (maktaba ya tensor kwa madhumuni ya jumla), llama.cpp inalenga kutoa utabiri wa haraka na matumizi madogo ya kumbukumbu ikilinganishwa na utekelezaji wa asili wa Python. Inaunga mkono uboreshaji wa vifaa, quantization, na inatoa API rahisi na mifano. Ikiwa unavutiwa na utabiri bora wa LLM, llama.cpp inafaa kuangaliwa kwani Phi3 inaweza kuendesha Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) ni muundo unaotumika kuwakilisha na kusasisha mifano ya ujifunzaji wa mashine. Ni muhimu hasa kwa mifano midogo ya lugha (SLMs) inayoweza kuendeshwa kwa ufanisi kwenye CPU kwa quantization ya 4-8bit. GGUF ni ya manufaa kwa prototyping ya haraka na kuendesha mifano kwenye vifaa vya edge au katika kazi za kundi kama vile pipelines za CI/CD.

**Kandido**:  
Hati hii imetafsiriwa kwa kutumia huduma ya tafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kuwa sahihi, tafadhali fahamu kuwa tafsiri za kiotomatiki zinaweza kuwa na makosa au upungufu wa usahihi. Nyaraka ya asili katika lugha yake ya asili inapaswa kuzingatiwa kama chanzo halali. Kwa taarifa muhimu, tafsiri ya kitaalamu ya binadamu inapendekezwa. Hatuwezi kuwajibika kwa kutoelewana au tafsiri potofu zitokanazo na matumizi ya tafsiri hii.