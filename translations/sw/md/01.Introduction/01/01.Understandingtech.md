# Teknolojia kuu zilizotajwa ni pamoja na

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - API ya ngazi ya chini kwa ajili ya ujifunzaji wa mashine unaoendeshwa kwa kasi na vifaa, iliyojengwa juu ya DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - jukwaa la kompyuta sambamba na mfano wa API uliotengenezwa na Nvidia, unaowezesha usindikaji wa jumla kwenye vitengo vya usindikaji picha (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - muundo wazi ulioundwa kuwakilisha mifano ya ujifunzaji wa mashine unaotoa ushirikiano kati ya mifumo tofauti ya ML.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - muundo unaotumika kuwakilisha na kusasisha mifano ya ujifunzaji wa mashine, hasa muhimu kwa mifano midogo ya lugha inayoweza kuendeshwa kwa ufanisi kwenye CPUs kwa quantization ya 4-8bit.

## DirectML

DirectML ni API ya ngazi ya chini inayowezesha ujifunzaji wa mashine unaoendeshwa kwa kasi na vifaa. Imejengwa juu ya DirectX 12 ili kutumia kasi ya GPU na haitegemei muuzaji mmoja, ikimaanisha haitaji mabadiliko ya msimbo ili ifanye kazi kwa wauzaji tofauti wa GPU. Inatumika hasa kwa mafunzo ya modeli na kazi za utambuzi kwenye GPUs.

Kuhusu msaada wa vifaa, DirectML imeundwa kufanya kazi na aina mbalimbali za GPUs, ikiwa ni pamoja na GPUs za AMD zilizojumuishwa na zile za nje, GPUs za Intel zilizojumuishwa, na GPUs za nje za NVIDIA. Ni sehemu ya Windows AI Platform na inasaidiwa kwenye Windows 10 & 11, ikiruhusu mafunzo ya modeli na utambuzi kwenye kifaa chochote cha Windows.

Kumekuwa na masasisho na fursa zinazohusiana na DirectML, kama kusaidia hadi waendeshaji 150 wa ONNX na kutumika na ONNX runtime pamoja na WinML. Inasaidiwa na wauzaji wakuu wa vifaa (IHVs), kila mmoja akitekeleza amri mbalimbali za metacommands.

## CUDA

CUDA, inayomaanisha Compute Unified Device Architecture, ni jukwaa la kompyuta sambamba na mfano wa API uliotengenezwa na Nvidia. Inawawezesha watengenezaji wa programu kutumia GPU yenye uwezo wa CUDA kwa usindikaji wa jumla â€“ njia inayojulikana kama GPGPU (General-Purpose computing on Graphics Processing Units). CUDA ni kichocheo kikuu cha kasi ya GPU ya Nvidia na inatumiwa sana katika nyanja mbalimbali, ikiwa ni pamoja na ujifunzaji wa mashine, hesabu za kisayansi, na usindikaji wa video.

Msaada wa vifaa kwa CUDA ni maalum kwa GPUs za Nvidia, kwani ni teknolojia miliki iliyotengenezwa na Nvidia. Kila usanifu unaunga mkono matoleo maalum ya zana za CUDA, zinazotoa maktaba na zana muhimu kwa watengenezaji kujenga na kuendesha programu za CUDA.

## ONNX

ONNX (Open Neural Network Exchange) ni muundo wazi ulioundwa kuwakilisha mifano ya ujifunzaji wa mashine. Unatoa ufafanuzi wa mfano wa grafu ya hesabu inayoweza kupanuliwa, pamoja na ufafanuzi wa waendeshaji waliomo na aina za kawaida za data. ONNX inaruhusu watengenezaji kuhamisha mifano kati ya mifumo tofauti ya ML, ikiruhusu ushirikiano na kurahisisha uundaji na usambazaji wa programu za AI.

Phi3 mini inaweza kuendesha kwa ONNX Runtime kwenye CPU na GPU katika vifaa mbalimbali, ikiwa ni pamoja na majukwaa ya seva, Windows, Linux na Mac desktops, pamoja na CPUs za simu za mkononi.  
Mipangilio iliyoboreshwa tuliyoongeza ni

- Mifano ya ONNX kwa int4 DML: Imequantize kwa int4 kupitia AWQ  
- Mfano wa ONNX kwa fp16 CUDA  
- Mfano wa ONNX kwa int4 CUDA: Imequantize kwa int4 kupitia RTN  
- Mfano wa ONNX kwa int4 CPU na Simu: Imequantize kwa int4 kupitia RTN  

## Llama.cpp

Llama.cpp ni maktaba ya programu chanzo wazi iliyoandikwa kwa C++. Inafanya utambuzi kwenye Mifano Mikubwa ya Lugha (LLMs) mbalimbali, ikiwa ni pamoja na Llama. Imeendelezwa sambamba na maktaba ya ggml (maktaba ya tensor kwa matumizi ya jumla), llama.cpp inalenga kutoa utambuzi wa haraka na matumizi ya chini ya kumbukumbu ikilinganishwa na utekelezaji wa awali wa Python. Inasaidia uboreshaji wa vifaa, quantization, na inatoa API rahisi pamoja na mifano. Ikiwa unavutiwa na utambuzi wa LLM kwa ufanisi, llama.cpp ni chaguo zuri kwani Phi3 inaweza kuendesha Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) ni muundo unaotumika kuwakilisha na kusasisha mifano ya ujifunzaji wa mashine. Ni muhimu hasa kwa mifano midogo ya lugha (SLMs) inayoweza kuendeshwa kwa ufanisi kwenye CPUs kwa quantization ya 4-8bit. GGUF ni ya manufaa kwa majaribio ya haraka na kuendesha mifano kwenye vifaa vya ukingo au katika kazi za kundi kama vile mizunguko ya CI/CD.

**Kiarifu cha Kutotegemea**:  
Hati hii imetafsiriwa kwa kutumia huduma ya tafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kwa usahihi, tafadhali fahamu kwamba tafsiri za kiotomatiki zinaweza kuwa na makosa au upungufu wa usahihi. Hati ya asili katika lugha yake ya asili inapaswa kuchukuliwa kama chanzo cha mamlaka. Kwa taarifa muhimu, tafsiri ya kitaalamu inayofanywa na binadamu inapendekezwa. Hatubebei dhamana kwa kutoelewana au tafsiri potofu zinazotokana na matumizi ya tafsiri hii.