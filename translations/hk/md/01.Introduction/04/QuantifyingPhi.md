<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-05-08T06:09:14+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "hk"
}
-->
# **量化 Phi 家族**

模型量化指將神經網絡模型中的參數（例如權重和激活值）由較大範圍（通常是連續值範圍）映射到較小的有限值範圍嘅過程。呢項技術可以減少模型嘅大小同計算複雜度，喺資源有限嘅環境，例如手機或者嵌入式系統，提高模型嘅運行效率。模型量化透過降低參數嘅精度嚟實現壓縮，但同時會帶來一定嘅精度損失。所以喺量化過程中，需要平衡模型大小、計算複雜度同精度。常見嘅量化方法包括定點量化、浮點量化等等，可以根據具體場景同需求選擇合適嘅量化策略。

我哋希望將 GenAI 模型部署喺邊緣設備，令更多設備可以進入 GenAI 場景，例如手機、AI PC/Copilot+PC 同傳統 IoT 設備。透過量化模型，可以根據唔同設備部署喺唔同嘅邊緣設備。結合硬件廠商提供嘅模型加速框架同量化模型，可以打造更好嘅 SLM 應用場景。

喺量化場景中，我哋有唔同嘅精度選擇（INT4、INT8、FP16、FP32）。以下係常用量化精度嘅說明：

### **INT4**

INT4 量化係一種激進嘅量化方法，將模型嘅權重同激活值量化成 4-bit 整數。由於表示範圍細同精度低，INT4 量化通常會帶來較大嘅精度損失。不過，相比 INT8 量化，INT4 可以進一步減少模型嘅存儲需求同計算複雜度。值得注意嘅係，實際應用中 INT4 量化比較罕見，因為太低嘅精度可能會導致模型性能顯著下降。另外，唔係所有硬件都支持 INT4 操作，所以選擇量化方法時要考慮硬件兼容性。

### **INT8**

INT8 量化係將模型嘅權重同激活值由浮點數轉換成 8-bit 整數嘅過程。雖然 INT8 整數表示嘅數值範圍較細同精度較低，但可以大幅減少存儲同計算需求。喺 INT8 量化中，模型嘅權重同激活值會經過量化處理，包括縮放同偏移，以盡量保留原本嘅浮點信息。推理時，呢啲量化值會被反量化返做浮點數計算，然後再量化返成 INT8 進行下一步。呢種方法喺大部分應用中可以提供足夠嘅精度同時保持高效嘅計算性能。

### **FP16**

FP16 格式，即 16-bit 浮點數（float16），相比 32-bit 浮點數（float32）將記憶體佔用減半，喺大規模深度學習應用中有明顯優勢。FP16 格式容許喺相同 GPU 記憶體限制下加載更大嘅模型或者處理更多數據。隨住現代 GPU 硬件持續支持 FP16 操作，使用 FP16 格式亦可能帶來計算速度提升。不過，FP16 格式亦有固有缺點，即精度較低，某啲情況下可能導致數值不穩定或者精度損失。

### **FP32**

FP32 格式提供較高嘅精度，可以準確表示較廣嘅數值範圍。喺需要進行複雜數學運算或者要求高精度結果嘅場景，會優先使用 FP32 格式。不過，高精度亦意味住更大嘅記憶體使用同更長嘅計算時間。對於大型深度學習模型，特別係參數多同數據龐大嘅情況，FP32 格式可能會導致 GPU 記憶體不足或者推理速度下降。

喺手機或者 IoT 設備上，我哋可以將 Phi-3.x 模型轉成 INT4，而 AI PC / Copilot PC 可以用更高精度，例如 INT8、FP16、FP32。

目前唔同硬件廠商有唔同嘅框架支持生成模型，例如 Intel 嘅 OpenVINO、Qualcomm 嘅 QNN、Apple 嘅 MLX 同 Nvidia 嘅 CUDA 等，結合模型量化完成本地部署。

技術方面，量化後我哋支持唔同格式，例如 PyTorch / Tensorflow 格式、GGUF 同 ONNX。我做過 GGUF 同 ONNX 嘅格式比較同應用場景推薦。呢度推薦 ONNX 量化格式，因為模型框架到硬件嘅支持都幾好。呢章我哋會聚焦喺 ONNX Runtime for GenAI、OpenVINO 同 Apple MLX 進行模型量化（如果你有更好方法，都可以透過提交 PR 同我哋分享）。

**本章包括**

1. [使用 llama.cpp 量化 Phi-3.5 / 4](./UsingLlamacppQuantifyingPhi.md)

2. [使用 onnxruntime 嘅 Generative AI 擴展量化 Phi-3.5 / 4](./UsingORTGenAIQuantifyingPhi.md)

3. [使用 Intel OpenVINO 量化 Phi-3.5 / 4](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [使用 Apple MLX 框架量化 Phi-3.5 / 4](./UsingAppleMLXQuantifyingPhi.md)

**免責聲明**：  
本文件係使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。雖然我哋盡力確保準確性，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件嘅母語版本應被視為權威來源。對於重要資訊，建議採用專業人工翻譯。因使用此翻譯而引致嘅任何誤解或誤釋，我哋概不負責。