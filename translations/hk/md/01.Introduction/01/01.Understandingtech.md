<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:41:40+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "hk"
}
-->
# 主要提及的技術包括

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - 一個基於 DirectX 12 的低階 API，用於硬件加速的機器學習。
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - 由 Nvidia 開發的平行運算平台及應用程式介面（API）模型，讓圖形處理器（GPU）能進行通用運算。
3. [ONNX](https://onnx.ai/)（Open Neural Network Exchange）- 一種開放格式，用於表示機器學習模型，促進不同機器學習框架之間的互通性。
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)（Generic Graph Update Format）- 一種用於表示和更新機器學習模型的格式，特別適合能在 CPU 上以 4-8 位元量化有效運行的小型語言模型。

## DirectML

DirectML 是一個低階 API，能實現硬件加速的機器學習。它建立於 DirectX 12 之上，利用 GPU 加速，且不依賴特定廠商，意味著在不同 GPU 廠商間使用時無需修改程式碼。它主要用於 GPU 上的模型訓練和推論工作負載。

在硬件支援方面，DirectML 設計能兼容多種 GPU，包括 AMD 的整合及獨立 GPU、Intel 的整合 GPU 以及 NVIDIA 的獨立 GPU。它是 Windows AI 平台的一部分，支援 Windows 10 和 11，允許在任何 Windows 裝置上進行模型訓練和推論。

DirectML 也有相關更新和機會，例如支援多達 150 個 ONNX 運算子，並被 ONNX runtime 和 WinML 使用。它由主要的整合硬件廠商（IHVs）支持，各自實現了多種元命令。

## CUDA

CUDA（Compute Unified Device Architecture）是 Nvidia 創建的平行運算平台及應用程式介面（API）模型。它讓軟件開發者能利用支援 CUDA 的圖形處理器（GPU）進行通用運算，這種方法稱為 GPGPU（通用圖形處理器運算）。CUDA 是 Nvidia GPU 加速的關鍵技術，廣泛應用於機器學習、科學計算及視頻處理等領域。

CUDA 的硬件支援專屬於 Nvidia 的 GPU，因為它是 Nvidia 的專有技術。每個架構支援特定版本的 CUDA 工具包，該工具包提供開發者構建和運行 CUDA 應用所需的庫和工具。

## ONNX

ONNX（Open Neural Network Exchange）是一種開放格式，用於表示機器學習模型。它定義了一種可擴展的計算圖模型，以及內建運算子和標準資料類型的定義。ONNX 讓開發者能在不同的機器學習框架間移動模型，促進互通性，並簡化 AI 應用的創建與部署。

Phi3 mini 可在 CPU 和 GPU 上使用 ONNX Runtime 運行，支援多種裝置，包括伺服器平台、Windows、Linux 和 Mac 桌面，以及行動 CPU。
我們新增的優化配置包括：

- 用於 int4 DML 的 ONNX 模型：透過 AWQ 量化為 int4
- 用於 fp16 CUDA 的 ONNX 模型
- 用於 int4 CUDA 的 ONNX 模型：透過 RTN 量化為 int4
- 用於 int4 CPU 和行動裝置的 ONNX 模型：透過 RTN 量化為 int4

## Llama.cpp

Llama.cpp 是一個用 C++ 編寫的開源軟件庫。它能對多種大型語言模型（LLMs）進行推論，包括 Llama。該庫與 ggml（通用張量庫）一同開發，目標是比原本的 Python 實現提供更快的推論速度和更低的記憶體使用。它支援硬件優化、量化，並提供簡單的 API 和範例。如果你對高效的 LLM 推論有興趣，llama.cpp 非常值得一試，因為 Phi3 可以運行 Llama.cpp。

## GGUF

GGUF（Generic Graph Update Format）是一種用於表示和更新機器學習模型的格式。它特別適合能在 CPU 上以 4-8 位元量化有效運行的小型語言模型（SLMs）。GGUF 對於快速原型開發以及在邊緣裝置或批次作業（如 CI/CD 流程）中運行模型非常有用。

**免責聲明**：  
本文件由 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。雖然我們致力於確保準確性，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於重要資訊，建議採用專業人工翻譯。我們不對因使用本翻譯而引起的任何誤解或誤釋承擔責任。