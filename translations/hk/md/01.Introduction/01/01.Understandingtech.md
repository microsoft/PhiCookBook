<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-08T06:15:43+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "hk"
}
-->
# 主要提及的關鍵技術包括

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - 一個基於 DirectX 12 的硬件加速機器學習低階 API。
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - 由 Nvidia 開發的平行運算平台及應用程式介面 (API) 模型，支援在圖形處理器 (GPU) 上進行通用運算。
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - 一種用於表示機器學習模型的開放格式，促進不同機器學習框架之間的互通性。
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - 一種用於表示及更新機器學習模型的格式，特別適合於可在 CPU 上以 4-8bit 量化有效運行的小型語言模型。

## DirectML

DirectML 是一個低階 API，支援硬件加速機器學習。它建立於 DirectX 12 之上，利用 GPU 加速，且不依賴特定廠商，無需修改程式碼即可跨不同 GPU 廠商運作。主要用於 GPU 上的模型訓練及推論工作負載。

在硬件支援方面，DirectML 設計可兼容多種 GPU，包括 AMD 的整合及獨立 GPU、Intel 的整合 GPU，以及 NVIDIA 的獨立 GPU。它是 Windows AI 平台的一部分，支援 Windows 10 及 11，讓任何 Windows 裝置都能進行模型訓練和推論。

DirectML 亦有相關更新及新機會，例如支援多達 150 個 ONNX 運算子，並被 ONNX runtime 及 WinML 採用。主要整合硬件廠商（IHV）均有支持並實作不同的元指令。

## CUDA

CUDA（Compute Unified Device Architecture）是 Nvidia 創建的平行運算平台及應用程式介面 (API) 模型。它讓軟件開發者可以利用 CUDA 支援的 GPU 進行通用運算，這種方法稱為 GPGPU（圖形處理器通用計算）。CUDA 是 Nvidia GPU 加速的核心技術，廣泛應用於機器學習、科學運算及視頻處理等領域。

CUDA 的硬件支援僅限於 Nvidia GPU，因為它是 Nvidia 的專有技術。每種架構支援特定版本的 CUDA 工具包，該工具包提供開發 CUDA 應用所需的庫和工具。

## ONNX

ONNX（Open Neural Network Exchange）是一種用於表示機器學習模型的開放格式。它定義了可擴展的計算圖模型，以及內置運算子和標準數據類型。ONNX 允許開發者在不同的機器學習框架間轉移模型，促進互通性，並簡化 AI 應用的開發和部署。

Phi3 mini 可以在 CPU 和 GPU 上使用 ONNX Runtime 運行，支援包括伺服器平台、Windows、Linux、Mac 桌面及移動 CPU 等多種設備。
我們新增的優化配置包括：

- 用於 int4 DML 的 ONNX 模型：通過 AWQ 量化至 int4
- 用於 fp16 CUDA 的 ONNX 模型
- 用於 int4 CUDA 的 ONNX 模型：通過 RTN 量化至 int4
- 用於 int4 CPU 和移動設備的 ONNX 模型：通過 RTN 量化至 int4

## Llama.cpp

Llama.cpp 是一個用 C++ 編寫的開源軟件庫，能對多種大型語言模型（LLM）進行推論，包括 Llama。它與 ggml 庫（通用張量庫）一同開發，目標是比原本的 Python 實現提供更快的推論速度和更低的記憶體使用。它支援硬件優化、量化，並提供簡單的 API 及範例。如果你對高效的 LLM 推論有興趣，Llama.cpp 值得一試，Phi3 亦可運行 Llama.cpp。

## GGUF

GGUF（Generic Graph Update Format）是一種用於表示及更新機器學習模型的格式。它特別適合於可在 CPU 上以 4-8bit 量化有效運行的小型語言模型（SLM）。GGUF 有助於快速原型開發，並適合在邊緣設備或 CI/CD 等批次作業中運行模型。

**免責聲明**：  
本文件由 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。雖然我們致力於確保準確性，但請注意自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應視為權威來源。對於重要資訊，建議採用專業人工翻譯。我們不對因使用本翻譯而引致的任何誤解或誤釋承擔責任。