<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "583e1ebd3884b47b43c883072eb8fa03",
  "translation_date": "2025-04-04T17:35:07+00:00",
  "source_file": "md\\01.Introduction\\01\\01.Understandingtech.md",
  "language_code": "hk"
}
-->
# 提到的主要技術包括

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - 一個基於 DirectX 12 的低層次 API，用於硬件加速的機器學習。
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - 一個由 Nvidia 開發的並行計算平台和應用程式介面 (API) 模型，支持在圖形處理單元 (GPU) 上進行通用處理。
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - 一個開放格式，用於表示機器學習模型，提供不同 ML 框架之間的互操作性。
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - 一個用於表示和更新機器學習模型的格式，特別適合在 CPU 上以 4-8bit 量化運行的小型語言模型。

## DirectML

DirectML 是一個低層次 API，支持硬件加速的機器學習。它基於 DirectX 12，利用 GPU 加速，並且與供應商無關，意味著無需修改代碼即可在不同 GPU 供應商之間運行。主要用於 GPU 上的模型訓練和推理工作負載。

在硬件支持方面，DirectML 設計用於支持多種 GPU，包括 AMD 集成和獨立 GPU、Intel 集成 GPU，以及 NVIDIA 獨立 GPU。它是 Windows AI 平台的一部分，支持 Windows 10 和 11，允許在任何 Windows 設備上進行模型訓練和推理。

DirectML 有一些更新和機會，例如支持多達 150 個 ONNX 操作符，並被 ONNX runtime 和 WinML 使用。它由主要的集成硬件供應商 (IHVs) 支持，每家供應商都實現了各種元命令。

## CUDA

CUDA，即 Compute Unified Device Architecture，是由 Nvidia 創建的一個並行計算平台和應用程式介面 (API) 模型。它允許軟件開發者使用支持 CUDA 的圖形處理單元 (GPU) 進行通用處理，這種方法被稱為 GPGPU (圖形處理單元上的通用計算)。CUDA 是 Nvidia GPU 加速的核心技術，被廣泛用於機器學習、科學計算和視頻處理等領域。

CUDA 的硬件支持特定於 Nvidia 的 GPU，因為它是 Nvidia 開發的專有技術。每個架構支持特定版本的 CUDA 工具包，該工具包提供必要的庫和工具，供開發者構建和運行 CUDA 應用程式。

## ONNX

ONNX (Open Neural Network Exchange) 是一個開放格式，用於表示機器學習模型。它提供了一個可擴展的計算圖模型的定義，以及內置操作符和標準數據類型的定義。ONNX 允許開發者在不同的 ML 框架之間移動模型，實現互操作性，並使 AI 應用的創建和部署更加容易。

Phi3 mini 可以使用 ONNX Runtime 在 CPU 和 GPU 上跨設備運行，包括服務器平台、Windows、Linux 和 Mac 桌面，以及移動 CPU。
我們添加的優化配置包括：

- 基於 DirectML 的 ONNX int4 模型：通過 AWQ 量化為 int4
- 基於 CUDA 的 ONNX fp16 模型
- 基於 CUDA 的 ONNX int4 模型：通過 RTN 量化為 int4
- 基於 CPU 和移動端的 ONNX int4 模型：通過 RTN 量化為 int4

## Llama.cpp

Llama.cpp 是一個用 C++ 編寫的開源軟件庫。它可以在各種大型語言模型 (LLMs) 上進行推理，包括 Llama。該庫與 ggml 庫（通用張量庫）一起開發，旨在提供比原始 Python 實現更快的推理速度和更低的內存使用。它支持硬件優化、量化，並提供簡單的 API 和示例。如果你對高效的 LLM 推理感興趣，可以探索 Llama.cpp，因為 Phi3 可以運行 Llama.cpp。

## GGUF

GGUF (Generic Graph Update Format) 是一個用於表示和更新機器學習模型的格式。它特別適合小型語言模型 (SLMs)，可以在 CPU 上以 4-8bit 量化高效運行。GGUF 對於快速原型開發以及在邊緣設備或批量任務（如 CI/CD 管道）中運行模型非常有用。

**免責聲明**:  
本文檔已使用AI翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。儘管我們努力確保翻譯的準確性，但請注意，自動翻譯可能會包含錯誤或不準確之處。原始語言版本應被視為權威來源。對於重要信息，建議尋求專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或錯誤解釋不承擔責任。