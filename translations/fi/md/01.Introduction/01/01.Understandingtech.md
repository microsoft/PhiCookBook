<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:45:37+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "fi"
}
-->
# Keskeiset mainitut teknologiat

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) – matalan tason API laitteistokiihdytettyyn koneoppimiseen, joka on rakennettu DirectX 12:n päälle.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) – rinnakkaislaskenta-alusta ja sovellusohjelmointirajapintamalli (API), jonka on kehittänyt Nvidia ja joka mahdollistaa yleiskäyttöisen laskennan grafiikkasuorittimilla (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) – avoin formaatti koneoppimismallien esittämiseen, joka tarjoaa yhteensopivuutta eri ML-kehysten välillä.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) – formaatti koneoppimismallien esittämiseen ja päivittämiseen, erityisen hyödyllinen pienemmille kielimalleille, jotka toimivat tehokkaasti CPU:lla 4–8-bittisellä kvantisoinnilla.

## DirectML

DirectML on matalan tason API, joka mahdollistaa laitteistokiihdytetyn koneoppimisen. Se on rakennettu DirectX 12:n päälle hyödyntämään GPU-kiihdytystä ja on laitevalmistajasta riippumaton, eli se ei vaadi koodimuutoksia toimiakseen eri GPU-valmistajien laitteilla. Sitä käytetään pääasiassa mallien koulutukseen ja päättelyyn GPU:lla.

Laitetuen osalta DirectML on suunniteltu toimimaan laajalla valikoimalla GPU:ita, mukaan lukien AMD:n integroidut ja erilliset GPU:t, Intelin integroidut GPU:t sekä NVIDIA:n erilliset GPU:t. Se on osa Windows AI Platformia ja tukee Windows 10 & 11 -käyttöjärjestelmiä, mahdollistaen mallien koulutuksen ja päättelyn millä tahansa Windows-laitteella.

DirectML:ään liittyen on tehty päivityksiä ja tarjolla uusia mahdollisuuksia, kuten tuki jopa 150 ONNX-operaattorille, ja sitä käyttävät sekä ONNX runtime että WinML. Sen takana ovat suuret integroidut laitevalmistajat (IHV), jotka toteuttavat erilaisia metakomentoja.

## CUDA

CUDA, eli Compute Unified Device Architecture, on Nvidia:n kehittämä rinnakkaislaskenta-alusta ja sovellusohjelmointirajapintamalli (API). Se mahdollistaa ohjelmistokehittäjien käyttää CUDA-yhteensopivaa grafiikkasuoritinta (GPU) yleiskäyttöiseen laskentaan – lähestymistapa, jota kutsutaan GPGPU:ksi (General-Purpose computing on Graphics Processing Units). CUDA on keskeinen tekijä Nvidia:n GPU-kiihdytyksessä ja sitä käytetään laajasti eri aloilla, kuten koneoppimisessa, tieteellisessä laskennassa ja videonkäsittelyssä.

Laitetuki CUDA:lle on rajattu Nvidia:n GPU:ihin, sillä se on Nvidia:n oma teknologia. Jokainen arkkitehtuuri tukee tiettyjä CUDA-työkalupakin versioita, jotka tarjoavat kehittäjille tarvittavat kirjastot ja työkalut CUDA-sovellusten rakentamiseen ja suorittamiseen.

## ONNX

ONNX (Open Neural Network Exchange) on avoin formaatti, joka on suunniteltu koneoppimismallien esittämiseen. Se tarjoaa laajennettavan laskentakaavion mallin määritelmän sekä sisäänrakennettujen operaattoreiden ja standardoitujen tietotyyppien määritelmät. ONNX mahdollistaa kehittäjien siirtää malleja eri ML-kehysten välillä, mikä parantaa yhteensopivuutta ja helpottaa tekoälysovellusten luomista ja käyttöönottoa.

Phi3 mini voi käyttää ONNX Runtimea CPU:lla ja GPU:lla eri laitteilla, mukaan lukien palvelinalustat, Windows-, Linux- ja Mac-työpöydät sekä mobiililaitteiden CPU:t.
Lisäämämme optimoidut kokoonpanot ovat

- ONNX-mallit int4 DML:lle – kvantisoitu int4:ään AWQ:n avulla
- ONNX-malli fp16 CUDA:lle
- ONNX-malli int4 CUDA:lle – kvantisoitu int4:ään RTN:n avulla
- ONNX-malli int4 CPU:lle ja mobiililaitteille – kvantisoitu int4:ään RTN:n avulla

## Llama.cpp

Llama.cpp on avoimen lähdekoodin ohjelmistokirjasto, joka on kirjoitettu C++:lla. Se suorittaa päättelyä useilla suurilla kielimalleilla (LLM), mukaan lukien Llama. Se on kehitetty yhdessä ggml-kirjaston (yleiskäyttöinen tensorikirjasto) kanssa, ja sen tavoitteena on tarjota nopeampi päättely ja pienempi muistinkulutus verrattuna alkuperäiseen Python-toteutukseen. Se tukee laitteistokiihdytystä, kvantisointia ja tarjoaa yksinkertaisen API:n sekä esimerkkejä. Jos olet kiinnostunut tehokkaasta LLM-päättelystä, llama.cpp on tutustumisen arvoinen, sillä Phi3 pystyy ajamaan Llama.cpp:ää.

## GGUF

GGUF (Generic Graph Update Format) on formaatti, jota käytetään koneoppimismallien esittämiseen ja päivittämiseen. Se on erityisen hyödyllinen pienemmille kielimalleille (SLM), jotka toimivat tehokkaasti CPU:lla 4–8-bittisellä kvantisoinnilla. GGUF on hyödyllinen nopeaan prototypointiin sekä mallien ajamiseen reunalaitteilla tai eräajoissa, kuten CI/CD-putkistoissa.

**Vastuuvapauslauseke**:  
Tämä asiakirja on käännetty käyttämällä tekoälypohjaista käännöspalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, että automaattikäännöksissä saattaa esiintyä virheitä tai epätarkkuuksia. Alkuperäistä asiakirjaa sen alkuperäiskielellä tulee pitää virallisena lähteenä. Tärkeissä asioissa suositellaan ammattimaista ihmiskäännöstä. Emme ole vastuussa tämän käännöksen käytöstä aiheutuvista väärinymmärryksistä tai tulkinnoista.