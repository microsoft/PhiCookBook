<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:25:19+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "fi"
}
-->
# Keskeiset mainitut teknologiat sisältävät

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) – matalan tason API laitteistokiihdytettyyn koneoppimiseen, joka on rakennettu DirectX 12:n päälle.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) – rinnakkaislaskenta-alusta ja sovellusohjelmointirajapintamalli (API), jonka Nvidia on kehittänyt ja joka mahdollistaa yleiskäyttöisen prosessoinnin grafiikkasuorittimilla (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) – avoin formaatti, joka on suunniteltu esittämään koneoppimismalleja ja tarjoaa yhteensopivuuden eri ML-kehysten välillä.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) – formaatti, jota käytetään koneoppimismallien esittämiseen ja päivittämiseen, erityisen hyödyllinen pienemmille kielimalleille, jotka toimivat tehokkaasti CPU:lla 4–8-bittisellä kvantisoinnilla.

## DirectML

DirectML on matalan tason API, joka mahdollistaa laitteistokiihdytetyn koneoppimisen. Se on rakennettu DirectX 12:n päälle hyödyntämään GPU-kiihdytystä ja on laitevalmistajasta riippumaton, eli se ei vaadi koodimuutoksia toimiakseen eri GPU-valmistajien laitteilla. Sitä käytetään pääasiassa mallien koulutukseen ja päättelyyn GPU:illa.

Laitetuen osalta DirectML on suunniteltu toimimaan laajalla valikoimalla GPU:ita, mukaan lukien AMD:n integroidut ja erilliset GPU:t, Intelin integroidut GPU:t sekä NVIDIAn erilliset GPU:t. Se on osa Windows AI Platformia ja tukee Windows 10 & 11 -käyttöjärjestelmiä, mahdollistaen mallien koulutuksen ja päättelyn millä tahansa Windows-laitteella.

DirectML:lle on julkaistu päivityksiä ja uusia mahdollisuuksia, kuten tuki jopa 150 ONNX-operaattorille, ja sitä käyttävät sekä ONNX runtime että WinML. Sen takana ovat suuret integroidut laitevalmistajat (IHVs), jotka toteuttavat erilaisia metakomentoja.

## CUDA

CUDA, joka tarkoittaa Compute Unified Device Architecturea, on Nvidia:n kehittämä rinnakkaislaskenta-alusta ja sovellusohjelmointirajapintamalli (API). Se antaa ohjelmistokehittäjille mahdollisuuden käyttää CUDA-yhteensopivaa grafiikkasuoritinta (GPU) yleiskäyttöiseen laskentaan – tätä kutsutaan nimellä GPGPU (General-Purpose computing on Graphics Processing Units). CUDA on keskeinen tekijä Nvidian GPU-kiihdytyksessä ja sitä käytetään laajasti eri aloilla, kuten koneoppimisessa, tieteellisessä laskennassa ja videonkäsittelyssä.

Laitetuki CUDA:lle on rajoitettu Nvidia:n GPU:ihin, sillä kyseessä on Nvidia:n oma teknologia. Jokainen arkkitehtuuri tukee tiettyjä CUDA-työkalupakin versioita, jotka tarjoavat kehittäjille tarvittavat kirjastot ja työkalut CUDA-sovellusten rakentamiseen ja ajamiseen.

## ONNX

ONNX (Open Neural Network Exchange) on avoin formaatti, joka on suunniteltu esittämään koneoppimismalleja. Se tarjoaa laajennettavan laskentakaaviomallin määritelmän sekä sisäänrakennettujen operaattoreiden ja standardoitujen tietotyyppien määrittelyt. ONNX mahdollistaa mallien siirtämisen eri ML-kehysten välillä, mikä parantaa yhteensopivuutta ja helpottaa tekoälysovellusten luomista ja käyttöönottoa.

Phi3 mini voi käyttää ONNX Runtimea CPU:lla ja GPU:lla eri laitteilla, mukaan lukien palvelinalustat, Windows, Linux ja Mac työpöydät sekä mobiilisuorittimet.
Olemme lisänneet optimoidut kokoonpanot:

- ONNX-mallit int4 DML: kvantisoitu int4 AWQ:n avulla
- ONNX-malli fp16 CUDA:lle
- ONNX-malli int4 CUDA:lle: kvantisoitu int4 RTN:n avulla
- ONNX-malli int4 CPU:lle ja mobiililaitteille: kvantisoitu int4 RTN:n avulla

## Llama.cpp

Llama.cpp on avoimen lähdekoodin ohjelmistokirjasto, joka on kirjoitettu C++:lla. Se suorittaa päättelyä eri suurilla kielimalleilla (LLM), mukaan lukien Llama. Se on kehitetty yhdessä ggml-kirjaston (yleiskäyttöinen tensorikirjasto) kanssa, ja sen tavoitteena on tarjota nopeampi päättely ja pienempi muistinkäyttö verrattuna alkuperäiseen Python-toteutukseen. Se tukee laitteistojen optimointia, kvantisointia sekä tarjoaa yksinkertaisen API:n ja esimerkkejä. Jos olet kiinnostunut tehokkaasta LLM-päättelystä, llama.cpp kannattaa tutkia, sillä Phi3 voi käyttää Llama.cpp:ää.

## GGUF

GGUF (Generic Graph Update Format) on formaatti, jota käytetään koneoppimismallien esittämiseen ja päivittämiseen. Se on erityisen hyödyllinen pienemmille kielimalleille (SLM), jotka toimivat tehokkaasti CPU:lla 4–8-bittisellä kvantisoinnilla. GGUF soveltuu hyvin nopeaan prototypointiin sekä mallien ajamiseen reunalaitteilla tai eräajotehtävissä, kuten CI/CD-putkissa.

**Vastuuvapauslauseke**:  
Tämä asiakirja on käännetty tekoälypohjaisella käännöspalvelulla [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, otathan huomioon, että automaattikäännöksissä saattaa esiintyä virheitä tai epätarkkuuksia. Alkuperäistä asiakirjaa sen alkuperäiskielellä tulisi pitää virallisena lähteenä. Tärkeissä tiedoissa suositellaan ammattimaisen ihmiskääntäjän käyttöä. Emme ole vastuussa tästä käännöksestä mahdollisesti aiheutuvista väärinymmärryksistä tai tulkinnoista.