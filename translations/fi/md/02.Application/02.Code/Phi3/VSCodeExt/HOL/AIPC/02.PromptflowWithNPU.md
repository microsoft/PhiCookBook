<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "bc29f7fe7fc16bed6932733eac8c81b8",
  "translation_date": "2025-05-09T19:24:27+00:00",
  "source_file": "md/02.Application/02.Code/Phi3/VSCodeExt/HOL/AIPC/02.PromptflowWithNPU.md",
  "language_code": "fi"
}
-->
# **Lab 2 - Suorita Prompt flow Phi-3-mini -mallilla AIPC:ssä**

## **Mikä on Prompt flow**

Prompt flow on kehitystyökaluvalikoima, joka on suunniteltu sujuvoittamaan koko LLM-pohjaisten tekoälysovellusten kehityssykliä ideoinnista, prototypointiin, testaukseen, arviointiin, tuotantoon ja seurantaan. Se helpottaa prompttien suunnittelua ja mahdollistaa tuotantotason LLM-sovellusten rakentamisen.

Prompt flow'n avulla voit:

- Luoda työnkulkuja, jotka yhdistävät LLM:t, promptit, Python-koodin ja muut työkalut suoritettavaksi kokonaisuudeksi.

- Debugata ja kehittää työnkulkujasi, erityisesti LLM-vuorovaikutuksia, helposti.

- Arvioida työnkulkuja, laskea laatu- ja suorituskykymittareita suurilla aineistoilla.

- Integroimaan testauksen ja arvioinnin CI/CD-järjestelmääsi työnkulun laadun varmistamiseksi.

- Julkaista työnkulut valitsemallesi palvelualustalle tai integroida ne helposti sovelluksesi koodipohjaan.

- (Valinnainen, mutta suositeltava) Tehdä yhteistyötä tiimisi kanssa hyödyntämällä Prompt flow’n pilvipalvelua Azure AI:ssa.

## **Mikä on AIPC**

AI PC:ssä on CPU, GPU ja NPU, joilla kaikilla on omat tekoälyä kiihdyttävät ominaisuutensa. NPU eli neuroverkkojen käsittelyyksikkö on erikoistunut kiihdytin, joka hoitaa tekoäly- (AI) ja koneoppimistehtäviä suoraan tietokoneellasi ilman, että dataa tarvitsee lähettää pilveen. GPU ja CPU voivat myös käsitellä näitä tehtäviä, mutta NPU on erityisen tehokas vähävirtaisissa AI-laskelmissa. AI PC merkitsee perustavanlaatuista muutosta tietokoneidemme toimintatavassa. Se ei ole ratkaisu aiemmin olemassa olleeseen ongelmaan, vaan lupaa merkittävän parannuksen päivittäiseen tietokoneen käyttöön.

Miten se toimii? Verrattuna generatiiviseen tekoälyyn ja valtaviin julkisiin aineistoihin koulutettuihin suuriin kielimalleihin (LLM), AI, joka toimii tietokoneellasi, on saavutettavissa monella tasolla. Konsepti on helpompi ymmärtää, ja koska se on koulutettu omalla datallasi ilman pilvipalveluiden tarvetta, hyödyt ovat heti houkuttelevampia laajemmalle käyttäjäjoukolle.

Lähitulevaisuudessa AI PC -maailmassa henkilökohtaiset avustajat ja pienemmät AI-mallit toimivat suoraan tietokoneellasi, hyödyntäen dataasi tarjotakseen henkilökohtaisia, yksityisiä ja turvallisempia AI-parannuksia arkisiin tehtäviisi – kokousmuistioiden laatimiseen, fantasiapalloliigan järjestämiseen, valokuva- ja videonmuokkauksen automatisointiin tai täydellisen matkasuunnitelman laatimiseen perhetapaamista varten saapumis- ja lähtöaikojen perusteella.

## **Generointikoodin työnkulkujen rakentaminen AIPC:ssä**

***Note*** ：Jos et ole vielä asentanut ympäristöä, käy osoitteessa [Lab 0 -Installations](./01.Installations.md)

1. Avaa Prompt flow -laajennus Visual Studio Codessa ja luo tyhjä työnkulkuprojekti

![create](../../../../../../../../../translated_images/pf_create.d6172d8277a78a7fa82cd6ff727ed44e037fa78b662f1f62d5963f36d712d229.fi.png)

2. Lisää Input- ja Output-parametrit ja lisää Python-koodi uutena työnkulkuun

![flow](../../../../../../../../../translated_images/pf_flow.d5646a323fb7f444c0b98b4521057a592325c583e7ba18bc31500bc0415e9ef3.fi.png)

Voit käyttää tätä rakennetta (flow.dag.yaml) työnkulun rakentamiseen

```yaml

inputs:
  question:
    type: string
    default: how to write Bubble Algorithm
outputs:
  answer:
    type: string
    reference: ${Chat_With_Phi3.output}
nodes:
- name: Chat_With_Phi3
  type: python
  source:
    type: code
    path: Chat_With_Phi3.py
  inputs:
    question: ${inputs.question}


```

3. Lisää koodi tiedostoon ***Chat_With_Phi3.py***

```python


from promptflow.core import tool

# import torch
from transformers import AutoTokenizer, pipeline,TextStreamer
import intel_npu_acceleration_library as npu_lib

import warnings

import asyncio
import platform

class Phi3CodeAgent:
    
    model = None
    tokenizer = None
    text_streamer = None
    
    model_id = "microsoft/Phi-3-mini-4k-instruct"

    @staticmethod
    def init_phi3():
        
        if Phi3CodeAgent.model is None or Phi3CodeAgent.tokenizer is None or Phi3CodeAgent.text_streamer is None:
            Phi3CodeAgent.model = npu_lib.NPUModelForCausalLM.from_pretrained(
                                    Phi3CodeAgent.model_id,
                                    torch_dtype="auto",
                                    dtype=npu_lib.int4,
                                    trust_remote_code=True
                                )
            Phi3CodeAgent.tokenizer = AutoTokenizer.from_pretrained(Phi3CodeAgent.model_id)
            Phi3CodeAgent.text_streamer = TextStreamer(Phi3CodeAgent.tokenizer, skip_prompt=True)

    

    @staticmethod
    def chat_with_phi3(prompt):
        
        Phi3CodeAgent.init_phi3()

        messages = "<|system|>You are a AI Python coding assistant. Please help me to generate code in Python.The answer only genertated Python code, but any comments and instructions do not need to be generated<|end|><|user|>" + prompt +"<|end|><|assistant|>"



        generation_args = {
            "max_new_tokens": 1024,
            "return_full_text": False,
            "temperature": 0.3,
            "do_sample": False,
            "streamer": Phi3CodeAgent.text_streamer,
        }

        pipe = pipeline(
            "text-generation",
            model=Phi3CodeAgent.model,
            tokenizer=Phi3CodeAgent.tokenizer,
            # **generation_args
        )

        result = ''

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            response = pipe(messages, **generation_args)
            result =response[0]['generated_text']
            return result


@tool
def my_python_tool(question: str) -> str:
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    return Phi3CodeAgent.chat_with_phi3(question)


```

4. Voit testata työnkulkua Debug- tai Run-toiminnolla varmistaaksesi, että generointikoodi toimii oikein

![RUN](../../../../../../../../../translated_images/pf_run.d918637dc00f61e9bdeec37d4cc9646f77d270ac9203bcce13569f3157202b6e.fi.png)

5. Suorita työnkulku kehityksen API:na terminaalissa

```

pf flow serve --source ./ --port 8080 --host localhost   

```

Voit testata sitä Postmanissa tai Thunder Clientissä

### **Note**

1. Ensimmäinen suoritus vie aikaa. Suositeltavaa on ladata phi-3-malli Hugging face CLI:llä.

2. Ottaen huomioon Intel NPU:n rajallisen laskentatehon, suositellaan käyttämään Phi-3-mini-4k-instruct -mallia.

3. Käytämme Intel NPU Accelerationia INT4-konversioon kvantisointiin, mutta jos käynnistät palvelun uudelleen, sinun täytyy poistaa cache- ja nc_workshop-kansiot.

## **Resurssit**

1. Opi Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Opi Intel NPU Acceleration [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Esimerkkikoodi, lataa [Local NPU Agent Sample Code](../../../../../../../../../code/07.Lab/01/AIPC)

**Vastuuvapauslauseke**:  
Tämä asiakirja on käännetty käyttämällä tekoälypohjaista käännöspalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, että automaattiset käännökset saattavat sisältää virheitä tai epätarkkuuksia. Alkuperäinen asiakirja omalla kielellään on pidettävä auktoriteettisena lähteenä. Tärkeissä tiedoissa suositellaan ammattimaista ihmiskäännöstä. Emme ole vastuussa tämän käännöksen käytöstä johtuvista väärinymmärryksistä tai virhetulkinnoista.