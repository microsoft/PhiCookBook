<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:47:42+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "sk"
}
-->
# Kľúčové technológie spomenuté zahŕňajú

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - nízkoúrovňové API pre hardvérovo akcelerované strojové učenie postavené na DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - paralelná výpočtová platforma a aplikačné programovacie rozhranie (API) vyvinuté spoločnosťou Nvidia, ktoré umožňuje všeobecné spracovanie na grafických procesoroch (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - otvorený formát navrhnutý na reprezentáciu modelov strojového učenia, ktorý zabezpečuje interoperabilitu medzi rôznymi ML frameworkmi.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - formát používaný na reprezentáciu a aktualizáciu modelov strojového učenia, obzvlášť užitočný pre menšie jazykové modely, ktoré môžu efektívne bežať na CPU s 4-8bitovou kvantizáciou.

## DirectML

DirectML je nízkoúrovňové API, ktoré umožňuje hardvérovo akcelerované strojové učenie. Je postavené na DirectX 12, aby využívalo akceleráciu GPU, a je nezávislé od výrobcu, čo znamená, že nevyžaduje zmeny v kóde na prácu s rôznymi GPU od rôznych výrobcov. Používa sa predovšetkým na trénovanie modelov a inferenciu na GPU.

Čo sa týka hardvérovej podpory, DirectML je navrhnuté tak, aby fungovalo s širokou škálou GPU, vrátane integrovaných a samostatných GPU od AMD, integrovaných GPU od Intelu a samostatných GPU od NVIDIA. Je súčasťou Windows AI Platform a je podporované na Windows 10 a 11, čo umožňuje trénovanie a inferenciu modelov na akomkoľvek zariadení s Windows.

Boli vydané aktualizácie a príležitosti súvisiace s DirectML, ako napríklad podpora až 150 ONNX operátorov a jeho využitie v ONNX runtime aj WinML. Podporujú ho hlavní výrobcovia hardvéru (IHVs), ktorí implementujú rôzne metaprikazy.

## CUDA

CUDA, čo znamená Compute Unified Device Architecture, je paralelná výpočtová platforma a aplikačné programovacie rozhranie (API) vytvorené spoločnosťou Nvidia. Umožňuje softvérovým vývojárom využiť grafický procesor (GPU) s podporou CUDA na všeobecné spracovanie – prístup nazývaný GPGPU (General-Purpose computing on Graphics Processing Units). CUDA je kľúčovým prvkom akcelerácie GPU od Nvidia a široko sa používa v rôznych oblastiach, vrátane strojového učenia, vedeckých výpočtov a spracovania videa.

Hardvérová podpora pre CUDA je špecifická pre GPU od Nvidia, keďže ide o proprietárnu technológiu vyvinutú spoločnosťou Nvidia. Každá architektúra podporuje konkrétne verzie CUDA toolkitu, ktorý poskytuje potrebné knižnice a nástroje pre vývojárov na tvorbu a spúšťanie CUDA aplikácií.

## ONNX

ONNX (Open Neural Network Exchange) je otvorený formát navrhnutý na reprezentáciu modelov strojového učenia. Poskytuje definíciu rozšíriteľného modelu výpočtového grafu, ako aj definície vstavaných operátorov a štandardných dátových typov. ONNX umožňuje vývojárom presúvať modely medzi rôznymi ML frameworkmi, čím zabezpečuje interoperabilitu a uľahčuje tvorbu a nasadzovanie AI aplikácií.

Phi3 mini môže bežať s ONNX Runtime na CPU aj GPU naprieč zariadeniami, vrátane serverových platforiem, Windows, Linux a Mac desktopov, ako aj mobilných CPU.
Optimalizované konfigurácie, ktoré sme pridali, sú

- ONNX modely pre int4 DML: kvantizované na int4 cez AWQ
- ONNX model pre fp16 CUDA
- ONNX model pre int4 CUDA: kvantizované na int4 cez RTN
- ONNX model pre int4 CPU a Mobile: kvantizované na int4 cez RTN

## Llama.cpp

Llama.cpp je open-source softvérová knižnica napísaná v C++. Vykonáva inferenciu na rôznych veľkých jazykových modeloch (LLMs), vrátane Llama. Vyvinutá spolu s knižnicou ggml (všeobecná tensorová knižnica), llama.cpp si kladie za cieľ poskytnúť rýchlejšiu inferenciu a nižšiu spotrebu pamäte v porovnaní s pôvodnou implementáciou v Pythone. Podporuje hardvérovú optimalizáciu, kvantizáciu a ponúka jednoduché API a príklady3. Ak máte záujem o efektívnu inferenciu LLM, llama.cpp stojí za preskúmanie, keďže Phi3 môže spúšťať Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) je formát používaný na reprezentáciu a aktualizáciu modelov strojového učenia. Je obzvlášť užitočný pre menšie jazykové modely (SLMs), ktoré môžu efektívne bežať na CPU s 4-8bitovou kvantizáciou. GGUF je výhodný pre rýchle prototypovanie a spúšťanie modelov na edge zariadeniach alebo v dávkových úlohách, ako sú CI/CD pipeline.

**Vyhlásenie o zodpovednosti**:  
Tento dokument bol preložený pomocou AI prekladateľskej služby [Co-op Translator](https://github.com/Azure/co-op-translator). Hoci sa snažíme o presnosť, prosím, majte na pamäti, že automatizované preklady môžu obsahovať chyby alebo nepresnosti. Pôvodný dokument v jeho rodnom jazyku by mal byť považovaný za autoritatívny zdroj. Pre kritické informácie sa odporúča profesionálny ľudský preklad. Nie sme zodpovední za akékoľvek nedorozumenia alebo nesprávne interpretácie vyplývajúce z použitia tohto prekladu.