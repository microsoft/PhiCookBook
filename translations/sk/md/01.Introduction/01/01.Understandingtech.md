<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:31:18+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "sk"
}
-->
# Kľúčové technológie uvedené zahŕňajú

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - nízkoúrovňové API pre hardvérovo akcelerované strojové učenie postavené na DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - paralelná výpočtová platforma a aplikačné programovacie rozhranie (API) vyvinuté spoločnosťou Nvidia, umožňujúce všeobecné spracovanie na grafických procesoroch (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - otvorený formát navrhnutý na reprezentáciu modelov strojového učenia, ktorý zabezpečuje interoperabilitu medzi rôznymi ML frameworkmi.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - formát používaný na reprezentáciu a aktualizáciu modelov strojového učenia, obzvlášť užitočný pre menšie jazykové modely, ktoré efektívne bežia na CPU s 4-8bitovou kvantizáciou.

## DirectML

DirectML je nízkoúrovňové API, ktoré umožňuje hardvérovo akcelerované strojové učenie. Je postavené na DirectX 12, aby využilo akceleráciu GPU a je nezávislé od výrobcu, čo znamená, že nevyžaduje zmeny kódu na prácu s rôznymi GPU značkami. Primárne sa používa na trénovanie modelov a inferenciu na GPU.

Čo sa týka hardvérovej podpory, DirectML je navrhnuté tak, aby fungovalo s širokou škálou GPU, vrátane integrovaných a dedikovaných GPU od AMD, integrovaných GPU od Intelu a dedikovaných GPU od NVIDIA. Je súčasťou Windows AI Platform a je podporované na Windows 10 a 11, čo umožňuje trénovanie a inferenciu modelov na akomkoľvek zariadení so systémom Windows.

Boli vydané aktualizácie a príležitosti týkajúce sa DirectML, napríklad podpora až 150 ONNX operátorov a jeho využitie v ONNX runtime a WinML. Podporujú ho hlavní dodávatelia hardvéru (IHVs), z ktorých každý implementuje rôzne metaprikazy.

## CUDA

CUDA, čo znamená Compute Unified Device Architecture, je paralelná výpočtová platforma a API model vytvorený spoločnosťou Nvidia. Umožňuje softvérovým vývojárom využiť GPU podporované CUDA na všeobecné spracovanie – prístup označovaný ako GPGPU (General-Purpose computing on Graphics Processing Units). CUDA je kľúčovým prvkom akcelerácie GPU od Nvidie a je široko používaná v rôznych oblastiach vrátane strojového učenia, vedeckých výpočtov a spracovania videa.

Hardvérová podpora pre CUDA je špecifická pre GPU od Nvidie, keďže ide o proprietárnu technológiu vyvinutú spoločnosťou Nvidia. Každá architektúra podporuje konkrétne verzie CUDA toolkit, ktorý poskytuje potrebné knižnice a nástroje pre vývojárov na vytváranie a spúšťanie CUDA aplikácií.

## ONNX

ONNX (Open Neural Network Exchange) je otvorený formát navrhnutý na reprezentáciu modelov strojového učenia. Poskytuje definíciu rozšíriteľného modelu výpočtového grafu, ako aj definície vstavaných operátorov a štandardných dátových typov. ONNX umožňuje vývojárom presúvať modely medzi rôznymi ML frameworkmi, čím zabezpečuje interoperabilitu a uľahčuje tvorbu a nasadzovanie AI aplikácií.

Phi3 mini dokáže bežať s ONNX Runtime na CPU aj GPU na rôznych zariadeniach vrátane serverových platforiem, Windows, Linux a Mac desktopov, ako aj mobilných CPU.
Optimalizované konfigurácie, ktoré sme pridali, sú

- ONNX modely pre int4 DML: kvantizované na int4 cez AWQ
- ONNX model pre fp16 CUDA
- ONNX model pre int4 CUDA: kvantizované na int4 cez RTN
- ONNX model pre int4 CPU a Mobile: kvantizované na int4 cez RTN

## Llama.cpp

Llama.cpp je open-source softvérová knižnica napísaná v C++. Vykonáva inferenciu na rôznych veľkých jazykových modeloch (LLMs), vrátane Llama. Vyvinutá spolu s knižnicou ggml (všeobecná knižnica pre tensory), llama.cpp sa snaží poskytnúť rýchlejšiu inferenciu a nižšiu spotrebu pamäte v porovnaní s pôvodnou Python implementáciou. Podporuje hardvérovú optimalizáciu, kvantizáciu a ponúka jednoduché API a príklady. Ak máte záujem o efektívnu inferenciu LLM, llama.cpp stojí za preskúmanie, keďže Phi3 dokáže spúšťať Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) je formát používaný na reprezentáciu a aktualizáciu modelov strojového učenia. Je obzvlášť užitočný pre menšie jazykové modely (SLMs), ktoré môžu efektívne bežať na CPU s 4-8bitovou kvantizáciou. GGUF je výhodný pre rýchle prototypovanie a spúšťanie modelov na edge zariadeniach alebo v dávkových úlohách ako CI/CD pipeline.

**Zrieknutie sa zodpovednosti**:  
Tento dokument bol preložený pomocou AI prekladateľskej služby [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keď sa snažíme o presnosť, prosím, majte na pamäti, že automatické preklady môžu obsahovať chyby alebo nepresnosti. Pôvodný dokument v jeho rodnom jazyku by mal byť považovaný za autoritatívny zdroj. Pre kritické informácie sa odporúča profesionálny ľudský preklad. Nie sme zodpovední za akékoľvek nedorozumenia alebo nesprávne interpretácie vyplývajúce z použitia tohto prekladu.