{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaktiivne Phi 3 Mini 4K Instruct vestlusrobot koos Whisper\n",
    "\n",
    "### Sissejuhatus:\n",
    "Interaktiivne Phi 3 Mini 4K Instruct vestlusrobot on tööriist, mis võimaldab kasutajatel suhelda Microsoft Phi 3 Mini 4K instruct demo abil teksti- või helisisendi kaudu. Vestlusrobotit saab kasutada mitmesugusteks ülesanneteks, näiteks tõlkimiseks, ilmateadete värskenduste saamiseks ja üldiseks teabe kogumiseks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[Loo oma Huggingface'i juurdepääsu token](https://huggingface.co/settings/tokens)\n",
    "\n",
    "Loo uus token \n",
    "Sisesta uus nimi \n",
    "Vali kirjutamisõigused\n",
    "Kopeeri token ja salvesta see turvalisse kohta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Järgmine Python-kood teeb kahte peamist ülesannet: impordib `os`-mooduli ja seab keskkonnamuutuja.\n",
    "\n",
    "1. `os` mooduli importimine:\n",
    "   - Python-i `os`-moodul pakub võimalust suhelda operatsioonisüsteemiga. See võimaldab teha erinevaid operatsioonisüsteemiga seotud toiminguid, näiteks pääseda juurde keskkonnamuutujatele, töötada failide ja kataloogidega jne.\n",
    "   - Selles koodis imporditakse `os`-moodul `import`-lausungi abil. See lause teeb `os`-mooduli funktsionaalsuse kättesaadavaks praeguses Python-skriptis.\n",
    "\n",
    "2. Keskkonnamuutuja seadmine:\n",
    "   - Keskkonnamuutuja on väärtus, millele saavad ligi operatsioonisüsteemil töötavad programmid. See on viis salvestada konfiguratsiooniseadeid või muud teavet, mida saavad kasutada mitmed programmid.\n",
    "   - Selles koodis luuakse uus keskkonnamuutuja, kasutades `os.environ` sõnastikku. Sõnastiku võti on `'HF_TOKEN'`, ja väärtus määratakse muutujast HUGGINGFACE_TOKEN.\n",
    "   - Muutuja HUGGINGFACE_TOKEN on määratletud vahetult selle koodilõigu kohal ning sellele omistatakse stringiväärtus \"hf_**************\" kasutades `#@param` süntaksit. Seda süntaksit kasutatakse sageli Jupyter märkmikel, et võimaldada kasutaja sisestust ja parameetrite seadistamist otse märkmiku liideses.\n",
    "   - Seades keskkonnamuutuja `'HF_TOKEN'`, saavad sellele ligi programmi muud osad või samal operatsioonisüsteemil töötavad teised programmid.\n",
    "\n",
    "Kokkuvõttes impordib see kood `os`-mooduli ja seab keskkonnamuutuja nimega `'HF_TOKEN'` väärtusega, mis on määratud muutujas HUGGINGFACE_TOKEN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selle koodilõigu abil määratletakse funktsioon clear_output, mida kasutatakse praeguse lahtri väljundi tühjendamiseks Jupyter Notebookis või IPythonis. Lähme läbi koodi ja mõistame selle funktsionaalsust:\n",
    "\n",
    "Funktsioon clear_output võtab ühe parameetri nimega wait, mis on loogiline väärtus. Vaikimisi on wait määratud väärtusele False. See parameeter määrab, kas funktsioon peaks enne tühjendamist ootama, kuni uus väljund on olemasoleva väljundi asendamiseks.\n",
    "\n",
    "Funktsioon ise kasutatakse praeguse lahtri väljundi tühjendamiseks. Jupyter Notebookis või IPythonis, kui lahter genereerib väljundi, nagu näiteks ekraanile trükitud tekst või graafilised joonised, siis see väljund kuvatakse lahtri all. Funktsioon clear_output võimaldab selle väljundi tühjendada.\n",
    "\n",
    "Funktsiooni implementatsioon ei ole koodilõigus esitatud, mida näitab ellipsis (...). Ellipsis tähistab kohatäitjat tegelikule koodile, mis täidab väljundi tühjendamise. Funktsiooni implementatsioon võib hõlmata suhtlust Jupyter Notebooki või IPythoni API-ga, et eemaldada olemasolev väljund lahtrist.\n",
    "\n",
    "Kokkuvõttes pakub see funktsioon mugavat viisi praeguse lahtri väljundi tühjendamiseks Jupyter Notebookis või IPythonis, muutes kuvatud väljundi haldamise ja uuendamise interaktiivsete kodeerimisseansside ajal lihtsamaks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tehke tekstist kõne (TTS) Edge TTS teenuse abil. Läheme läbi vastavate funktsioonide implementatsioonid üks haaval:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: See funktsioon võtab sisendi väärtuse ja arvutab TTS-hääle kiiruse stringi. Sisendi väärtus tähistab soovitud kõnekiirust, kus väärtus 1 tähendab normaalkiirust. Funktsioon arvutab kiiruse stringi, lahutades sisendväärtusest 1, korrutades tulemuse 100-ga ja määrates seejärel märgi sõltuvalt sellest, kas sisendväärtus on suurem või võrdne 1-ga. Funktsioon tagastab kiiruse stringi formaadis \"{sign}{rate}\".\n",
    "\n",
    "2.`make_chunks(input_text, language)`: See funktsioon võtab parameetriteks sisendi teksti ja keele. See jagab sisendi teksti lõikudeks keele-spetsiifiliste reeglite alusel. Selles implementatsioonis, kui keel on \"English\", jagab funktsioon teksti igal tähel (\".\") ja eemaldab igast lõigust juhtiva ja lõpliku tühiku. Seejärel lisab ta igale lõigule punkti ja tagastab filtreeritud lõikude loendi.\n",
    "\n",
    "3. `tts_file_name(text)`: See funktsioon genereerib TTS-audiofailile nime sisestatud teksti alusel. Ta teostab teksti peal mitmeid teisendusi: eemaldab lõppemispunkti (kui see on olemas), teisendab teksti väikesteks tähtedeks, lõikab juhtivad ja lõplikud tühikud ning asendab tühikud alakriipsudega. Seejärel lühendab teksti maksimaalselt 25 märgini (kui pikem) või kasutab kogu teksti, kui see on tühi. Lõpuks genereerib ta juhusliku stringi kasutades [`uuid`] moodulit ja kombineerib selle lühendatud tekstiga, et luua failinimi formaadis \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: See funktsioon liidab mitu audiofaili üheks audiofailiks. See võtab parameetriteks audiofailide teekondade loendi ja väljundteekonna. Funktsioon initsialiseerib tühja `AudioSegment` objekti nimega [`merged_audio`]. Seejärel iteriseerib ta läbi iga audiofaili teekonna, laadib audiofaili kasutades `AudioSegment.from_file()` meetodit `pydub`-raamatukogust ja lisab praeguse audiofaili [`merged_audio`] objektile. Lõpuks ekspordib ta liidetud audio määratud väljundteekonnale MP3-formaadis.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path): See funktsioon teostab TTS-operatsiooni kasutades Edge TTS teenust. See võtab parameetriteks tekstilõikude loendi, kõne kiiruse, hääle nime ja salvestamise teekonna. Kui lõikude arv on suurem kui 1, loob funktsioon kataloogi individuaalsete lõigufailide salvestamiseks. Seejärel iteriseerib ta läbi iga lõigu, konstrueerib Edge TTS käsu kasutades `calculate_rate_string()` funktsiooni, hääle nime ja lõigu teksti ning käivitab käsu kasutades `os.system()` funktsiooni. Kui käsu täitmine on edukas, lisab ta genereeritud audiofaili teekonna loendisse. Pärast kõigi lõikude töötlemist ühendab ta individuaalsed audiofailid funktsiooni `merge_audio_files()` abil ja salvestab liidetud audio määratud `save_path` teekonnale. Kui lõik on ainult üks, genereerib ta otse Edge TTS käsu ja salvestab audio `save_path`-i. Lõpuks tagastab see genereeritud audiofaili salvestusteekonna.\n",
    "\n",
    "6. `random_audio_name_generate()`: See funktsioon genereerib juhusliku audiofaili nime kasutades [`uuid`] moodulit. See genereerib juhusliku UUID-i, teisendab selle stringiks, võtab esimesed 8 märki, lisab \".mp3\" laienduse ja tagastab juhusliku audiofaili nime.\n",
    "\n",
    "7. `talk(input_text)`: See funktsioon on peamine sisenemispunkt TTS-operatsiooni teostamiseks. See võtab parameetrina sisendi teksti. Esiteks kontrollib ta sisendi teksti pikkust, et määrata, kas tegemist on pika lausungiga (rohkem või võrdne 600 märgiga). Sõltuvalt pikkusest ja `translate_text_flag` muutuja väärtusest määrab ta keele ja genereerib tekstilõikude loendi kasutades `make_chunks()` funktsiooni. Seejärel genereerib ta audiofaili salvestusteekonna kasutades `random_audio_name_generate()` funktsiooni. Lõpuks kutsub ta välja `edge_free_tts()` funktsiooni TTS-operatsiooni teostamiseks ja tagastab genereeritud audiofaili salvestusteekonna.\n",
    "\n",
    "Kokkuvõttes töötavad need funktsioonid koos, et jagada sisendi tekst lõikudeks, genereerida audiofaili nimi, teostada TTS-operatsioon Edge TTS teenuse abil ja ühendada individuaalsed audiofailid üheks lõplikuks audiofailiks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kahe funktsiooni implementatsioon: convert_to_text ja run_text_prompt, samuti kahe klassi deklaratsioon: str ja Audio.\n",
    "\n",
    "The convert_to_text function takes an audio_path as input and transcribes the audio to text using a model called whisper_model. The function first checks if the gpu flag is set to True. If it is, the whisper_model is used with certain parameters such as word_timestamps=True, fp16=True, language='English', and task='translate'. If the gpu flag is False, the whisper_model is used with fp16=False. The resulting transcription is then saved to a file named 'scan.txt' and returned as the text.\n",
    "\n",
    "The run_text_prompt function takes a message and a chat_history as input. It uses the phi_demo function to generate a response from a chatbot based on the input message. The generated response is then passed to the talk function, which converts the response into an audio file and returns the file path. The Audio class is used to display and play the audio file. The audio is displayed using the display function from the IPython.display module, and the Audio object is created with the autoplay=True parameter, so the audio starts playing automatically. The chat_history is updated with the input message and the generated response, and an empty string and the updated chat_history are returned.\n",
    "\n",
    "The str class is a built-in class in Python that represents a sequence of characters. It provides various methods for manipulating and working with strings, such as capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill, and more. These methods allow you to perform operations like searching, replacing, formatting, and manipulating strings.\n",
    "\n",
    "The Audio class is a custom class that represents an audio object. It is used to create an audio player in the Jupyter Notebook environment. The class accepts various parameters such as data, filename, url, embed, rate, autoplay, and normalize. The data parameter can be a numpy array, a list of samples, a string representing a filename or URL, or raw PCM data. The filename parameter is used to specify a local file to load the audio data from, and the url parameter is used to specify a URL to download the audio data from. The embed parameter determines whether the audio data should be embedded using a data URI or referenced from the original source. The rate parameter specifies the sampling rate of the audio data. The autoplay parameter determines whether the audio should start playing automatically. The normalize parameter specifies whether the audio data should be normalized (rescaled) to the maximum possible range. The Audio class also provides methods like reload to reload the audio data from file or URL, and attributes like src_attr, autoplay_attr, and element_id_attr to retrieve the corresponding attributes for the audio element in HTML.\n",
    "\n",
    "Overall, these functions and classes are used to transcribe audio to text, generate audio responses from a chatbot, and display and play audio in the Jupyter Notebook environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\nVastutusest loobumine:\nSee dokument on tõlgitud tehisintellekti tõlke­teenuse Co-op Translator (https://github.com/Azure/co-op-translator) abil. Kuigi me püüame tagada täpsust, palun arvestage, et automaatsed tõlked võivad sisaldada vigu või ebatäpsusi. Originaaldokument selle algkeeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul on soovitatav kasutada professionaalset inimtõlget. Me ei vastuta võimalike arusaamatuste ega valesti tõlgendamise eest, mis sellest tõlkest võivad tuleneda.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-12-22T04:57:33+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "et"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}