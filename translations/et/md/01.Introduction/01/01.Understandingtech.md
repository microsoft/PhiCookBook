<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-10-11T12:15:35+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "et"
}
-->
# Peamised mainitud tehnoloogiad

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - madala taseme API riistvarakiirendusega masinõppe jaoks, mis on üles ehitatud DirectX 12 peale.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - paralleelarvutuse platvorm ja rakendusliidese (API) mudel, mille on välja töötanud Nvidia, võimaldades üldotstarbelist töötlemist graafikaprotsessoritel (GPU-del).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - avatud formaat, mis on loodud masinõppemudelite esitamiseks ja mis tagab erinevate ML-raamistike vahelise koostalitlusvõime.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - formaat, mida kasutatakse masinõppemudelite esitamiseks ja uuendamiseks, eriti kasulik väiksemate keelemudelite jaoks, mis töötavad tõhusalt protsessoritel 4-8bitise kvantiseerimisega.

## DirectML

DirectML on madala taseme API, mis võimaldab riistvarakiirendusega masinõpet. See on üles ehitatud DirectX 12 peale, et kasutada GPU kiirendust, ja on müüjast sõltumatu, mis tähendab, et see ei nõua koodimuudatusi erinevate GPU müüjate vahel töötamiseks. Seda kasutatakse peamiselt mudelite treenimiseks ja järelduste tegemiseks GPU-del.

Riistvaratoe osas on DirectML loodud töötama laia valiku GPU-dega, sealhulgas AMD integreeritud ja eraldiseisvad GPU-d, Inteli integreeritud GPU-d ja NVIDIA eraldiseisvad GPU-d. See on osa Windows AI platvormist ja seda toetatakse Windows 10 & 11 operatsioonisüsteemides, võimaldades mudelite treenimist ja järelduste tegemist igas Windowsi seadmes.

DirectML-iga on seotud mitmeid uuendusi ja võimalusi, näiteks kuni 150 ONNX operaatori tugi ning kasutamine nii ONNX runtime'i kui ka WinML-i poolt. Seda toetavad peamised integreeritud riistvaratootjad (IHV-d), kes rakendavad erinevaid metakäske.

## CUDA

CUDA, mis tähistab Compute Unified Device Architecture, on paralleelarvutuse platvorm ja rakendusliidese (API) mudel, mille on loonud Nvidia. See võimaldab tarkvaraarendajatel kasutada CUDA-toega graafikaprotsessoreid (GPU-sid) üldotstarbeliseks töötlemiseks – lähenemine, mida nimetatakse GPGPU-ks (General-Purpose computing on Graphics Processing Units). CUDA on Nvidia GPU kiirenduse võtmetehnoloogia ja seda kasutatakse laialdaselt erinevates valdkondades, sealhulgas masinõppes, teadusarvutustes ja videote töötlemisel.

Riistvaratugi CUDA jaoks on spetsiifiline Nvidia GPU-dele, kuna see on Nvidia poolt välja töötatud patenteeritud tehnoloogia. Iga arhitektuur toetab konkreetseid CUDA tööriistakomplekti versioone, mis pakuvad vajalikke teeke ja tööriistu arendajatele CUDA rakenduste loomiseks ja käitamiseks.

## ONNX

ONNX (Open Neural Network Exchange) on avatud formaat, mis on loodud masinõppemudelite esitamiseks. See pakub laiendatava arvutusgraafiku mudeli definitsiooni, samuti sisseehitatud operaatorite ja standardsete andmetüüpide definitsioone. ONNX võimaldab arendajatel liigutada mudeleid erinevate ML-raamistike vahel, tagades koostalitlusvõime ja lihtsustades tehisintellekti rakenduste loomist ja juurutamist.

Phi3 mini saab töötada ONNX Runtime'iga protsessoritel ja GPU-del erinevates seadmetes, sealhulgas serveriplatvormidel, Windowsi, Linuxi ja Maci lauaarvutitel ning mobiilsetel protsessoritel.
Optimeeritud konfiguratsioonid, mille oleme lisanud, on:

- ONNX mudelid int4 DML jaoks: kvantiseeritud int4-ks AWQ abil
- ONNX mudel fp16 CUDA jaoks
- ONNX mudel int4 CUDA jaoks: kvantiseeritud int4-ks RTN abil
- ONNX mudel int4 protsessorite ja mobiilseadmete jaoks: kvantiseeritud int4-ks RTN abil

## Llama.cpp

Llama.cpp on avatud lähtekoodiga tarkvararaamatukogu, mis on kirjutatud C++ keeles. See teostab järeldusi erinevate suurte keelemudelite (LLM-ide), sealhulgas Llama, põhjal. Koos ggml-raamatukoguga (üldotstarbeline tensorite raamatukogu) välja töötatud Llama.cpp eesmärk on pakkuda kiiremat järeldamist ja väiksemat mälukasutust võrreldes algse Pythonis kirjutatud rakendusega. See toetab riistvara optimeerimist, kvantiseerimist ja pakub lihtsat API-d ning näiteid. Kui olete huvitatud tõhusast LLM-i järeldamisest, tasub Llama.cpp-d uurida, kuna Phi3 saab töötada Llama.cpp-ga.

## GGUF

GGUF (Generic Graph Update Format) on formaat, mida kasutatakse masinõppemudelite esitamiseks ja uuendamiseks. See on eriti kasulik väiksemate keelemudelite (SLM-ide) jaoks, mis töötavad tõhusalt protsessoritel 4-8bitise kvantiseerimisega. GGUF on kasulik kiireks prototüüpimiseks ja mudelite käitamiseks servaseadmetel või partiitöödes, näiteks CI/CD torujuhtmetes.

---

**Lahtiütlus**:  
See dokument on tõlgitud AI tõlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi püüame tagada täpsust, palume arvestada, et automaatsed tõlked võivad sisaldada vigu või ebatäpsusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtõlget. Me ei vastuta selle tõlke kasutamisest tulenevate arusaamatuste või valesti tõlgenduste eest.