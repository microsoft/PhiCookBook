# **量化 Phi 系列**

模型量化是指将神经网络模型中的参数（如权重和激活值）从较大的取值范围（通常是连续值范围）映射到较小的有限取值范围的过程。该技术可以减少模型的大小和计算复杂度，并提升模型在移动设备或嵌入式系统等资源受限环境中的运行效率。模型量化通过降低参数的精度实现压缩，但这也引入了一定的精度损失。因此，在量化过程中，需要在模型大小、计算复杂度和精度之间进行平衡。常见的量化方法包括定点量化、浮点量化等。您可以根据具体场景和需求选择合适的量化策略。

我们希望将生成式人工智能模型部署到边缘设备上，让更多设备进入生成式AI场景，如移动设备、AI PC/Copilot+PC，以及传统物联网设备。通过量化模型，我们可以基于不同设备将模型部署到不同的边缘设备。结合硬件厂商提供的模型加速框架和量化模型，我们能够构建更好的SLM应用场景。

在量化场景中，我们有不同的精度（INT4、INT8、FP16、FP32）。以下是常用量化精度的说明。

### **INT4**

INT4量化是一种激进的量化方法，将模型的权重和激活值量化为4位整数。由于表示范围更小、精度更低，INT4量化通常会导致较大的精度损失。然而，与INT8量化相比，INT4量化能够进一步降低模型的存储需求和计算复杂度。需要注意的是，INT4量化在实际应用中相对少见，因为过低的精度可能导致模型性能显著下降。此外，并非所有硬件都支持INT4操作，因此在选择量化方法时需要考虑硬件兼容性。

### **INT8**

INT8量化是将模型的权重和激活值从浮点数转换为8位整数的过程。虽然INT8整数表示的数值范围较小且精度较低，但它可以显著减少存储和计算需求。在INT8量化中，模型的权重和激活值经过量化处理，包括缩放和偏移，以尽可能保留原始浮点信息。在推理过程中，这些量化值将被反量化回浮点数进行计算，然后再重新量化为INT8以进行下一步操作。该方法在大多数应用中能提供足够的精度，同时保持较高的计算效率。

### **FP16**

FP16格式，即16位浮点数（float16），相比32位浮点数（float32）将内存占用减半，在大规模深度学习应用中具有显著优势。FP16格式允许在相同GPU内存限制下加载更大的模型或处理更多数据。随着现代GPU硬件不断支持FP16操作，使用FP16格式也可能带来计算速度的提升。然而，FP16格式也有其固有缺点，即较低的精度，可能导致数值不稳定或部分情况下精度损失。

### **FP32**

FP32格式提供更高的精度，能够准确表示更广泛的数值范围。在执行复杂数学运算或需要高精度结果的场景中，优先采用FP32格式。但高精度同时意味着更高的内存使用和更长的计算时间。对于大规模深度学习模型，尤其是参数众多和数据量庞大的情况，使用FP32格式可能会导致GPU内存不足或推理速度降低。

在移动设备或物联网设备上，我们可以将Phi-3.x模型转换为INT4，而AI PC / Copilot PC则可以使用较高精度，如INT8、FP16、FP32。

目前，不同硬件厂商有不同的框架支持生成式模型，如Intel的OpenVINO、Qualcomm的QNN、Apple的MLX和Nvidia的CUDA等，结合模型量化完成本地部署。

在技术层面，量化后我们支持不同格式，如PyTorch / TensorFlow格式、GGUF和ONNX。我做过GGUF和ONNX的格式对比及应用场景分析。这里推荐ONNX量化格式，因其从模型框架到硬件都有很好的支持。本章我们将重点介绍基于ONNX Runtime for GenAI、OpenVINO和Apple MLX进行模型量化（如果您有更好的方案，也欢迎通过提交PR提供）。

**本章内容包括**

1. [使用 llama.cpp 量化 Phi-3.5 / 4](./UsingLlamacppQuantifyingPhi.md)

2. [使用 onnxruntime 生成式AI扩展量化 Phi-3.5 / 4](./UsingORTGenAIQuantifyingPhi.md)

3. [使用 Intel OpenVINO 量化 Phi-3.5 / 4](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [使用 Apple MLX 框架量化 Phi-3.5 / 4](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**免责声明**：  
本文件通过 AI 翻译服务 [Co-op Translator](https://github.com/Azure/co-op-translator) 翻译完成。我们尽力保证翻译的准确性，但请注意，自动翻译可能存在错误或不准确之处。原始语言的文档应被视为权威来源。对于关键信息，建议采用专业人工翻译。我们不对因使用本翻译而引起的任何误解或错误解释承担责任。
<!-- CO-OP TRANSLATOR DISCLAIMER END -->