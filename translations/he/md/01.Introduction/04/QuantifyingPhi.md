<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-05T08:48:01+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "he"
}
-->
# **כימות משפחת פי**

כימות מודל מתייחס לתהליך של מיפוי הפרמטרים (כגון משקלים וערכי הפעלה) במודל רשת עצבית מטווח ערכים רחב (בדרך כלל טווח ערכים רציף) לטווח ערכים סופי קטן יותר. טכנולוגיה זו יכולה להפחית את גודל המודל ואת המורכבות החישובית שלו ולשפר את יעילות ההפעלה של המודל בסביבות עם משאבים מוגבלים כגון מכשירים ניידים או מערכות משובצות. כימות המודל משיג דחיסה על ידי הפחתת הדיוק של הפרמטרים, אך גם גורם לאיבוד דיוק מסוים. לכן, בתהליך הכימות יש לאזן בין גודל המודל, המורכבות החישובית והדיוק. שיטות כימות נפוצות כוללות כימות נקודה-קבועה, כימות נקודה-צפה ועוד. ניתן לבחור באסטרטגיית הכימות המתאימה בהתאם לתרחיש ולצרכים הספציפיים.

אנו שואפים לפרוס מודל GenAI למכשירי קצה ולאפשר למכשירים רבים יותר להיכנס לתרחישי GenAI, כגון מכשירים ניידים, AI PC/Copilot+PC, ומכשירי IoT מסורתיים. דרך מודל הכימות, נוכל לפרוס אותו למכשירי קצה שונים על פי ההבדלים בין המכשירים. בשילוב עם מסגרת האצת מודלים ומודל הכימות שמספקים יצרני חומרה, נוכל לבנות תרחישי יישומים טובים יותר ל-SLM.

בתרחיש הכימות קיימים לנו דיוקים שונים (INT4, INT8, FP16, FP32). להלן הסבר על דיוקי הכימות הנפוצים.

### **INT4**

כימות INT4 היא שיטת כימות ממושמעת שמכמתת את המשקלים וערכי ההפעלה של המודל למספרים שלמים בגודל 4 ביט. כימות INT4 בדרך כלל גורם לאיבוד דיוק גדול יותר בשל טווח הייצוג הקטן יותר והדיוק הנמוך. עם זאת, בהשוואה לכימות INT8, כימות INT4 יכול להפחית עוד יותר את דרישות האחסון ואת המורכבות החישובית של המודל. יש להדגיש כי כימות INT4 הוא יחסית נדיר ביישומים מעשיים, משום שדיוק נמוך מדי עלול לגרום לירידה משמעותית בביצועי המודל. בנוסף, לא כל חומרה תומכת בפעולות INT4, ולכן יש להתחשב בהתאמת החומרה בעת בחירת שיטת כימות.

### **INT8**

כימות INT8 הוא התהליך של המרת המשקלים וערכי ההפעלה של מודל ממספרים נקודה צפה למספרים שלמים בגודל 8 ביט. למרות שטווח המספרים שמיוצג על ידי מספרים אלו קטן יותר ופחות מדויק, הוא יכול להפחית משמעותית את דרישות האחסון והחישוב. בכימות INT8, המשקלים וערכי ההפעלה עוברים תהליך כימות הכולל מיזעור והיסט, כדי לשמר כמה שאפשר את המידע המקורי בנקודה הצפה. במהלך ההסקה, ערכים אלה מנותחים חזרה לנקודה צפה לחישוב, ולאחר מכן מכומתים שוב ל-INT8 לשלב הבא. שיטה זו יכולה לספק דיוק מספק ברוב היישומים תוך שמירה על יעילות חישובית גבוהה.

### **FP16**

פורמט FP16, כלומר מספרים בולטים בגודל 16 ביט (float16), מפחית את דרישת הזיכרון למחצית בהשוואה למספרים בולטים בגודל 32 ביט (float32), מה שמעניק יתרונות משמעותיים ביישומי למידה עמוקה בקנה מידה גדול. פורמט FP16 מאפשר לטעון מודלים גדולים יותר או לעבד יותר נתונים במסגרת מגבלות הזיכרון של ה-GPU. ככל שחומרות GPU מודרניות ממשיכות לתמוך בפעולות FP16, השימוש בפורמט FP16 עשוי גם להביא לשיפורים במהירות החישובים. עם זאת, פורמט FP16 גם סובל מחסרונות מולדים, כמו דיוק נמוך יותר, שעלול לגרום לאי יציבות מספרית או לאובדן דיוק במקרים מסוימים.

### **FP32**

פורמט FP32 מספק דיוק גבוה יותר ויכול לייצג טווח רחב של ערכים בדיוק. בתרחישים שבהם מתבצעים פעולות מתמטיות מורכבות או נדרשים תוצאות בעלי דיוק גבוה, פורמט FP32 מועדף. עם זאת, הדיוק הגבוה מלווה גם בשימוש גבוה יותר בזיכרון ובזמני חישוב ארוכים יותר. במודלים גדולים ללמידה עמוקה, במיוחד כאשר קיימים פרמטרים רבים וכמות עצומה של נתונים, פורמט FP32 עלול לגרום למחסור בזיכרון ה-GPU או לירידה במהירות ההסקה.

במכשירים ניידים או במכשירי IoT, נוכל להמיר מודלים של Phi-3.x ל-INT4, בעוד ש-AI PC / Copilot PC יכולים להשתמש בדיוק גבוה יותר כמו INT8, FP16 או FP32.

כיום, יצרני חומרה שונים מציעים מסגרות שונות לתמיכה במודלים גנרטיביים, כמו OpenVINO של אינטל, QNN של קוואלקום, MLX של אפל ו-CUDA של נווידיה, בשילוב עם כימות המודל להשלמת פריסה מקומית.

בטכנולוגיה, קיימות לנו תמיכות פורמט שונות לאחר הכימות, כגון פורמט PyTorch / TensorFlow, GGUF, ו-ONNX. ערכתי השוואת פורמטים ותסריטי שימוש בין GGUF ו-ONNX. כאן אני ממליץ על פורמט כימות ONNX, המונע תמיכה טובה מהמסגרת של המודל ועד לחומרה. בפרק זה נתמקד ב-ONNX Runtime ל-GenAI, OpenVINO ואפל MLX לביצוע כימות מודל (אם יש לכם דרך טובה יותר, אתם יכולים גם לספק אותה על ידי שליחת PR).

**פרק זה כולל**

1. [כימות Phi-3.5 / 4 באמצעות llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [כימות Phi-3.5 / 4 באמצעות הרחבות AI גנרטיביות ל-onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [כימות Phi-3.5 / 4 באמצעות Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [כימות Phi-3.5 / 4 באמצעות Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**כתב ויתור**:
מסמך זה תורגם באמצעות שירות תרגום בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי-דיוקים. יש להתייחס למסמך המקורי בשפת המקור כמקור הסמכותי. למידע קריטי מומלץ להיעזר בתרגום מקצועי על ידי אדם. אנו לא אחראים לכל אי-הבנה או פרשנות שגויה הנובעים מהשימוש בתרגום זה.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->