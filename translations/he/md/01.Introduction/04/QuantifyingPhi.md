<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-05-09T13:31:29+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "he"
}
-->
# **כימות משפחת Phi**

כימות מודל מתייחס לתהליך של מיפוי הפרמטרים (כגון משקלים וערכי הפעלה) במודל רשת עצבית מטווח ערכים גדול (בדרך כלל טווח ערכים רציף) לטווח ערכים סופי וקטן יותר. טכנולוגיה זו יכולה לצמצם את גודל המודל ואת המורכבות החישובית שלו, ולשפר את יעילות הפעולה של המודל בסביבות מוגבלות במשאבים כמו מכשירים ניידים או מערכות מוטמעות. כימות המודל משיג דחיסה על ידי הפחתת הדיוק של הפרמטרים, אך גם גורם לאובדן דיוק מסוים. לכן, בתהליך הכימות יש צורך לאזן בין גודל המודל, המורכבות החישובית והדיוק. שיטות כימות נפוצות כוללות כימות נקודת קבועה, כימות נקודת צף וכו'. ניתן לבחור את אסטרטגיית הכימות המתאימה בהתאם לתרחיש ולצרכים הספציפיים.

אנו שואפים לפרוס את מודל GenAI במכשירי קצה ולאפשר למכשירים רבים יותר להיכנס לתרחישי GenAI, כגון מכשירים ניידים, AI PC/Copilot+PC, ומכשירי IoT מסורתיים. באמצעות מודל הכימות ניתן לפרוס אותו למכשירי קצה שונים בהתבסס על סוגי המכשירים. בשילוב עם מסגרת האצת המודל ומודל הכימות המסופקים על ידי יצרני חומרה, נוכל לבנות תרחישי יישום SLM טובים יותר.

בתרחיש הכימות קיימים רמות דיוק שונות (INT4, INT8, FP16, FP32). להלן הסבר על רמות הדיוק הנפוצות בכימות

### **INT4**

כימות INT4 הוא שיטת כימות קיצונית שממירה את המשקלים וערכי ההפעלה של המודל למספרים שלמים בגודל 4 ביט. כימות INT4 בדרך כלל גורם לאובדן דיוק גדול יותר בגלל טווח הייצוג הקטן יותר והדיוק הנמוך יותר. עם זאת, בהשוואה לכימות INT8, כימות INT4 יכול להפחית עוד יותר את דרישות האחסון והמורכבות החישובית של המודל. חשוב לציין שכימות INT4 יחסית נדיר בשימוש מעשי, מכיוון שדיוק נמוך מדי עלול לגרום לירידה משמעותית בביצועי המודל. בנוסף, לא כל החומרות תומכות בפעולות INT4, ולכן יש לקחת בחשבון תאימות חומרה בבחירת שיטת הכימות.

### **INT8**

כימות INT8 הוא תהליך המרת המשקלים וערכי ההפעלה של המודל ממספרים בנקודת צף למספרים שלמים בגודל 8 ביט. אף שטווח המספרים המיוצגים ב-INT8 קטן ופחות מדויק, הוא יכול להפחית משמעותית את דרישות האחסון והחישוב. בכימות INT8, המשקלים וערכי ההפעלה עוברים תהליך כימות הכולל סקלינג והסטה, במטרה לשמור על מידע נקודת הצף המקורי ככל האפשר. במהלך האינפרנס, הערכים המוכמתים מפוענחים חזרה למספרי נקודת צף לצורך חישוב, ואז מוכמתים שוב ל-INT8 לשלב הבא. שיטה זו מספקת דיוק מספק ברוב היישומים תוך שמירה על יעילות חישובית גבוהה.

### **FP16**

פורמט FP16, כלומר מספרים בנקודת צף בגודל 16 ביט (float16), מצמצם את צריכת הזיכרון בחצי בהשוואה למספרים בגודל 32 ביט (float32), יתרון משמעותי ביישומי למידה עמוקה בקנה מידה גדול. פורמט FP16 מאפשר טעינת מודלים גדולים יותר או עיבוד נתונים רבים יותר במסגרת מגבלות הזיכרון של ה-GPU. עם המשך התמיכה של חומרות GPU מודרניות בפעולות FP16, השימוש בפורמט FP16 עשוי להביא גם לשיפורים במהירות החישוב. עם זאת, לפורמט FP16 גם חסרונות מולדים, כלומר דיוק נמוך יותר, שעשוי לגרום לאי יציבות מספרית או לאובדן דיוק במקרים מסוימים.

### **FP32**

פורמט FP32 מספק דיוק גבוה יותר ויכול לייצג טווח רחב של ערכים בדיוק. בתרחישים שבהם מתבצעות פעולות מתמטיות מורכבות או נדרשים תוצאות מדויקות מאוד, פורמט FP32 מועדף. עם זאת, דיוק גבוה גם אומר שימוש גבוה יותר בזיכרון וזמן חישוב ארוך יותר. עבור מודלים גדולים של למידה עמוקה, במיוחד כאשר קיימים פרמטרים רבים ומידע רב, פורמט FP32 עלול לגרום למחסור בזיכרון GPU או להאטת מהירות האינפרנס.

במכשירים ניידים או מכשירי IoT, ניתן להמיר מודלים של Phi-3.x ל-INT4, בעוד ש-AI PC / Copilot PC יכולים להשתמש ברמות דיוק גבוהות יותר כמו INT8, FP16, FP32.

כיום, ליצרני חומרה שונים יש מסגרות שונות לתמיכה במודלים גנרטיביים, כגון OpenVINO של אינטל, QNN של Qualcomm, MLX של Apple, ו-CUDA של Nvidia, בשילוב עם כימות המודל להשלמת פריסה מקומית.

מבחינה טכנולוגית, קיימת תמיכה בפורמטים שונים לאחר הכימות, כגון פורמטים של PyTorch / Tensorflow, GGUF, ו-ONNX. ביצעתי השוואת פורמטים ותרחישי יישום בין GGUF ל-ONNX. כאן אני ממליץ על פורמט הכימות ONNX, שמקבל תמיכה טובה ממסגרת המודל ועד לחומרה. בפרק זה נתמקד ב-ONNX Runtime עבור GenAI, OpenVINO ו-Apple MLX לביצוע כימות מודל (אם יש לכם שיטה טובה יותר, תוכלו גם לשלוח לנו PR).

**פרק זה כולל**

1. [כימות Phi-3.5 / 4 באמצעות llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [כימות Phi-3.5 / 4 באמצעות הרחבות AI גנרטיביות ל-onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [כימות Phi-3.5 / 4 באמצעות Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [כימות Phi-3.5 / 4 באמצעות מסגרת Apple MLX](./UsingAppleMLXQuantifyingPhi.md)

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון כי תרגומים אוטומטיים עלולים להכיל שגיאות או אי-דיוקים. המסמך המקורי בשפת המקור שלו הוא המקור הסמכותי. למידע קריטי מומלץ להשתמש בתרגום מקצועי אנושי. אנו לא נושאים באחריות על אי-הבנות או פרשנויות שגויות הנובעות משימוש בתרגום זה.