<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-07-16T21:48:17+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "he"
}
-->
# **כימות משפחת Phi**

כימות מודל מתייחס לתהליך של מיפוי הפרמטרים (כגון משקלים וערכי הפעלה) במודל רשת עצבית מטווח ערכים רחב (בדרך כלל טווח ערכים רציף) לטווח ערכים סופי וקטן יותר. טכנולוגיה זו יכולה להפחית את גודל המודל ואת המורכבות החישובית שלו, ולשפר את יעילות הפעולה של המודל בסביבות עם משאבים מוגבלים כמו מכשירים ניידים או מערכות משובצות. כימות מודל משיג דחיסה על ידי הפחתת הדיוק של הפרמטרים, אך גם גורם לאובדן דיוק מסוים. לכן, בתהליך הכימות יש צורך לאזן בין גודל המודל, המורכבות החישובית והדיוק. שיטות כימות נפוצות כוללות כימות נקודה קבועה, כימות נקודה צפה ועוד. ניתן לבחור את אסטרטגיית הכימות המתאימה בהתאם לתרחיש ולצרכים הספציפיים.

אנו שואפים לפרוס את מודל GenAI במכשירי קצה ולאפשר למכשירים רבים יותר להיכנס לתרחישי GenAI, כגון מכשירים ניידים, AI PC/Copilot+PC ומכשירי IoT מסורתיים. באמצעות מודל הכימות, נוכל לפרוס אותו במכשירי קצה שונים בהתאם לסוג המכשיר. בשילוב עם מסגרת האצת המודל ומודל הכימות המסופקים על ידי יצרני החומרה, נוכל לבנות תרחישי יישום SLM טובים יותר.

בתרחיש הכימות, קיימים דיוקים שונים (INT4, INT8, FP16, FP32). להלן הסבר על דיוקי הכימות הנפוצים

### **INT4**

כימות INT4 הוא שיטת כימות קיצונית שמכמתת את המשקלים וערכי ההפעלה של המודל למספרים שלמים בגודל 4 ביט. כימות INT4 בדרך כלל גורם לאובדן דיוק גדול יותר בגלל טווח הייצוג הקטן יותר והדיוק הנמוך יותר. עם זאת, בהשוואה לכימות INT8, כימות INT4 יכול להפחית עוד יותר את דרישות האחסון והמורכבות החישובית של המודל. יש לציין שכימות INT4 הוא יחסית נדיר בשימוש מעשי, מכיוון שדיוק נמוך מדי עלול לגרום לירידה משמעותית בביצועי המודל. בנוסף, לא כל החומרה תומכת בפעולות INT4, ולכן יש לקחת בחשבון את תאימות החומרה בבחירת שיטת הכימות.

### **INT8**

כימות INT8 הוא תהליך המרה של משקלי וערכי ההפעלה של המודל ממספרים בנקודה צפה למספרים שלמים בגודל 8 ביט. למרות שטווח המספרים המיוצגים על ידי INT8 קטן ופחות מדויק, הוא יכול להפחית משמעותית את דרישות האחסון והחישוב. בכימות INT8, המשקלים וערכי ההפעלה עוברים תהליך כימות הכולל סקלינג והסטה, כדי לשמר את המידע המקורי בנקודה צפה ככל האפשר. במהלך ההסקה, ערכים מכויימים אלה מפוענחים חזרה למספרים בנקודה צפה לצורך חישוב, ואז מכויימים שוב ל-INT8 לשלב הבא. שיטה זו מספקת דיוק מספק ברוב היישומים תוך שמירה על יעילות חישובית גבוהה.

### **FP16**

פורמט FP16, כלומר מספרים בנקודה צפה בגודל 16 ביט (float16), מפחית את צריכת הזיכרון בחצי לעומת מספרים בנקודה צפה בגודל 32 ביט (float32), מה שמביא יתרונות משמעותיים ביישומי למידה עמוקה בקנה מידה גדול. פורמט FP16 מאפשר טעינת מודלים גדולים יותר או עיבוד כמות גדולה יותר של נתונים במסגרת מגבלות הזיכרון של ה-GPU. ככל שחומרת ה-GPU המודרנית ממשיכה לתמוך בפעולות FP16, השימוש בפורמט FP16 עשוי גם לשפר את מהירות החישוב. עם זאת, לפורמט FP16 יש גם חסרונות מובנים, כלומר דיוק נמוך יותר, שעשוי לגרום לאי יציבות מספרית או לאובדן דיוק במקרים מסוימים.

### **FP32**

פורמט FP32 מספק דיוק גבוה יותר ויכול לייצג טווח רחב של ערכים בדיוק רב. בתרחישים שבהם מבוצעים חישובים מתמטיים מורכבים או נדרשים תוצאות מדויקות מאוד, פורמט FP32 מועדף. עם זאת, דיוק גבוה גם אומר שימוש גבוה יותר בזיכרון וזמן חישוב ארוך יותר. עבור מודלים גדולים של למידה עמוקה, במיוחד כאשר יש הרבה פרמטרים וכמות עצומה של נתונים, פורמט FP32 עלול לגרום למחסור בזיכרון GPU או להאטה במהירות ההסקה.

במכשירים ניידים או במכשירי IoT, ניתן להמיר מודלים של Phi-3.x ל-INT4, בעוד ש-AI PC / Copilot PC יכולים להשתמש בדיוק גבוה יותר כמו INT8, FP16, FP32.

כיום, ליצרני חומרה שונים יש מסגרות שונות לתמיכה במודלים גנרטיביים, כגון OpenVINO של Intel, QNN של Qualcomm, MLX של Apple ו-CUDA של Nvidia, בשילוב עם כימות מודל להשלמת פריסה מקומית.

מבחינה טכנולוגית, יש לנו תמיכה בפורמטים שונים לאחר הכימות, כגון פורמט PyTorch / Tensorflow, GGUF ו-ONNX. ערכתי השוואת פורמטים ותרחישי יישום בין GGUF ל-ONNX. כאן אני ממליץ על פורמט הכימות ONNX, שמקבל תמיכה טובה ממסגרת המודל ועד החומרה. בפרק זה נתמקד ב-ONNX Runtime עבור GenAI, OpenVINO ו-Apple MLX לביצוע כימות מודל (אם יש לכם שיטה טובה יותר, תוכלו גם לשלוח לנו PR).

**פרק זה כולל**

1. [כימות Phi-3.5 / 4 באמצעות llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [כימות Phi-3.5 / 4 באמצעות הרחבות AI גנרטיבי ל-onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [כימות Phi-3.5 / 4 באמצעות Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [כימות Phi-3.5 / 4 באמצעות מסגרת Apple MLX](./UsingAppleMLXQuantifyingPhi.md)

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון כי תרגומים אוטומטיים עלולים להכיל שגיאות או אי-דיוקים. המסמך המקורי בשפת המקור שלו נחשב למקור הסמכותי. למידע קריטי מומלץ להשתמש בתרגום מקצועי על ידי מתרגם אנושי. אנו לא נושאים באחריות לכל אי-הבנה או פרשנות שגויה הנובעת משימוש בתרגום זה.