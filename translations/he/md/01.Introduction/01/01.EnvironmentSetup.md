<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-07-16T18:10:56+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "he"
}
-->
# התחילו עם Phi-3 באופן מקומי

המדריך הזה יעזור לכם להגדיר את הסביבה המקומית שלכם כדי להריץ את מודל Phi-3 באמצעות Ollama. ניתן להריץ את המודל בכמה דרכים שונות, כולל שימוש ב-GitHub Codespaces, VS Code Dev Containers, או בסביבה המקומית שלכם.

## הגדרת הסביבה

### GitHub Codespaces

ניתן להריץ את התבנית הזו באופן וירטואלי באמצעות GitHub Codespaces. הכפתור יפתח מופע של VS Code מבוסס דפדפן:

1. פתחו את התבנית (זה עשוי לקחת כמה דקות):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. פתחו חלון טרמינל

### VS Code Dev Containers

⚠️ אפשרות זו תעבוד רק אם ל-Docker Desktop שלכם מוקצה לפחות 16 גיגה-בייט של זיכרון RAM. אם יש לכם פחות מ-16 גיגה-בייט, תוכלו לנסות את [אפשרות GitHub Codespaces](../../../../../md/01.Introduction/01) או [להגדיר את זה באופן מקומי](../../../../../md/01.Introduction/01).

אפשרות קשורה היא VS Code Dev Containers, שתפתח את הפרויקט ב-VS Code המקומי שלכם באמצעות [הרחבת Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. הפעלו את Docker Desktop (התקינו אותו אם עדיין לא מותקן)
2. פתחו את הפרויקט:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. בחלון VS Code שנפתח, ברגע שקבצי הפרויקט יופיעו (זה עשוי לקחת כמה דקות), פתחו חלון טרמינל.
4. המשיכו עם [שלבי הפריסה](../../../../../md/01.Introduction/01)

### סביבה מקומית

1. ודאו שהכלים הבאים מותקנים:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## בדיקת המודל

1. בקשו מ-Ollama להוריד ולהריץ את מודל phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    זה ייקח כמה דקות להוריד את המודל.

2. ברגע שתראו "success" בפלט, תוכלו לשלוח הודעה למודל דרך הפרומפט.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. לאחר כמה שניות, אמור להופיע זרם תגובה מהמודל.

4. כדי ללמוד על טכניקות שונות בשימוש עם מודלים לשוניים, פתחו את פנקס הפייתון [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) והריצו כל תא. אם השתמשתם במודל שונה מ-'phi3:mini', שנו את `MODEL_NAME` בתא הראשון.

5. כדי לנהל שיחה עם מודל phi3:mini מפייתון, פתחו את קובץ הפייתון [chat.py](../../../../../code/01.Introduce/chat.py) והריצו אותו. ניתן לשנות את `MODEL_NAME` בראש הקובץ לפי הצורך, וגם לשנות את הודעת המערכת או להוסיף דוגמאות few-shot במידת הרצון.

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון כי תרגומים אוטומטיים עלולים להכיל שגיאות או אי-דיוקים. המסמך המקורי בשפת המקור שלו נחשב למקור הסמכותי. למידע קריטי מומלץ להשתמש בתרגום מקצועי על ידי אדם. אנו לא נושאים באחריות לכל אי-הבנה או פרשנות שגויה הנובעת משימוש בתרגום זה.