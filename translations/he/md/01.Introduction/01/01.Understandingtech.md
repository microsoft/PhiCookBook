<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:26:31+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "he"
}
-->
# טכנולוגיות מרכזיות שהוזכרו כוללות

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - API ברמת נמוכה ללמידת מכונה מואצת חומרה, המבוסס על DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - פלטפורמת חישוב מקבילית ומודל API שפותחו על ידי Nvidia, שמאפשרים עיבוד כללי על יחידות עיבוד גרפיות (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - פורמט פתוח שנועד לייצוג מודלים של למידת מכונה ומספק אינטרופרביליות בין מסגרות ML שונות.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - פורמט לייצוג ועדכון מודלים של למידת מכונה, שימושי במיוחד למודלים שפתיים קטנים שיכולים לפעול ביעילות על CPUs עם כימות של 4-8 ביט.

## DirectML

DirectML הוא API ברמת נמוכה שמאפשר למידת מכונה מואצת חומרה. הוא מבוסס על DirectX 12 כדי לנצל את האצת ה-GPU והוא עצמאי מספק, כלומר אינו דורש שינויי קוד כדי לפעול על גבי ספקי GPU שונים. הוא משמש בעיקר לאימון מודלים ולעיבוד תוצאות על GPUs.

לגבי תמיכת החומרה, DirectML תוכנן לפעול עם מגוון רחב של GPUs, כולל GPUs משולבים ודיסקרטיים של AMD, GPUs משולבים של Intel, ו-GPUs דיסקרטיים של NVIDIA. הוא חלק מפלטפורמת Windows AI ותומך ב-Windows 10 ו-11, מה שמאפשר אימון מודלים ועיבוד תוצאות על כל מכשיר Windows.

היו עדכונים והזדמנויות הקשורות ל-DirectML, כמו תמיכה עד ל-150 אופרטורים של ONNX ושימוש גם ב-ONNX runtime וגם ב-WinML. הוא נתמך על ידי ספקי חומרה מובילים (IHVs), שכל אחד מהם מיישם מגוון מטא-פקודות.

## CUDA

CUDA, שמייצג Compute Unified Device Architecture, היא פלטפורמת חישוב מקבילית ומודל API שפותחו על ידי Nvidia. היא מאפשרת למפתחי תוכנה להשתמש ב-GPU עם תמיכת CUDA לעיבוד כללי – גישה המכונה GPGPU (חישוב כללי על יחידות עיבוד גרפיות). CUDA היא מרכיב מרכזי באצת GPU של Nvidia ומשמשת בתחומים רבים, כולל למידת מכונה, חישובים מדעיים ועיבוד וידאו.

תמיכת החומרה ל-CUDA מוגבלת ל-GPUs של Nvidia, כיוון שמדובר בטכנולוגיה קניינית שפותחה על ידם. כל ארכיטקטורה תומכת בגרסאות ספציפיות של ערכת הכלים CUDA, שמספקת את הספריות והכלים הנדרשים למפתחים לבניית והרצת יישומי CUDA.

## ONNX

ONNX (Open Neural Network Exchange) הוא פורמט פתוח שנועד לייצוג מודלים של למידת מכונה. הוא מספק הגדרה של מודל גרף חישובי הניתן להרחבה, כמו גם הגדרות של אופרטורים מובנים וסוגי נתונים סטנדרטיים. ONNX מאפשר למפתחים להעביר מודלים בין מסגרות ML שונות, מה שמאפשר אינטרופרביליות ומקל על יצירה ופריסה של יישומי AI.

Phi3 mini יכול לפעול עם ONNX Runtime על CPU ו-GPU במכשירים שונים, כולל פלטפורמות שרת, Windows, Linux, שולחנות עבודה Mac ו-CPUs ניידים.  
התצורות המותאמות שהוספנו הן

- מודלים ONNX ל-int4 DML: כימות ל-int4 באמצעות AWQ  
- מודל ONNX ל-fp16 CUDA  
- מודל ONNX ל-int4 CUDA: כימות ל-int4 באמצעות RTN  
- מודל ONNX ל-int4 CPU ונייד: כימות ל-int4 באמצעות RTN

## Llama.cpp

Llama.cpp היא ספריית תוכנה בקוד פתוח שנכתבה ב-C++. היא מבצעת אינפרנס על מודלים שפתיים גדולים (LLMs), כולל Llama. פותחה לצד ספריית ggml (ספריית טנסורים למטרות כלליות), ומטרתה לספק אינפרנס מהיר יותר ושימוש זיכרון נמוך יותר לעומת המימוש המקורי בפייתון. היא תומכת באופטימיזציה לחומרה, כימות, ומציעה API פשוט ודוגמאות. אם אתם מעוניינים באינפרנס יעיל של LLM, שווה לבדוק את llama.cpp כיוון ש-Phi3 יכול להריץ אותה.

## GGUF

GGUF (Generic Graph Update Format) הוא פורמט המשמש לייצוג ועדכון מודלים של למידת מכונה. הוא שימושי במיוחד למודלים שפתיים קטנים (SLMs) שיכולים לפעול ביעילות על CPUs עם כימות 4-8 ביט. GGUF מועיל לפרוטוטייפינג מהיר ולהרצת מודלים על מכשירי קצה או בעבודות אצווה כמו צינורות CI/CD.

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש להיות מודעים לכך שתרגומים אוטומטיים עלולים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית הוא המקור הסמכותי. למידע קריטי מומלץ להשתמש בתרגום מקצועי על ידי אדם. אנו לא אחראים על כל אי הבנה או פרשנות שגויה הנובעת משימוש בתרגום זה.