# הטכנולוגיות המרכזיות שהוזכרו כוללות

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - API ברמת נמוכה ללמידת מכונה מואצת חומרה, המבוסס על DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - פלטפורמת חישוב מקבילית ומודל API שפותח על ידי Nvidia, המאפשר עיבוד כללי על יחידות עיבוד גרפיות (GPUs).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - פורמט פתוח שנועד לייצג מודלים של למידת מכונה ומספק אינטרופרביליות בין מסגרות ML שונות.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - פורמט המשמש לייצוג ועדכון מודלים של למידת מכונה, במיוחד שימושי למודלים שפתיים קטנים שיכולים לפעול ביעילות על מעבדים עם קוונטיזציה של 4-8 ביט.

## DirectML

DirectML הוא API ברמת נמוכה שמאפשר למידת מכונה מואצת חומרה. הוא בנוי על DirectX 12 כדי לנצל את האצת ה-GPU והוא עצמאי מספק, כלומר אינו דורש שינויים בקוד כדי לפעול על GPUs של ספקים שונים. הוא משמש בעיקר לעומסי עבודה של אימון מודלים והסקת מסקנות על GPUs.

לגבי תמיכת החומרה, DirectML תוכנן לעבוד עם מגוון רחב של GPUs, כולל GPUs משולבים ודיסקרטיים של AMD, GPUs משולבים של Intel ו-GPUs דיסקרטיים של NVIDIA. הוא חלק מפלטפורמת Windows AI ומגובה ב-Windows 10 ו-11, ומאפשר אימון מודלים והסקת מסקנות על כל מכשיר Windows.

היו עדכונים והזדמנויות הקשורות ל-DirectML, כמו תמיכה עד ל-150 אופרטורים של ONNX ושימוש הן ב-ONNX runtime והן ב-WinML. הוא נתמך על ידי ספקי חומרה מרכזיים (IHVs), שכל אחד מהם מיישם פקודות מטא שונות.

## CUDA

CUDA, שמייצג Compute Unified Device Architecture, היא פלטפורמת חישוב מקבילית ומודל API שפותח על ידי Nvidia. היא מאפשרת למפתחי תוכנה להשתמש ב-GPU עם תמיכת CUDA לעיבוד כללי – גישה המכונה GPGPU (General-Purpose computing on Graphics Processing Units). CUDA היא מנוע מרכזי להאצת GPU של Nvidia ונמצאת בשימוש נרחב בתחומים שונים, כולל למידת מכונה, חישובים מדעיים ועיבוד וידאו.

תמיכת החומרה של CUDA מוגבלת ל-GPUs של Nvidia, שכן זו טכנולוגיה קניינית שפותחה על ידה. כל ארכיטקטורה תומכת בגרסאות ספציפיות של ערכת הכלים CUDA, המספקת את הספריות והכלים הדרושים למפתחים לבניית והרצת יישומי CUDA.

## ONNX

ONNX (Open Neural Network Exchange) הוא פורמט פתוח שנועד לייצג מודלים של למידת מכונה. הוא מספק הגדרה של מודל גרף חישובי הניתן להרחבה, וכן הגדרות של אופרטורים מובנים וסוגי נתונים סטנדרטיים. ONNX מאפשר למפתחים להעביר מודלים בין מסגרות ML שונות, מה שמאפשר אינטרופרביליות ומקל על יצירה ופריסה של יישומי AI.

Phi3 mini יכול לפעול עם ONNX Runtime על CPU ו-GPU במגוון מכשירים, כולל פלטפורמות שרת, שולחנות עבודה ב-Windows, Linux ו-Mac, ומעבדי מובייל.  
הקונפיגורציות המותאמות שהוספנו הן:

- מודלים ONNX ל-int4 DML: קוונטיזציה ל-int4 באמצעות AWQ  
- מודל ONNX ל-fp16 CUDA  
- מודל ONNX ל-int4 CUDA: קוונטיזציה ל-int4 באמצעות RTN  
- מודל ONNX ל-int4 CPU ומובייל: קוונטיזציה ל-int4 באמצעות RTN

## Llama.cpp

Llama.cpp היא ספריית תוכנה בקוד פתוח שנכתבה ב-C++. היא מבצעת הסקה על מודלים שפתיים גדולים (LLMs) שונים, כולל Llama. הספרייה פותחה במקביל לספריית ggml (ספריית טנסורים כללית), ומטרתה לספק הסקה מהירה יותר ושימוש נמוך יותר בזיכרון בהשוואה למימוש המקורי בפייתון. היא תומכת באופטימיזציה לחומרה, קוונטיזציה, ומציעה API פשוט ודוגמאות. אם אתם מעוניינים בהסקה יעילה של LLM, כדאי לבדוק את llama.cpp, שכן Phi3 יכול להריץ Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) הוא פורמט המשמש לייצוג ועדכון מודלים של למידת מכונה. הוא שימושי במיוחד למודלים שפתיים קטנים (SLMs) שיכולים לפעול ביעילות על מעבדים עם קוונטיזציה של 4-8 ביט. GGUF מועיל לפרוטוטייפינג מהיר ולהרצת מודלים על מכשירי קצה או בעבודות אצווה כמו צינורות CI/CD.

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון כי תרגומים אוטומטיים עלולים להכיל שגיאות או אי-דיוקים. המסמך המקורי בשפת המקור שלו נחשב למקור הסמכותי. למידע קריטי מומלץ להשתמש בתרגום מקצועי על ידי מתרגם אנושי. אנו לא נושאים באחריות לכל אי-הבנה או פרשנות שגויה הנובעת משימוש בתרגום זה.