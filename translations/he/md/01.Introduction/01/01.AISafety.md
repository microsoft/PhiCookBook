<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "839ccc4b3886ef10cfd4e64977f5792d",
  "translation_date": "2026-01-05T08:46:31+00:00",
  "source_file": "md/01.Introduction/01/01.AISafety.md",
  "language_code": "he"
}
-->
# בטיחות בינה מלאכותית לדגמי Phi
משפחת דגמי Phi פותחה בהתאם ל[תקן הבינה המלאכותית האחראית של מיקרוסופט](https://www.microsoft.com/ai/principles-and-approach#responsible-ai-standard), שהוא סט דרישות ברחבי החברה המבוסס על ששת העקרונות הבאים: אחריות, שקיפות, הוגנות, אמינות ובטיחות, פרטיות ואבטחה, וכלליות שמרכיבות את [עקרונות הבינה המלאכותית האחראית של מיקרוסופט](https://www.microsoft.com/ai/responsible-ai).

כמו בדגמי Phi הקודמים, אומצה גישה רב-פניונית להערכת בטיחות וגישה לאחר אימון בטיחות, עם צעדים נוספים שנקטו כדי להתחשב ביכולות הרב-לשוניות של הגרסה הזו. הגישה שלנו לאימון והערכות בטיחות כולל בדיקות בשפות מרובות וקטגוריות סיכון מתוארת ב[מסמך הבטיחות לאחר האימון של Phi](https://arxiv.org/abs/2407.13833). למרות שדגמי Phi נהנים מגישה זו, על מפתחים להחיל שיטות עבודה מומלצות לבינה מלאכותית אחראית, הכוללות מיפוי, מדידה, והפחתה של סיכונים הקשורים למקרה השימוש הספציפי שלהם ולהקשר התרבותי והלשוני.

## שיטות עבודה מומלצות

כמו דגמים אחרים, משפחת דגמי Phi עלולה להתנהג בצורה בלתי הוגנת, לא אמינה או פוגענית.

כמה מההתנהגויות המגבילות של SLM ו-LLM שכדאי להיות מודעים להן כוללות:

- **איכות השירות:** דגמי Phi מאומנים בעיקר על טקסט באנגלית. שפות אחרות מאנגלית יחוו ביצועים פחות טובים. שפות אנגלית עם ייצוג נמוך יותר בנתוני האימון עשויות לחוות ביצועים נמוכים יותר מאנגלית אמריקאית סטנדרטית.
- **ייצוג נזקים והמשכיות סטריאוטיפים:** דגמים אלה עלולים לייצג יתר או חוסר ייצוג של קבוצות אנשים, למחוק ייצוג של קבוצות מסוימות, או לחזק סטריאוטיפים משפילים או שליליים. למרות האימון לאחר הבטיחות, מגבלות אלה עדיין יכולות להיות נוכחות עקב רמות ייצוג שונות של קבוצות שונות או שכיחות דוגמאות לסטריאוטיפים שליליים בנתוני האימון שמשקפים דפוסים חברתיים והטיות מהמציאות.
- **תוכן בלתי הולם או פוגעני:** דגמים אלה עלולים לייצר סוגים אחרים של תוכן בלתי הולם או פוגעני, מה שעשוי להפוך את השימוש בהם ללא מתאים בהקשרים רגישים ללא אמצעי הפחתה נוספים הספציפיים למקרה השימוש.
- **אמינות מידע:** דגמי שפה יכולים לייצר תוכן לא הגיוני או להטעות בתוכן שנשמע סביר אך אינו מדויק או מיושן.
- **תחום מוגבל לקוד:** רוב נתוני האימון של Phi-3 מבוססים על Python ומשתמשים בחבילות נפוצות כגון "typing, math, random, collections, datetime, itertools". אם הדגם מייצר סקריפטים בפייתון שמשתמשים בחבילות אחרות או סקריפטים בשפות אחרות, אנו ממליצים בחום למשתמשים לאמת באופן ידני את כל השימושים ב-API.

מפתחים צריכים להחיל שיטות עבודה מומלצות לבינה מלאכותית אחראית ולקחת אחריות על עמידת מקרה שימוש ספציפי בחוקים ותקנות רלוונטיים (כגון פרטיות, סחר וכו').

## שיקולים בבינה מלאכותית אחראית

כמו דגמי שפה אחרים, דגמי סדרת Phi עלולים להתנהג בדרכים בלתי הוגנות, לא אמינות או פוגעניות. כמה מההתנהגויות המגבילות שכדאי להיות מודעים להן כוללות:

**איכות השירות:** דגמי Phi מאומנים בעיקר על טקסט באנגלית. שפות אחרות מאנגלית יחוו ביצועים פחות טובים. שפות אנגלית עם ייצוג נמוך יותר בנתוני האימון עשויות לחוות ביצועים נמוכים יותר מאנגלית אמריקאית סטנדרטית.

**ייצוג נזקים והמשכיות סטריאוטיפים:** דגמים אלה עלולים לייצג יתר או חוסר ייצוג של קבוצות אנשים, למחוק ייצוג של קבוצות מסוימות, או לחזק סטריאוטיפים משפילים או שליליים. למרות האימון לאחר הבטיחות, מגבלות אלה עדיין יכולות להיות נוכחות עקב רמות ייצוג שונות של קבוצות שונות או שכיחות דוגמאות לסטריאוטיפים שליליים בנתוני האימון שמשקפים דפוסים חברתיים והטיות מהמציאות.

**תוכן בלתי הולם או פוגעני:** דגמים אלה עלולים לייצר סוגים אחרים של תוכן בלתי הולם או פוגעני, מה שעשוי להפוך את השימוש בהם ללא מתאים בהקשרים רגישים ללא אמצעי הפחתה נוספים הספציפיים למקרה השימוש.
אמינות מידע: דגמי שפה יכולים לייצר תוכן לא הגיוני או להטעות בתוכן שנשמע סביר אך אינו מדויק או מיושן.

**תחום מוגבל לקוד:** רוב נתוני האימון של Phi-3 מבוססים על Python ומשתמשים בחבילות נפוצות כגון "typing, math, random, collections, datetime, itertools". אם הדגם מייצר סקריפטים בפייתון שמשתמשים בחבילות אחרות או סקריפטים בשפות אחרות, אנו ממליצים בחום למשתמשים לאמת באופן ידני את כל השימושים ב-API.

מפתחים צריכים להחיל שיטות עבודה מומלצות לבינה מלאכותית אחראית ולוודא שמקרה שימוש ספציפי עומד בחוקים ובתקנות הרלוונטיים (כגון פרטיות, סחר וכו'). אזורים חשובים למחשבה כוללים:

**הקצאה:** דגמים עשויים שלא להתאים לתרחישים שעלולים להשפיע באופן משמעותי על הסטטוס החוקי או על הקצאת משאבים או הזדמנויות חיים (כגון: דיור, תעסוקה, אשראי וכו') ללא הערכות נוספות וטכניקות הפחתת הטיות נוספות.

**תרחישים בעלי סיכון גבוה:** על מפתחים להעריך את ההתאמה של שימוש בדגמים בתרחישים בעלי סיכון גבוה שבהם פלט בלתי הוגן, לא אמין או פוגעני עלול להיות יקר במיוחד או לגרום לנזק. זה כולל מתן ייעוץ בתחומים רגישים או מקצועיים שבהם דיוק ואמינות קריטיים (כגון: ייעוץ משפטי או רפואי). יש ליישם אמצעי זהירות נוספים ברמת היישום בהתאם להקשר הפריסה.

**מידע שגוי:** דגמים עלולים לייצר מידע לא מדויק. מפתחים צריכים לעקוב אחר שיטות עבודה מומלצות לשקיפות ולהודיע למשתמשי הקצה שהם מתקשרים עם מערכת בינה מלאכותית. ברמת היישום, מפתחים יכולים לבנות מנגנוני משוב וצינורות להטמעת תשובות במידע הקשרי ומותאם למקרה השימוש, טכניקה הידועה כ-Retrieval Augmented Generation (RAG).

**ייצור תוכן מזיק:** על מפתחים להעריך פלטים בהתאם להקשרם ולהשתמש בסיווגי בטיחות זמינים או בפתרונות מותאמים למקרה השימוש שלהם.

**שימוש לרעה:** צורות נוספות של שימוש לרעה כגון הונאה, ספאם או הפצת תוכנות זדוניות עשויים להיות אפשריים, ומפתחים צריכים להבטיח שהיישומים שלהם אינם מפרים חוקים ותקנות רלוונטיים.

### כיוונון עדין ובטיחות תוכן בינה מלאכותית

לאחר כיוונון עדין של דגם, אנו ממליצים בחום להשתמש בצעדים של [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) כדי לפקח על התוכן המיוצר על ידי הדגמים, לזהות ולחסום סיכונים, איומים ובעיות איכות פוטנציאליים.

![Phi3AISafety](../../../../../translated_images/he/01.phi3aisafety.c0d7fc42f5a5c405.png)

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) תומך בתוכן טקסטואלי ותמונות. ניתן לפרוס אותו בענן, במכלים מנותקים, ובמכשירים בקצה / משובצים.

## סיכום על Azure AI Content Safety

Azure AI Content Safety אינה פתרון אחיד לכולם; ניתן להתאימה כך שתתאים למדיניות הספציפית של עסקים. בנוסף, דגמיה הרב-לשוניים מאפשרים לה להבין מספר שפות במקביל.

![AIContentSafety](../../../../../translated_images/he/01.AIcontentsafety.a288819b8ce8da1a.png)

- **Azure AI Content Safety**
- **מפתחי מיקרוסופט**
- **5 סרטונים**

שירות Azure AI Content Safety מזהה תוכן מזיק שיצרו משתמשים ותוכן שנוצר על ידי בינה מלאכותית באפליקציות ובשירותים. הוא כולל APIs לטקסט ולתמונות המאפשרים לזהות חומר מזיק או לא הולם.

[AI Content Safety Playlist](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**הצהרת אחריות**:  
מסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עלולים להכיל שגיאות או אי-דיוקים. המסמך המקורי בשפה המקורית שלו נחשב למקור הסמכותי. לקבלת מידע קריטי, מומלץ להיעזר בתרגום מקצועי של אדם. אנו אינם אחראים לכל אי-הבנה או פרשנות שגויה הנובעים משימוש בתרגום זה.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->