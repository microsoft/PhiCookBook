{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## צ'אטבוט אינטראקטיבי Phi 3 Mini 4K עם Whisper\n",
    "\n",
    "### הקדמה:\n",
    "צ'אטבוט אינטראקטיבי Phi 3 Mini 4K הוא כלי שמאפשר למשתמשים לתקשר עם הדמו של Microsoft Phi 3 Mini 4K באמצעות קלט טקסט או אודיו. ניתן להשתמש בצ'אטבוט למגוון משימות, כמו תרגום, עדכוני מזג אוויר ואיסוף מידע כללי.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "צור אסימון גישה חדש  \n",
    "ספק שם חדש  \n",
    "בחר הרשאות כתיבה  \n",
    "העתק את האסימון ושמור אותו במקום בטוח\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "הקוד הבא כתוב בפייתון ומבצע שתי משימות עיקריות: ייבוא המודול `os` והגדרת משתנה סביבה.\n",
    "\n",
    "1. ייבוא המודול `os`:\n",
    "   - המודול `os` בפייתון מספק דרך לתקשר עם מערכת ההפעלה. הוא מאפשר לבצע משימות שונות הקשורות למערכת ההפעלה, כמו גישה למשתני סביבה, עבודה עם קבצים ותיקיות, ועוד.\n",
    "   - בקוד הזה, המודול `os` מיובא באמצעות הפקודה `import`. פקודה זו מאפשרת להשתמש בפונקציונליות של המודול `os` בתוך הסקריפט הנוכחי בפייתון.\n",
    "\n",
    "2. הגדרת משתנה סביבה:\n",
    "   - משתנה סביבה הוא ערך שניתן לגשת אליו על ידי תוכניות שרצות על מערכת ההפעלה. זהו אמצעי לאחסון הגדרות תצורה או מידע אחר שניתן להשתמש בו על ידי תוכניות שונות.\n",
    "   - בקוד הזה, משתנה סביבה חדש מוגדר באמצעות המילון `os.environ`. המפתח של המילון הוא `'HF_TOKEN'`, והערך מוקצה מתוך המשתנה `HUGGINGFACE_TOKEN`.\n",
    "   - המשתנה `HUGGINGFACE_TOKEN` מוגדר ממש מעל קטע הקוד הזה, והוא מקבל ערך מחרוזת `\"hf_**************\"` באמצעות התחביר `#@param`. תחביר זה משמש לעיתים קרובות במחברות Jupyter כדי לאפשר קלט משתמש והגדרת פרמטרים ישירות בממשק המחברת.\n",
    "   - על ידי הגדרת משתנה הסביבה `'HF_TOKEN'`, ניתן לגשת אליו על ידי חלקים אחרים בתוכנית או על ידי תוכניות אחרות שרצות על אותה מערכת הפעלה.\n",
    "\n",
    "בסך הכל, הקוד הזה מייבא את המודול `os` ומגדיר משתנה סביבה בשם `'HF_TOKEN'` עם הערך שסופק במשתנה `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "קטע הקוד הזה מגדיר פונקציה בשם clear_output שמשמשת לניקוי הפלט של התא הנוכחי ב-Jupyter Notebook או IPython. בואו נפרק את הקוד ונבין את הפונקציונליות שלו:\n",
    "\n",
    "הפונקציה clear_output מקבלת פרמטר אחד בשם wait, שהוא ערך בוליאני. כברירת מחדל, wait מוגדר כ-False. פרמטר זה קובע האם הפונקציה צריכה להמתין עד שיהיה פלט חדש להחליף את הפלט הקיים לפני שהיא מנקה אותו.\n",
    "\n",
    "הפונקציה עצמה משמשת לניקוי הפלט של התא הנוכחי. ב-Jupyter Notebook או IPython, כאשר תא מייצר פלט, כמו טקסט מודפס או גרפים, הפלט הזה מוצג מתחת לתא. הפונקציה clear_output מאפשרת לנקות את הפלט הזה.\n",
    "\n",
    "מימוש הפונקציה לא מסופק בקטע הקוד, כפי שמצוין על ידי שלוש הנקודות (...). שלוש הנקודות מייצגות מציין מקום לקוד האמיתי שמבצע את פעולת הניקוי של הפלט. ייתכן שמימוש הפונקציה כולל אינטראקציה עם ה-API של Jupyter Notebook או IPython כדי להסיר את הפלט הקיים מהתא.\n",
    "\n",
    "בסך הכל, הפונקציה הזו מספקת דרך נוחה לנקות את הפלט של התא הנוכחי ב-Jupyter Notebook או IPython, מה שמקל על ניהול ועדכון הפלט המוצג במהלך סשנים אינטראקטיביים של קידוד.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "בצע המרת טקסט לדיבור (TTS) באמצעות שירות Edge TTS. בואו נעבור על יישומי הפונקציות הרלוונטיות אחת אחת:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: פונקציה זו מקבלת ערך קלט ומחשבת את מחרוזת הקצב עבור הקול של TTS. ערך הקלט מייצג את מהירות הדיבור הרצויה, כאשר ערך של 1 מייצג מהירות רגילה. הפונקציה מחשבת את מחרוזת הקצב על ידי חיסור 1 מערך הקלט, הכפלתו ב-100, ולאחר מכן קובעת את הסימן בהתאם לשאלה האם ערך הקלט גדול או שווה ל-1. הפונקציה מחזירה את מחרוזת הקצב בפורמט \"{sign}{rate}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: פונקציה זו מקבלת טקסט קלט ושפה כפרמטרים. היא מחלקת את טקסט הקלט לקטעים על פי כללים ספציפיים לשפה. ביישום זה, אם השפה היא \"אנגלית\", הפונקציה מחלקת את הטקסט בכל נקודה (\".\") ומסירה רווחים מובילים או נגררים. לאחר מכן היא מוסיפה נקודה לכל קטע ומחזירה את רשימת הקטעים המסוננים.\n",
    "\n",
    "3. `tts_file_name(text)`: פונקציה זו יוצרת שם קובץ עבור קובץ האודיו של TTS בהתבסס על טקסט הקלט. היא מבצעת מספר טרנספורמציות על הטקסט: הסרת נקודה נגררת (אם קיימת), המרת הטקסט לאותיות קטנות, הסרת רווחים מובילים ונגררים, והחלפת רווחים בקו תחתון. לאחר מכן היא מקצרת את הטקסט לאורך מקסימלי של 25 תווים (אם ארוך יותר) או משתמשת בטקסט המלא אם הוא ריק. לבסוף, היא יוצרת מחרוזת אקראית באמצעות מודול [`uuid`] ומשלבת אותה עם הטקסט המקוצר כדי ליצור את שם הקובץ בפורמט \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: פונקציה זו ממזגת מספר קבצי אודיו לקובץ אודיו אחד. היא מקבלת רשימה של נתיבי קבצי אודיו ונתיב פלט כפרמטרים. הפונקציה מאתחלת אובייקט ריק מסוג `AudioSegment` בשם [`merged_audio`]. לאחר מכן היא עוברת על כל נתיב קובץ אודיו, טוענת את קובץ האודיו באמצעות השיטה `AudioSegment.from_file()` מספריית `pydub`, ומוסיפה את קובץ האודיו הנוכחי לאובייקט [`merged_audio`]. לבסוף, היא מייצאת את האודיו הממוזג לנתיב הפלט שצוין בפורמט MP3.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: פונקציה זו מבצעת את פעולת TTS באמצעות שירות Edge TTS. היא מקבלת רשימה של קטעי טקסט, מהירות הדיבור, שם הקול ונתיב שמירה כפרמטרים. אם מספר הקטעים גדול מ-1, הפונקציה יוצרת תיקייה לאחסון קבצי האודיו של הקטעים הבודדים. לאחר מכן היא עוברת על כל קטע, בונה פקודת Edge TTS באמצעות הפונקציה `calculate_rate_string()`, שם הקול וטקסט הקטע, ומבצעת את הפקודה באמצעות הפונקציה `os.system()`. אם ביצוע הפקודה מצליח, היא מוסיפה את נתיב קובץ האודיו שנוצר לרשימה. לאחר עיבוד כל הקטעים, היא ממזגת את קבצי האודיו הבודדים באמצעות הפונקציה `merge_audio_files()` ושומרת את האודיו הממוזג לנתיב השמירה שצוין. אם יש רק קטע אחד, היא יוצרת ישירות את פקודת Edge TTS ושומרת את האודיו לנתיב השמירה. לבסוף, היא מחזירה את נתיב השמירה של קובץ האודיו שנוצר.\n",
    "\n",
    "6. `random_audio_name_generate()`: פונקציה זו יוצרת שם קובץ אודיו אקראי באמצעות מודול [`uuid`]. היא יוצרת UUID אקראי, ממירה אותו למחרוזת, לוקחת את שמונה התווים הראשונים, מוסיפה את הסיומת \".mp3\", ומחזירה את שם קובץ האודיו האקראי.\n",
    "\n",
    "7. `talk(input_text)`: פונקציה זו היא נקודת הכניסה הראשית לביצוע פעולת TTS. היא מקבלת טקסט קלט כפרמטר. היא קודם כל בודקת את אורך טקסט הקלט כדי לקבוע אם מדובר במשפט ארוך (600 תווים או יותר). בהתבסס על האורך ועל הערך של המשתנה `translate_text_flag`, היא קובעת את השפה ויוצרת את רשימת קטעי הטקסט באמצעות הפונקציה `make_chunks()`. לאחר מכן היא יוצרת נתיב שמירה עבור קובץ האודיו באמצעות הפונקציה `random_audio_name_generate()`. לבסוף, היא קוראת לפונקציה `edge_free_tts()` כדי לבצע את פעולת TTS ומחזירה את נתיב השמירה של קובץ האודיו שנוצר.\n",
    "\n",
    "בסך הכל, פונקציות אלו פועלות יחד כדי לחלק את טקסט הקלט לקטעים, ליצור שם קובץ עבור קובץ האודיו, לבצע את פעולת TTS באמצעות שירות Edge TTS, ולמזג את קבצי האודיו הבודדים לקובץ אודיו אחד.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "היישום של שתי פונקציות: convert_to_text ו-run_text_prompt, וכן ההכרזה על שתי מחלקות: str ו-Audio.\n",
    "\n",
    "הפונקציה convert_to_text מקבלת כקלט את audio_path ומבצעת תמלול של האודיו לטקסט באמצעות מודל בשם whisper_model. הפונקציה קודם כל בודקת אם הדגל gpu מוגדר כ-True. אם כן, נעשה שימוש ב-whisper_model עם פרמטרים מסוימים כמו word_timestamps=True, fp16=True, language='English', ו-task='translate'. אם הדגל gpu מוגדר כ-False, נעשה שימוש ב-whisper_model עם fp16=False. התמלול המתקבל נשמר בקובץ בשם 'scan.txt' ומוחזר כטקסט.\n",
    "\n",
    "הפונקציה run_text_prompt מקבלת הודעה ו-chat_history כקלט. היא משתמשת בפונקציה phi_demo כדי ליצור תגובה מצ'אטבוט בהתבסס על ההודעה שהתקבלה. התגובה שנוצרה מועברת לפונקציה talk, שממירה את התגובה לקובץ אודיו ומחזירה את נתיב הקובץ. מחלקת Audio משמשת להצגת קובץ האודיו ולהשמעתו. האודיו מוצג באמצעות הפונקציה display מהמודול IPython.display, ואובייקט Audio נוצר עם הפרמטר autoplay=True, כך שהאודיו מתחיל להתנגן באופן אוטומטי. ה-chat_history מתעדכן עם ההודעה שהתקבלה והתגובה שנוצרה, ומוחזרים מחרוזת ריקה ו-chat_history המעודכן.\n",
    "\n",
    "מחלקת str היא מחלקה מובנית בפייתון שמייצגת רצף של תווים. היא מספקת מגוון שיטות לעבודה עם מחרוזות, כמו capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill, ועוד. שיטות אלו מאפשרות לבצע פעולות כמו חיפוש, החלפה, עיצוב ועיבוד של מחרוזות.\n",
    "\n",
    "מחלקת Audio היא מחלקה מותאמת אישית שמייצגת אובייקט אודיו. היא משמשת ליצירת נגן אודיו בסביבת Jupyter Notebook. המחלקה מקבלת מגוון פרמטרים כמו data, filename, url, embed, rate, autoplay, ו-normalize. הפרמטר data יכול להיות מערך numpy, רשימת דגימות, מחרוזת שמייצגת שם קובץ או URL, או נתוני PCM גולמיים. הפרמטר filename משמש לציון קובץ מקומי לטעינת נתוני האודיו, והפרמטר url משמש לציון URL להורדת נתוני האודיו. הפרמטר embed קובע האם נתוני האודיו צריכים להיות מוטמעים באמצעות URI של נתונים או להיות מופנים למקור המקורי. הפרמטר rate מציין את קצב הדגימה של נתוני האודיו. הפרמטר autoplay קובע האם האודיו צריך להתחיל להתנגן באופן אוטומטי. הפרמטר normalize מציין האם נתוני האודיו צריכים להיות מנורמלים (מכוילים) לטווח האפשרי המרבי. מחלקת Audio מספקת גם שיטות כמו reload לטעינת נתוני האודיו מחדש מקובץ או URL, ותכונות כמו src_attr, autoplay_attr, ו-element_id_attr כדי לשלוף את התכונות המתאימות לאלמנט האודיו ב-HTML.\n",
    "\n",
    "בסך הכל, הפונקציות והמחלקות הללו משמשות לתמלול אודיו לטקסט, יצירת תגובות אודיו מצ'אטבוט, והצגה והשמעה של אודיו בסביבת Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**כתב ויתור**:  \nמסמך זה תורגם באמצעות שירות תרגום מבוסס בינה מלאכותית [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי על ידי אדם. איננו נושאים באחריות לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T23:17:53+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "he"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}