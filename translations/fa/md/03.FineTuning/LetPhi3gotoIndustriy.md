<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "743d7e9cb9c4e8ea642d77bee657a7fa",
  "translation_date": "2025-07-17T09:51:58+00:00",
  "source_file": "md/03.FineTuning/LetPhi3gotoIndustriy.md",
  "language_code": "fa"
}
-->
# **اجازه دهید Phi-3 به یک متخصص صنعت تبدیل شود**

برای وارد کردن مدل Phi-3 به یک صنعت، باید داده‌های کسب‌وکار صنعتی را به مدل Phi-3 اضافه کنید. ما دو گزینه مختلف داریم، اولی RAG (تولید تقویت‌شده با بازیابی) و دومی Fine Tuning (تنظیم دقیق).

## **RAG در مقابل Fine-Tuning**

### **تولید تقویت‌شده با بازیابی**

RAG ترکیبی از بازیابی داده و تولید متن است. داده‌های ساختاریافته و غیرساختاریافته شرکت در پایگاه داده برداری ذخیره می‌شوند. هنگام جستجوی محتوای مرتبط، خلاصه و محتوای مرتبط پیدا می‌شود تا یک زمینه شکل بگیرد و قابلیت تکمیل متن مدل‌های LLM/SLM برای تولید محتوا ترکیب می‌شود.

### **تنظیم دقیق**

تنظیم دقیق بر بهبود یک مدل خاص تمرکز دارد. نیازی به شروع از الگوریتم مدل نیست، اما داده‌ها باید به طور مداوم جمع‌آوری شوند. اگر به دنبال اصطلاحات دقیق‌تر و بیان زبانی بهتر در کاربردهای صنعتی هستید، تنظیم دقیق گزینه بهتری است. اما اگر داده‌های شما به طور مکرر تغییر می‌کنند، تنظیم دقیق می‌تواند پیچیده شود.

### **چگونه انتخاب کنیم**

1. اگر پاسخ ما نیاز به معرفی داده‌های خارجی دارد، RAG بهترین انتخاب است

2. اگر نیاز به خروجی دانش صنعتی پایدار و دقیق دارید، تنظیم دقیق انتخاب خوبی است. RAG اولویت را به کشیدن محتوای مرتبط می‌دهد اما ممکن است همیشه نکات تخصصی را به درستی درک نکند.

3. تنظیم دقیق به مجموعه داده با کیفیت بالا نیاز دارد و اگر داده‌ها محدود باشند، تفاوت زیادی ایجاد نمی‌کند. RAG انعطاف‌پذیرتر است.

4. تنظیم دقیق یک جعبه سیاه است، یک متافیزیک، و درک مکانیزم داخلی آن دشوار است. اما RAG می‌تواند منبع داده را آسان‌تر پیدا کند و به این ترتیب توهمات یا خطاهای محتوا را به طور مؤثر تنظیم کرده و شفافیت بهتری ارائه دهد.

### **سناریوها**

1. صنایع عمودی که نیاز به واژگان و اصطلاحات تخصصی دارند، ***تنظیم دقیق*** بهترین انتخاب است

2. سیستم‌های پرسش و پاسخ که ترکیب نقاط دانش مختلف را شامل می‌شوند، ***RAG*** بهترین انتخاب است

3. ترکیب جریان خودکار کسب‌وکار، ***RAG + تنظیم دقیق*** بهترین انتخاب است

## **چگونه از RAG استفاده کنیم**

![rag](../../../../translated_images/rag.2014adc59e6f6007bafac13e800a6cbc3e297fbb9903efe20a93129bd13987e9.fa.png)

پایگاه داده برداری مجموعه‌ای از داده‌ها است که به صورت ریاضی ذخیره شده‌اند. پایگاه‌های داده برداری به مدل‌های یادگیری ماشین کمک می‌کنند تا ورودی‌های قبلی را بهتر به خاطر بسپارند و امکان استفاده از یادگیری ماشین را برای مواردی مانند جستجو، پیشنهادات و تولید متن فراهم می‌کنند. داده‌ها بر اساس معیارهای شباهت شناسایی می‌شوند نه تطابق دقیق، که به مدل‌های کامپیوتری اجازه می‌دهد زمینه داده‌ها را درک کنند.

پایگاه داده برداری کلید تحقق RAG است. ما می‌توانیم داده‌ها را از طریق مدل‌های برداری مانند text-embedding-3، jina-ai-embedding و غیره به ذخیره‌سازی برداری تبدیل کنیم.

برای یادگیری بیشتر درباره ایجاد برنامه RAG به [https://github.com/microsoft/Phi-3CookBook](https://github.com/microsoft/Phi-3CookBook?WT.mc_id=aiml-138114-kinfeylo) مراجعه کنید.

## **چگونه از Fine-tuning استفاده کنیم**

الگوریتم‌های رایج در تنظیم دقیق، Lora و QLora هستند. چگونه انتخاب کنیم؟
- [برای یادگیری بیشتر با این دفترچه نمونه](../../../../code/04.Finetuning/Phi_3_Inference_Finetuning.ipynb)
- [مثال نمونه تنظیم دقیق پایتون](../../../../code/04.Finetuning/FineTrainingScript.py)

### **Lora و QLora**

![lora](../../../../translated_images/qlora.e6446c988ee04ca08807488bb7d9e2c0ea7ef4af9d000fc6d13032b4ac2de18d.fa.png)

LoRA (تطبیق کم‌رتبه) و QLoRA (تطبیق کم‌رتبه کوانتیزه شده) هر دو تکنیک‌هایی برای تنظیم دقیق مدل‌های زبان بزرگ (LLM) با استفاده از تنظیم دقیق پارامتر بهینه (PEFT) هستند. تکنیک‌های PEFT برای آموزش مدل‌ها به صورت بهینه‌تر نسبت به روش‌های سنتی طراحی شده‌اند.  
LoRA یک تکنیک تنظیم دقیق مستقل است که با اعمال تقریب کم‌رتبه به ماتریس به‌روزرسانی وزن، مصرف حافظه را کاهش می‌دهد. این روش زمان آموزش سریع و عملکردی نزدیک به روش‌های تنظیم دقیق سنتی ارائه می‌دهد.

QLoRA نسخه توسعه یافته LoRA است که تکنیک‌های کوانتیزه‌سازی را برای کاهش بیشتر مصرف حافظه به کار می‌برد. QLoRA دقت پارامترهای وزن در LLM پیش‌آموزش دیده را به دقت ۴ بیتی کوانتیزه می‌کند که نسبت به LoRA حافظه کمتری مصرف می‌کند. با این حال، آموزش QLoRA حدود ۳۰٪ کندتر از LoRA است به دلیل مراحل اضافی کوانتیزه‌سازی و دکوآنتیزه‌سازی.

QLoRA از LoRA به عنوان مکملی برای اصلاح خطاهای ناشی از کوانتیزه‌سازی استفاده می‌کند. QLoRA امکان تنظیم دقیق مدل‌های عظیم با میلیاردها پارامتر را روی GPUهای نسبتاً کوچک و در دسترس فراهم می‌کند. برای مثال، QLoRA می‌تواند مدل ۷۰ میلیارد پارامتری که به ۳۶ GPU نیاز دارد را تنها با ۲ GPU تنظیم دقیق کند.

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما در تلاش برای دقت هستیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نواقصی باشند. سند اصلی به زبان بومی خود باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئول هیچ گونه سوءتفاهم یا تفسیر نادرستی که از استفاده این ترجمه ناشی شود، نیستیم.