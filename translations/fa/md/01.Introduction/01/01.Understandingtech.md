<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-03-27T06:07:47+00:00",
  "source_file": "md\\01.Introduction\\01\\01.Understandingtech.md",
  "language_code": "fa"
}
-->
# فناوری‌های کلیدی ذکر شده عبارتند از

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - یک API سطح پایین برای یادگیری ماشین شتاب‌یافته سخت‌افزاری که بر پایه DirectX 12 ساخته شده است.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - یک پلتفرم محاسبات موازی و مدل API که توسط Nvidia توسعه یافته و امکان پردازش عمومی روی واحدهای پردازش گرافیکی (GPU) را فراهم می‌کند.
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - یک فرمت باز که برای نمایش مدل‌های یادگیری ماشین طراحی شده و امکان همکاری بین فریم‌ورک‌های مختلف ML را فراهم می‌کند.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - فرمتی برای نمایش و به‌روزرسانی مدل‌های یادگیری ماشین، به‌ویژه مفید برای مدل‌های زبانی کوچکی که می‌توانند با کوانتایز 4-8 بیتی به‌صورت مؤثر روی CPU اجرا شوند.

## DirectML

DirectML یک API سطح پایین است که یادگیری ماشین شتاب‌یافته سخت‌افزاری را ممکن می‌سازد. این API بر پایه DirectX 12 ساخته شده تا از شتاب‌دهی GPU استفاده کند و مستقل از فروشنده است، به این معنا که برای کار با GPUهای مختلف نیازی به تغییر کد ندارد. این API عمدتاً برای بارهای کاری آموزش مدل و استنتاج روی GPUها استفاده می‌شود.

از نظر پشتیبانی سخت‌افزاری، DirectML برای کار با طیف گسترده‌ای از GPUها طراحی شده است، از جمله GPUهای یکپارچه و مجزای AMD، GPUهای یکپارچه Intel، و GPUهای مجزای NVIDIA. این فناوری بخشی از پلتفرم Windows AI است و در ویندوز 10 و 11 پشتیبانی می‌شود، که امکان آموزش مدل و استنتاج را روی هر دستگاه ویندوزی فراهم می‌کند.

به‌روزرسانی‌ها و فرصت‌هایی در رابطه با DirectML وجود داشته است، از جمله پشتیبانی از حداکثر 150 عملگر ONNX و استفاده توسط ONNX runtime و WinML. این فناوری توسط فروشندگان بزرگ سخت‌افزار یکپارچه (IHVها) پشتیبانی می‌شود که هر یک دستورات متا مختلفی را پیاده‌سازی کرده‌اند.

## CUDA

CUDA که مخفف Compute Unified Device Architecture است، یک پلتفرم محاسبات موازی و مدل API است که توسط Nvidia ایجاد شده است. این فناوری به توسعه‌دهندگان نرم‌افزار امکان می‌دهد از GPUهای پشتیبانی‌کننده CUDA برای پردازش عمومی استفاده کنند – رویکردی که به آن GPGPU (محاسبات عمومی روی واحدهای پردازش گرافیکی) گفته می‌شود. CUDA یک عامل کلیدی در شتاب‌دهی GPU توسط Nvidia است و به طور گسترده در زمینه‌های مختلفی از جمله یادگیری ماشین، محاسبات علمی و پردازش ویدئو استفاده می‌شود.

پشتیبانی سخت‌افزاری CUDA به GPUهای Nvidia محدود است، زیرا این فناوری توسط Nvidia توسعه داده شده و اختصاصی است. هر معماری نسخه‌های خاصی از جعبه‌ابزار CUDA را پشتیبانی می‌کند که کتابخانه‌ها و ابزارهای لازم برای توسعه‌دهندگان جهت ساخت و اجرای برنامه‌های CUDA را فراهم می‌کند.

## ONNX

ONNX (Open Neural Network Exchange) یک فرمت باز است که برای نمایش مدل‌های یادگیری ماشین طراحی شده است. این فرمت تعریفی از یک مدل گراف محاسباتی قابل توسعه، به همراه تعریف عملگرهای داخلی و انواع داده استاندارد ارائه می‌دهد. ONNX به توسعه‌دهندگان امکان می‌دهد مدل‌ها را بین فریم‌ورک‌های مختلف ML جابه‌جا کنند، که این قابلیت همکاری را ممکن کرده و ایجاد و استقرار برنامه‌های هوش مصنوعی را آسان‌تر می‌کند.

Phi3 mini می‌تواند با ONNX Runtime روی CPU و GPU در دستگاه‌های مختلف از جمله پلتفرم‌های سرور، دسکتاپ‌های ویندوز، لینوکس و مک، و CPUهای موبایل اجرا شود. پیکربندی‌های بهینه‌ای که اضافه کرده‌ایم عبارتند از:

- مدل‌های ONNX برای int4 DML: کوانتایز شده به int4 از طریق AWQ
- مدل ONNX برای fp16 CUDA
- مدل ONNX برای int4 CUDA: کوانتایز شده به int4 از طریق RTN
- مدل ONNX برای int4 CPU و موبایل: کوانتایز شده به int4 از طریق RTN

## Llama.cpp

Llama.cpp یک کتابخانه نرم‌افزاری متن‌باز است که به زبان C++ نوشته شده است. این کتابخانه استنتاج را روی مدل‌های زبانی بزرگ (LLMها) مختلف از جمله Llama انجام می‌دهد. این کتابخانه که همراه با کتابخانه ggml (یک کتابخانه تنسور عمومی) توسعه یافته، هدفش ارائه استنتاج سریع‌تر و استفاده کمتر از حافظه نسبت به پیاده‌سازی اصلی پایتون است. این کتابخانه از بهینه‌سازی سخت‌افزاری، کوانتایز کردن، و یک API ساده به همراه مثال‌ها پشتیبانی می‌کند. اگر به استنتاج مؤثر LLM علاقه‌مندید، Llama.cpp ارزش بررسی دارد، چرا که Phi3 می‌تواند Llama.cpp را اجرا کند.

## GGUF

GGUF (Generic Graph Update Format) یک فرمت است که برای نمایش و به‌روزرسانی مدل‌های یادگیری ماشین استفاده می‌شود. این فرمت به‌ویژه برای مدل‌های زبانی کوچک (SLMها) مفید است که می‌توانند با کوانتایز 4-8 بیتی به‌صورت مؤثر روی CPUها اجرا شوند. GGUF برای نمونه‌سازی سریع و اجرای مدل‌ها روی دستگاه‌های لبه یا در وظایف دسته‌ای مانند خطوط CI/CD مفید است.

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما برای دقت تلاش می‌کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل اشتباهات یا نادقتی‌ها باشند. سند اصلی به زبان مادری آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما هیچ مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.