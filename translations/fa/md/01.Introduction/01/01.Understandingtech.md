<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:40:48+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "fa"
}
-->
# فناوری‌های کلیدی ذکر شده شامل

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - یک API سطح پایین برای یادگیری ماشین با شتاب سخت‌افزاری که بر پایه DirectX 12 ساخته شده است.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - یک پلتفرم محاسبات موازی و مدل رابط برنامه‌نویسی کاربردی (API) توسعه یافته توسط Nvidia که امکان پردازش عمومی روی واحدهای پردازش گرافیکی (GPU) را فراهم می‌کند.
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - یک فرمت باز طراحی شده برای نمایش مدل‌های یادگیری ماشین که قابلیت تعامل بین فریم‌ورک‌های مختلف ML را فراهم می‌کند.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - فرمت مورد استفاده برای نمایش و به‌روزرسانی مدل‌های یادگیری ماشین، به‌ویژه برای مدل‌های زبانی کوچک‌تر که می‌توانند با کوانتیزاسیون ۴-۸ بیت به‌خوبی روی CPU اجرا شوند.

## DirectML

DirectML یک API سطح پایین است که یادگیری ماشین با شتاب سخت‌افزاری را ممکن می‌سازد. این API بر پایه DirectX 12 ساخته شده تا از شتاب GPU بهره ببرد و مستقل از سازنده است، یعنی برای کار روی GPUهای مختلف نیازی به تغییر کد ندارد. عمدتاً برای آموزش مدل‌ها و انجام استنتاج روی GPUها استفاده می‌شود.

در مورد پشتیبانی سخت‌افزاری، DirectML برای کار با طیف گسترده‌ای از GPUها طراحی شده است، از جمله GPUهای مجتمع و مجزا AMD، GPUهای مجتمع اینتل و GPUهای مجزای NVIDIA. این API بخشی از پلتفرم هوش مصنوعی ویندوز است و روی ویندوز ۱۰ و ۱۱ پشتیبانی می‌شود، که امکان آموزش و استنتاج مدل‌ها را روی هر دستگاه ویندوزی فراهم می‌کند.

به‌روزرسانی‌ها و فرصت‌هایی مرتبط با DirectML وجود داشته است، مانند پشتیبانی از تا ۱۵۰ اپراتور ONNX و استفاده توسط هر دو ONNX runtime و WinML. این API توسط تولیدکنندگان سخت‌افزار اصلی (IHVs) پشتیبانی می‌شود که هر کدام متاکماندهای مختلفی را پیاده‌سازی کرده‌اند.

## CUDA

CUDA که مخفف Compute Unified Device Architecture است، یک پلتفرم محاسبات موازی و مدل API است که توسط Nvidia ایجاد شده است. این امکان را به توسعه‌دهندگان نرم‌افزار می‌دهد تا از GPUهای مجهز به CUDA برای پردازش‌های عمومی استفاده کنند — رویکردی که GPGPU (محاسبات عمومی روی واحدهای پردازش گرافیکی) نامیده می‌شود. CUDA یکی از عوامل کلیدی شتاب GPUهای Nvidia است و در زمینه‌های مختلفی از جمله یادگیری ماشین، محاسبات علمی و پردازش ویدئو به‌طور گسترده استفاده می‌شود.

پشتیبانی سخت‌افزاری CUDA مختص GPUهای Nvidia است، زیرا این فناوری اختصاصی توسط Nvidia توسعه یافته است. هر معماری از نسخه‌های خاصی از ابزارهای CUDA پشتیبانی می‌کند که کتابخانه‌ها و ابزارهای لازم برای توسعه‌دهندگان جهت ساخت و اجرای برنامه‌های CUDA را فراهم می‌آورد.

## ONNX

ONNX (Open Neural Network Exchange) یک فرمت باز است که برای نمایش مدل‌های یادگیری ماشین طراحی شده است. این فرمت تعریفی از مدل گراف محاسباتی قابل توسعه، به همراه تعاریف اپراتورهای داخلی و انواع داده‌های استاندارد ارائه می‌دهد. ONNX به توسعه‌دهندگان اجازه می‌دهد مدل‌ها را بین فریم‌ورک‌های مختلف ML جابجا کنند، که باعث تعامل‌پذیری و آسان‌تر شدن ساخت و استقرار برنامه‌های هوش مصنوعی می‌شود.

Phi3 mini می‌تواند با ONNX Runtime روی CPU و GPU در دستگاه‌های مختلف، از جمله پلتفرم‌های سرور، ویندوز، لینوکس و مک دسکتاپ و CPUهای موبایل اجرا شود.
پیکربندی‌های بهینه‌ای که اضافه کرده‌ایم عبارتند از:

- مدل‌های ONNX برای int4 DML: کوانتیزه شده به int4 از طریق AWQ
- مدل ONNX برای fp16 CUDA
- مدل ONNX برای int4 CUDA: کوانتیزه شده به int4 از طریق RTN
- مدل ONNX برای int4 CPU و موبایل: کوانتیزه شده به int4 از طریق RTN

## Llama.cpp

Llama.cpp یک کتابخانه نرم‌افزاری متن‌باز نوشته شده به زبان C++ است. این کتابخانه استنتاج را روی مدل‌های بزرگ زبانی مختلف (LLMs)، از جمله Llama، انجام می‌دهد. این پروژه همراه با کتابخانه ggml (یک کتابخانه تنسور عمومی) توسعه یافته است و هدف آن ارائه استنتاج سریع‌تر و مصرف حافظه کمتر نسبت به پیاده‌سازی اصلی پایتون است. این کتابخانه از بهینه‌سازی سخت‌افزاری، کوانتیزاسیون پشتیبانی می‌کند و API ساده و مثال‌هایی ارائه می‌دهد. اگر به استنتاج کارآمد مدل‌های بزرگ زبانی علاقه‌مند هستید، llama.cpp ارزش بررسی دارد چون Phi3 می‌تواند Llama.cpp را اجرا کند.

## GGUF

GGUF (Generic Graph Update Format) فرمت مورد استفاده برای نمایش و به‌روزرسانی مدل‌های یادگیری ماشین است. این فرمت به‌ویژه برای مدل‌های زبانی کوچک‌تر (SLMs) که می‌توانند با کوانتیزاسیون ۴-۸ بیت به‌خوبی روی CPU اجرا شوند، مفید است. GGUF برای نمونه‌سازی سریع و اجرای مدل‌ها روی دستگاه‌های لبه‌ای یا در کارهای دسته‌ای مانند خطوط CI/CD بسیار کاربردی است.

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما در تلاش برای دقت هستیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نواقصی باشند. سند اصلی به زبان بومی خود باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئول هیچ گونه سوءتفاهم یا تفسیر نادرستی که از استفاده از این ترجمه ناشی شود، نیستیم.