<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-07T14:58:27+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "fa"
}
-->
# فناوری‌های کلیدی ذکر شده شامل

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - یک API سطح پایین برای یادگیری ماشین با شتاب سخت‌افزاری که بر پایه DirectX 12 ساخته شده است.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - یک پلتفرم محاسبات موازی و مدل رابط برنامه‌نویسی کاربردی (API) توسعه یافته توسط Nvidia که امکان پردازش عمومی روی واحدهای پردازش گرافیکی (GPU) را فراهم می‌کند.
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - یک فرمت باز طراحی شده برای نمایش مدل‌های یادگیری ماشین که قابلیت تعامل بین فریم‌ورک‌های مختلف ML را فراهم می‌کند.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - فرمت مورد استفاده برای نمایش و به‌روزرسانی مدل‌های یادگیری ماشین، به‌ویژه برای مدل‌های زبانی کوچک‌تر که می‌توانند به طور مؤثر روی CPUها با کوانتیزاسیون ۴-۸ بیت اجرا شوند.

## DirectML

DirectML یک API سطح پایین است که یادگیری ماشین با شتاب سخت‌افزاری را ممکن می‌سازد. این API بر پایه DirectX 12 ساخته شده تا از شتاب GPU استفاده کند و مستقل از فروشنده است، به این معنی که برای کار روی GPUهای مختلف نیازی به تغییر کد ندارد. این API عمدتاً برای آموزش مدل‌ها و بارهای کاری استنتاج روی GPUها به کار می‌رود.

از نظر پشتیبانی سخت‌افزاری، DirectML برای کار با طیف وسیعی از GPUها طراحی شده است، شامل GPUهای مجتمع و مجزا AMD، GPUهای مجتمع اینتل و GPUهای مجزای NVIDIA. این API بخشی از پلتفرم AI ویندوز است و روی ویندوز ۱۰ و ۱۱ پشتیبانی می‌شود، که امکان آموزش و استنتاج مدل روی هر دستگاه ویندوزی را فراهم می‌کند.

به‌روزرسانی‌ها و فرصت‌هایی مرتبط با DirectML وجود داشته است، مانند پشتیبانی از تا ۱۵۰ اپراتور ONNX و استفاده توسط هر دو ONNX runtime و WinML. این API توسط تولیدکنندگان سخت‌افزار بزرگ (IHVs) پشتیبانی می‌شود که هر کدام دستورات متنوعی را پیاده‌سازی می‌کنند.

## CUDA

CUDA که مخفف Compute Unified Device Architecture است، یک پلتفرم محاسبات موازی و مدل رابط برنامه‌نویسی کاربردی (API) است که توسط Nvidia ایجاد شده است. این پلتفرم به توسعه‌دهندگان نرم‌افزار اجازه می‌دهد تا از واحد پردازش گرافیکی مجهز به CUDA برای پردازش‌های عمومی استفاده کنند — رویکردی که به عنوان GPGPU (محاسبات عمومی روی واحدهای پردازش گرافیکی) شناخته می‌شود. CUDA محرک اصلی شتاب GPUهای Nvidia است و در حوزه‌های مختلفی از جمله یادگیری ماشین، محاسبات علمی و پردازش ویدئو به طور گسترده استفاده می‌شود.

پشتیبانی سخت‌افزاری CUDA مخصوص GPUهای Nvidia است، زیرا این فناوری اختصاصی توسط Nvidia توسعه یافته است. هر معماری از نسخه‌های خاصی از جعبه‌ابزار CUDA پشتیبانی می‌کند که کتابخانه‌ها و ابزارهای لازم برای توسعه‌دهندگان جهت ساخت و اجرای برنامه‌های CUDA را فراهم می‌آورد.

## ONNX

ONNX (Open Neural Network Exchange) یک فرمت باز است که برای نمایش مدل‌های یادگیری ماشین طراحی شده است. این فرمت تعریفی از مدل گراف محاسباتی قابل گسترش، به همراه تعاریف اپراتورهای داخلی و انواع داده‌های استاندارد ارائه می‌دهد. ONNX به توسعه‌دهندگان اجازه می‌دهد مدل‌ها را بین فریم‌ورک‌های مختلف ML جابه‌جا کنند، که قابلیت تعامل را افزایش داده و ایجاد و استقرار برنامه‌های هوش مصنوعی را آسان‌تر می‌کند.

Phi3 mini می‌تواند با ONNX Runtime روی CPU و GPU در دستگاه‌های مختلف از جمله پلتفرم‌های سرور، ویندوز، لینوکس، دسکتاپ‌های مک و CPUهای موبایل اجرا شود.
پیکربندی‌های بهینه‌سازی شده‌ای که اضافه کرده‌ایم عبارتند از

- مدل‌های ONNX برای int4 DML: کوانتیزه شده به int4 از طریق AWQ
- مدل ONNX برای fp16 CUDA
- مدل ONNX برای int4 CUDA: کوانتیزه شده به int4 از طریق RTN
- مدل ONNX برای int4 CPU و موبایل: کوانتیزه شده به int4 از طریق RTN

## Llama.cpp

Llama.cpp یک کتابخانه نرم‌افزاری متن‌باز نوشته شده به زبان C++ است. این کتابخانه استنتاج را روی مدل‌های زبان بزرگ مختلف (LLMها)، از جمله Llama انجام می‌دهد. این پروژه همراه با کتابخانه ggml (یک کتابخانه تنسور چندمنظوره) توسعه یافته است و هدف آن ارائه استنتاج سریع‌تر و مصرف حافظه کمتر نسبت به پیاده‌سازی اصلی پایتون است. این کتابخانه از بهینه‌سازی سخت‌افزاری، کوانتیزاسیون پشتیبانی می‌کند و API ساده و مثال‌هایی ارائه می‌دهد. اگر به استنتاج کارآمد LLMها علاقه‌مندید، llama.cpp ارزش بررسی دارد زیرا Phi3 می‌تواند Llama.cpp را اجرا کند.

## GGUF

GGUF (Generic Graph Update Format) فرمت مورد استفاده برای نمایش و به‌روزرسانی مدل‌های یادگیری ماشین است. این فرمت به‌ویژه برای مدل‌های زبانی کوچک‌تر (SLMها) که می‌توانند به طور مؤثر روی CPUها با کوانتیزاسیون ۴-۸ بیت اجرا شوند، مفید است. GGUF برای نمونه‌سازی سریع و اجرای مدل‌ها روی دستگاه‌های لبه یا در کارهای دسته‌ای مانند خطوط CI/CD بسیار کاربردی است.

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما در تلاش برای دقت هستیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نادرستی‌هایی باشند. سند اصلی به زبان مادری خود باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئول هیچ گونه سوء تفاهم یا تفسیر نادرستی که از استفاده از این ترجمه ناشی شود، نیستیم.