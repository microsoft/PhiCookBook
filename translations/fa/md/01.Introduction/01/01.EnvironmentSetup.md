<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-07T14:59:50+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "fa"
}
-->
# شروع کار با Phi-3 به صورت محلی

این راهنما به شما کمک می‌کند محیط محلی خود را برای اجرای مدل Phi-3 با استفاده از Ollama تنظیم کنید. شما می‌توانید مدل را به چند روش مختلف اجرا کنید، از جمله استفاده از GitHub Codespaces، VS Code Dev Containers، یا محیط محلی خودتان.

## تنظیم محیط

### GitHub Codespaces

می‌توانید این قالب را به صورت مجازی با استفاده از GitHub Codespaces اجرا کنید. دکمه زیر یک نسخه وب VS Code را در مرورگر شما باز می‌کند:

1. قالب را باز کنید (این ممکن است چند دقیقه طول بکشد):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. یک پنجره ترمینال باز کنید

### VS Code Dev Containers

⚠️ این گزینه فقط در صورتی کار می‌کند که Docker Desktop شما حداقل ۱۶ گیگابایت رم اختصاص داده باشد. اگر کمتر از ۱۶ گیگابایت رم دارید، می‌توانید گزینه [GitHub Codespaces](../../../../../md/01.Introduction/01) را امتحان کنید یا [به صورت محلی تنظیم کنید](../../../../../md/01.Introduction/01).

گزینه مرتبط، VS Code Dev Containers است که پروژه را در VS Code محلی شما با استفاده از [افزونه Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) باز می‌کند:

1. Docker Desktop را راه‌اندازی کنید (اگر نصب نشده است، ابتدا نصب کنید)
2. پروژه را باز کنید:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. در پنجره VS Code که باز می‌شود، وقتی فایل‌های پروژه نمایش داده شدند (این ممکن است چند دقیقه طول بکشد)، یک پنجره ترمینال باز کنید.
4. ادامه دهید با [مراحل استقرار](../../../../../md/01.Introduction/01)

### محیط محلی

1. مطمئن شوید ابزارهای زیر نصب شده‌اند:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## آزمایش مدل

1. از Ollama بخواهید مدل phi3:mini را دانلود و اجرا کند:

    ```shell
    ollama run phi3:mini
    ```

    دانلود مدل چند دقیقه طول خواهد کشید.

2. وقتی در خروجی عبارت "success" را دیدید، می‌توانید از طریق پرامپت به آن مدل پیام ارسال کنید.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. پس از چند ثانیه، باید جریان پاسخ از مدل را مشاهده کنید.

4. برای آشنایی با تکنیک‌های مختلف استفاده شده در مدل‌های زبانی، دفترچه پایتون [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) را باز کرده و هر سلول را اجرا کنید. اگر از مدلی غیر از 'phi3:mini' استفاده کردید، در بالای فایل مقدار `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` را به دلخواه تغییر دهید، همچنین می‌توانید پیام سیستم یا مثال‌های چند شاتی را در صورت تمایل اصلاح کنید.

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما در تلاش برای دقت هستیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نادرستی‌هایی باشند. سند اصلی به زبان بومی خود باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئول هیچ گونه سوءتفاهم یا تفسیر نادرستی که از استفاده این ترجمه ناشی شود، نیستیم.