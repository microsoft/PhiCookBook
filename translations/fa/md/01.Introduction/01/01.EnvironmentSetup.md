<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-07-16T18:06:58+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "fa"
}
-->
# شروع کار با Phi-3 به صورت محلی

این راهنما به شما کمک می‌کند محیط محلی خود را برای اجرای مدل Phi-3 با استفاده از Ollama راه‌اندازی کنید. می‌توانید مدل را به چند روش مختلف اجرا کنید، از جمله استفاده از GitHub Codespaces، VS Code Dev Containers، یا محیط محلی خودتان.

## راه‌اندازی محیط

### GitHub Codespaces

می‌توانید این قالب را به صورت مجازی با استفاده از GitHub Codespaces اجرا کنید. دکمه زیر یک نسخه وب VS Code را در مرورگر شما باز می‌کند:

1. قالب را باز کنید (این ممکن است چند دقیقه طول بکشد):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. یک پنجره ترمینال باز کنید

### VS Code Dev Containers

⚠️ این گزینه فقط در صورتی کار می‌کند که Docker Desktop شما حداقل ۱۶ گیگابایت رم اختصاص داده باشد. اگر کمتر از ۱۶ گیگابایت رم دارید، می‌توانید گزینه [GitHub Codespaces](../../../../../md/01.Introduction/01) را امتحان کنید یا [محیط محلی را راه‌اندازی کنید](../../../../../md/01.Introduction/01).

گزینه مرتبط، VS Code Dev Containers است که پروژه را در VS Code محلی شما با استفاده از [افزونه Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) باز می‌کند:

1. Docker Desktop را اجرا کنید (اگر نصب نیست، ابتدا نصب کنید)
2. پروژه را باز کنید:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. در پنجره VS Code که باز می‌شود، پس از نمایش فایل‌های پروژه (که ممکن است چند دقیقه طول بکشد)، یک پنجره ترمینال باز کنید.
4. ادامه دهید با [مراحل استقرار](../../../../../md/01.Introduction/01)

### محیط محلی

1. مطمئن شوید ابزارهای زیر نصب شده‌اند:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## آزمایش مدل

1. از Ollama بخواهید مدل phi3:mini را دانلود و اجرا کند:

    ```shell
    ollama run phi3:mini
    ```

    دانلود مدل چند دقیقه طول خواهد کشید.

2. وقتی در خروجی "success" را دیدید، می‌توانید از طریق پرامپت به آن مدل پیام ارسال کنید.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. پس از چند ثانیه، باید جریان پاسخ از مدل را مشاهده کنید.

4. برای آشنایی با تکنیک‌های مختلف استفاده شده در مدل‌های زبانی، دفترچه پایتون [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) را باز کرده و هر سلول را اجرا کنید. اگر از مدلی غیر از 'phi3:mini' استفاده کردید، مقدار `MODEL_NAME` را در سلول اول تغییر دهید.

5. برای گفتگو با مدل phi3:mini از طریق پایتون، فایل پایتون [chat.py](../../../../../code/01.Introduce/chat.py) را باز کرده و اجرا کنید. می‌توانید در بالای فایل مقدار `MODEL_NAME` را به دلخواه تغییر دهید و همچنین پیام سیستم یا مثال‌های few-shot را در صورت تمایل ویرایش کنید.

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما در تلاش برای دقت هستیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نادرستی‌هایی باشند. سند اصلی به زبان بومی خود باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئول هیچ گونه سوءتفاهم یا تفسیر نادرستی که از استفاده از این ترجمه ناشی شود، نیستیم.