<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-03-27T05:43:06+00:00",
  "source_file": "md\\01.Introduction\\01\\01.EnvironmentSetup.md",
  "language_code": "fa"
}
-->
# شروع به کار با Phi-3 به صورت محلی

این راهنما به شما کمک می‌کند محیط محلی خود را برای اجرای مدل Phi-3 با استفاده از Ollama تنظیم کنید. شما می‌توانید مدل را به چند روش مختلف اجرا کنید، از جمله استفاده از GitHub Codespaces، VS Code Dev Containers، یا محیط محلی خود.

## تنظیم محیط

### GitHub Codespaces

شما می‌توانید این قالب را به صورت مجازی با استفاده از GitHub Codespaces اجرا کنید. دکمه زیر یک نمونه وب‌محور از VS Code را در مرورگر شما باز می‌کند:

1. قالب را باز کنید (ممکن است چند دقیقه طول بکشد):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. یک پنجره ترمینال باز کنید

### VS Code Dev Containers

⚠️ این گزینه فقط در صورتی کار می‌کند که Docker Desktop شما حداقل 16 گیگابایت رم تخصیص داده باشد. اگر کمتر از 16 گیگابایت رم دارید، می‌توانید گزینه [GitHub Codespaces](../../../../../md/01.Introduction/01) را امتحان کنید یا [آن را به صورت محلی تنظیم کنید](../../../../../md/01.Introduction/01).

یک گزینه مرتبط VS Code Dev Containers است که پروژه را در VS Code محلی شما با استفاده از افزونه [Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) باز می‌کند:

1. Docker Desktop را راه‌اندازی کنید (اگر نصب نشده است، آن را نصب کنید)
2. پروژه را باز کنید:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. در پنجره VS Code که باز می‌شود، پس از اینکه فایل‌های پروژه ظاهر شدند (ممکن است چند دقیقه طول بکشد)، یک پنجره ترمینال باز کنید.
4. با [مراحل استقرار](../../../../../md/01.Introduction/01) ادامه دهید

### محیط محلی

1. مطمئن شوید ابزارهای زیر نصب شده‌اند:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## آزمایش مدل

1. از Ollama بخواهید مدل phi3:mini را دانلود و اجرا کند:

    ```shell
    ollama run phi3:mini
    ```

    این فرآیند چند دقیقه طول می‌کشد تا مدل دانلود شود.

2. وقتی در خروجی "success" را مشاهده کردید، می‌توانید از طریق prompt پیامی به مدل ارسال کنید.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. پس از چند ثانیه، باید جریان پاسخ مدل را مشاهده کنید.

4. برای یادگیری تکنیک‌های مختلف مورد استفاده در مدل‌های زبانی، دفترچه Python [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) را باز کنید و هر سلول را اجرا کنید. اگر از مدلی غیر از 'phi3:mini' استفاده کرده‌اید، `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` را در بالای فایل تغییر دهید و همچنین می‌توانید پیام سیستم را اصلاح کنید یا مثال‌های few-shot اضافه کنید.

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما برای دقت تلاش می‌کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نادرستی‌هایی باشند. سند اصلی به زبان بومی خود باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، توصیه می‌شود از ترجمه انسانی حرفه‌ای استفاده کنید. ما هیچ مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.