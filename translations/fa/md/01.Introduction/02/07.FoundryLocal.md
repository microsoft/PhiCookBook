<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "52973a5680a65a810aa80b7036afd31f",
  "translation_date": "2025-06-27T13:32:24+00:00",
  "source_file": "md/01.Introduction/02/07.FoundryLocal.md",
  "language_code": "fa"
}
-->
## شروع کار با مدل‌های خانواده Phi در Foundry Local

### معرفی Foundry Local

Foundry Local یک راهکار قدرتمند استنتاج هوش مصنوعی روی دستگاه است که قابلیت‌های هوش مصنوعی سطح سازمانی را مستقیماً به سخت‌افزار محلی شما می‌آورد. این آموزش شما را در راه‌اندازی و استفاده از مدل‌های خانواده Phi با Foundry Local راهنمایی می‌کند و کنترل کامل روی بارهای کاری هوش مصنوعی را با حفظ حریم خصوصی و کاهش هزینه‌ها در اختیارتان قرار می‌دهد.

Foundry Local با اجرای مدل‌های هوش مصنوعی به صورت محلی روی دستگاه شما، مزایای عملکرد، حریم خصوصی، سفارشی‌سازی و کاهش هزینه را فراهم می‌کند. این راهکار به‌طور یکپارچه در جریان‌های کاری و برنامه‌های موجود شما از طریق یک CLI، SDK و REST API ساده ادغام می‌شود.

![arch](../../../../../translated_images/foundry-local-arch.8823e321dd8258d7d68815ddb0153503587142ff32e6997041c7cf0c9df24b49.fa.png)

### چرا Foundry Local را انتخاب کنیم؟

آشنایی با مزایای Foundry Local به شما کمک می‌کند تصمیمات آگاهانه‌ای درباره استراتژی استقرار هوش مصنوعی خود بگیرید:

- **استنتاج روی دستگاه:** مدل‌ها را به‌صورت محلی روی سخت‌افزار خود اجرا کنید، هزینه‌ها را کاهش دهید و همه داده‌ها را روی دستگاه خود نگه دارید.

- **سفارشی‌سازی مدل:** از مدل‌های پیش‌فرض انتخاب کنید یا مدل‌های خود را برای برآورده کردن نیازها و موارد استفاده خاص به کار ببرید.

- **صرفه‌جویی در هزینه:** با استفاده از سخت‌افزار موجود خود، هزینه‌های مکرر خدمات ابری را حذف کنید و هوش مصنوعی را در دسترس‌تر کنید.

- **ادغام بی‌دردسر:** از طریق SDK، نقاط انتهایی API یا CLI به برنامه‌های خود متصل شوید و در صورت نیاز به آسانی به Azure AI Foundry مقیاس دهید.

> **یادداشت شروع:** این آموزش بر استفاده از Foundry Local از طریق رابط‌های CLI و SDK تمرکز دارد. شما هر دو روش را یاد می‌گیرید تا بهترین روش را برای مورد استفاده خود انتخاب کنید.

## بخش ۱: راه‌اندازی CLI Foundry Local

### گام ۱: نصب

CLI Foundry Local دروازه شما برای مدیریت و اجرای مدل‌های هوش مصنوعی به‌صورت محلی است. بیایید با نصب آن روی سیستم خود شروع کنیم.

**پلتفرم‌های پشتیبانی‌شده:** ویندوز و macOS

برای دستورالعمل‌های نصب دقیق، لطفاً به [مستندات رسمی Foundry Local](https://github.com/microsoft/Foundry-Local/blob/main/README.md) مراجعه کنید.

### گام ۲: بررسی مدل‌های موجود

پس از نصب CLI Foundry Local، می‌توانید مدل‌های موجود برای مورد استفاده خود را کشف کنید. این دستور تمام مدل‌های پشتیبانی‌شده را نشان می‌دهد:

```bash
foundry model list
```

### گام ۳: آشنایی با مدل‌های خانواده Phi

خانواده Phi مجموعه‌ای از مدل‌ها را ارائه می‌دهد که برای موارد استفاده و پیکربندی‌های سخت‌افزاری مختلف بهینه شده‌اند. در اینجا مدل‌های Phi موجود در Foundry Local آمده است:

**مدل‌های Phi موجود:**

- **phi-3.5-mini** - مدل جمع‌وجور برای وظایف پایه  
- **phi-3-mini-128k** - نسخه با زمینه گسترده‌تر برای گفتگوهای طولانی‌تر  
- **phi-3-mini-4k** - مدل با زمینه استاندارد برای استفاده عمومی  
- **phi-4** - مدل پیشرفته با قابلیت‌های بهبود یافته  
- **phi-4-mini** - نسخه سبک‌تر Phi-4  
- **phi-4-mini-reasoning** - تخصصی برای وظایف استدلال پیچیده

> **سازگاری سخت‌افزاری:** هر مدل می‌تواند برای شتاب‌دهی سخت‌افزاری مختلف (CPU، GPU) بر اساس توانایی‌های سیستم شما پیکربندی شود.

### گام ۴: اجرای اولین مدل Phi

بیایید با یک مثال عملی شروع کنیم. مدل `phi-4-mini-reasoning` را اجرا می‌کنیم که در حل مسائل پیچیده گام به گام بسیار خوب عمل می‌کند.

**دستور اجرای مدل:**

```bash
foundry model run Phi-4-mini-reasoning-generic-cpu
```

> **راه‌اندازی اولیه:** هنگام اجرای مدل برای اولین بار، Foundry Local به‌صورت خودکار آن را روی دستگاه محلی شما دانلود می‌کند. زمان دانلود بسته به سرعت شبکه شما متفاوت است، بنابراین لطفاً در طول راه‌اندازی اولیه صبور باشید.

### گام ۵: آزمایش مدل با یک مسئله واقعی

اکنون بیایید مدل خود را با یک مسئله منطقی کلاسیک تست کنیم تا ببینیم چگونه استدلال گام به گام را انجام می‌دهد:

**مثال مسئله:**

```txt
Please calculate the following step by step: Now there are pheasants and rabbits in the same cage, there are thirty-five heads on top and ninety-four legs on the bottom, how many pheasants and rabbits are there?
```

**رفتار مورد انتظار:** مدل باید این مسئله را به مراحل منطقی تقسیم کند و با استفاده از این واقعیت که قرقاول‌ها دو پا و خرگوش‌ها چهار پا دارند، سیستم معادلات را حل کند.

**نتایج:**

![cli](../../../../../translated_images/cli.862ec6b55c2b5d916093866d4df99190150d4198fd33ab79e586f9d6f5403089.fa.png)

## بخش ۲: ساخت برنامه‌ها با SDK Foundry Local

### چرا از SDK استفاده کنیم؟

در حالی که CLI برای آزمایش و تعامل سریع مناسب است، SDK به شما امکان می‌دهد Foundry Local را به صورت برنامه‌نویسی در برنامه‌های خود ادغام کنید. این امکان‌ها را فراهم می‌کند برای:

- ساخت برنامه‌های سفارشی مبتنی بر هوش مصنوعی  
- ایجاد جریان‌های کاری خودکار  
- ادغام قابلیت‌های هوش مصنوعی در سیستم‌های موجود  
- توسعه چت‌بات‌ها و ابزارهای تعاملی

### زبان‌های برنامه‌نویسی پشتیبانی‌شده

Foundry Local پشتیبانی SDK را برای چند زبان برنامه‌نویسی ارائه می‌دهد تا با ترجیحات توسعه شما سازگار باشد:

**📦 SDKهای موجود:**

- **C# (.NET):** [مستندات و مثال‌های SDK](https://github.com/microsoft/Foundry-Local/tree/main/sdk/cs)  
- **Python:** [مستندات و مثال‌های SDK](https://github.com/microsoft/Foundry-Local/tree/main/sdk/python)  
- **JavaScript:** [مستندات و مثال‌های SDK](https://github.com/microsoft/Foundry-Local/tree/main/sdk/js)  
- **Rust:** [مستندات و مثال‌های SDK](https://github.com/microsoft/Foundry-Local/tree/main/sdk/rust)

### مراحل بعدی

1. **SDK مورد نظر خود را** بر اساس محیط توسعه خود انتخاب کنید  
2. **مستندات مخصوص SDK را** برای راهنمایی‌های پیاده‌سازی دقیق دنبال کنید  
3. **با مثال‌های ساده شروع کنید** قبل از ساخت برنامه‌های پیچیده  
4. **کد نمونه ارائه‌شده** در هر مخزن SDK را بررسی کنید

## نتیجه‌گیری

اکنون یاد گرفته‌اید چگونه:

- ✅ CLI Foundry Local را نصب و راه‌اندازی کنید  
- ✅ مدل‌های خانواده Phi را کشف و اجرا کنید  
- ✅ مدل‌ها را با مسائل واقعی تست کنید  
- ✅ گزینه‌های SDK برای توسعه برنامه‌ها را درک کنید

Foundry Local پایه‌ای قدرتمند برای آوردن قابلیت‌های هوش مصنوعی مستقیماً به محیط محلی شما فراهم می‌کند، کنترل کامل روی عملکرد، حریم خصوصی و هزینه‌ها را در اختیار شما می‌گذارد و در عین حال انعطاف‌پذیری لازم برای مقیاس‌پذیری به راهکارهای ابری را نیز حفظ می‌کند.

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما در تلاش برای دقت هستیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نادرستی‌هایی باشند. سند اصلی به زبان بومی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، استفاده از ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئول هیچ گونه سوءتفاهم یا تفسیر نادرستی که از استفاده از این ترجمه ناشی شود، نیستیم.