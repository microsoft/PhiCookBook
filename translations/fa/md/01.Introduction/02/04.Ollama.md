<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2aa35f3c8b437fd5dc9995d53909d495",
  "translation_date": "2025-12-21T10:10:11+00:00",
  "source_file": "md/01.Introduction/02/04.Ollama.md",
  "language_code": "fa"
}
-->
## خانوادهٔ Phi در Ollama


[Ollama](https://ollama.com) به افراد بیشتری اجازه می‌دهد تا به‌طور مستقیم مدل‌های متن‌باز LLM یا SLM را از طریق اسکریپت‌های ساده مستقر کنند، و همچنین می‌تواند APIهایی بسازد تا به سناریوهای کاربردی محلی مانند Copilot کمک کند.

## **1. نصب**

Ollama از اجرای روی Windows، macOS و Linux پشتیبانی می‌کند. می‌توانید Ollama را از این لینک نصب کنید ([https://ollama.com/download](https://ollama.com/download)). پس از نصب موفق، می‌توانید مستقیماً از اسکریپت Ollama برای فراخوانی Phi-3 از طریق یک پنجره ترمینال استفاده کنید. می‌توانید تمام [کتابخانه‌های موجود در Ollama](https://ollama.com/library) را ببینید. اگر این مخزن را در یک Codespace باز کنید، Ollama از قبل نصب شده خواهد بود.

```bash

ollama run phi4

```

> [!NOTE]
> مدل هنگام اجرای اولین بار ابتدا دانلود خواهد شد. البته می‌توانید به‌طور مستقیم مدل دانلودشده Phi-4 را هم مشخص کنید. ما WSL را به‌عنوان مثال برای اجرای دستور می‌گیریم. پس از موفقیت‌آمیز بودن دانلود مدل، می‌توانید مستقیماً در ترمینال تعامل داشته باشید.

![اجرای دستور](../../../../../translated_images/fa/ollama_run.e9755172b162b381.webp)

## **2. فراخوانی API phi-4 از Ollama**

اگر می‌خواهید API مربوط به Phi-4 که توسط Ollama تولید شده را فراخوانی کنید، می‌توانید از این دستور در ترمینال برای راه‌اندازی سرور Ollama استفاده کنید.

```bash

ollama serve

```

> [!NOTE]
> اگر در macOS یا Linux اجرا می‌کنید، توجه داشته باشید که ممکن است با خطای زیر روبه‌رو شوید **"Error: listen tcp 127.0.0.1:11434: bind: address already in use"**. ممکن است این خطا هنگام اجرای دستور دریافت شود. می‌توانید یا آن خطا را نادیده بگیرید، زیرا معمولاً نشان می‌دهد که سرور از قبل در حال اجرا است، یا می‌توانید Ollama را متوقف کرده و دوباره راه‌اندازی کنید:

**macOS**

```bash

brew services restart ollama

```

**Linux**

```bash

sudo systemctl stop ollama

```

Ollama دو API را پشتیبانی می‌کند: generate و chat. می‌توانید مطابق نیازتان با ارسال درخواست‌ها به سرویس محلی در حال اجرا روی پورت 11434 از API مدل ارائه‌شده توسط Ollama استفاده کنید.

**Chat**

```bash

curl http://127.0.0.1:11434/api/chat -d '{
  "model": "phi3",
  "messages": [
    {
      "role": "system",
      "content": "Your are a python developer."
    },
    {
      "role": "user",
      "content": "Help me generate a bubble algorithm"
    }
  ],
  "stream": false
  
}'
```

این نتیجه در Postman است

![اسکرین‌شات نتایج JSON برای درخواست generate](../../../../../translated_images/fa/ollama_gen.bda5d4e715366cc9.webp)

## منابع اضافی

لیست مدل‌های موجود در Ollama را در [کتابخانهٔ آنها](https://ollama.com/library) بررسی کنید.

مدل خود را از سرور Ollama با استفاده از این دستور دریافت کنید

```bash
ollama pull phi4
```

مدل را با استفاده از این دستور اجرا کنید

```bash
ollama run phi4
```

***توجه:*** برای کسب اطلاعات بیشتر به این لینک مراجعه کنید [https://github.com/ollama/ollama/blob/main/docs/api.md](https://github.com/ollama/ollama/blob/main/docs/api.md)

## فراخوانی Ollama از پایتون

می‌توانید از `requests` یا `urllib3` برای ارسال درخواست به نقاط انتهایی سرور محلی که در بالا استفاده شد، استفاده کنید. با این حال، یک راه محبوب برای استفاده از Ollama در پایتون، استفاده از SDK [openai](https://pypi.org/project/openai/) است، چون Ollama همچنین نقاط انتهایی سازگار با OpenAI را فراهم می‌کند.

در اینجا یک نمونه برای phi3-mini آورده شده است:

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="nokeyneeded",
)

response = client.chat.completions.create(
    model="phi4",
    temperature=0.7,
    n=1,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a haiku about a hungry cat"},
    ],
)

print("Response:")
print(response.choices[0].message.content)
```

## فراخوانی Ollama از جاوااسکریپت 

```javascript
// نمونه‌ای از خلاصه‌سازی یک فایل با Phi-4
script({
    model: "ollama:phi4",
    title: "Summarize with Phi-4",
    system: ["system"],
})

// مثال خلاصه‌سازی
const file = def("FILE", env.files)
$`Summarize ${file} in a single paragraph.`
```

## فراخوانی Ollama از C#

یک برنامه C# Console جدید ایجاد کنید و بسته NuGet زیر را اضافه کنید:

```bash
dotnet add package Microsoft.SemanticKernel --version 1.34.0
```

سپس این کد را در فایل `Program.cs` جایگزین کنید

```csharp
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.ChatCompletion;

// add chat completion service using the local ollama server endpoint
#pragma warning disable SKEXP0001, SKEXP0003, SKEXP0010, SKEXP0011, SKEXP0050, SKEXP0052
builder.AddOpenAIChatCompletion(
    modelId: "phi4",
    endpoint: new Uri("http://localhost:11434/"),
    apiKey: "non required");

// invoke a simple prompt to the chat service
string prompt = "Write a joke about kittens";
var response = await kernel.InvokePromptAsync(prompt);
Console.WriteLine(response.GetValue<string>());
```

برنامه را با این دستور اجرا کنید:

```bash
dotnet run
```

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**سلب مسئولیت**:
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. هرچند ما در تلاش برای دقت هستیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است دارای خطاها یا نادرستی‌هایی باشند. سند اصلی به زبان اصلی‌اش باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس یا حیاتی، استفاده از ترجمه حرفه‌ای انسانی توصیه می‌شود. ما در قبال هرگونه سوء‌فهم یا تفسیر نادرستی که از استفاده از این ترجمه ناشی شود مسئولیتی نداریم.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->