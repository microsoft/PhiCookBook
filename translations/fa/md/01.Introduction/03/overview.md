<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f1ff728038c4f554b660a36b76cbdd6e",
  "translation_date": "2025-05-07T14:41:20+00:00",
  "source_file": "md/01.Introduction/03/overview.md",
  "language_code": "fa"
}
-->
در زمینه Phi-3-mini، استنتاج به فرایند استفاده از مدل برای پیش‌بینی یا تولید خروجی‌ها بر اساس داده‌های ورودی اشاره دارد. اجازه دهید جزئیات بیشتری درباره Phi-3-mini و قابلیت‌های استنتاج آن ارائه دهم.

Phi-3-mini بخشی از سری مدل‌های Phi-3 است که توسط مایکروسافت منتشر شده‌اند. این مدل‌ها برای بازتعریف آنچه با مدل‌های زبان کوچک (SLM) ممکن است، طراحی شده‌اند.

در اینجا چند نکته کلیدی درباره Phi-3-mini و قابلیت‌های استنتاج آن آمده است:

## **مروری بر Phi-3-mini:**
- Phi-3-mini دارای اندازه پارامتر ۳.۸ میلیارد است.
- این مدل نه تنها روی دستگاه‌های محاسباتی سنتی، بلکه روی دستگاه‌های لبه‌ای مانند دستگاه‌های موبایل و دستگاه‌های اینترنت اشیا نیز قابل اجرا است.
- انتشار Phi-3-mini به افراد و شرکت‌ها امکان می‌دهد تا SLMها را روی سخت‌افزارهای مختلف، به‌ویژه در محیط‌های محدود از نظر منابع، مستقر کنند.
- این مدل فرمت‌های مختلفی را پوشش می‌دهد، از جمله فرمت سنتی PyTorch، نسخه کم‌حجم شده فرمت gguf، و نسخه کم‌حجم شده مبتنی بر ONNX.

## **دسترسی به Phi-3-mini:**
برای دسترسی به Phi-3-mini، می‌توانید از [Semantic Kernel](https://github.com/microsoft/SemanticKernelCookBook?WT.mc_id=aiml-138114-kinfeylo) در یک برنامه Copilot استفاده کنید. Semantic Kernel به طور کلی با Azure OpenAI Service، مدل‌های متن‌باز روی Hugging Face و مدل‌های محلی سازگار است.
همچنین می‌توانید از [Ollama](https://ollama.com) یا [LlamaEdge](https://llamaedge.com) برای فراخوانی مدل‌های کم‌حجم استفاده کنید. Ollama به کاربران فردی اجازه می‌دهد مدل‌های کم‌حجم مختلف را فراخوانی کنند، در حالی که LlamaEdge امکان دسترسی چندسکویی به مدل‌های GGUF را فراهم می‌کند.

## **مدل‌های کم‌حجم شده:**
بسیاری از کاربران ترجیح می‌دهند برای استنتاج محلی از مدل‌های کم‌حجم شده استفاده کنند. به عنوان مثال، می‌توانید مستقیماً Ollama را اجرا کنید تا Phi-3 را فراخوانی کند یا آن را به صورت آفلاین با استفاده از یک Modelfile پیکربندی کنید. Modelfile مسیر فایل GGUF و فرمت درخواست را مشخص می‌کند.

## **امکانات هوش مصنوعی مولد:**
ترکیب مدل‌های زبان کوچک مانند Phi-3-mini امکانات جدیدی برای هوش مصنوعی مولد ایجاد می‌کند. استنتاج تنها اولین گام است؛ این مدل‌ها می‌توانند در وظایف مختلف در سناریوهای محدود از نظر منابع، با تأخیر پایین و محدودیت‌های هزینه‌ای به کار گرفته شوند.

## **باز کردن قفل هوش مصنوعی مولد با Phi-3-mini: راهنمای استنتاج و استقرار**
یاد بگیرید چگونه از Semantic Kernel، Ollama/LlamaEdge و ONNX Runtime برای دسترسی و استنتاج مدل‌های Phi-3-mini استفاده کنید و امکانات هوش مصنوعی مولد را در سناریوهای مختلف کاربردی کشف کنید.

**ویژگی‌ها**
استنتاج مدل phi3-mini در:

- [Semantic Kernel](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/semantickernel?WT.mc_id=aiml-138114-kinfeylo)
- [Ollama](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ollama?WT.mc_id=aiml-138114-kinfeylo)
- [LlamaEdge WASM](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm?WT.mc_id=aiml-138114-kinfeylo)
- [ONNX Runtime](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/onnx?WT.mc_id=aiml-138114-kinfeylo)
- [iOS](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ios?WT.mc_id=aiml-138114-kinfeylo)

در جمع‌بندی، Phi-3-mini به توسعه‌دهندگان امکان می‌دهد فرمت‌های مختلف مدل را بررسی کنند و از هوش مصنوعی مولد در سناریوهای مختلف کاربردی بهره ببرند.

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما در تلاش برای دقت هستیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نواقصی باشند. سند اصلی به زبان مادری خود باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئول هیچ گونه سوءتفاهم یا تفسیر نادرستی که ناشی از استفاده از این ترجمه باشد، نیستیم.