<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f1ff728038c4f554b660a36b76cbdd6e",
  "translation_date": "2025-03-27T07:57:18+00:00",
  "source_file": "md\\01.Introduction\\03\\overview.md",
  "language_code": "fa"
}
-->
در زمینه Phi-3-mini، استنتاج به فرآیند استفاده از مدل برای پیش‌بینی یا تولید خروجی‌ها بر اساس داده‌های ورودی اشاره دارد. اجازه دهید جزئیات بیشتری درباره Phi-3-mini و قابلیت‌های استنتاج آن ارائه دهم.

Phi-3-mini بخشی از سری مدل‌های Phi-3 است که توسط مایکروسافت منتشر شده‌اند. این مدل‌ها طراحی شده‌اند تا امکان‌پذیری‌های جدیدی را با مدل‌های زبان کوچک (SLM) تعریف کنند.

در اینجا برخی از نکات کلیدی درباره Phi-3-mini و قابلیت‌های استنتاج آن آورده شده است:

## **نمای کلی Phi-3-mini:**
- Phi-3-mini دارای اندازه پارامتر 3.8 میلیارد است.
- این مدل می‌تواند نه تنها بر روی دستگاه‌های محاسباتی سنتی اجرا شود، بلکه بر روی دستگاه‌های لبه‌ای مانند دستگاه‌های موبایل و دستگاه‌های اینترنت اشیاء (IoT) نیز اجرا شود.
- انتشار Phi-3-mini به افراد و شرکت‌ها این امکان را می‌دهد که مدل‌های SLM را بر روی دستگاه‌های سخت‌افزاری مختلف، به‌ویژه در محیط‌های محدود از نظر منابع، مستقر کنند.
- این مدل فرمت‌های مختلفی را پوشش می‌دهد، از جمله فرمت سنتی PyTorch، نسخه کوانتیده شده فرمت gguf و نسخه کوانتیده شده مبتنی بر ONNX.

## **دسترسی به Phi-3-mini:**
برای دسترسی به Phi-3-mini، می‌توانید از [Semantic Kernel](https://github.com/microsoft/SemanticKernelCookBook?WT.mc_id=aiml-138114-kinfeylo) در یک اپلیکیشن Copilot استفاده کنید. Semantic Kernel به طور کلی با سرویس Azure OpenAI، مدل‌های متن‌باز در Hugging Face، و مدل‌های محلی سازگار است.
همچنین می‌توانید از [Ollama](https://ollama.com) یا [LlamaEdge](https://llamaedge.com) برای فراخوانی مدل‌های کوانتیده شده استفاده کنید. Ollama به کاربران فردی اجازه می‌دهد مدل‌های کوانتیده مختلف را فراخوانی کنند، در حالی که LlamaEdge امکان دسترسی چند‌پلتفرمی برای مدل‌های GGUF فراهم می‌کند.

## **مدل‌های کوانتیده شده:**
بسیاری از کاربران ترجیح می‌دهند از مدل‌های کوانتیده شده برای استنتاج محلی استفاده کنند. به عنوان مثال، می‌توانید مستقیم Ollama run Phi-3 را اجرا کنید یا آن را به صورت آفلاین با استفاده از یک Modelfile پیکربندی کنید. Modelfile مسیر فایل GGUF و فرمت پرسش را مشخص می‌کند.

## **امکانات هوش مصنوعی مولد:**
ترکیب مدل‌های زبان کوچک مانند Phi-3-mini امکانات جدیدی برای هوش مصنوعی مولد باز می‌کند. استنتاج تنها گام اول است؛ این مدل‌ها می‌توانند برای وظایف مختلف در سناریوهای محدود از نظر منابع، وابسته به تأخیر و محدود به هزینه استفاده شوند.

## **باز کردن قابلیت‌های هوش مصنوعی مولد با Phi-3-mini: راهنمای استنتاج و استقرار**
یاد بگیرید چگونه از Semantic Kernel، Ollama/LlamaEdge و ONNX Runtime برای دسترسی و استنتاج مدل‌های Phi-3-mini استفاده کنید و امکانات هوش مصنوعی مولد را در سناریوهای مختلف اپلیکیشن بررسی کنید.

**ویژگی‌ها**
استنتاج مدل phi3-mini در:

- [Semantic Kernel](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/semantickernel?WT.mc_id=aiml-138114-kinfeylo)
- [Ollama](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ollama?WT.mc_id=aiml-138114-kinfeylo)
- [LlamaEdge WASM](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm?WT.mc_id=aiml-138114-kinfeylo)
- [ONNX Runtime](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/onnx?WT.mc_id=aiml-138114-kinfeylo)
- [iOS](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ios?WT.mc_id=aiml-138114-kinfeylo)

به طور خلاصه، Phi-3-mini به توسعه‌دهندگان اجازه می‌دهد تا فرمت‌های مختلف مدل را بررسی کرده و از هوش مصنوعی مولد در سناریوهای مختلف اپلیکیشن بهره ببرند.

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم تا دقت ترجمه را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.