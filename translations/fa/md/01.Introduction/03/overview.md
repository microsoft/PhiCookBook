در زمینه Phi-3-mini، استنتاج به فرآیند استفاده از مدل برای پیش‌بینی یا تولید خروجی بر اساس داده‌های ورودی اشاره دارد. اجازه دهید جزئیات بیشتری درباره Phi-3-mini و قابلیت‌های استنتاج آن ارائه دهم.

Phi-3-mini بخشی از سری مدل‌های Phi-3 است که توسط مایکروسافت عرضه شده‌اند. این مدل‌ها برای بازتعریف امکانات مدل‌های زبان کوچک (SLM) طراحی شده‌اند.

در اینجا چند نکته کلیدی درباره Phi-3-mini و قابلیت‌های استنتاج آن آورده شده است:

## **مروری بر Phi-3-mini:**
- Phi-3-mini دارای اندازه پارامتر ۳.۸ میلیارد است.
- این مدل نه تنها روی دستگاه‌های محاسباتی سنتی بلکه روی دستگاه‌های لبه‌ای مانند موبایل و دستگاه‌های اینترنت اشیا نیز قابل اجرا است.
- عرضه Phi-3-mini به افراد و شرکت‌ها امکان می‌دهد تا مدل‌های زبان کوچک را روی سخت‌افزارهای مختلف، به‌ویژه در محیط‌های با منابع محدود، پیاده‌سازی کنند.
- این مدل از فرمت‌های مختلفی پشتیبانی می‌کند، از جمله فرمت سنتی PyTorch، نسخه کوانتیزه شده فرمت gguf و نسخه کوانتیزه شده مبتنی بر ONNX.

## **دسترسی به Phi-3-mini:**
برای دسترسی به Phi-3-mini می‌توانید از [Semantic Kernel](https://github.com/microsoft/SemanticKernelCookBook?WT.mc_id=aiml-138114-kinfeylo) در یک برنامه Copilot استفاده کنید. Semantic Kernel به طور کلی با Azure OpenAI Service، مدل‌های متن‌باز در Hugging Face و مدل‌های محلی سازگار است.
همچنین می‌توانید از [Ollama](https://ollama.com) یا [LlamaEdge](https://llamaedge.com) برای فراخوانی مدل‌های کوانتیزه شده استفاده کنید. Ollama به کاربران فردی اجازه می‌دهد مدل‌های کوانتیزه مختلف را فراخوانی کنند، در حالی که LlamaEdge دسترسی چندسکویی به مدل‌های GGUF را فراهم می‌کند.

## **مدل‌های کوانتیزه شده:**
بسیاری از کاربران ترجیح می‌دهند برای استنتاج محلی از مدل‌های کوانتیزه شده استفاده کنند. به عنوان مثال، می‌توانید مستقیماً Ollama را برای اجرای Phi-3 راه‌اندازی کنید یا آن را به صورت آفلاین با استفاده از یک Modelfile پیکربندی کنید. Modelfile مسیر فایل GGUF و فرمت پرامپت را مشخص می‌کند.

## **امکانات هوش مصنوعی مولد:**
ترکیب مدل‌های زبان کوچک مانند Phi-3-mini امکانات جدیدی برای هوش مصنوعی مولد فراهم می‌کند. استنتاج تنها اولین گام است؛ این مدل‌ها می‌توانند در وظایف مختلف در محیط‌های با منابع محدود، با تأخیر کم و هزینه محدود به کار گرفته شوند.

## **بازکردن قفل هوش مصنوعی مولد با Phi-3-mini: راهنمای استنتاج و استقرار**  
یاد بگیرید چگونه از Semantic Kernel، Ollama/LlamaEdge و ONNX Runtime برای دسترسی و استنتاج مدل‌های Phi-3-mini استفاده کنید و امکانات هوش مصنوعی مولد را در سناریوهای مختلف کاربردی کشف کنید.

**ویژگی‌ها**  
استنتاج مدل phi3-mini در:

- [Semantic Kernel](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/semantickernel?WT.mc_id=aiml-138114-kinfeylo)  
- [Ollama](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ollama?WT.mc_id=aiml-138114-kinfeylo)  
- [LlamaEdge WASM](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm?WT.mc_id=aiml-138114-kinfeylo)  
- [ONNX Runtime](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/onnx?WT.mc_id=aiml-138114-kinfeylo)  
- [iOS](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ios?WT.mc_id=aiml-138114-kinfeylo)  

در خلاصه، Phi-3-mini به توسعه‌دهندگان امکان می‌دهد فرمت‌های مختلف مدل را بررسی کرده و از هوش مصنوعی مولد در سناریوهای مختلف کاربردی بهره ببرند.

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما در تلاش برای دقت هستیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نواقصی باشند. سند اصلی به زبان بومی خود باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئول هیچ گونه سوءتفاهم یا تفسیر نادرستی که از استفاده این ترجمه ناشی شود، نیستیم.