<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e5bb9190ef9d149d28037a768c6b62b2",
  "translation_date": "2025-03-27T08:40:17+00:00",
  "source_file": "md\\01.Introduction\\04\\UsingLlamacppQuantifyingPhi.md",
  "language_code": "fa"
}
-->
# **کوآنتایز کردن خانواده Phi با استفاده از llama.cpp**

## **llama.cpp چیست؟**

llama.cpp یک کتابخانه نرم‌افزاری متن‌باز است که عمدتاً با زبان C++ نوشته شده و برای استنتاج روی مدل‌های زبانی بزرگ (LLM) مانند Llama استفاده می‌شود. هدف اصلی آن ارائه عملکرد پیشرفته برای استنتاج مدل‌های زبانی بزرگ روی انواع سخت‌افزار با حداقل تنظیمات است. همچنین، برای این کتابخانه بایندینگ‌های پایتون وجود دارد که یک API سطح بالا برای تکمیل متن و یک سرور وب سازگار با OpenAI ارائه می‌دهند.

هدف اصلی llama.cpp این است که استنتاج مدل‌های زبانی بزرگ را با حداقل تنظیمات و عملکرد پیشرفته روی سخت‌افزارهای متنوع - چه به صورت محلی و چه در فضای ابری - ممکن سازد.

- پیاده‌سازی ساده با C/C++ بدون وابستگی‌ها
- اپل سیلیکون به عنوان یک پلتفرم درجه یک - بهینه‌سازی شده از طریق ARM NEON، Accelerate و Metal
- پشتیبانی از AVX، AVX2 و AVX512 برای معماری‌های x86
- کوآنتایز کردن با اعداد صحیح 1.5 بیت، 2 بیت، 3 بیت، 4 بیت، 5 بیت، 6 بیت و 8 بیت برای استنتاج سریع‌تر و کاهش مصرف حافظه
- هسته‌های سفارشی CUDA برای اجرای مدل‌های زبانی بزرگ روی GPUهای NVIDIA (پشتیبانی از GPUهای AMD از طریق HIP)
- پشتیبانی از بک‌اندهای Vulkan و SYCL
- استنتاج هیبریدی CPU+GPU برای تسریع نسبی مدل‌هایی که بزرگ‌تر از ظرفیت کل VRAM هستند

## **کوآنتایز کردن Phi-3.5 با llama.cpp**

مدل Phi-3.5-Instruct را می‌توان با استفاده از llama.cpp کوآنتایز کرد، اما مدل‌های Phi-3.5-Vision و Phi-3.5-MoE هنوز پشتیبانی نمی‌شوند. فرمت تبدیل‌شده توسط llama.cpp، GGUF است که همچنین رایج‌ترین فرمت کوآنتایز شده محسوب می‌شود.

تعداد زیادی مدل با فرمت کوآنتایز شده GGUF در Hugging Face موجود است. AI Foundry، Ollama و LlamaEdge به llama.cpp متکی هستند، بنابراین مدل‌های GGUF نیز اغلب مورد استفاده قرار می‌گیرند.

### **GGUF چیست؟**

GGUF یک فرمت باینری است که برای بارگذاری و ذخیره سریع مدل‌ها بهینه شده و برای اهداف استنتاج بسیار کارآمد است. GGUF برای استفاده با GGML و دیگر اجراکننده‌ها طراحی شده است. این فرمت توسط @ggerganov توسعه یافته که همچنین توسعه‌دهنده llama.cpp، یک چارچوب محبوب استنتاج مدل‌های زبانی بزرگ با C/C++، است. مدل‌هایی که ابتدا در چارچوب‌هایی مانند PyTorch توسعه داده شده‌اند، می‌توانند برای استفاده با این موتورها به فرمت GGUF تبدیل شوند.

### **ONNX در مقابل GGUF**

ONNX یک فرمت سنتی یادگیری ماشین/یادگیری عمیق است که در چارچوب‌های مختلف هوش مصنوعی به خوبی پشتیبانی می‌شود و در دستگاه‌های لبه کاربردهای خوبی دارد. اما GGUF مبتنی بر llama.cpp است و می‌توان گفت در عصر GenAI تولید شده است. این دو کاربردهای مشابهی دارند. اگر به عملکرد بهتر در سخت‌افزارهای تعبیه‌شده و لایه‌های کاربردی نیاز دارید، ONNX ممکن است انتخاب شما باشد. اگر از چارچوب‌ها و فناوری‌های مشتق شده از llama.cpp استفاده می‌کنید، GGUF ممکن است گزینه بهتری باشد.

### **کوآنتایز کردن Phi-3.5-Instruct با استفاده از llama.cpp**

**1. پیکربندی محیط**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. کوآنتایز کردن**

تبدیل Phi-3.5-Instruct به FP16 GGUF با استفاده از llama.cpp


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

کوآنتایز کردن Phi-3.5 به INT4


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. آزمایش**

نصب llama-cpp-python


```bash

pip install llama-cpp-python -U

```

***نکته*** 

اگر از اپل سیلیکون استفاده می‌کنید، لطفاً llama-cpp-python را به این صورت نصب کنید


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

آزمایش


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **منابع**

1. اطلاعات بیشتر درباره llama.cpp [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. اطلاعات بیشتر درباره GGUF [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را رعایت کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نادقتی‌هایی باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما هیچ مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نمی‌پذیریم.