<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-05-07T14:48:23+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "fa"
}
-->
# **کمّی‌سازی خانواده فی**

کمّی‌سازی مدل به فرآیند نگاشت پارامترها (مانند وزن‌ها و مقادیر فعال‌سازی) در یک مدل شبکه عصبی از بازه‌ای بزرگ از مقادیر (معمولاً بازه‌ای پیوسته) به بازه‌ای کوچک‌تر و محدود اشاره دارد. این فناوری می‌تواند اندازه و پیچیدگی محاسباتی مدل را کاهش داده و کارایی اجرای مدل را در محیط‌های محدود به منابع مانند دستگاه‌های موبایل یا سیستم‌های تعبیه‌شده بهبود بخشد. کمّی‌سازی مدل با کاهش دقت پارامترها فشرده‌سازی را انجام می‌دهد، اما همچنین مقداری از دقت را از دست می‌دهد. بنابراین، در فرآیند کمّی‌سازی باید تعادلی میان اندازه مدل، پیچیدگی محاسباتی و دقت برقرار شود. روش‌های رایج کمّی‌سازی شامل کمّی‌سازی نقطه ثابت، کمّی‌سازی نقطه شناور و غیره هستند. می‌توانید بر اساس سناریو و نیازهای خاص، استراتژی کمّی‌سازی مناسب را انتخاب کنید.

ما امیدواریم مدل‌های GenAI را در دستگاه‌های لبه‌ای پیاده‌سازی کنیم و اجازه دهیم دستگاه‌های بیشتری وارد سناریوهای GenAI شوند، مانند دستگاه‌های موبایل، AI PC/Copilot+PC و دستگاه‌های سنتی IoT. از طریق مدل کمّی‌سازی‌شده می‌توانیم آن را بر اساس دستگاه‌های مختلف روی دستگاه‌های لبه‌ای مختلف پیاده‌سازی کنیم. با ترکیب چارچوب تسریع مدل و مدل کمّی‌سازی ارائه‌شده توسط تولیدکنندگان سخت‌افزار، می‌توانیم سناریوهای کاربردی SLM بهتری بسازیم.

در سناریوی کمّی‌سازی، دقت‌های مختلفی داریم (INT4، INT8، FP16، FP32). در ادامه توضیحاتی درباره دقت‌های رایج کمّی‌سازی آمده است.

### **INT4**

کمّی‌سازی INT4 روشی بسیار شدید برای کمّی‌سازی است که وزن‌ها و مقادیر فعال‌سازی مدل را به اعداد صحیح ۴ بیتی تبدیل می‌کند. کمّی‌سازی INT4 معمولاً به دلیل بازه نمایشی کوچکتر و دقت پایین‌تر، باعث از دست رفتن دقت بیشتری می‌شود. با این حال، در مقایسه با کمّی‌سازی INT8، کمّی‌سازی INT4 می‌تواند نیازهای ذخیره‌سازی و پیچیدگی محاسباتی مدل را بیشتر کاهش دهد. باید توجه داشت که کمّی‌سازی INT4 در کاربردهای عملی نسبتاً نادر است، زیرا دقت بسیار پایین ممکن است باعث افت قابل توجه عملکرد مدل شود. علاوه بر این، همه سخت‌افزارها از عملیات INT4 پشتیبانی نمی‌کنند، بنابراین هنگام انتخاب روش کمّی‌سازی باید سازگاری سخت‌افزاری را در نظر گرفت.

### **INT8**

کمّی‌سازی INT8 فرآیند تبدیل وزن‌ها و فعال‌سازی‌های مدل از اعداد شناور به اعداد صحیح ۸ بیتی است. اگرچه بازه عددی نمایشی اعداد صحیح INT8 کوچکتر و دقت کمتری دارد، اما می‌تواند به طور قابل توجهی نیازهای ذخیره‌سازی و محاسبات را کاهش دهد. در کمّی‌سازی INT8، وزن‌ها و مقادیر فعال‌سازی مدل از طریق فرآیند کمّی‌سازی شامل مقیاس‌بندی و جابجایی عبور می‌کنند تا اطلاعات شناور اصلی تا حد امکان حفظ شود. در هنگام استنتاج، این مقادیر کمّی‌شده به اعداد شناور بازگردانده شده برای محاسبه و سپس مجدداً برای مرحله بعدی به INT8 کمّی می‌شوند. این روش می‌تواند دقت کافی را در بیشتر کاربردها فراهم کند و در عین حال کارایی محاسباتی بالایی حفظ نماید.

### **FP16**

فرمت FP16، یعنی اعداد شناور ۱۶ بیتی (float16)، نسبت به اعداد شناور ۳۲ بیتی (float32) حافظه را نصف می‌کند که در کاربردهای یادگیری عمیق بزرگ‌مقیاس مزایای قابل توجهی دارد. فرمت FP16 اجازه می‌دهد مدل‌های بزرگ‌تر بارگذاری شوند یا داده‌های بیشتری در محدودیت حافظه GPU پردازش شوند. با ادامه پشتیبانی سخت‌افزار GPUهای مدرن از عملیات FP16، استفاده از فرمت FP16 ممکن است باعث افزایش سرعت محاسبات نیز شود. با این حال، فرمت FP16 معایب ذاتی خود را دارد، یعنی دقت کمتر که ممکن است در برخی موارد منجر به ناپایداری عددی یا از دست رفتن دقت شود.

### **FP32**

فرمت FP32 دقت بالاتری ارائه می‌دهد و می‌تواند بازه وسیعی از مقادیر را به دقت نمایش دهد. در سناریوهایی که عملیات ریاضی پیچیده انجام می‌شود یا نتایج با دقت بالا مورد نیاز است، فرمت FP32 ترجیح داده می‌شود. با این حال، دقت بالا به معنای استفاده بیشتر از حافظه و زمان محاسبه طولانی‌تر است. برای مدل‌های یادگیری عمیق بزرگ‌مقیاس، به ویژه زمانی که پارامترهای مدل زیاد و داده‌ها حجیم باشند، فرمت FP32 ممکن است باعث کمبود حافظه GPU یا کاهش سرعت استنتاج شود.

در دستگاه‌های موبایل یا دستگاه‌های IoT، می‌توانیم مدل‌های Phi-3.x را به INT4 تبدیل کنیم، در حالی که AI PC / Copilot PC می‌توانند از دقت‌های بالاتری مانند INT8، FP16، FP32 استفاده کنند.

در حال حاضر، تولیدکنندگان سخت‌افزار مختلف چارچوب‌های متفاوتی برای پشتیبانی از مدل‌های مولد دارند، مانند OpenVINO از Intel، QNN از Qualcomm، MLX از Apple و CUDA از Nvidia، که با کمّی‌سازی مدل ترکیب شده و استقرار محلی را کامل می‌کنند.

از نظر فناوری، پس از کمّی‌سازی از فرمت‌های مختلفی پشتیبانی می‌کنیم، مانند فرمت PyTorch / Tensorflow، GGUF و ONNX. من مقایسه فرمت و سناریوهای کاربردی بین GGUF و ONNX را انجام داده‌ام. اینجا فرمت کمّی‌سازی ONNX را توصیه می‌کنم که از چارچوب مدل تا سخت‌افزار پشتیبانی خوبی دارد. در این فصل، تمرکز ما بر ONNX Runtime برای GenAI، OpenVINO و Apple MLX برای انجام کمّی‌سازی مدل خواهد بود (اگر راه بهتری دارید، می‌توانید از طریق ارسال PR به ما ارائه دهید).

**این فصل شامل موارد زیر است**

1. [کمّی‌سازی Phi-3.5 / 4 با استفاده از llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [کمّی‌سازی Phi-3.5 / 4 با استفاده از افزونه‌های AI مولد برای onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [کمّی‌سازی Phi-3.5 / 4 با استفاده از Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [کمّی‌سازی Phi-3.5 / 4 با استفاده از چارچوب Apple MLX](./UsingAppleMLXQuantifyingPhi.md)

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما برای دقت تلاش می‌کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی اشتباهات یا نواقصی باشند. سند اصلی به زبان بومی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئول هیچ گونه سوءتفاهم یا برداشت نادرستی که از استفاده این ترجمه ناشی شود، نیستیم.