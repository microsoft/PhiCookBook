<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-03-27T08:21:52+00:00",
  "source_file": "md\\01.Introduction\\04\\QuantifyingPhi.md",
  "language_code": "fa"
}
-->
# **کوانتیزه کردن خانواده Phi**

کوانتیزه کردن مدل به فرآیند نگاشت پارامترها (مانند وزن‌ها و مقادیر فعال‌سازی) در یک مدل شبکه عصبی از یک دامنه مقدار بزرگ (معمولاً دامنه مقدار پیوسته) به یک دامنه مقدار کوچک‌تر و محدود اشاره دارد. این فناوری می‌تواند اندازه و پیچیدگی محاسباتی مدل را کاهش دهد و کارایی مدل را در محیط‌های محدود از نظر منابع مانند دستگاه‌های موبایل یا سیستم‌های تعبیه‌شده بهبود دهد. کوانتیزه کردن مدل از طریق کاهش دقت پارامترها به فشرده‌سازی دست پیدا می‌کند، اما همچنین مقداری افت دقت را نیز به همراه دارد. بنابراین، در فرآیند کوانتیزه کردن، لازم است بین اندازه مدل، پیچیدگی محاسباتی و دقت تعادل برقرار شود. روش‌های رایج کوانتیزه کردن شامل کوانتیزه کردن با نقطه ثابت، کوانتیزه کردن با نقطه شناور و غیره می‌شود. شما می‌توانید براساس سناریو و نیازهای خاص، استراتژی کوانتیزه کردن مناسب را انتخاب کنید.

ما امیدواریم مدل‌های GenAI را در دستگاه‌های لبه‌ای مستقر کنیم و اجازه دهیم دستگاه‌های بیشتری وارد سناریوهای GenAI شوند، مانند دستگاه‌های موبایل، کامپیوترهای هوش مصنوعی/کاپیلات + کامپیوتر، و دستگاه‌های سنتی اینترنت اشیا. از طریق مدل کوانتیزه شده، می‌توانیم آن را براساس دستگاه‌های مختلف در دستگاه‌های لبه‌ای مختلف مستقر کنیم. با ترکیب چارچوب‌های شتاب‌دهی مدل و مدل‌های کوانتیزه شده ارائه‌شده توسط تولیدکنندگان سخت‌افزار، می‌توانیم سناریوهای کاربردی SLM بهتری ایجاد کنیم.

در سناریوی کوانتیزه کردن، دقت‌های مختلفی داریم (INT4، INT8، FP16، FP32). در ادامه توضیحی درباره دقت‌های رایج کوانتیزه کردن ارائه شده است:

### **INT4**

کوانتیزه کردن INT4 یک روش کوانتیزه کردن جسورانه است که وزن‌ها و مقادیر فعال‌سازی مدل را به اعداد صحیح ۴ بیتی کوانتیزه می‌کند. کوانتیزه کردن INT4 معمولاً به دلیل دامنه نمایش کوچک‌تر و دقت پایین‌تر، منجر به افت دقت بیشتری می‌شود. با این حال، در مقایسه با کوانتیزه کردن INT8، کوانتیزه کردن INT4 می‌تواند بیشتر نیازهای ذخیره‌سازی و پیچیدگی محاسباتی مدل را کاهش دهد. لازم به ذکر است که کوانتیزه کردن INT4 در کاربردهای عملی نسبتاً نادر است، زیرا دقت بسیار پایین ممکن است باعث کاهش قابل توجه عملکرد مدل شود. علاوه بر این، همه سخت‌افزارها از عملیات INT4 پشتیبانی نمی‌کنند، بنابراین هنگام انتخاب روش کوانتیزه کردن باید سازگاری سخت‌افزاری را در نظر گرفت.

### **INT8**

کوانتیزه کردن INT8 فرآیند تبدیل وزن‌ها و فعال‌سازی‌های مدل از اعداد نقطه شناور به اعداد صحیح ۸ بیتی است. اگرچه دامنه عددی که توسط اعداد صحیح INT8 نمایش داده می‌شود کوچک‌تر و کم‌دقت‌تر است، اما می‌تواند نیازهای ذخیره‌سازی و محاسباتی را به طور قابل توجهی کاهش دهد. در کوانتیزه کردن INT8، وزن‌ها و مقادیر فعال‌سازی مدل از یک فرآیند کوانتیزه کردن، شامل مقیاس‌بندی و آفست، عبور می‌کنند تا اطلاعات اصلی نقطه شناور تا حد امکان حفظ شود. در طول استنتاج، این مقادیر کوانتیزه شده دوباره به اعداد نقطه شناور بازکوانتیزه می‌شوند تا محاسبه انجام شود و سپس برای مرحله بعدی دوباره به INT8 کوانتیزه می‌شوند. این روش می‌تواند در اکثر کاربردها دقت کافی را فراهم کند و در عین حال کارایی محاسباتی بالایی را حفظ کند.

### **FP16**

فرمت FP16، یعنی اعداد نقطه شناور ۱۶ بیتی (float16)، ردپای حافظه را نسبت به اعداد نقطه شناور ۳۲ بیتی (float32) به نصف کاهش می‌دهد، که در کاربردهای یادگیری عمیق بزرگ‌مقیاس مزایای قابل توجهی دارد. فرمت FP16 امکان بارگذاری مدل‌های بزرگ‌تر یا پردازش داده‌های بیشتر را در محدودیت‌های حافظه GPU مشابه فراهم می‌کند. با ادامه پشتیبانی سخت‌افزارهای مدرن GPU از عملیات FP16، استفاده از فرمت FP16 ممکن است همچنین باعث بهبود سرعت محاسبات شود. با این حال، فرمت FP16 نیز دارای معایب ذاتی خود است، یعنی دقت پایین‌تر، که ممکن است در برخی موارد منجر به بی‌ثباتی عددی یا افت دقت شود.

### **FP32**

فرمت FP32 دقت بالاتری را فراهم می‌کند و می‌تواند دامنه گسترده‌ای از مقادیر را به طور دقیق نمایش دهد. در سناریوهایی که عملیات ریاضی پیچیده انجام می‌شود یا نتایج با دقت بالا مورد نیاز است، فرمت FP32 ترجیح داده می‌شود. با این حال، دقت بالا همچنین به معنای استفاده بیشتر از حافظه و زمان محاسبات طولانی‌تر است. برای مدل‌های یادگیری عمیق بزرگ‌مقیاس، به ویژه زمانی که تعداد پارامترهای مدل زیاد است و حجم داده‌ها عظیم است، فرمت FP32 ممکن است باعث کمبود حافظه GPU یا کاهش سرعت استنتاج شود.

در دستگاه‌های موبایل یا دستگاه‌های اینترنت اشیا، می‌توانیم مدل‌های Phi-3.x را به INT4 تبدیل کنیم، در حالی که کامپیوترهای هوش مصنوعی / کاپیلات کامپیوتر می‌توانند از دقت‌های بالاتری مانند INT8، FP16، FP32 استفاده کنند.

در حال حاضر، تولیدکنندگان مختلف سخت‌افزار چارچوب‌های متفاوتی برای پشتیبانی از مدل‌های تولیدی دارند، مانند OpenVINO اینتل، QNN کوالکام، MLX اپل، و CUDA انویدیا و غیره، که با ترکیب کوانتیزه کردن مدل می‌توانند استقرار محلی را کامل کنند.

از نظر فناوری، پس از کوانتیزه کردن، پشتیبانی از فرمت‌های مختلفی داریم، مانند فرمت‌های PyTorch / Tensorflow، GGUF و ONNX. من یک مقایسه فرمت و سناریوهای کاربردی بین GGUF و ONNX انجام داده‌ام. اینجا فرمت کوانتیزه کردن ONNX را توصیه می‌کنم، که از پشتیبانی خوبی از چارچوب مدل تا سخت‌افزار برخوردار است. در این فصل، ما بر روی ONNX Runtime برای GenAI، OpenVINO، و Apple MLX تمرکز خواهیم کرد تا کوانتیزه کردن مدل را انجام دهیم (اگر روش بهتری دارید، می‌توانید با ارسال PR آن را به ما ارائه دهید).

**این فصل شامل موارد زیر است**

1. [کوانتیزه کردن Phi-3.5 / 4 با استفاده از llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [کوانتیزه کردن Phi-3.5 / 4 با استفاده از افزونه‌های هوش تولیدی برای onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [کوانتیزه کردن Phi-3.5 / 4 با استفاده از OpenVINO اینتل](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [کوانتیزه کردن Phi-3.5 / 4 با استفاده از چارچوب MLX اپل](./UsingAppleMLXQuantifyingPhi.md)

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم ترجمه‌ها دقیق باشند، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌هایی باشند. سند اصلی به زبان مادری باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما هیچ مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.