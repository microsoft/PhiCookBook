<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-05T17:03:40+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "fa"
}
-->
# **کوانتیزه کردن خانواده فی**

کوانتیزه کردن مدل به فرآیند نگاشت پارامترها (مانند وزن‌ها و مقادیر فعال‌سازی) در مدل شبکه عصبی از یک بازه مقادیر بزرگ (معمولاً بازه مقادیر پیوسته) به یک بازه مقادیر محدود کوچکتر اطلاق می‌شود. این فناوری می‌تواند اندازه و پیچیدگی محاسباتی مدل را کاهش داده و بهره‌وری عملکرد مدل را در محیط‌های دارای محدودیت منابع مانند دستگاه‌های موبایل یا سیستم‌های تعبیه‌شده بهبود بخشد. کوانتیزه کردن مدل با کاهش دقت پارامترها فشرده‌سازی را محقق می‌کند، ولی همچنین باعث ایجاد مقداری افت دقت می‌شود. بنابراین، در فرایند کوانتیزه کردن لازم است بین اندازه مدل، پیچیدگی محاسباتی و دقت تعادل برقرار شود. روش‌های رایج کوانتیزه کردن شامل کوانتیزه کردن نقطه ثابت، کوانتیزه کردن نقطه شناور و غیره می‌باشد. شما می‌توانید بر اساس سناریوی مشخص و نیازها، استراتژی کوانتیزه کردن مناسب را انتخاب کنید.

ما امیدواریم که مدل GenAI را به دستگاه‌های لبه‌ای پیاده‌سازی کنیم و اجازه دهیم دستگاه‌های بیشتری وارد سناریوهای GenAI شوند، مانند دستگاه‌های موبایل، رایانه‌های شخصی هوش مصنوعی / Copilot+PC و دستگاه‌های سنتی اینترنت اشیا. از طریق مدل‌های کوانتیزه شده، می‌توانیم آن را بر اساس دستگاه‌های مختلف بر روی دستگاه‌های لبه‌ای مختلف پیاده کنیم. همراه با چارچوب سرعت‌بخشی مدل و مدل کوانتیزه شده ارائه شده توسط تولیدکنندگان سخت‌افزار، می‌توانیم سناریوهای کاربردی بهتر SLM بسازیم.

در سناریوی کوانتیزه کردن، دقت‌های متفاوتی داریم (INT4، INT8، FP16، FP32). در ادامه توضیح دقت‌های رایج کوانتیزه کردن آمده است.

### **INT4**

کوانتیزه کردن INT4 روشی رادیکال برای کوانتیزه کردن است که وزن‌ها و مقادیر فعال‌سازی مدل را به اعداد صحیح ۴ بیتی تبدیل می‌کند. کوانتیزه کردن INT4 معمولاً به دلیل بازه نمایش کوچکتر و دقت پایین‌تر باعث افت دقت بیشتر می‌شود. اما در مقایسه با کوانتیزه کردن INT8، کوانتیزه کردن INT4 می‌تواند نیازهای ذخیره‌سازی و پیچیدگی محاسباتی مدل را بیش از پیش کاهش دهد. لازم به ذکر است که کوانتیزه کردن INT4 در کاربردهای عملی نسبتاً نادر است، زیرا دقت بسیار پایین ممکن است باعث افت قابل توجه عملکرد مدل شود. همچنین، همه سخت‌افزارها از عملیات INT4 پشتیبانی نمی‌کنند، بنابراین سازگاری سخت‌افزاری هنگام انتخاب روش کوانتیزه کردن باید مدنظر قرار گیرد.

### **INT8**

کوانتیزه کردن INT8 فرآیند تبدیل وزن‌ها و مقادیر فعال‌سازی مدل از اعداد ممیز شناور به اعداد صحیح ۸ بیتی است. اگرچه بازه عددی نمایش داده شده توسط اعداد صحیح INT8 کوچکتر و دقت کمتری دارند، اما به طور قابل توجهی نیاز به ذخیره‌سازی و محاسبات را کاهش می‌دهند. در کوانتیزه کردن INT8، وزن‌ها و مقادیر فعال‌سازی مدل از طریق فرآیند کوانتیزه کردن شامل مقیاس‌بندی و جابجایی عبور می‌کنند تا اطلاعات عددی اصلی ممیز شناور تا حد امکان حفظ شود. در زمان استنتاج، این مقادیر کوانتیزه شده به اعداد ممیز شناور بازگردانده می‌شوند و سپس برای گام بعدی مجدد به INT8 کوانتیزه می‌شوند. این روش می‌تواند در اکثر کاربردها دقت کافی را فراهم کند و در عین حال به بهره‌وری محاسباتی بالا دست یابد.

### **FP16**

فرمت FP16، یعنی اعداد ممیز شناور ۱۶ بیتی (float16)، اندازه حافظه را نسبت به اعداد ممیز شناور ۳۲ بیتی (float32) نصف می‌کند که در کاربردهای یادگیری عمیق در مقیاس بزرگ مزایای قابل توجهی دارد. فرمت FP16 امکان بارگذاری مدل‌های بزرگتر یا پردازش داده‌های بیشتر را در محدودیت‌های حافظه GPU مشابه فراهم می‌کند. با ادامه حمایت سخت‌افزارهای GPU مدرن از عملیات FP16، استفاده از فرمت FP16 ممکن است باعث بهبود سرعت محاسبه نیز شود. اما فرمت FP16 دارای معایب ذاتی خود یعنی دقت پایین‌تر است که ممکن است در برخی موارد منجر به ناپایداری عددی یا افت دقت شود.

### **FP32**

فرمت FP32 دقت بالاتری ارائه می‌دهد و می‌تواند گستره وسیعی از مقادیر را با دقت بالا نمایش دهد. در سناریوهایی که عملیات ریاضی پیچیده انجام می‌شود یا نتایج با دقت بالا مورد نیاز است، فرمت FP32 ترجیح داده می‌شود. اما دقت بالا به معنای مصرف بیشتر حافظه و زمان محاسبه طولانی‌تر است. برای مدل‌های یادگیری عمیق در مقیاس وسیع، خصوصاً زمانی که پارامترهای مدل زیاد و داده‌ها حجیم هستند، فرمت FP32 ممکن است باعث کمبود حافظه GPU یا کاهش سرعت استنتاج شود.

در دستگاه‌های موبایل یا دستگاه‌های IoT، می‌توان مدل‌های Phi-3.x را به INT4 تبدیل کرد، در حالی که رایانه‌های هوش مصنوعی / Copilot PC می‌توانند از دقت‌های بالاتر مانند INT8، FP16، یا FP32 استفاده کنند.

در حال حاضر، تولیدکنندگان سخت‌افزار مختلف چارچوب‌های مختلفی برای پشتیبانی از مدل‌های مولد دارند، مانند OpenVINO شرکت Intel، QNN شرکت Qualcomm، MLX شرکت Apple و CUDA شرکت Nvidia، که همراه با کوانتیزه کردن مدل، پیاده‌سازی محلی را تکمیل می‌کنند.

از نظر فناوری، پس از کوانتیزه کردن، فرمت‌های پشتیبانی متفاوتی داریم مانند فرمت PyTorch / TensorFlow، GGUF و ONNX. من یک مقایسه فرمت و سناریوهای کاربردی بین GGUF و ONNX انجام داده‌ام. در اینجا فرمت کوانتیزه کردن ONNX را توصیه می‌کنم که از چارچوب مدل تا سخت‌افزار پشتیبانی خوبی دارد. در این فصل، تمرکز ما بر ONNX Runtime برای GenAI، OpenVINO، و Apple MLX برای انجام کوانتیزه کردن مدل خواهد بود (اگر روش بهتری دارید، می‌توانید با ارسال PR به ما ارائه دهید).

**این فصل شامل موارد زیر است**

1. [کوانتیزه کردن Phi-3.5 / 4 با استفاده از llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [کوانتیزه کردن Phi-3.5 / 4 با استفاده از افزونه‌های هوش مصنوعی مولد برای onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [کوانتیزه کردن Phi-3.5 / 4 با استفاده از Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [کوانتیزه کردن Phi-3.5 / 4 با استفاده از چارچوب Apple MLX](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**سلب‌مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما برای دقت تلاش می‌کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نواقصی باشند. سند اصلی به زبان بومی آن باید به عنوان مرجع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، استفاده از ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئول سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نیستیم.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->