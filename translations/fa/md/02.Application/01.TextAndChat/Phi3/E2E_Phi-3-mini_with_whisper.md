<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7f72d7981ed3640865700f51ae407da4",
  "translation_date": "2026-01-14T15:09:56+00:00",
  "source_file": "md/02.Application/01.TextAndChat/Phi3/E2E_Phi-3-mini_with_whisper.md",
  "language_code": "fa"
}
-->
# چت‌بات آموزشی تعاملی Phi 3 Mini 4K با Whisper

## بررسی کلی

چت‌بات آموزشی تعاملی Phi 3 Mini 4K ابزاری است که به کاربران اجازه می‌دهد با دموی آموزشی Microsoft Phi 3 Mini 4K به صورت متنی یا صوتی تعامل داشته باشند. این چت‌بات می‌تواند برای وظایف متنوعی مثل ترجمه، به‌روزرسانی وضعیت آب و هوا و جمع‌آوری اطلاعات عمومی استفاده شود.

### شروع سریع

برای استفاده از این چت‌بات، کافی است این دستورالعمل‌ها را دنبال کنید:

1. یک [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) جدید باز کنید
2. در پنجره اصلی نوت‌بوک، رابط چت‌باکسی با یک جعبه ورودی متن و دکمه "ارسال" را خواهید دید.
3. برای استفاده از چت‌بات متنی، به سادگی پیام خود را در جعبه ورودی متن تایپ کرده و دکمه "ارسال" را کلیک کنید. چت‌بات با یک فایل صوتی پاسخ خواهد داد که می‌توان آن را مستقیماً در داخل نوت‌بوک پخش کرد.

**نکته**: این ابزار به یک GPU و دسترسی به مدل‌های Microsoft Phi-3 و OpenAI Whisper نیاز دارد که برای تشخیص گفتار و ترجمه استفاده می‌شود.

### نیازمندی‌های GPU

برای اجرای این دموی آزمایشی به 12 گیگابایت حافظه GPU نیاز دارید.

نیازهای حافظه برای اجرای دمو **Microsoft-Phi-3-Mini-4K instruct** روی GPU به عوامل مختلفی بستگی دارد، مانند اندازه داده ورودی (صوت یا متن)، زبان مورد استفاده برای ترجمه، سرعت مدل، و حافظه موجود روی GPU.

کلاً، مدل Whisper برای اجرا بر روی GPU طراحی شده است. حداقل میزان حافظه GPU توصیه شده برای اجرای مدل Whisper هشت گیگابایت است، اما این مدل می‌تواند مقادیر بیشتری حافظه را در صورت نیاز مدیریت کند.

مهم است بدانید که اجرای حجم زیادی از داده‌ها یا تعداد زیادی درخواست روی مدل ممکن است به حافظه GPU بیشتری نیاز داشته باشد و یا باعث مشکلات عملکردی شود. توصیه می‌شود مورد استفاده خود را با تنظیمات مختلف آزمایش کنید و مصرف حافظه را پایش نمایید تا به بهترین تنظیمات برای نیازهای خاص خود برسید.

## نمونه E2E برای چت‌بات آموزشی تعاملی Phi 3 Mini 4K با Whisper

نوت‌بوک جویتری با عنوان [چت‌بات آموزشی تعاملی Phi 3 Mini 4K با Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) نشان می‌دهد چگونه از دموی Microsoft Phi 3 Mini 4K برای تولید متن از ورودی صوتی یا متنی استفاده کنیم. این نوت‌بوک چندین تابع را تعریف می‌کند:

1. `tts_file_name(text)`: این تابع نام فایلی بر اساس متن ورودی تولید می‌کند تا فایل صوتی تولید شده ذخیره شود.
1. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: این تابع با استفاده از API Edge TTS، یک فایل صوتی از لیستی از قطعات متن ورودی تولید می‌کند. پارامترهای ورودی شامل لیست قطعات، سرعت گفتار، نام صدا و مسیر خروجی برای ذخیره فایل تولید شده است.
1. `talk(input_text)`: این تابع با استفاده از API Edge TTS یک فایل صوتی تولید کرده و آن را با نام تصادفی در دایرکتوری /content/audio ذخیره می‌کند. پارامتر ورودی متن برای تبدیل به گفتار است.
1. `run_text_prompt(message, chat_history)`: این تابع از دموی Microsoft Phi 3 Mini 4K برای تولید یک فایل صوتی از پیام ورودی استفاده کرده و آن را به تاریخچه چت اضافه می‌کند.
1. `run_audio_prompt(audio, chat_history)`: این تابع یک فایل صوتی را با استفاده از API مدل Whisper به متن تبدیل کرده و آن را به تابع `run_text_prompt()` ارسال می‌کند.
1. کد یک اپلیکیشن Gradio را راه‌اندازی می‌کند که به کاربران اجازه می‌دهد با دموی Phi 3 Mini 4K به صورت تایپ پیام‌ها یا آپلود فایل‌های صوتی تعامل داشته باشند. خروجی به صورت پیام متنی درون برنامه نمایش داده می‌شود.

## عیب‌یابی

نصب درایورهای Cuda GPU

1. اطمینان حاصل کنید که نرم‌افزار لینوکس شما به‌روز است

    ```bash
    sudo apt update
    ```

1. نصب درایورهای Cuda

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. ثبت مکان درایور cuda

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. بررسی اندازه حافظه GPU انویدیا (نیازمند ۱۲ گیگابایت حافظه GPU)

    ```bash
    nvidia-smi
    ```

1. خالی کردن کش: اگر از PyTorch استفاده می‌کنید، می‌توانید با فراخوانی torch.cuda.empty_cache() تمام حافظه کش بلااستفاده را آزاد کنید تا توسط برنامه‌های GPU دیگر استفاده شود

    ```python
    torch.cuda.empty_cache() 
    ```

1. بررسی Nvidia Cuda

    ```bash
    nvcc --version
    ```

1. برای ساخت توکن Hugging Face مراحل زیر را انجام دهید:

    - به صفحه [تنظیمات توکن Hugging Face](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo) بروید.
    - گزینه **توکن جدید** را انتخاب کنید.
    - نام پروژه‌ای که می‌خواهید استفاده کنید را وارد کنید.
    - نوع را روی **نوشتن** تنظیم کنید.

> [!NOTE]
>
> اگر با خطای زیر مواجه شدید:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> برای رفع آن، دستور زیر را در ترمینال خود تایپ کنید.
>
> ```bash
> sudo ldconfig
> ```

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما برای دقت تلاش می‌کنیم، لطفاً آگاه باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نادرستی‌هایی باشند. سند اصلی به زبان بومی خود باید به‌عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئول هیچ‌گونه سو تفاهم یا تفسیر نادرستی که از استفاده این ترجمه ناشی شود، نمی‌باشیم.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->