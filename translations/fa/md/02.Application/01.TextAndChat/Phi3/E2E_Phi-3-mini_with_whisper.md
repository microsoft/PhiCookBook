<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "006e8cf75211d3297f24e1b22e38955f",
  "translation_date": "2025-07-17T02:13:56+00:00",
  "source_file": "md/02.Application/01.TextAndChat/Phi3/E2E_Phi-3-mini_with_whisper.md",
  "language_code": "fa"
}
-->
# ربات گفتگو تعاملی Phi 3 Mini 4K Instruct با Whisper

## مرور کلی

ربات گفتگو تعاملی Phi 3 Mini 4K Instruct ابزاری است که به کاربران امکان می‌دهد با نسخه نمایشی Microsoft Phi 3 Mini 4K instruct از طریق ورودی متنی یا صوتی تعامل داشته باشند. این ربات گفتگو برای انجام وظایف مختلفی مانند ترجمه، به‌روزرسانی وضعیت هوا و جمع‌آوری اطلاعات عمومی قابل استفاده است.

### شروع کار

برای استفاده از این ربات گفتگو، کافی است مراحل زیر را دنبال کنید:

1. یک نوت‌بوک جدید [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) باز کنید
2. در پنجره اصلی نوت‌بوک، یک رابط چت باکس شامل کادر ورودی متن و دکمه "Send" مشاهده خواهید کرد.
3. برای استفاده از ربات گفتگو متنی، کافی است پیام خود را در کادر ورودی متن تایپ کرده و روی دکمه "Send" کلیک کنید. ربات گفتگو با یک فایل صوتی پاسخ می‌دهد که می‌توانید مستقیماً در داخل نوت‌بوک آن را پخش کنید.

**توجه**: این ابزار نیازمند GPU و دسترسی به مدل‌های Microsoft Phi-3 و OpenAI Whisper است که برای تشخیص گفتار و ترجمه استفاده می‌شوند.

### نیازمندی‌های GPU

برای اجرای این نسخه نمایشی به ۱۲ گیگابایت حافظه GPU نیاز دارید.

نیازمندی‌های حافظه برای اجرای نسخه نمایشی **Microsoft-Phi-3-Mini-4K instruct** روی GPU به عوامل مختلفی بستگی دارد، مانند اندازه داده ورودی (صوت یا متن)، زبان مورد استفاده برای ترجمه، سرعت مدل و حافظه موجود روی GPU.

به طور کلی، مدل Whisper برای اجرا روی GPU طراحی شده است. حداقل حافظه GPU توصیه شده برای اجرای مدل Whisper، ۸ گیگابایت است، اما در صورت نیاز می‌تواند حافظه بیشتری را مدیریت کند.

مهم است بدانید که اجرای حجم زیادی از داده‌ها یا تعداد زیادی درخواست روی مدل ممکن است به حافظه GPU بیشتری نیاز داشته باشد و/یا باعث بروز مشکلات عملکردی شود. توصیه می‌شود مورد استفاده خود را با پیکربندی‌های مختلف آزمایش کرده و مصرف حافظه را زیر نظر بگیرید تا بهترین تنظیمات را برای نیازهای خاص خود پیدا کنید.

## نمونه E2E برای ربات گفتگو تعاملی Phi 3 Mini 4K Instruct با Whisper

نوت‌بوک Jupyter با عنوان [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) نشان می‌دهد چگونه می‌توان از نسخه نمایشی Microsoft Phi 3 Mini 4K instruct برای تولید متن از ورودی صوتی یا متنی استفاده کرد. این نوت‌بوک چندین تابع را تعریف می‌کند:

1. `tts_file_name(text)`: این تابع نام فایلی بر اساس متن ورودی تولید می‌کند تا فایل صوتی تولید شده را ذخیره کند.
1. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: این تابع از API Edge TTS برای تولید فایل صوتی از لیستی از بخش‌های متن ورودی استفاده می‌کند. پارامترهای ورودی شامل لیست بخش‌ها، سرعت گفتار، نام صدا و مسیر ذخیره فایل صوتی تولید شده است.
1. `talk(input_text)`: این تابع با استفاده از API Edge TTS یک فایل صوتی تولید کرده و آن را با نام تصادفی در دایرکتوری /content/audio ذخیره می‌کند. پارامتر ورودی متن ورودی برای تبدیل به گفتار است.
1. `run_text_prompt(message, chat_history)`: این تابع از نسخه نمایشی Microsoft Phi 3 Mini 4K instruct برای تولید فایل صوتی از پیام ورودی استفاده کرده و آن را به تاریخچه چت اضافه می‌کند.
1. `run_audio_prompt(audio, chat_history)`: این تابع یک فایل صوتی را با استفاده از API مدل Whisper به متن تبدیل کرده و آن را به تابع `run_text_prompt()` ارسال می‌کند.
1. کد یک اپلیکیشن Gradio را راه‌اندازی می‌کند که به کاربران اجازه می‌دهد با نسخه نمایشی Phi 3 Mini 4K instruct از طریق تایپ پیام یا بارگذاری فایل صوتی تعامل داشته باشند. خروجی به صورت پیام متنی در داخل اپ نمایش داده می‌شود.

## رفع اشکال

نصب درایورهای Cuda GPU

1. اطمینان حاصل کنید که برنامه‌های لینوکس شما به‌روز هستند

    ```bash
    sudo apt update
    ```

1. درایورهای Cuda را نصب کنید

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. محل درایور cuda را ثبت کنید

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. بررسی اندازه حافظه GPU انویدیا (نیازمند ۱۲ گیگابایت حافظه GPU)

    ```bash
    nvidia-smi
    ```

1. خالی کردن کش: اگر از PyTorch استفاده می‌کنید، می‌توانید با فراخوانی torch.cuda.empty_cache() تمام حافظه کش استفاده نشده را آزاد کنید تا توسط برنامه‌های دیگر GPU قابل استفاده باشد

    ```python
    torch.cuda.empty_cache() 
    ```

1. بررسی Cuda انویدیا

    ```bash
    nvcc --version
    ```

1. برای ایجاد توکن Hugging Face مراحل زیر را انجام دهید:

    - به صفحه [تنظیمات توکن Hugging Face](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo) بروید.
    - گزینه **New token** را انتخاب کنید.
    - نام پروژه‌ای که می‌خواهید استفاده کنید را وارد کنید.
    - نوع را روی **Write** تنظیم کنید.

> **توجه**
>
> اگر با خطای زیر مواجه شدید:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> برای رفع آن، دستور زیر را در ترمینال خود وارد کنید.
>
> ```bash
> sudo ldconfig
> ```

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما در تلاش برای دقت هستیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نواقصی باشند. سند اصلی به زبان بومی خود باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئول هیچ گونه سوءتفاهم یا تفسیر نادرستی که از استفاده این ترجمه ناشی شود، نیستیم.