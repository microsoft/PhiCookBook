<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "006e8cf75211d3297f24e1b22e38955f",
  "translation_date": "2025-05-07T14:10:14+00:00",
  "source_file": "md/02.Application/01.TextAndChat/Phi3/E2E_Phi-3-mini_with_whisper.md",
  "language_code": "fa"
}
-->
# ربات گفتگوگر تعاملی Phi 3 Mini 4K Instruct با Whisper

## مرور کلی

ربات گفتگوگر تعاملی Phi 3 Mini 4K Instruct ابزاری است که به کاربران اجازه می‌دهد با نسخه نمایشی Microsoft Phi 3 Mini 4K instruct از طریق ورودی متنی یا صوتی تعامل داشته باشند. این ربات برای انجام وظایف مختلفی مانند ترجمه، به‌روزرسانی وضعیت آب و هوا و جمع‌آوری اطلاعات عمومی قابل استفاده است.

### شروع کار

برای استفاده از این ربات گفتگوگر، کافی است مراحل زیر را دنبال کنید:

1. یک [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) جدید باز کنید
2. در پنجره اصلی نوت‌بوک، یک رابط گفتگو با کادر ورودی متن و دکمه "Send" مشاهده خواهید کرد.
3. برای استفاده از ربات گفتگوگر متنی، کافی است پیام خود را در کادر ورودی متن تایپ کرده و روی دکمه "Send" کلیک کنید. ربات پاسخ را به صورت یک فایل صوتی ارائه می‌دهد که می‌توان مستقیماً در داخل نوت‌بوک پخش کرد.

**Note**: این ابزار به یک GPU و دسترسی به مدل‌های Microsoft Phi-3 و OpenAI Whisper نیاز دارد که برای تشخیص گفتار و ترجمه استفاده می‌شوند.

### نیازمندی‌های GPU

برای اجرای این نسخه نمایشی به 12 گیگابایت حافظه GPU نیاز دارید.

نیازمندی‌های حافظه برای اجرای نسخه نمایشی **Microsoft-Phi-3-Mini-4K instruct** روی GPU به عوامل مختلفی بستگی دارد، مانند حجم داده ورودی (صوتی یا متنی)، زبان مورد استفاده برای ترجمه، سرعت مدل و حافظه موجود روی GPU.

به طور کلی، مدل Whisper برای اجرا روی GPU طراحی شده است. حداقل حافظه توصیه شده برای اجرای مدل Whisper، 8 گیگابایت است، اما در صورت نیاز می‌تواند حافظه بیشتری را نیز مدیریت کند.

مهم است بدانید که اجرای حجم زیادی از داده یا تعداد زیادی درخواست ممکن است به حافظه GPU بیشتری نیاز داشته باشد و/یا باعث بروز مشکلات عملکردی شود. توصیه می‌شود مورد استفاده خود را با پیکربندی‌های مختلف آزمایش کرده و مصرف حافظه را نظارت کنید تا بهترین تنظیمات متناسب با نیازهای خاص خود را پیدا کنید.

## نمونه E2E برای ربات گفتگوگر تعاملی Phi 3 Mini 4K Instruct با Whisper

نوت‌بوک Jupyter با عنوان [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) نحوه استفاده از نسخه نمایشی Microsoft Phi 3 Mini 4K instruct برای تولید متن از ورودی صوتی یا متنی را نشان می‌دهد. این نوت‌بوک چندین تابع را تعریف می‌کند:

1. `tts_file_name(text)`: این تابع بر اساس متن ورودی، نام فایلی برای ذخیره فایل صوتی تولید شده ایجاد می‌کند.
1. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: این تابع با استفاده از API Edge TTS، فایل صوتی را از لیستی از بخش‌های متن ورودی تولید می‌کند. پارامترهای ورودی شامل لیست بخش‌ها، سرعت گفتار، نام صدا و مسیر خروجی برای ذخیره فایل صوتی تولید شده است.
1. `talk(input_text)`: این تابع با استفاده از API Edge TTS، فایل صوتی تولید کرده و آن را با نام فایلی تصادفی در دایرکتوری /content/audio ذخیره می‌کند. پارامتر ورودی متن ورودی برای تبدیل به گفتار است.
1. `run_text_prompt(message, chat_history)`: این تابع از نسخه نمایشی Microsoft Phi 3 Mini 4K instruct برای تولید فایل صوتی از پیام ورودی استفاده کرده و آن را به تاریخچه گفتگو اضافه می‌کند.
1. `run_audio_prompt(audio, chat_history)`: این تابع فایل صوتی را با استفاده از API مدل Whisper به متن تبدیل کرده و آن را به تابع `run_text_prompt()` ارسال می‌کند.
1. کد یک برنامه Gradio را اجرا می‌کند که به کاربران اجازه می‌دهد با نسخه نمایشی Phi 3 Mini 4K instruct از طریق تایپ پیام یا بارگذاری فایل صوتی تعامل داشته باشند. خروجی به صورت پیام متنی در داخل برنامه نمایش داده می‌شود.

## عیب‌یابی

نصب درایورهای Cuda GPU

1. اطمینان حاصل کنید که برنامه‌های لینوکس شما به‌روز هستند

    ```bash
    sudo apt update
    ```

1. نصب درایورهای Cuda

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. ثبت محل درایور cuda

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. بررسی اندازه حافظه GPU انویدیا (نیازمند 12 گیگابایت حافظه GPU)

    ```bash
    nvidia-smi
    ```

1. خالی کردن کش: اگر از PyTorch استفاده می‌کنید، می‌توانید با فراخوانی torch.cuda.empty_cache() تمام حافظه کش استفاده‌نشده را آزاد کنید تا توسط سایر برنامه‌های GPU استفاده شود

    ```python
    torch.cuda.empty_cache() 
    ```

1. بررسی Cuda انویدیا

    ```bash
    nvcc --version
    ```

1. برای ایجاد توکن Hugging Face کارهای زیر را انجام دهید:

    - به [صفحه تنظیمات توکن Hugging Face](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo) بروید.
    - گزینه **New token** را انتخاب کنید.
    - نام پروژه‌ای که می‌خواهید استفاده کنید را وارد کنید.
    - نوع را روی **Write** تنظیم کنید.

> **Note**
>
> اگر با خطای زیر مواجه شدید:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> برای رفع آن، دستور زیر را در ترمینال خود وارد کنید.
>
> ```bash
> sudo ldconfig
> ```

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما در تلاش برای دقت هستیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نواقصی باشند. سند اصلی به زبان بومی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما در قبال هرگونه سوء تفاهم یا تفسیر نادرست ناشی از استفاده از این ترجمه مسئولیتی نداریم.