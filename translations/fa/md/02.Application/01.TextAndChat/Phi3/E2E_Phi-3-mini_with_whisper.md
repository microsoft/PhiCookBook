<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "006e8cf75211d3297f24e1b22e38955f",
  "translation_date": "2025-03-27T10:49:38+00:00",
  "source_file": "md\\02.Application\\01.TextAndChat\\Phi3\\E2E_Phi-3-mini_with_whisper.md",
  "language_code": "fa"
}
-->
# چت‌بات تعاملی Phi 3 Mini 4K Instruct با Whisper

## معرفی

چت‌بات تعاملی Phi 3 Mini 4K Instruct ابزاری است که به کاربران اجازه می‌دهد با دمو Microsoft Phi 3 Mini 4K instruct از طریق ورودی متنی یا صوتی تعامل کنند. این چت‌بات می‌تواند برای وظایف مختلفی از جمله ترجمه، به‌روزرسانی وضعیت آب‌وهوا و جمع‌آوری اطلاعات عمومی مورد استفاده قرار گیرد.

### شروع به کار

برای استفاده از این چت‌بات، مراحل زیر را دنبال کنید:

1. یک [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) جدید باز کنید.
2. در پنجره اصلی نوت‌بوک، یک رابط چت با یک جعبه ورودی متن و دکمه "ارسال" مشاهده خواهید کرد.
3. برای استفاده از چت‌بات متنی، پیام خود را در جعبه ورودی متن تایپ کرده و دکمه "ارسال" را کلیک کنید. چت‌بات با یک فایل صوتی پاسخ می‌دهد که می‌توان آن را مستقیماً از داخل نوت‌بوک پخش کرد.

**توجه**: این ابزار نیاز به GPU و دسترسی به مدل‌های Microsoft Phi-3 و OpenAI Whisper دارد که برای تشخیص گفتار و ترجمه استفاده می‌شوند.

### نیازمندی‌های GPU

برای اجرای این دمو به 12 گیگابایت حافظه GPU نیاز دارید.

میزان حافظه موردنیاز برای اجرای دمو **Microsoft-Phi-3-Mini-4K instruct** بر روی GPU به چندین عامل بستگی دارد، از جمله اندازه داده ورودی (صوتی یا متنی)، زبانی که برای ترجمه استفاده می‌شود، سرعت مدل و حافظه موجود روی GPU.

به‌طور کلی، مدل Whisper برای اجرا روی GPU طراحی شده است. حداقل مقدار حافظه GPU پیشنهادی برای اجرای این مدل 8 گیگابایت است، اما در صورت نیاز می‌تواند حافظه بیشتری را مدیریت کند.

توجه داشته باشید که اجرای حجم بالایی از داده‌ها یا درخواست‌ها ممکن است به حافظه GPU بیشتری نیاز داشته باشد و/یا باعث مشکلات عملکردی شود. پیشنهاد می‌شود که مورد استفاده خود را با پیکربندی‌های مختلف تست کرده و مصرف حافظه را برای تعیین تنظیمات بهینه برای نیازهای خاص خود بررسی کنید.

## نمونه E2E برای چت‌بات تعاملی Phi 3 Mini 4K Instruct با Whisper

نوت‌بوک جویتر تحت عنوان [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) نشان می‌دهد که چگونه می‌توان از دمو Microsoft Phi 3 Mini 4K instruct برای تولید متن از ورودی صوتی یا متنی استفاده کرد. این نوت‌بوک چندین تابع را تعریف می‌کند:

1. `tts_file_name(text)`: این تابع نام فایلی بر اساس متن ورودی برای ذخیره فایل صوتی تولید شده ایجاد می‌کند.
2. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: این تابع با استفاده از API Edge TTS یک فایل صوتی از یک لیست تکه‌های متن ورودی تولید می‌کند. پارامترهای ورودی شامل لیست تکه‌ها، نرخ گفتار، نام صدا و مسیر خروجی برای ذخیره فایل صوتی تولید شده هستند.
3. `talk(input_text)`: این تابع یک فایل صوتی با استفاده از API Edge TTS تولید کرده و آن را به یک نام فایل تصادفی در پوشه /content/audio ذخیره می‌کند. پارامتر ورودی متن ورودی برای تبدیل به گفتار است.
4. `run_text_prompt(message, chat_history)`: این تابع با استفاده از دمو Microsoft Phi 3 Mini 4K instruct یک فایل صوتی از یک پیام ورودی تولید کرده و آن را به تاریخچه چت اضافه می‌کند.
5. `run_audio_prompt(audio, chat_history)`: این تابع یک فایل صوتی را با استفاده از API مدل Whisper به متن تبدیل کرده و آن را به تابع `run_text_prompt()` ارسال می‌کند.
6. کد یک اپلیکیشن Gradio را راه‌اندازی می‌کند که به کاربران اجازه می‌دهد با دمو Phi 3 Mini 4K instruct از طریق تایپ پیام‌ها یا آپلود فایل‌های صوتی تعامل کنند. خروجی به‌صورت یک پیام متنی درون اپلیکیشن نمایش داده می‌شود.

## رفع اشکال

نصب درایورهای Cuda GPU

1. اطمینان حاصل کنید که برنامه‌های لینوکس شما به‌روز هستند:

    ```bash
    sudo apt update
    ```

2. نصب درایورهای Cuda:

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

3. ثبت محل درایورهای Cuda:

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

4. بررسی اندازه حافظه Nvidia GPU (نیاز به 12 گیگابایت حافظه GPU):

    ```bash
    nvidia-smi
    ```

5. خالی کردن کش: اگر از PyTorch استفاده می‌کنید، می‌توانید با فراخوانی تابع torch.cuda.empty_cache() تمام حافظه کش نشده استفاده نشده را آزاد کنید تا توسط برنامه‌های دیگر GPU مورد استفاده قرار گیرد.

    ```python
    torch.cuda.empty_cache() 
    ```

6. بررسی Nvidia Cuda:

    ```bash
    nvcc --version
    ```

7. انجام وظایف زیر برای ایجاد یک توکن Hugging Face:

    - به [صفحه تنظیمات توکن Hugging Face](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo) بروید.
    - گزینه **توکن جدید** را انتخاب کنید.
    - **نام پروژه** موردنظر خود را وارد کنید.
    - نوع **Write** را انتخاب کنید.

> **توجه**
>
> اگر با خطای زیر مواجه شدید:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> برای رفع این مشکل، دستور زیر را در ترمینال خود تایپ کنید:
>
> ```bash
> sudo ldconfig
> ```

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادقتی‌هایی باشند. سند اصلی به زبان بومی خود باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه انسانی حرفه‌ای توصیه می‌شود. ما هیچ مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نمی‌پذیریم.