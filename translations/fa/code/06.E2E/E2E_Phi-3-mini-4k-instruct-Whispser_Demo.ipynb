{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## چت‌بات تعاملی Phi 3 Mini 4K Instruct با Whisper\n",
    "\n",
    "### مقدمه:\n",
    "چت‌بات تعاملی Phi 3 Mini 4K Instruct ابزاری است که به کاربران امکان می‌دهد با نسخه نمایشی Microsoft Phi 3 Mini 4K instruct از طریق ورودی متنی یا صوتی تعامل داشته باشند. این چت‌بات می‌تواند برای انجام وظایف مختلفی مانند ترجمه، به‌روزرسانی‌های آب‌وهوا و جمع‌آوری اطلاعات عمومی استفاده شود.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "ایجاد توکن دسترسی Huggingface خود\n",
    "\n",
    "یک توکن جدید ایجاد کنید  \n",
    "یک نام جدید وارد کنید  \n",
    "دسترسی‌های نوشتن را انتخاب کنید  \n",
    "توکن را کپی کرده و در یک مکان امن ذخیره کنید  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "این فایل پایتون دو وظیفه اصلی را انجام می‌دهد: وارد کردن ماژول `os` و تنظیم یک متغیر محیطی.\n",
    "\n",
    "1. وارد کردن ماژول `os`:\n",
    "   - ماژول `os` در پایتون راهی برای تعامل با سیستم‌عامل فراهم می‌کند. این ماژول امکان انجام وظایف مختلف مرتبط با سیستم‌عامل را فراهم می‌کند، مانند دسترسی به متغیرهای محیطی، کار با فایل‌ها و پوشه‌ها و غیره.\n",
    "   - در این کد، ماژول `os` با استفاده از دستور `import` وارد شده است. این دستور قابلیت‌های ماژول `os` را برای استفاده در اسکریپت پایتون فعلی در دسترس قرار می‌دهد.\n",
    "\n",
    "2. تنظیم یک متغیر محیطی:\n",
    "   - متغیر محیطی مقداری است که می‌تواند توسط برنامه‌هایی که روی سیستم‌عامل اجرا می‌شوند، دسترسی پیدا کند. این روشی برای ذخیره تنظیمات پیکربندی یا اطلاعات دیگری است که می‌تواند توسط چندین برنامه استفاده شود.\n",
    "   - در این کد، یک متغیر محیطی جدید با استفاده از دیکشنری `os.environ` تنظیم شده است. کلید این دیکشنری `'HF_TOKEN'` است و مقدار آن از متغیر `HUGGINGFACE_TOKEN` اختصاص داده می‌شود.\n",
    "   - متغیر `HUGGINGFACE_TOKEN` درست بالای این قطعه کد تعریف شده است و با استفاده از سینتکس `#@param` مقدار رشته‌ای `\"hf_**************\"` به آن اختصاص داده شده است. این سینتکس معمولاً در نوت‌بوک‌های Jupyter استفاده می‌شود تا امکان ورودی کاربر و تنظیم پارامترها مستقیماً در رابط نوت‌بوک فراهم شود.\n",
    "   - با تنظیم متغیر محیطی `'HF_TOKEN'`، این متغیر می‌تواند توسط بخش‌های دیگر برنامه یا برنامه‌های دیگر که روی همان سیستم‌عامل اجرا می‌شوند، دسترسی پیدا کند.\n",
    "\n",
    "به طور کلی، این کد ماژول `os` را وارد می‌کند و یک متغیر محیطی به نام `'HF_TOKEN'` را با مقداری که در متغیر `HUGGINGFACE_TOKEN` ارائه شده است، تنظیم می‌کند.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "این قطعه کد یک تابع به نام clear_output تعریف می‌کند که برای پاک کردن خروجی سلول فعلی در Jupyter Notebook یا IPython استفاده می‌شود. بیایید کد را بررسی کنیم و عملکرد آن را درک کنیم:\n",
    "\n",
    "تابع clear_output یک پارامتر به نام wait می‌گیرد که یک مقدار بولی است. به طور پیش‌فرض، مقدار wait برابر با False تنظیم شده است. این پارامتر تعیین می‌کند که آیا تابع باید منتظر بماند تا خروجی جدید آماده شود و جایگزین خروجی موجود شود، قبل از اینکه آن را پاک کند یا خیر.\n",
    "\n",
    "خود تابع برای پاک کردن خروجی سلول فعلی استفاده می‌شود. در Jupyter Notebook یا IPython، زمانی که یک سلول خروجی تولید می‌کند، مانند متن چاپ شده یا نمودارهای گرافیکی، آن خروجی زیر سلول نمایش داده می‌شود. تابع clear_output به شما امکان می‌دهد تا آن خروجی را پاک کنید.\n",
    "\n",
    "پیاده‌سازی تابع در قطعه کد ارائه نشده است، همان‌طور که با علامت سه نقطه (...) نشان داده شده است. این علامت سه نقطه به عنوان یک جایگزین برای کد واقعی که عملیات پاک کردن خروجی را انجام می‌دهد، استفاده شده است. پیاده‌سازی تابع ممکن است شامل تعامل با API Jupyter Notebook یا IPython برای حذف خروجی موجود از سلول باشد.\n",
    "\n",
    "به طور کلی، این تابع یک روش راحت برای پاک کردن خروجی سلول فعلی در Jupyter Notebook یا IPython فراهم می‌کند و مدیریت و به‌روزرسانی خروجی نمایش داده شده را در جلسات کدنویسی تعاملی آسان‌تر می‌کند.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "استفاده از سرویس تبدیل متن به گفتار (TTS) با Edge TTS. بیایید پیاده‌سازی‌های مربوط به هر تابع را به صورت جداگانه بررسی کنیم:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: این تابع یک مقدار ورودی دریافت می‌کند و رشته نرخ برای صدای TTS را محاسبه می‌کند. مقدار ورودی سرعت مورد نظر گفتار را نشان می‌دهد، که مقدار 1 سرعت معمولی را نشان می‌دهد. این تابع رشته نرخ را با کم کردن 1 از مقدار ورودی، ضرب کردن آن در 100، و سپس تعیین علامت بر اساس اینکه مقدار ورودی بزرگتر یا مساوی 1 است، محاسبه می‌کند. این تابع رشته نرخ را در قالب \"{sign}{rate}\" برمی‌گرداند.\n",
    "\n",
    "2. `make_chunks(input_text, language)`: این تابع متن ورودی و زبان را به عنوان پارامتر دریافت می‌کند. متن ورودی را بر اساس قوانین خاص زبان به بخش‌هایی تقسیم می‌کند. در این پیاده‌سازی، اگر زبان \"انگلیسی\" باشد، تابع متن را در هر نقطه (\".\") تقسیم می‌کند و هر فضای اضافی در ابتدا یا انتهای متن را حذف می‌کند. سپس یک نقطه به هر بخش اضافه می‌کند و لیست فیلتر شده بخش‌ها را برمی‌گرداند.\n",
    "\n",
    "3. `tts_file_name(text)`: این تابع نام فایل صوتی TTS را بر اساس متن ورودی تولید می‌کند. چندین تغییر روی متن انجام می‌دهد: حذف نقطه پایانی (اگر وجود داشته باشد)، تبدیل متن به حروف کوچک، حذف فضای اضافی در ابتدا و انتها، و جایگزینی فضاها با زیرخط. سپس متن را به حداکثر 25 کاراکتر کوتاه می‌کند (اگر طولانی‌تر باشد) یا از متن کامل استفاده می‌کند اگر خالی باشد. در نهایت، یک رشته تصادفی با استفاده از ماژول [`uuid`] تولید می‌کند و آن را با متن کوتاه شده ترکیب می‌کند تا نام فایل را در قالب \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\" ایجاد کند.\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: این تابع چندین فایل صوتی را به یک فایل صوتی واحد ترکیب می‌کند. لیستی از مسیرهای فایل صوتی و یک مسیر خروجی را به عنوان پارامتر دریافت می‌کند. تابع یک شیء خالی [`AudioSegment`] به نام [`merged_audio`] ایجاد می‌کند. سپس از طریق هر مسیر فایل صوتی تکرار می‌کند، فایل صوتی را با استفاده از متد `AudioSegment.from_file()` از کتابخانه `pydub` بارگذاری می‌کند، و فایل صوتی فعلی را به شیء [`merged_audio`] اضافه می‌کند. در نهایت، فایل صوتی ترکیب شده را به مسیر خروجی مشخص شده در قالب MP3 صادر می‌کند.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: این تابع عملیات TTS را با استفاده از سرویس Edge TTS انجام می‌دهد. لیستی از بخش‌های متن، سرعت گفتار، نام صدا، و مسیر ذخیره‌سازی را به عنوان پارامتر دریافت می‌کند. اگر تعداد بخش‌ها بیشتر از 1 باشد، تابع یک دایرکتوری برای ذخیره فایل‌های صوتی بخش‌های جداگانه ایجاد می‌کند. سپس از طریق هر بخش تکرار می‌کند، یک دستور Edge TTS با استفاده از تابع `calculate_rate_string()`، نام صدا، و متن بخش ایجاد می‌کند، و دستور را با استفاده از تابع `os.system()` اجرا می‌کند. اگر اجرای دستور موفقیت‌آمیز باشد، مسیر فایل صوتی تولید شده را به لیستی اضافه می‌کند. پس از پردازش تمام بخش‌ها، فایل‌های صوتی جداگانه را با استفاده از تابع `merge_audio_files()` ترکیب می‌کند و فایل صوتی ترکیب شده را به مسیر ذخیره‌سازی مشخص شده ذخیره می‌کند. اگر فقط یک بخش وجود داشته باشد، مستقیماً دستور Edge TTS را تولید می‌کند و فایل صوتی را به مسیر ذخیره‌سازی ذخیره می‌کند. در نهایت، مسیر ذخیره‌سازی فایل صوتی تولید شده را برمی‌گرداند.\n",
    "\n",
    "6. `random_audio_name_generate()`: این تابع یک نام فایل صوتی تصادفی با استفاده از ماژول [`uuid`] تولید می‌کند. یک UUID تصادفی تولید می‌کند، آن را به رشته تبدیل می‌کند، 8 کاراکتر اول را می‌گیرد، پسوند \".mp3\" را اضافه می‌کند، و نام فایل صوتی تصادفی را برمی‌گرداند.\n",
    "\n",
    "7. `talk(input_text)`: این تابع نقطه ورود اصلی برای انجام عملیات TTS است. یک متن ورودی را به عنوان پارامتر دریافت می‌کند. ابتدا طول متن ورودی را بررسی می‌کند تا تعیین کند آیا جمله طولانی است (بزرگتر یا مساوی 600 کاراکتر). بر اساس طول و مقدار متغیر `translate_text_flag`، زبان را تعیین می‌کند و لیست بخش‌های متن را با استفاده از تابع `make_chunks()` تولید می‌کند. سپس یک مسیر ذخیره‌سازی برای فایل صوتی با استفاده از تابع `random_audio_name_generate()` تولید می‌کند. در نهایت، تابع `edge_free_tts()` را برای انجام عملیات TTS فراخوانی می‌کند و مسیر ذخیره‌سازی فایل صوتی تولید شده را برمی‌گرداند.\n",
    "\n",
    "به طور کلی، این توابع با هم کار می‌کنند تا متن ورودی را به بخش‌هایی تقسیم کنند، نام فایل صوتی را تولید کنند، عملیات TTS را با استفاده از سرویس Edge TTS انجام دهند، و فایل‌های صوتی جداگانه را به یک فایل صوتی واحد ترکیب کنند.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "توضیح پیاده‌سازی دو تابع: convert_to_text و run_text_prompt، و همچنین تعریف دو کلاس: str و Audio.\n",
    "\n",
    "تابع convert_to_text یک مسیر صوتی (audio_path) را به عنوان ورودی دریافت می‌کند و با استفاده از مدلی به نام whisper_model، صوت را به متن تبدیل می‌کند. ابتدا بررسی می‌شود که آیا پرچم gpu روی True تنظیم شده است یا خیر. اگر True باشد، whisper_model با پارامترهایی مانند word_timestamps=True، fp16=True، language='English' و task='translate استفاده می‌شود. اگر gpu روی False تنظیم شده باشد، whisper_model با fp16=False استفاده می‌شود. متن تبدیل‌شده سپس در فایلی به نام 'scan.txt' ذخیره شده و به عنوان خروجی بازگردانده می‌شود.\n",
    "\n",
    "تابع run_text_prompt یک پیام و یک تاریخچه چت (chat_history) را به عنوان ورودی دریافت می‌کند. این تابع از تابع phi_demo برای تولید پاسخ از یک چت‌بات بر اساس پیام ورودی استفاده می‌کند. پاسخ تولید‌شده سپس به تابع talk ارسال می‌شود که پاسخ را به یک فایل صوتی تبدیل کرده و مسیر فایل را بازمی‌گرداند. کلاس Audio برای نمایش و پخش فایل صوتی استفاده می‌شود. صوت با استفاده از تابع display از ماژول IPython.display نمایش داده می‌شود و شیء Audio با پارامتر autoplay=True ایجاد می‌شود تا صوت به صورت خودکار پخش شود. تاریخچه چت با پیام ورودی و پاسخ تولید‌شده به‌روزرسانی می‌شود و یک رشته خالی و تاریخچه چت به‌روزرسانی‌شده بازگردانده می‌شود.\n",
    "\n",
    "کلاس str یک کلاس داخلی در پایتون است که نمایانگر یک رشته از کاراکترها است. این کلاس روش‌های مختلفی برای کار با رشته‌ها ارائه می‌دهد، مانند capitalize، casefold، center، count، encode، endswith، expandtabs، find، format، index، isalnum، isalpha، isascii، isdecimal، isdigit، isidentifier، islower، isnumeric، isprintable، isspace، istitle، isupper، join، ljust، lower، lstrip، partition، replace، removeprefix، removesuffix، rfind، rindex، rjust، rpartition، rsplit، rstrip، split، splitlines، startswith، strip، swapcase، title، translate، upper، zfill و موارد دیگر. این روش‌ها امکان انجام عملیات‌هایی مانند جستجو، جایگزینی، قالب‌بندی و دستکاری رشته‌ها را فراهم می‌کنند.\n",
    "\n",
    "کلاس Audio یک کلاس سفارشی است که نمایانگر یک شیء صوتی است. این کلاس برای ایجاد یک پخش‌کننده صوتی در محیط Jupyter Notebook استفاده می‌شود. کلاس Audio پارامترهای مختلفی مانند data، filename، url، embed، rate، autoplay و normalize را می‌پذیرد. پارامتر data می‌تواند یک آرایه numpy، یک لیست از نمونه‌ها، یک رشته نمایانگر نام فایل یا URL، یا داده خام PCM باشد. پارامتر filename برای مشخص کردن یک فایل محلی جهت بارگذاری داده‌های صوتی استفاده می‌شود و پارامتر url برای مشخص کردن یک URL جهت دانلود داده‌های صوتی استفاده می‌شود. پارامتر embed تعیین می‌کند که آیا داده‌های صوتی باید با استفاده از یک URI داده جاسازی شوند یا از منبع اصلی ارجاع شوند. پارامتر rate نرخ نمونه‌برداری داده‌های صوتی را مشخص می‌کند. پارامتر autoplay تعیین می‌کند که آیا صوت باید به صورت خودکار پخش شود یا خیر. پارامتر normalize مشخص می‌کند که آیا داده‌های صوتی باید به محدوده ممکن حداکثر مقیاس‌بندی شوند یا خیر. کلاس Audio همچنین روش‌هایی مانند reload برای بارگذاری مجدد داده‌های صوتی از فایل یا URL و ویژگی‌هایی مانند src_attr، autoplay_attr و element_id_attr برای بازیابی ویژگی‌های مربوط به عنصر صوتی در HTML ارائه می‌دهد.\n",
    "\n",
    "به طور کلی، این توابع و کلاس‌ها برای تبدیل صوت به متن، تولید پاسخ‌های صوتی از یک چت‌بات، و نمایش و پخش صوت در محیط Jupyter Notebook استفاده می‌شوند.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**سلب مسئولیت**:  \nاین سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T16:38:58+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "fa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}