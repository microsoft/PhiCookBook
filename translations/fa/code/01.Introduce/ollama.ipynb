{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama + OpenAI + Python\n",
    "\n",
    "## 1. مشخص کردن نام مدل\n",
    "\n",
    "اگر مدل دیگری به جز \"phi3:mini\" را وارد کرده‌اید، مقدار موجود در سلول زیر را تغییر دهید. این متغیر در کدهای سراسر نوت‌بوک استفاده خواهد شد.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"phi3:mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ۲. تنظیم کلاینت Open AI\n",
    "\n",
    "به طور معمول کلاینت OpenAI برای تعامل با مدل‌های زبان بزرگ از OpenAI.com یا Azure OpenAI استفاده می‌شود.  \n",
    "با این حال، می‌توان از آن با Ollama نیز استفاده کرد، زیرا Ollama یک نقطه پایانی سازگار با OpenAI در \"http://localhost:11434/v1\" ارائه می‌دهد.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"nokeyneeded\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. تولید یک پاسخ چت\n",
    "\n",
    "اکنون می‌توانیم از OpenAI SDK برای تولید پاسخ یک مکالمه استفاده کنیم. این درخواست باید یک هایکو درباره گربه‌ها ایجاد کند:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about a hungry cat\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. مهندسی درخواست\n",
    "\n",
    "اولین پیامی که به مدل زبان ارسال می‌شود \"پیام سیستم\" یا \"درخواست سیستم\" نامیده می‌شود و دستورالعمل‌های کلی برای مدل را تعیین می‌کند.  \n",
    "شما می‌توانید درخواست سیستم خود را سفارشی کنید تا مدل زبان را به تولید خروجی به شیوه‌ای متفاوت هدایت کنید.  \n",
    "`SYSTEM_MESSAGE` زیر را تغییر دهید تا مدل به سبک شخصیت مورد علاقه شما از یک فیلم یا سریال معروف پاسخ دهد، یا از درخواست‌های دیگر در [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts?tab=readme-ov-file#prompts) الهام بگیرید.\n",
    "\n",
    "پس از سفارشی کردن پیام سیستم، اولین سوال کاربر را در `USER_MESSAGE` ارائه دهید.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "I want you to act like Elmo from Sesame Street.\n",
    "I want you to respond and answer like Elmo using the tone, manner and vocabulary that Elmo would use.\n",
    "Do not write any explanations. Only answer like Elmo.\n",
    "You must know all of the knowledge of Elmo, and nothing more.\n",
    "\"\"\"\n",
    "\n",
    "USER_MESSAGE = \"\"\"\n",
    "Hi Elmo, how are you doing today?\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ۵. مثال‌های محدود\n",
    "\n",
    "یکی دیگر از روش‌های هدایت مدل زبان، ارائه \"چند مثال\" است، یعنی یک سری سوال/پاسخ نمونه که نشان می‌دهد مدل چگونه باید پاسخ دهد.\n",
    "\n",
    "مثال زیر تلاش می‌کند تا مدل زبان را به عنوان یک دستیار آموزشی شبیه‌سازی کند، با ارائه چند نمونه از سوالات و پاسخ‌هایی که ممکن است یک دستیار آموزشی بدهد، و سپس مدل را با سوالی که ممکن است یک دانش‌آموز بپرسد، تحریک می‌کند.\n",
    "\n",
    "ابتدا آن را امتحان کنید، سپس `SYSTEM_MESSAGE`، `EXAMPLES` و `USER_MESSAGE` را برای یک سناریوی جدید تغییر دهید.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are a helpful assistant that helps students with their homework.\n",
    "Instead of providing the full answer, you respond with a hint or a clue.\n",
    "\"\"\"\n",
    "\n",
    "EXAMPLES = [\n",
    "    (\n",
    "        \"What is the capital of France?\",\n",
    "        \"Can you remember the name of the city that is known for the Eiffel Tower?\"\n",
    "    ),\n",
    "    (\n",
    "        \"What is the square root of 144?\",\n",
    "        \"What number multiplied by itself equals 144?\"\n",
    "    ),\n",
    "    (   \"What is the atomic number of oxygen?\",\n",
    "        \"How many protons does an oxygen atom have?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "USER_MESSAGE = \"What is the largest planet in our solar system?\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": EXAMPLES[0][0]},\n",
    "        {\"role\": \"assistant\", \"content\": EXAMPLES[0][1]},\n",
    "        {\"role\": \"user\", \"content\": EXAMPLES[1][0]},\n",
    "        {\"role\": \"assistant\", \"content\": EXAMPLES[1][1]},\n",
    "        {\"role\": \"user\", \"content\": EXAMPLES[2][0]},\n",
    "        {\"role\": \"assistant\", \"content\": EXAMPLES[2][1]},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE},\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. تولید مبتنی بر بازیابی\n",
    "\n",
    "RAG (تولید مبتنی بر بازیابی) یک تکنیک است که به مدل‌های زبانی کمک می‌کند تا به سوالات مربوط به یک حوزه خاص با دقت پاسخ دهند. این کار با بازیابی اطلاعات مرتبط از یک منبع دانش و سپس تولید پاسخ بر اساس آن اطلاعات انجام می‌شود.\n",
    "\n",
    "ما یک فایل CSV محلی با داده‌هایی درباره خودروهای هیبریدی فراهم کرده‌ایم. کد زیر فایل CSV را می‌خواند، به دنبال تطابق‌هایی با سوال کاربر می‌گردد و سپس بر اساس اطلاعات یافت‌شده پاسخ تولید می‌کند. توجه داشته باشید که این فرآیند نسبت به مثال‌های قبلی زمان بیشتری می‌برد، زیرا داده‌های بیشتری به مدل ارسال می‌شود. اگر متوجه شدید که پاسخ هنوز بر اساس داده‌ها نیست، می‌توانید مهندسی سیستم را امتحان کنید یا از مدل‌های دیگر استفاده کنید. به طور کلی، RAG با مدل‌های بزرگ‌تر یا نسخه‌های تنظیم‌شده SLM‌ها مؤثرتر است.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are a helpful assistant that answers questions about cars based off a hybrid car data set.\n",
    "You must use the data set to answer the questions, you should not provide any information that is not in the provided sources.\n",
    "\"\"\"\n",
    "\n",
    "USER_MESSAGE = \"how fast is a prius?\"\n",
    "\n",
    "# Open the CSV and store in a list\n",
    "with open(\"hybrid.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    rows = list(reader)\n",
    "\n",
    "# Normalize the user question to replace punctuation and make lowercase\n",
    "normalized_message = USER_MESSAGE.lower().replace(\"?\", \"\").replace(\"(\", \" \").replace(\")\", \" \")\n",
    "\n",
    "# Search the CSV for user question using very naive search\n",
    "words = normalized_message.split()\n",
    "matches = []\n",
    "for row in rows[1:]:\n",
    "    # if the word matches any word in row, add the row to the matches\n",
    "    if any(word in row[0].lower().split() for word in words) or any(word in row[5].lower().split() for word in words):\n",
    "        matches.append(row)\n",
    "\n",
    "# Format as a markdown table, since language models understand markdown\n",
    "matches_table = \" | \".join(rows[0]) + \"\\n\" + \" | \".join(\" --- \" for _ in range(len(rows[0]))) + \"\\n\"\n",
    "matches_table += \"\\n\".join(\" | \".join(row) for row in matches)\n",
    "print(f\"Found {len(matches)} matches:\")\n",
    "print(matches_table)\n",
    "\n",
    "# Now we can use the matches to generate a response\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE + \"\\nSources: \" + matches_table},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**سلب مسئولیت**:  \nاین سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده کنید. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "coopTranslator": {
   "original_hash": "6f9e40a7dbbd892aae50aff77da4b4be",
   "translation_date": "2025-09-12T16:55:58+00:00",
   "source_file": "code/01.Introduce/ollama.ipynb",
   "language_code": "fa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}