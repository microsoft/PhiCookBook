<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-05-08T06:09:38+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "tw"
}
-->
# **量化 Phi 家族**

模型量化是指將神經網路模型中的參數（例如權重和激活值）從較大的數值範圍（通常是連續數值範圍）映射到較小的有限數值範圍的過程。這項技術可以減少模型的大小和計算複雜度，並提升模型在資源受限環境（如行動裝置或嵌入式系統）中的運行效率。模型量化透過降低參數的精度來達成壓縮，但同時也會引入一定的精度損失。因此，在量化過程中，需要在模型大小、計算複雜度和精度之間取得平衡。常見的量化方法包括定點量化、浮點量化等。你可以根據具體場景和需求選擇合適的量化策略。

我們希望將 GenAI 模型部署到邊緣裝置，讓更多裝置能進入 GenAI 場景，例如行動裝置、AI PC/Copilot+PC 以及傳統 IoT 裝置。透過量化模型，我們可以依據不同裝置部署到不同邊緣裝置。結合硬體廠商提供的模型加速框架和量化模型，我們能打造更好的 SLM 應用場景。

在量化場景中，我們有不同的精度選擇（INT4、INT8、FP16、FP32）。以下是常用量化精度的說明。

### **INT4**

INT4 量化是一種激進的量化方法，將模型的權重和激活值量化成 4 位元整數。由於表示範圍較小且精度較低，INT4 量化通常會帶來較大的精度損失。但相比 INT8 量化，INT4 可以進一步降低模型的儲存需求和計算複雜度。需要注意的是，INT4 量化在實際應用中相對罕見，因為精度過低可能導致模型性能明顯下降。此外，並非所有硬體都支援 INT4 運算，選擇量化方法時需考慮硬體相容性。

### **INT8**

INT8 量化是將模型的權重和激活值從浮點數轉換為 8 位元整數的過程。雖然 INT8 整數所代表的數值範圍較小且精度較低，但能顯著降低儲存和計算需求。在 INT8 量化中，模型的權重和激活值會經過量化處理，包括縮放和偏移，以盡可能保留原始浮點資訊。推論時，這些量化值會被反量化回浮點數進行計算，接著再量化回 INT8 進行下一步。這種方法在多數應用中能提供足夠的精度，同時保持高效的計算效率。

### **FP16**

FP16 格式，即 16 位元浮點數（float16），相較於 32 位元浮點數（float32）可將記憶體佔用減半，在大規模深度學習應用中具有顯著優勢。FP16 格式允許在相同 GPU 記憶體限制下載入更大的模型或處理更多資料。隨著現代 GPU 硬體持續支援 FP16 運算，使用 FP16 格式也可能提升計算速度。不過，FP16 格式也有其固有缺點，即精度較低，某些情況下可能導致數值不穩定或精度損失。

### **FP32**

FP32 格式提供較高精度，能準確表示廣泛的數值範圍。在進行複雜數學運算或需要高精度結果的場景中，FP32 格式是首選。不過，高精度也意味著較多的記憶體使用和較長的計算時間。對於大型深度學習模型，特別是模型參數眾多且資料量龐大時，FP32 格式可能導致 GPU 記憶體不足或推論速度下降。

在行動裝置或 IoT 裝置上，我們可以將 Phi-3.x 模型轉換為 INT4，而 AI PC / Copilot PC 則可以使用較高精度，如 INT8、FP16、FP32。

目前，不同硬體廠商有不同的框架支援生成模型，例如 Intel 的 OpenVINO、Qualcomm 的 QNN、Apple 的 MLX 以及 Nvidia 的 CUDA 等，結合模型量化完成本地部署。

在技術層面，我們有不同的量化後格式支援，如 PyTorch / Tensorflow 格式、GGUF 與 ONNX。我做過 GGUF 與 ONNX 的格式比較與應用場景介紹。這裡推薦 ONNX 量化格式，因為它從模型框架到硬體都有良好支援。本章將聚焦於使用 ONNX Runtime for GenAI、OpenVINO 與 Apple MLX 進行模型量化（如果你有更好的方法，也歡迎透過提交 PR 分享）。

**本章內容包含**

1. [使用 llama.cpp 量化 Phi-3.5 / 4](./UsingLlamacppQuantifyingPhi.md)

2. [使用 Generative AI extensions for onnxruntime 量化 Phi-3.5 / 4](./UsingORTGenAIQuantifyingPhi.md)

3. [使用 Intel OpenVINO 量化 Phi-3.5 / 4](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [使用 Apple MLX Framework 量化 Phi-3.5 / 4](./UsingAppleMLXQuantifyingPhi.md)

**免責聲明**：  
本文件係使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 所翻譯。雖然我們致力於翻譯的準確性，但請注意自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應視為權威依據。對於重要資訊，建議尋求專業人工翻譯。我們不對因使用本翻譯所產生的任何誤解或誤釋負責。