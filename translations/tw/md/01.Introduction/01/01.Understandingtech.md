<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-08T06:16:05+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "tw"
}
-->
# 主要提到的關鍵技術包括

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - 一個基於 DirectX 12 的低階硬體加速機器學習 API。
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - 由 Nvidia 開發的平行運算平台和應用程式介面（API）模型，讓圖形處理器（GPU）能進行通用運算。
3. [ONNX](https://onnx.ai/)（Open Neural Network Exchange）- 一種用來表示機器學習模型的開放格式，提供不同 ML 框架間的互通性。
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)（Generic Graph Update Format）- 一種用於表示和更新機器學習模型的格式，特別適合可在 CPU 上以 4-8bit 量化有效運行的小型語言模型。

## DirectML

DirectML 是一個低階 API，能啟用硬體加速的機器學習。它建立於 DirectX 12 之上，利用 GPU 加速，且不依賴特定廠商，意即不用修改程式碼就能在不同 GPU 廠商間運作。主要用於 GPU 上的模型訓練與推論工作負載。

關於硬體支援，DirectML 設計能支援多種 GPU，包括 AMD 的整合與獨立 GPU、Intel 整合 GPU，以及 NVIDIA 獨立 GPU。它是 Windows AI 平台的一部分，支援 Windows 10 和 11，能在任何 Windows 裝置上進行模型訓練與推論。

DirectML 也有持續更新與機會，例如支援多達 150 個 ONNX 運算子，並被 ONNX runtime 和 WinML 使用。它由主要的整合硬體廠商（IHVs）支持，各廠商實作不同的 metacommands。

## CUDA

CUDA，全名 Compute Unified Device Architecture，是 Nvidia 創造的平行運算平台和 API 模型。它允許軟體開發者利用支援 CUDA 的 GPU 進行通用運算，這種做法稱為 GPGPU（通用圖形處理器運算）。CUDA 是 Nvidia GPU 加速的關鍵技術，廣泛應用於機器學習、科學計算和影像處理等領域。

CUDA 的硬體支援專屬於 Nvidia GPU，因為它是 Nvidia 的專有技術。每種架構支援特定版本的 CUDA 工具包，該工具包提供開發者所需的函式庫和工具來建構和執行 CUDA 應用程式。

## ONNX

ONNX（Open Neural Network Exchange）是一種用來表示機器學習模型的開放格式。它定義了可擴充的計算圖模型，以及內建運算子和標準資料類型的定義。ONNX 讓開發者能在不同 ML 框架間移動模型，促進互通性，並使建立與部署 AI 應用更容易。

Phi3 mini 能在 CPU 和 GPU 上使用 ONNX Runtime 運行，支援伺服器平台、Windows、Linux、Mac 桌面和行動 CPU。
我們新增的優化配置包括

- 針對 int4 DML 的 ONNX 模型：透過 AWQ 量化為 int4
- fp16 CUDA 的 ONNX 模型
- int4 CUDA 的 ONNX 模型：透過 RTN 量化為 int4
- int4 CPU 和行動裝置的 ONNX 模型：透過 RTN 量化為 int4

## Llama.cpp

Llama.cpp 是用 C++ 撰寫的開源軟體庫。它對多種大型語言模型（LLMs）進行推論，包括 Llama。此軟體與 ggml 函式庫（通用張量函式庫）一同開發，目標是比原始 Python 實作提供更快的推論速度與更低的記憶體使用。它支援硬體優化、量化，並提供簡單的 API 和範例。如果你對高效的 LLM 推論有興趣，llama.cpp 非常值得一試，因為 Phi3 可執行 Llama.cpp。

## GGUF

GGUF（Generic Graph Update Format）是一種用於表示和更新機器學習模型的格式。它特別適合可在 CPU 上以 4-8bit 量化有效運行的小型語言模型（SLMs）。GGUF 有助於快速原型開發，並適用於邊緣裝置或像 CI/CD 流程中批次作業的模型執行。

**免責聲明**：  
本文件係使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 所翻譯。雖然我們致力於提供準確的翻譯，但請注意，自動翻譯可能會包含錯誤或不準確之處。原始文件的母語版本應視為權威來源。對於重要資訊，建議採用專業人工翻譯。我們不對因使用本翻譯而產生的任何誤解或誤譯負責。