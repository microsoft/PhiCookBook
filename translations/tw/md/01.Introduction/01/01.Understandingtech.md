<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "583e1ebd3884b47b43c883072eb8fa03",
  "translation_date": "2025-04-04T05:44:48+00:00",
  "source_file": "md\\01.Introduction\\01\\01.Understandingtech.md",
  "language_code": "tw"
}
-->
# 提到的關鍵技術包括

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - 一個基於 DirectX 12 的底層 API，用於硬體加速的機器學習。
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia 開發的平行運算平台和應用程式介面 (API) 模型，支持在圖形處理器 (GPU) 上進行通用處理。
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - 一種開放格式，用於表示機器學習模型，提供不同 ML 框架之間的互操作性。
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - 一種用於表示和更新機器學習模型的格式，特別適合能在 CPU 上有效運行的小型語言模型，支持 4-8 位量化。

## DirectML

DirectML 是一個底層 API，能夠實現硬體加速的機器學習。它基於 DirectX 12 構建，利用 GPU 加速，並且是供應商無關的，這意味著它不需要修改代碼即可在不同的 GPU 供應商上運行。主要用於 GPU 上的模型訓練和推理工作負載。

在硬體支持方面，DirectML 設計能夠兼容多種 GPU，包括 AMD 的集成和獨立 GPU、Intel 的集成 GPU，以及 NVIDIA 的獨立 GPU。它是 Windows AI 平台的一部分，支持 Windows 10 和 11，能夠在任何 Windows 設備上進行模型訓練和推理。

DirectML 提供了一些更新和機會，例如支持高達 150 個 ONNX 操作符，並被 ONNX runtime 和 WinML 使用。它得到了主要整合硬體供應商 (IHVs) 的支持，每家供應商都實現了各種元命令。

## CUDA

CUDA，全名為 Compute Unified Device Architecture，是由 Nvidia 開發的一個平行運算平台和應用程式介面 (API) 模型。它允許軟體開發人員使用支持 CUDA 的圖形處理器 (GPU) 進行通用處理，這種方法被稱為 GPGPU (通用圖形處理器運算)。CUDA 是 Nvidia GPU 加速的核心技術，被廣泛應用於機器學習、科學計算和視頻處理等領域。

CUDA 的硬體支持特定於 Nvidia 的 GPU，因為它是一種由 Nvidia 開發的專有技術。每種架構支持特定版本的 CUDA 工具包，該工具包提供了必要的庫和工具，供開發人員構建和運行 CUDA 應用程式。

## ONNX

ONNX (Open Neural Network Exchange) 是一種開放格式，用於表示機器學習模型。它提供了可擴展的計算圖模型定義，以及內建操作符和標準數據類型的定義。ONNX 允許開發人員在不同的 ML 框架之間移動模型，實現互操作性，使創建和部署 AI 應用程式變得更加容易。

Phi3 mini 可以在 CPU 和 GPU 上使用 ONNX Runtime，支持多種設備，包括伺服器平台、Windows、Linux 和 Mac 桌面，以及移動 CPU。
我們添加的優化配置包括：

- 適用於 int4 DML 的 ONNX 模型：通過 AWQ 量化為 int4
- 適用於 fp16 CUDA 的 ONNX 模型
- 適用於 int4 CUDA 的 ONNX 模型：通過 RTN 量化為 int4
- 適用於 int4 CPU 和移動設備的 ONNX 模型：通過 RTN 量化為 int4

## Llama.cpp

Llama.cpp 是一個用 C++ 編寫的開源軟體庫。它能夠對多種大型語言模型 (LLMs) 進行推理，包括 Llama。Llama.cpp 與 ggml 庫（通用張量庫）共同開發，旨在提供比原始 Python 實現更快的推理速度和更低的內存使用量。它支持硬體優化、量化，並提供簡單的 API 和範例。如果你對高效的 LLM 推理感興趣，Llama.cpp 值得探索，因為 Phi3 可以運行 Llama.cpp。

## GGUF

GGUF (Generic Graph Update Format) 是一種用於表示和更新機器學習模型的格式。它特別適合能在 CPU 上有效運行的小型語言模型 (SLMs)，支持 4-8 位量化。GGUF 對於快速原型設計以及在邊緣設備或批量作業（如 CI/CD 管道）中運行模型非常有用。

**免責聲明**:  
本文件使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。雖然我們致力於提高翻譯的準確性，但請注意，自動翻譯可能包含錯誤或不精確之處。原始語言的文件應被視為權威來源。對於關鍵信息，建議尋求專業人工翻譯。我們不對因使用此翻譯而產生的任何誤解或錯誤解釋負責。