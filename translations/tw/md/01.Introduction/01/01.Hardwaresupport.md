<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8cdc17ce0f10535da30b53d23fe1a795",
  "translation_date": "2025-07-16T18:23:44+00:00",
  "source_file": "md/01.Introduction/01/01.Hardwaresupport.md",
  "language_code": "tw"
}
-->
# Phi 硬體支援

Microsoft Phi 已針對 ONNX Runtime 進行優化，並支援 Windows DirectML。它能在多種硬體類型上良好運作，包括 GPU、CPU，甚至是行動裝置。

## 裝置硬體  
具體支援的硬體包括：

- GPU 型號：RTX 4090（DirectML）
- GPU 型號：1 個 A100 80GB（CUDA）
- CPU 型號：Standard F64s v2（64 個 vCPU，128 GiB 記憶體）

## 行動裝置型號

- Android - Samsung Galaxy S21
- Apple iPhone 14 或更新款 A16/A17 處理器

## Phi 硬體規格

- 最低需求配置。
- Windows：支援 DirectX 12 的 GPU，且合計記憶體至少 4GB

CUDA：NVIDIA GPU，計算能力 >= 7.02

![HardwareSupport](../../../../../translated_images/01.phihardware.5d51b2377cba18afc6949074542f290c56bb278dac3f4f86302aca6d80fffeb9.tw.png)

## 在多 GPU 上執行 onnxruntime

目前可用的 Phi ONNX 模型僅支援單一 GPU。雖然理論上可以支援 Phi 模型的多 GPU，但使用兩個 GPU 的 ORT 並不保證效能會比執行兩個獨立的 ort 實例更好。請參考 [ONNX Runtime](https://onnxruntime.ai/) 以獲取最新資訊。

在 [Build 2024 the GenAI ONNX Team](https://youtu.be/WLW4SE8M9i8?si=EtG04UwDvcjunyfC) 中宣布，他們已針對 Phi 模型啟用了多實例（multi-instance）而非多 GPU。

目前這允許你使用 CUDA_VISIBLE_DEVICES 環境變數，執行一個 onnxruntime 或 onnxruntime-genai 實例，如下所示。

```Python
CUDA_VISIBLE_DEVICES=0 python infer.py
CUDA_VISIBLE_DEVICES=1 python infer.py
```

歡迎在 [Azure AI Foundry](https://ai.azure.com) 中進一步探索 Phi。

**免責聲明**：  
本文件係使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。雖然我們致力於確保翻譯的準確性，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應視為權威來源。對於重要資訊，建議採用專業人工翻譯。我們不對因使用本翻譯而產生的任何誤解或誤釋負責。