# కీలక సాంకేతికతలు సూచించబడ్డవి

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12 పై నిర్మించిన, హార్డ్వేర్-ఆధారిత యాక్సలరేషన్‌తో మెషీన్ లెర్నింగ్ కోసం తక్కువ-స్థాయి API.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia అభివృద్ధి చేసిన ఒక సమాంతర గణన వేదిక మరియు అప్లికేషన్ ప్రోగ్రామింగ్ ఇంటర్ఫేస్ (API) మోడల్, ఇది గ్రాఫిక్స్ ప్రాసెసింగ్ యూనిట్స్ (GPUs) పై సాధారణ-ఉద్దేశ్య ప్రాసెసింగ్‌ను ప్రారంభం చేస్తుంది.
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - మెషీన్ లెర్నింగ్ మోడల్స్‌ను ప్రతినిధ్యం చేయడానికి రూపొందించిన ఓపెన్ ఫార్మాట్, వివిధ ML ఫ్రేమ్‌వర్క్‌ల మధ్య ఇంటరొపరబిలిటీని అందిస్తుంది.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - మెషీన్ లెర్నింగ్ మోడల్స్‌ను ప్రతినిధ్యం చేయడానికి మరియు అప్డేట్ చేయడానికి ఉపయోగించే ఒక ఫార్మాట్, ముఖ్యంగా 4-8 బిట్ క్వాంటైజేషన్‌తో CPUలపై సమర్థవంతంగా నడిచే చిన్న భాషా మోడల్‌లకు ఉపయోగకరంగా ఉంటుంది.

## DirectML

DirectML ఒక తక్కువ-స్థాయి API, ఇది హార్డ్వేర్-ఆధారిత యాక్సలరేటెడ్ మెషీన్ లెర్నింగ్‌ను సక్రీయం చేస్తుంది. GPU యాక్సలరేషన్‌ను ఉపయోగించడానికి ఇది DirectX 12 పై నిర్మించబడింది మరియు వెన్డర్-నిరపేక్షంగా ఉంటుంది, అంటే వివిధ GPU వెండర్లపై పనిచేయడానికి కోడ్ మార్పులు అవసరం ఉండవు. ఇది ప్రధానంగా GPUs పై మోడల్ ట్రైనింగ్ మరియు ఇన్ఫరెన్సింగ్ పనులకు ఉపయోగించబడుతుంది.

హార్డ్వేర్ మద్దతు విషయానికి వస్తే, DirectML అన్ని రకాల GPUsతో పనిచేయడానికి రూపొందించబడింది, అవి AMD ఇంటిగ్రేటెడ్ మరియు డిస్క్రీట్ GPUs, Intel ఇంటిగ్రేటెడ్ GPUs మరియు NVIDIA డిస్క్రీట్ GPUs ను కూడా కలిగి ఉంటాయి. ఇది Windows AI Platform భాగంగా ఉంది మరియు Windows 10 & 11 లో మద్దతు పొందినందున ఏ Windows డివైస్‌పైని మోడల్ ట్రైనింగ్ మరియు ఇన్ఫరెన్సింగ్‌కు అనుమతి ఇస్తుంది.

DirectML కు సంబంధించిన అప్‌డేట్స్ మరియు అవకాశాలు లేవి, ఉదాహరణకు 150 వరకూ ONNX ఆపరేటర్లను సపోర్ట్ చేయడం మరియు ONNX runtime మరియు WinML రెండింటిచే ఉపయోగించబడటం. ఇది ముఖ్యమైన ఇంటిగ్రేటెడ్ హార్డ్‌వేర్ విక్రేతులచే (IHVs) మద్దతు పొందింది, ప్రతి ఒకరు వివిధ మెటా కమాండ్స్ అమలు చేస్తారు.

## CUDA

CUDA, అంటే Compute Unified Device Architecture, Nvidia రూపొందించిన ఒక సమాంతర గణన వేదిక మరియు API మోడల్. ఇది సాఫ్ట్‌వేర్ అభివృద్ధికర్తలకు CUDA-సమర్థ గ్రాఫిక్స్ ప్రాసెసింగ్ యూనిట్ (GPU) ను సాధారణ-ఉద్దేశ్య ప్రాసెసింగ్ కోసం ఉపయోగించేందుకు అనుమతిస్తుంది — ఈ దృష్టికోణాన్ని GPGPU (గ్రాఫిక్స్ ప్రాసెసింగ్ యూనిట్స్‌పై సాధారణ ఉద్దేశ్య గణన) అంటారు. CUDA అనేది Nvidia యొక్క GPU యాక్సలరేషన్‌కు కీలక భాగం మరియు మెషీన్ లెర్నింగ్, శాస్త్రీయ గణితం మరియు వీడియో ప్రాసెసింగ్ వంటి విభిన్న రంగాలలో విస్తృతంగా ఉపయోగించబడుతుంది.

CUDA కు సంబంధించిన హార్డ్వేర్ మద్దతు ప్రత్యేకంగా Nvidia యొక్క GPUs కు సరిపోతుంది, ఎందుకంటే ఇది Nvidia అభివృద్ధి చేసిన ప్రాప్రైటరీ సాంకేతికత. ప్రతి ఆర్కిటెక్చర్ CUDA టూల్‌కిట్ యొక్క నిర్దిష్ట సంస్కరణలను మద్దతు ఇస్తుంది, ఇవి అభివృద్ధికర్తలకి CUDA అప్లికేషన్లు తయారుచేయుటకు మరియు నడపుటకు కావలసిన లైబ్రరీలు మరియు టూల్స్‌ను అందిస్తాయి.

## ONNX

ONNX (Open Neural Network Exchange) ఒక ఓపెన్ ఫార్మాట్, ఇది మెషీన్ లెర్నింగ్ మోడల్స్‌ను ప్రతినిధ్యం చేయడానికి రూపొందించబడింది. ఇది విస్తరించగల కంప్యూటేషన్ గ్రాఫ్ మోడల్ నిర్వచనాన్ని మరియు బిల్ట్-ఇన్ ఆపరేటర్లు మరియు స్టాండర్డ్ డేటా టైప్స్ నిర్వచనాలను అందిస్తుంది. ONNX ద్వారా అభివృద్ధికర్తలు మోడల్స్‌ను వివిధ ML ఫ్రేమ్‌వర్క్‌ల మధ్య తరలించ వచ్చు, ఇది ఇంటరొపరబిలిటీని సులభతరం చేస్తుంది మరియు AI అప్లికేషన్లను సృష్టించటం మరియు డిప్లాయ్ చేయటాన్ని సులభతరం చేస్తుంది.

Phi3 mini ONNX Runtime తో CPU మరియు GPU పై, సర్వర్ ప్లాట్‌ఫార్మ్స్‌, Windows, Linux మరియు Mac డెస్క్‌టాప్స్, మరియు మొబైల్ CPUలపై నడపగలదు.
మేము జోడించిన ఆప్టిమైజ్డ్ కాన్ఫిగరేషన్లు ఇవి

- ONNX మోడల్స్ కోసం int4 DML: AWQ ద్వారా int4 కి క్వాంటైజ్ చేయబడింది
- fp16 CUDA కోసం ONNX మోడల్
- int4 CUDA కోసం ONNX మోడల్: RTN ద్వారా int4 కి క్వాంటైజ్ చేయబడింది
- int4 CPU మరియు Mobile కోసం ONNX మోడల్: RTN ద్వారా int4 కి క్వాంటైజ్ చేయబడింది

## Llama.cpp

Llama.cpp C++ లో రాయబడ్డ ఓ ఓపెన్-సోర్స్ సాఫ్ట్‌వేర్ లైబ్రరీ. ఇది Llama సహా వివిధ పెద్ద భాషా మోడల్స్ (LLMs) పై ఇన్ఫరెన్సు నిర్వహిస్తుంది. ggml లైబ్రరీ (ఒక సాధారణ-ఉద్దేశ్య టెన్సార్ లైబ్రరీ) తో కలిసి అభివృద్ధి చేయబడిన llama.cpp ప్రధాన Python అమలు కంటే వేగవంతమైన ఇన్ఫరెన్సు మరియు తక్కువ మెమరీ వినియోగం అందించడాన్ని లక్ష్యంగా పెట్టుకుంటుంది. ఇది హార్డ్‌వేర్ ఆప్టిమైజేషన్, క్వాంటైజేషన్‌ను మద్దతు చేస్తుంది మరియు ఒక సింపుల్ API మరియు examples3 ను అందిస్తుంది. సమర్థవంతమైన LLM ఇన్ఫరెన్సులో ఆసక్తి ఉంటే, Phi3లో Llama.cpp నడపగలగడం వల్ల llama.cpp ను పరిశీలించడం విలువైనది.

## GGUF

GGUF (Generic Graph Update Format) మెషీన్ లెర్నింగ్ మోడల్స్‌ను ప్రతినిధ్యం చేయడానికి మరియు అప్డేట్ చేయడానికి ఉపయోగించే ఒక ఫార్మాట్. ఇది ముఖ్యంగా 4-8 బిట్ క్వాంటైజేషన్‌తో CPUలపై సమర్థవంతంగా నడిచే చిన్న భాషా మోడళ్లకు ఉపయుక్తం. GGUF వేగవంతమైన ప్రోటోటైపింగ్ కోసం మరియు ఎడ్జ్ డివైసెస్ లేదా CI/CD వంటి బ్యాచ్ పనులలో మోడల్స్‌ను నడపడానికి ఉపయోగపడుతుంది.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
నిరాకరణ:
ఈ పత్రాన్ని AI అనువాద సేవ [Co-op Translator](https://github.com/Azure/co-op-translator) ఉపయోగించి అనువదించబడింది. మేము ఖచ్చితత్వం కోసం ప్రయత్నించినప్పటికీ, ఆటోమేటెడ్ అనువాదాల్లో తప్పులు లేదా అపరాధతలు ఉండవచ్చును అని దయచేసి గమనించండి. అసలైన పత్రాన్ని దాని స్థానిక భాషలో ఉన్నట్లే అధికారిక మూలంగా పరిగణించాలి. ముఖ్యమైన సమాచారానికి వృత్తిపరమైన మానవ అనువాదాన్ని సూచించబడుతుంది. ఈ అనువాదం ఉపయోగం వలన ఏర్పడిన అవగాహనా లోపాలు లేదా తప్పుగా అర్థం చేసుకోవడాల కోసం మేము బాధ్యులేమి కాదు.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->