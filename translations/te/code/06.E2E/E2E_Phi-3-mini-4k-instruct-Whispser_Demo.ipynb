{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ఇంటరాక్టివ్ Phi 3 Mini 4K Instruct చాట్‌బాట్ with Whisper\n",
    "\n",
    "### పరిచయం:\n",
    "ఇంటరాక్టివ్ Phi 3 Mini 4K Instruct చాట్‌బాట్ ఒక సాధనం, ఇది వినియోగదారులకు టెక్స్ట్ లేదా ఆడియో ఇన్‌పుట్ ద్వారా Microsoft Phi 3 Mini 4K instruct డెమోతో పరస్పర చర్య చేయడానికి అనుమతిస్తుంది. ఈ చాట్‌బాట్ అనువాదం, వాతావరణ నవీకరణలు, మరియు సాధారణ సమాచారం సేకరణ వంటి విభిన్న పనుల కోసం ఉపయోగించుకోవచ్చు.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[మీ Huggingface యాక్సెస్ టోకెన్‌ను సృష్టించండి](https://huggingface.co/settings/tokens)\n",
    "\n",
    "కొత్త టోకెన్‌ను సృష్టించండి \n",
    "కొత్త పేరు ఇవ్వండి \n",
    "వ్రాయడానికి అనుమతులను ఎంచుకోండి\n",
    "టోకెన్‌ను కాపీ చేసి, సురక్షిత స్థలంలో భద్రపరచండి\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "క్రింద ఇచ్చిన Python కోడ్ రెండు ప్రధాన పనులు చేస్తుంది: `os` మాడ్యూల్‌ను దిగుమతి చేయడం మరియు ఒక పరిసర వేరియబుల్ సెట్ చేయడం.\n",
    "\n",
    "1. Importing the `os` module:\n",
    "   - Pythonలోని `os` మాడ్యూల్ ఆపరేటింగ్ సిస్టమ్‌తో పరస్పరం చేయడానికి ఒక మార్గాన్ని అందిస్తుంది. ఇది పర్యావరణ వేరియబుల్స్‌కి యాక్సెస్ చేయడం, ఫైళ్లు మరియు డైరెక్టరీలతో పని చేయటం మొదలైన వివిధ ఆపరేటింగ్ సిస్టమ్ సంబంధిత పనులు చేయడానికి అనుమతిస్తుంది.\n",
    "   - ఈ కోడ్‌లో, `os` మాడ్యూల్‌ను `import` స్టేట్‌మెంట్ ద్వారా దిగుమతి చేస్తున్నాడు. ఈ స్టేట్‌మెంట్ ప్రస్తుత Python స్క్రిప్ట్‌లో `os` మాడ్యూల్ ఫంక్షనాలిటీని ఉపయోగానికి అందుబాటులో ఉంచుతుంది.\n",
    "\n",
    "2. Setting an environment variable:\n",
    "   - పర్యావరణ వేరియబుల్ అనేది ఆపరేటింగ్ సిస్టమ్‌పై నడిచే ప్రోగ్రామ్‌లు యాక్సెస్ చేసుకునే ఒక విలువ. ఇది కాన్ఫిగరేషన్ సెట్టింగ్స్ లేదా ఒకటి కన్నా ఎక్కువ ప్రోగ్రామ్‌లు ఉపయోగించగలిగే ఇతర సమాచారాన్ని నిల్వ చేయడానికి ఒక మార్గం.\n",
    "   - ఈ కోడ్‌లో, కొత్త పర్యావరణ వేరియబుల్ `os.environ` డిక్షనరీ ఉపయోగించి సెట్ చేయబడుతోంది. డిక్షనరీ యొక్క కీ `'HF_TOKEN'`, మరియు విలువ `HUGGINGFACE_TOKEN` వేరియబుల్ నుండి కేటాయించబడింది.\n",
    "   - `HUGGINGFACE_TOKEN` వేరియబుల్ ఈ కోడ్ స్నిపెట్ తలకు కొద్దిగా పైగా నిర్వచించబడింది, మరియు అది `#@param` సింటాక్స్ ఉపయోగించి `\"hf_**************\"` అనే స్ట్రింగ్ విలువకు కేటాయించబడింది. ఈ సింటాక్స్ సాధారణంగా Jupyter నోట్‌బుక్స్‌లో యూజర్ ఇన్‌పుట్ మరియు పరామితి కాన్ఫిగరేషన్‌ను నోట్బుక్ ఇంటర్‌ఫేస్‌లో నేరుగా అనుమతించడానికి ఉపయోగిస్తారు.\n",
    "   - `'HF_TOKEN'` పర్యావరణ వేరియబుల్‌ను సెట్ చేయడం ద్వారా, అది ప్రోగ్రామ్ యొక్క ఇతర భాగాలు లేదా ఒకే ఆపరేటింగ్ సిస్టమ్‌పై నడిచే ఇతర ప్రోగ్రామ్‌లు ద్వారా యాక్సెస్ చేయబడవచ్చు.\n",
    "\n",
    "మొత్తంగా, ఈ కోడ్ `os` మాడ్యూల్‌ను దిగుమతి చేస్తుంది మరియు `HUGGINGFACE_TOKEN` వేరియబుల్‌లో ఇచ్చిన విలువతో `'HF_TOKEN'` అనే పేరుతో ఒక పర్యావరణ వేరియబుల్‌ను సెట్ చేస్తుంది.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ఈ కోడ్ స్నిపెట్ Jupyter Notebook లేదా IPython లో ప్రస్తుత సెల్ యొక్క అవుట్‌పుట్‌ను క్లియర్ చేయడానికి ఉపయోగించే clear_output అనే ఫంక్షన్‌ను నిర్వచిస్తుంది. కోడ్‌ను విడదీసి దాని పనితీరును అర్థం చేసుకుందాం:\n",
    "\n",
    "clear_output ఫంక్షన్ ఒక పారామీటరు wait ను తీసుకుంటుంది, ఇది ఒక boolean విలువ. డిఫాల్ట్‌గా wait ను False గా సెట్ చేస్తారు. ఈ పారామీటరు ఫంక్షన్ మరిన్ని అవుట్‌పుట్ వచ్చేసరికి ఉన్న అవుట్‌పుట్‌ను మార్చడానికి ఎదురుచూడవచ్చా లేదా అన్నదానిని నిర్ణయిస్తుంది.\n",
    "\n",
    "ఫంక్షన్ స్వయంగా ప్రస్తుత సెల్ యొక్క అవుట్‌పుట్‌ను క్లియర్ చేయడానికి ఉపయోగిస్తారు. Jupyter Notebook లేదా IPython‌లో, ఒక సెల్ అవుట్‌పుట్ ఉత్పత్తి చేసినప్పుడు, ఉదాహరణకు ముద్రించిన టెక్స్ట్ లేదా గ్రాఫికల్ ప్లాట్లు వంటి అవుట్‌పుట్ సెల్ దిగువన ప్రదర్శించబడతాయి. clear_output ఫంక్షన్ ఆ అవుట్‌పుట్‌ను క్లియర్ చేయడానికి మీకు అనుమతిస్తుంది.\n",
    "\n",
    "ఫంక్షన్ యొక్క అమలు కోడ్ స్నిపెట్‌లో అందించబడలేదు, ఇది ... ద్వారా సూచించబడింది. ఇది ... అవుట్‌పుట్‌ను క్లియర్ చేసే వాస్తవ కోడ్ కోసం ఒక ప్లేస్‌హోల్డర్‌ని సూచిస్తుంది. ఫంక్షన్ యొక్క అమలు ప్రస్తుత సెల్ నుంచి ఉన్న అవుట్‌పుట్‌ని తొలగించడానికి Jupyter Notebook లేదా IPython APIతో పరస్పరం వ్యవహరించడం (interacting) అవసరం కావచ్చు.\n",
    "\n",
    "మొత్తానికి, ఈ ఫంక్షన్ Jupyter Notebook లేదా IPythonలో ప్రస్తుత సెల్ యొక్క అవుట్‌పుట్‌ను క్లియర్ చేయడానికి ఒక సౌకర్యవంతమైన మార్గాన్ని అందిస్తుంది, ఇది ఇంటరాక్టివ్ కోడింగ్ సెషన్ల సమయంలో ప్రదర్శించబడుతున్న అవుట్‌పుట్‌ను నిర్వహించడం మరియు అప్డేట్ చేయడం సులభతరం చేస్తుంది.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge TTS సేవను ఉపయోగించి టెక్స్ట్-టు-స్పీచ్ (TTS) నిర్వహించండి. సంబంధిత ఫంక్షన్ అమలులను ఒక్కోటి ఒక్కోటి గా పరిశీలしましょう:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: ఈ ఫంక్షన్ ఒక ఇన్‌పుట్ విలువను తీసుకుని TTS వాయిస్ కోసం రేటు స్ట్రింగ్‌ను లెక్కిస్తుంది. ఇన్‌పుట్ విలువ మాట్లాడే వేగాన్ని సూచిస్తుంది, ఇక్కడ 1 అంటే సాధారణ వేగం. ఫంక్షన్ రేటు స్ట్రింగ్‌ను లెక్కించడానికి ఇన్‌పుట్ విలువ నుండి 1 తీసివేస్తుంది, దాన్ని 100తో గుణిస్తుంది, మరియు ఆ తర్వాత ఇన్‌పుట్ విలువ 1 కన్నా పెద్దదా లేదా సమానమా అని ఆధారంగా సైన్‌ను నిర్ణయిస్తుంది. ఫంక్షన్ \"{sign}{rate}\" ఫార్మాట్‌లో రేటు స్ట్రింగ్‌ను రిటర్న్ చేస్తుంది.\n",
    "\n",
    "2.`make_chunks(input_text, language)`: ఈ ఫంక్షన్ ఇన్‌పుట్ టెక్స్ట్ మరియు భాషను పరామీటర్లుగా తీసుకుంటుంది. ఇది భాష-స్పెసిఫిక్ నియమాల ప్రకారం టెక్స్ట్‌ను చంక్‌లుగా విభజిస్తుంది. ఈ అమలులో, భాష \"English\" అయితే, ఫంక్షన్ టెక్స్ట్‌ను ప్రతి period (\".\") వద్ద విభజించి ముందునుండి లేదా వెనుకునుండి ఉన్న ఏదైనా ఖాళీలను తొలగిస్తుంది. తర్వాత ఇది ప్రతి చంక్‌కు ఒక period జతచేసి ఫిల్టర్ చేయబడ్డ చంక్‌ల జాబితాను రిటర్న్ చేస్తుంది.\n",
    "\n",
    "3. `tts_file_name(text)`: ఈ ఫంక్షన్ ఇన్‌పుట్ టెక్స్ట్ ఆధారంగా TTS ఆడియో ఫైల్ కోసం ఒక ఫైల్ పేరు ఉత్పత్తి చేస్తుంది. ఇది టెక్స్ట్‌పై అనేక రూపాంతరాలు చేస్తుంది: చివరలో ఉన్న period ను తీసివేయడం (ఉంటే), టెక్స్ట్‌ను lowercase గా మార్చడం, ముందునుండి మరియు వెనుకునుండి ఖాళీలను స్ట్రిప్ చేయడం, మరియు స్పేస్‌లను underscores తో మార్పిడి చేయడం. తరువాత ఇది టెక్స్ట్‌ను గరిష్టంగా 25 క్యారెక్టర్లు వరకు కొట్టి (దీని కంటే ఎక్కువ అయితే) లేదా అది ఖాళీ అయితే పూర్తి టెక్స్ట్‌ను ఉపయోగిస్తుంది. చివరిగా, ఇది [`uuid`] మాడ్యూల్ ఉపయోగించి ఒక రాండమ్ స్ట్రింగ్‌ను ఉత్పత్తి చేసి truncated టెక్స్ట్‌తో కలిపి ఫైల్ పేరును \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\" ఫార్మాట్‌లో సృష్టిస్తుంది.\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: ఈ ఫంక్షన్ అనేక ఆడియో ఫైళ్లను ఒకే ఆడియో ఫైల్‌గా విలీనం చేస్తుంది. ఇది ఆడియో ఫైల్ పగతులు మరియు ఒక output path ను పరామీటర్లుగా తీసుకుంటుంది. ఫంక్షన్ ఒక ఖాళీ `AudioSegment` ఆబ్జెక్ట్‌ను [`merged_audio`] అని ప్రారంభిస్తుంది. ఆ తర్వాత అది ప్రతి ఆడియో ఫైల్ పాత్‌ను తిరిగి చూసి, `AudioSegment.from_file()` మెథడ్ (ఇదిని `pydub` లైబ్రరీ నుండి ఉపయోగిస్తుంది) ఉపయోగించి ఆ ఆడియో ఫైల్‌ను లోడ్ చేసి, ప్రస్తుత ఆడియో ఫైల్‌ను [`merged_audio`] ఆబ్జెక్ట్‌కు జత చేస్తుంది. చివరగా, అది విలీనం చేసిన ఆడియోను పేర్కొన్న output pathకి MP3 ఫార్మాట్‌లో ఎక్స్‌పోర్ట్ చేస్తుంది.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path): This function performs the TTS operation using the Edge TTS service. It takes a list of text chunks, the speed of the speech, the voice name, and the save path as parameters. If the number of chunks is greater than 1, the function creates a directory for storing the individual chunk audio files. It then iterates through each chunk, constructs an Edge TTS command using the `calculate_rate_string()' function, the voice name, and the chunk text, and executes the command using the `os.system()` function. If the command execution is successful, it appends the path of the generated audio file to a list. After processing all the chunks, it merges the individual audio files using the `merge_audio_files()` function and saves the merged audio to the specified save path. If there is only one chunk, it directly generates the Edge TTS command and saves the audio to the save path. Finally, it returns the save path of the generated audio file.\n",
    "\n",
    "6. `random_audio_name_generate()`: ఈ ఫంక్షన్ [`uuid`] మాడ్యూల్ ఉపయోగించి ఒక రాండమ్ ఆడియో ఫైల్ పేరు జనరేట్ చేస్తుంది. ఇది ఒక రాండమ్ UUIDను ఉత్పత్తి చేసి దాన్ని స్ట్రింగ్‌గా మార్చి, మొదటి 8 అక్షరాలను తీసుకుని, \".mp3\" ఎక్స్టెన్షన్‌ను జత చేసి ఆ రాండమ్ ఆడియో ఫైల్ పేరును రిటర్న్ చేస్తుంది.\n",
    "\n",
    "7. `talk(input_text)`: ఈ ఫంక్షన్ TTS ఆపరేషన్‌ను నిర్వహించడానికి ప్రధాన ఎంట్రీ పాయింట్. ఇది ఇన్‌పుట్ టెక్స్ట్‌ను ఒక పారామీటర్‌గా తీసుకుంటుంది. మొదట ఇది ఇన్‌పుట్ టెక్స్ట్ యొక్క పొడవును తనిఖీ చేసి అది ఒక దీర్ఘ వాక్యముగా ఉందా (600 అక్షరాలకంటే ఎక్కువ లేదా సమానం) అని నిర్ణయిస్తుంది. పొడవు మరియు `translate_text_flag` వేరియబుల్ విలువ ఆధారంగా, ఇది భాషను నిర్ణయించి `make_chunks()` ఫంక్షన్ ఉపయోగించి టెక్స్ట్ చంక్‌ల జాబితాను తయారుచేస్తుంది. తరువాత, ఇది ఆడియో ఫైల్ కోసం ఒక సేవ్ పాత్‌ను `random_audio_name_generate()` ఫంక్షన్ ఉపయోగించి ఉత్పత్తి చేస్తుంది. చివరగా, ఇది `edge_free_tts()` ఫంక్షన్‌ని పిలిచి TTS ఆపరేషన్‌ను నిర్వహించి ఉత్పత్తి అయిన ఆడియో ఫైల్ యొక్క సేవ్ పాత్‌ని రిటర్న్ చేస్తుంది.\n",
    "\n",
    "సారాంశంగా, ఈ ఫంక్షన్‌లు కలిసి ఇన్‌పుట్ టెక్స్ట్‌ను చంక్‌లుగా విభజించి, ఆడియో ఫైల్ కోసం ఫైల్ పేరు ఉత్పత్తి చేసి, Edge TTS సేవను ఉపయోగించి TTS ఆపరేషన్‌ను నిర్వహించి, వ్యక్తిగత ఆడియో ఫైళ్లను ఒకే ఆడియో ఫైల్‌గా విలీనం చేస్తాయి.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert_to_text మరియు run_text_prompt అనే రెండు ఫంక్షన్ల అమలు మరియు str మరియు Audio అనే రెండు క్లాసుల ప్రకటన.\n",
    "\n",
    "convert_to_text ఫంక్షన్ audio_path ను ఇన్‌పుట్‌గా తీసుకొని whisper_model అనే మోడల్‌ను ఉపయోగించి ఆడియోను టెక్స్ట్‌గా ట్రాన్స్‌క్రైబ్ చేస్తుంది. ఫంక్షన్ మొదట gpu ఫ్లాగ్ True గా సెట్ చేయబడిందా అని తనిఖీ చేస్తుంది. అది అయితే, whisper_model ను word_timestamps=True, fp16=True, language='English', మరియు task='translate' వంటి నిర్దేశిత పారామీటర్లతో ఉపయోగిస్తారు. gpu ఫ్లాగ్ False అయితే, whisper_model ను fp16=False తో ఉపయోగిస్తారు. ఫలితంగా వచ్చిన ట్రాన్స్‌క్రిప్షన్‌ను 'scan.txt' అనే ఫైల్‌లో సేవ్ చేసి, టెక్స్ట్‌గా రిటర్న్ చేస్తారు.\n",
    "\n",
    "run_text_prompt ఫంక్షన్ message మరియు chat_history‌ను ఇన్‌పుట్‌గా తీసుకుంటుంది. ఇది phi_demo ఫంక్షన్‌ను ఉపయోగించి chatbot నుండి ఇన్‌పుట్ మెसेజ్ ఆధారంగా ప్రతిస్పందనను ఉత్పత్తి చేస్తుంది. ఉత్పత్తి అయిన స్పందన తరువాత talk ఫంక్షన్‌కు పంపబడుతుంది, ఇది స్పందనను ఓ ఆడియో ఫైల్‌గా మార్చి ఆ ఫైల్ పాత్‌ను రిటర్న్ చేస్తుంది. Audio క్లాస్ ఆ ఆడియో ఫైల్‌ను ప్రదర్శించడానికి మరియు ప్లే చేయడానికి ఉపయోగించబడుతుంది. ఆడియోను IPython.display మాడ్యూల్ నుండి display ఫంక్షన్ ఉపయోగించి ప్రదర్శిస్తారు, మరియు Audio αντικార్తిని autoplay=True పారామీటరుతో సృష్టిస్తారు, అందువల్ల ఆడియో ఆటోమాటిక్‌గా ప్లే అవ్వడం ప్రారంభమవుతుంది. chat_history ను ఇన్‌పుట్ మెసేజ్ మరియు ఉత్పత్తి అయిన స్పందనతో అప్డేట్ చేస్తారు, మరియు ఖాళీ స్ట్రింగ్ మరియు అప్డేట్ చేసిన chat_history ని రిటర్న్ చేస్తారు.\n",
    "\n",
    "str క్లాస్ Python లోని బిల్ట్-ఇన్ క్లాస్‌గా చిరునామా ఎత్తిన క్యారెక్టర్ల సీక్వెన్స్‌ను సూచిస్తుంది. ఇది స్ట్రింగ్స్‌పై పని చేయడానికి మరియు నిర్వహించడానికి వివిధ మెథడ్స్‌ని అందిస్తుంది, ఉదాహరణకు capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill, మరియు మరిన్ని. ఈ మెథడ్స్ ద్వారా సర్చ్, రీప్లేస్, ఫార్మాటింగ్ మరియు స్ట్రింగ్స్ నిర్వహణ వంటి ఆపరేషన్లు చేయవచ్చు.\n",
    "\n",
    "Audio క్లాస్ ఒక కస్టమ్ క్లాస్ కాగా అది ఒక ఆడియో ఆబ్జెక్ట్‌ను సూచిస్తుంది. ఇది Jupyter Notebook వాతావరణంలో ఆడియో ప్లేయర్‌ను సృష్టించడానికి ఉపయోగించబడుతుంది. క్లాస్ data, filename, url, embed, rate, autoplay, మరియు normalize వంటి వివిధ పారామీటర్లను అంగీకరిస్తుంది. data పారామీటర్ numpy array, శాంపిల్‌ల జాబితా, ఫైల్ పేరు లేదా URL ని సూచించే స్ట్రింగ్, లేదా rå PCM డేటా అవ్వచ్చు. filename పారామీటరు స్థానిక ఫైల్ నుండి ఆడియో డేటాను లోడ్ చేయడానికి ఉపయోగిస్తారు, మరియు url పారామీటరు ఆడియో డేటాను డౌన్లోడ్ చేయడానికి URL ను సూచించడానికి ఉపయోగిస్తారు. embed పారామీటరు ఆడియో డేటాను data URI ద్వారా ఎంబెడ్ చేయాలో లేదా మూల సోర్స్ నుండి సూచించాలో నిర్ణయిస్తుంది. rate పారామీటర్ ఆడియో డేటా యొక్క సాంప్లింగ్ రేట్‌ను సూచిస్తుంది. autoplay పారామీటరు ఆడియో ఆటోమాటిక్‌గా ప్లే అవ్వాలని నిర్ణయిస్తుంది. normalize పారామీటర్ ఆడియో డేటాను గరిష్ఠ האפשרైన రేంజ్‌కి రీస్కేలు చేయాలా లేదో సూచిస్తుంది. Audio క్లాస్ reload వంటి మెథడ్స్ కూడా అందిస్తుంది ఫైల్ లేదా URL నుంచి ఆడియో డేటాను రీలోడ్ చేయడానికి, మరియు src_attr, autoplay_attr, మరియు element_id_attr వంటి అట్రిబ్యూట్స్ HTML లోని ఆడియో ఎలిమెంట్‌కు సంబంధించి అనుకూల అట్రిబ్యూట్స్‌ని పొందడానికి అందుబాటులో ఉంటాయి.\n",
    "\n",
    "మొత్తానికి, ఈ ఫంక్షన్లు మరియు క్లాసులు ఆడియోను టెక్స్ట్‌గా ట్రాన్స్‌క్రైబ్ చేయడానికి, chatbot నుండి ఆడియో ప్రతిస్పందనలు రూపొందించాలని, మరియు Jupyter Notebook వాతావరణంలో ఆడియోను ప్రదర్శించి ప్లే చేయడానికి ఉపయోగిస్తారు.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\nస్పష్టీకరణ:\nఈ పత్రం AI అనువాద సేవ [Co-op Translator](https://github.com/Azure/co-op-translator) ఉపయోగించి అనువదించబడింది. మేము ఖచ్చితత్వానికి ప్రయత్నించినప్పటికీ, ఆటోమేటెడ్ అనువాదాల్లో లోపాలు లేదా పొరపాట్లు ఉండవచ్చని దయచేసి గమనించండి. మూల పత్రాన్ని దాని మూల భాషలోనే అధికారిక మూలంగా పరిగణించాలి. కీలకమైన సమాచారానికి వృత్తిపరమైన మానవ అనువాదం సిఫార్సు చేయబడుతుంది. ఈ అనువాదం వలన ఏర్పడిన ఏవైనా అవగాహనా లోపాలు లేదా తప్పుగా అర్థం చేసుకోవడంపై మేము బాధ్యత వహించము.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-12-22T05:02:38+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "te"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}