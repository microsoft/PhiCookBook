{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Інтерактивний Phi 3 Mini 4K Instruct Chatbot з Whisper\n",
    "\n",
    "### Вступ:\n",
    "Інтерактивний Phi 3 Mini 4K Instruct Chatbot — це інструмент, який дозволяє користувачам взаємодіяти з демонстрацією Microsoft Phi 3 Mini 4K Instruct за допомогою текстового або аудіовведення. Чатбот можна використовувати для різноманітних завдань, таких як переклад, оновлення погоди та загальне збирання інформації.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "Створіть свій токен доступу Huggingface\n",
    "\n",
    "Створіть новий токен  \n",
    "Введіть нову назву  \n",
    "Виберіть права на запис  \n",
    "Скопіюйте токен і збережіть його в безпечному місці\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наступний код на Python виконує дві основні задачі: імпортує модуль `os` та встановлює змінну середовища.\n",
    "\n",
    "1. Імпорт модуля `os`:\n",
    "   - Модуль `os` у Python забезпечує можливість взаємодії з операційною системою. Він дозволяє виконувати різні завдання, пов’язані з операційною системою, такі як доступ до змінних середовища, робота з файлами та каталогами тощо.\n",
    "   - У цьому коді модуль `os` імпортується за допомогою оператора `import`. Цей оператор робить функціональність модуля `os` доступною для використання у поточному Python-скрипті.\n",
    "\n",
    "2. Встановлення змінної середовища:\n",
    "   - Змінна середовища — це значення, до якого можуть звертатися програми, що працюють в операційній системі. Це спосіб зберігати налаштування конфігурації або іншу інформацію, яка може використовуватися кількома програмами.\n",
    "   - У цьому коді нова змінна середовища встановлюється за допомогою словника `os.environ`. Ключем словника є `'HF_TOKEN'`, а значення присвоюється зі змінної `HUGGINGFACE_TOKEN`.\n",
    "   - Змінна `HUGGINGFACE_TOKEN` визначена безпосередньо перед цим фрагментом коду, і їй присвоєно рядкове значення `\"hf_**************\"` за допомогою синтаксису `#@param`. Цей синтаксис часто використовується у Jupyter-ноутбуках для введення користувацьких даних і налаштування параметрів безпосередньо через інтерфейс ноутбука.\n",
    "   - Встановивши змінну середовища `'HF_TOKEN'`, до неї можуть звертатися інші частини програми або інші програми, що працюють у тій самій операційній системі.\n",
    "\n",
    "Загалом, цей код імпортує модуль `os` і встановлює змінну середовища з назвою `'HF_TOKEN'`, використовуючи значення, надане у змінній `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цей фрагмент коду визначає функцію під назвою clear_output, яка використовується для очищення виводу поточної комірки в Jupyter Notebook або IPython. Давайте розберемо код і зрозуміємо його функціональність:\n",
    "\n",
    "Функція clear_output приймає один параметр під назвою wait, який є булевим значенням. За замовчуванням wait встановлено як False. Цей параметр визначає, чи повинна функція чекати, поки новий вивід стане доступним для заміни існуючого виводу перед його очищенням.\n",
    "\n",
    "Сама функція використовується для очищення виводу поточної комірки. У Jupyter Notebook або IPython, коли комірка генерує вивід, наприклад, текст або графічні побудови, цей вивід відображається під коміркою. Функція clear_output дозволяє очистити цей вивід.\n",
    "\n",
    "Реалізація функції не надана у фрагменті коду, як зазначено через три крапки (...). Три крапки представляють заповнювач для фактичного коду, який виконує очищення виводу. Реалізація функції може включати взаємодію з API Jupyter Notebook або IPython для видалення існуючого виводу з комірки.\n",
    "\n",
    "Загалом, ця функція забезпечує зручний спосіб очищення виводу поточної комірки в Jupyter Notebook або IPython, що полегшує управління та оновлення відображеного виводу під час інтерактивних сеансів програмування.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виконання тексту в мовлення (TTS) за допомогою сервісу Edge TTS. Давайте розглянемо відповідні реалізації функцій одну за одною:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Ця функція приймає вхідне значення і обчислює рядок швидкості для голосу TTS. Вхідне значення представляє бажану швидкість мовлення, де значення 1 відповідає нормальній швидкості. Функція обчислює рядок швидкості, віднімаючи 1 від вхідного значення, множачи його на 100, а потім визначаючи знак залежно від того, чи вхідне значення більше або дорівнює 1. Функція повертає рядок швидкості у форматі \"{sign}{rate}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: Ця функція приймає текст і мову як параметри. Вона розбиває текст на частини відповідно до мовних правил. У цьому випадку, якщо мова — \"English\", функція розбиває текст на кожній крапці (\".\") і видаляє будь-які пробіли на початку або в кінці. Потім вона додає крапку до кожної частини і повертає відфільтрований список частин.\n",
    "\n",
    "3. `tts_file_name(text)`: Ця функція генерує ім'я файлу для аудіофайлу TTS на основі введеного тексту. Вона виконує кілька трансформацій тексту: видаляє крапку в кінці (якщо є), перетворює текст у нижній регістр, видаляє пробіли на початку і в кінці, а також замінює пробіли на підкреслення. Потім вона скорочує текст до максимум 25 символів (якщо довший) або використовує весь текст, якщо він порожній. Нарешті, вона генерує випадковий рядок за допомогою модуля [`uuid`] і об'єднує його зі скороченим текстом, щоб створити ім'я файлу у форматі \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Ця функція об'єднує кілька аудіофайлів в один. Вона приймає список шляхів до аудіофайлів і шлях для збереження як параметри. Функція ініціалізує порожній об'єкт `AudioSegment` під назвою [`merged_audio`]. Потім вона ітерує кожен шлях до аудіофайлу, завантажує аудіофайл за допомогою методу `AudioSegment.from_file()` з бібліотеки `pydub` і додає поточний аудіофайл до об'єкта [`merged_audio`]. Нарешті, вона експортує об'єднаний аудіофайл до вказаного шляху у форматі MP3.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: Ця функція виконує операцію TTS за допомогою сервісу Edge TTS. Вона приймає список текстових частин, швидкість мовлення, ім'я голосу та шлях для збереження як параметри. Якщо кількість частин більше 1, функція створює каталог для зберігання окремих аудіофайлів частин. Потім вона ітерує кожну частину, створює команду Edge TTS за допомогою функції `calculate_rate_string()`, імені голосу та тексту частини, і виконує команду за допомогою функції `os.system()`. Якщо виконання команди успішне, вона додає шлях до створеного аудіофайлу до списку. Після обробки всіх частин вона об'єднує окремі аудіофайли за допомогою функції `merge_audio_files()` і зберігає об'єднаний аудіофайл до вказаного шляху. Якщо є лише одна частина, вона безпосередньо створює команду Edge TTS і зберігає аудіо до шляху збереження. Нарешті, вона повертає шлях збереження створеного аудіофайлу.\n",
    "\n",
    "6. `random_audio_name_generate()`: Ця функція генерує випадкове ім'я аудіофайлу за допомогою модуля [`uuid`]. Вона генерує випадковий UUID, перетворює його на рядок, бере перші 8 символів, додає розширення \".mp3\" і повертає випадкове ім'я аудіофайлу.\n",
    "\n",
    "7. `talk(input_text)`: Ця функція є основною точкою входу для виконання операції TTS. Вона приймає текст як параметр. Спочатку вона перевіряє довжину тексту, щоб визначити, чи є це довгим реченням (600 символів або більше). Залежно від довжини та значення змінної `translate_text_flag`, вона визначає мову і генерує список текстових частин за допомогою функції `make_chunks()`. Потім вона генерує шлях для збереження аудіофайлу за допомогою функції `random_audio_name_generate()`. Нарешті, вона викликає функцію `edge_free_tts()` для виконання операції TTS і повертає шлях збереження створеного аудіофайлу.\n",
    "\n",
    "Загалом, ці функції працюють разом, щоб розбити введений текст на частини, створити ім'я файлу для аудіофайлу, виконати операцію TTS за допомогою сервісу Edge TTS і об'єднати окремі аудіофайли в один аудіофайл.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реалізація двох функцій: convert_to_text і run_text_prompt, а також оголошення двох класів: str і Audio.\n",
    "\n",
    "Функція convert_to_text приймає audio_path як вхідний параметр і транскрибує аудіо в текст за допомогою моделі під назвою whisper_model. Спочатку функція перевіряє, чи встановлений прапорець gpu на True. Якщо так, то whisper_model використовується з певними параметрами, такими як word_timestamps=True, fp16=True, language='English' і task='translate'. Якщо прапорець gpu встановлений на False, whisper_model використовується з параметром fp16=False. Отримана транскрипція зберігається у файл з назвою 'scan.txt' і повертається як текст.\n",
    "\n",
    "Функція run_text_prompt приймає message і chat_history як вхідні параметри. Вона використовує функцію phi_demo для генерації відповіді від чат-бота на основі вхідного повідомлення. Згенерована відповідь передається у функцію talk, яка конвертує відповідь у аудіофайл і повертає шлях до файлу. Клас Audio використовується для відображення і відтворення аудіофайлу. Аудіо відображається за допомогою функції display з модуля IPython.display, а об'єкт Audio створюється з параметром autoplay=True, щоб аудіо починало відтворюватися автоматично. chat_history оновлюється вхідним повідомленням і згенерованою відповіддю, після чого повертається порожній рядок і оновлений chat_history.\n",
    "\n",
    "Клас str — це вбудований клас у Python, який представляє послідовність символів. Він надає різні методи для маніпуляції і роботи з рядками, такі як capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill і багато інших. Ці методи дозволяють виконувати операції, такі як пошук, заміна, форматування і маніпуляція рядками.\n",
    "\n",
    "Клас Audio — це користувацький клас, який представляє аудіооб'єкт. Він використовується для створення аудіоплеєра в середовищі Jupyter Notebook. Клас приймає різні параметри, такі як data, filename, url, embed, rate, autoplay і normalize. Параметр data може бути numpy-масивом, списком семплів, рядком, що представляє ім'я файлу або URL, або сирими даними PCM. Параметр filename використовується для вказівки локального файлу, з якого завантажуються аудіодані, а параметр url — для вказівки URL, з якого завантажуються аудіодані. Параметр embed визначає, чи повинні аудіодані бути вбудовані за допомогою URI даних або посилатися на оригінальне джерело. Параметр rate визначає частоту семплювання аудіоданих. Параметр autoplay визначає, чи повинно аудіо починати відтворюватися автоматично. Параметр normalize визначає, чи повинні аудіодані бути нормалізовані (перемасштабовані) до максимально можливого діапазону. Клас Audio також надає методи, такі як reload для перезавантаження аудіоданих з файлу або URL, і атрибути, такі як src_attr, autoplay_attr і element_id_attr для отримання відповідних атрибутів аудіоелемента в HTML.\n",
    "\n",
    "Загалом, ці функції і класи використовуються для транскрибування аудіо в текст, генерації аудіовідповідей від чат-бота, а також для відображення і відтворення аудіо в середовищі Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Відмова від відповідальності**:  \nЦей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають у результаті використання цього перекладу.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-13T07:05:23+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "uk"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}