<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "bc29f7fe7fc16bed6932733eac8c81b8",
  "translation_date": "2025-07-09T19:34:12+00:00",
  "source_file": "md/02.Application/02.Code/Phi3/VSCodeExt/HOL/AIPC/02.PromptflowWithNPU.md",
  "language_code": "uk"
}
-->
# **Лабораторна робота 2 - Запуск Prompt flow з Phi-3-mini на AIPC**

## **Що таке Prompt flow**

Prompt flow — це набір інструментів для розробки, створений для спрощення повного циклу розробки AI-додатків на основі LLM: від ідеї, прототипування, тестування, оцінки до розгортання в продакшн і моніторингу. Він значно полегшує роботу з prompt engineering і дозволяє створювати LLM-додатки з якістю, придатною для продакшн.

За допомогою prompt flow ви зможете:

- Створювати потоки, які поєднують LLM, промпти, Python-код та інші інструменти в єдиний виконуваний робочий процес.

- Відлагоджувати та ітерувати ваші потоки, особливо взаємодію з LLM, з легкістю.

- Оцінювати ваші потоки, розраховувати метрики якості та продуктивності на великих наборах даних.

- Інтегрувати тестування та оцінку у вашу CI/CD систему для забезпечення якості потоку.

- Розгортати ваші потоки на обраній платформі для сервінгу або легко інтегрувати у кодову базу вашого додатку.

- (Опційно, але дуже рекомендовано) Співпрацювати з командою, використовуючи хмарну версію Prompt flow в Azure AI.

## **Що таке AIPC**

AI PC має CPU, GPU та NPU, кожен з яких має спеціалізовані можливості для прискорення AI. NPU, або нейронний процесорний блок, — це спеціалізований прискорювач, який виконує завдання штучного інтелекту (AI) та машинного навчання (ML) безпосередньо на вашому ПК, замість того, щоб відправляти дані для обробки в хмару. GPU та CPU також можуть обробляти ці завдання, але NPU особливо ефективний для AI-обчислень з низьким енергоспоживанням. AI PC означає фундаментальну зміну в роботі наших комп’ютерів. Це не рішення для проблеми, яка раніше не існувала, а значне покращення для повсякденного використання ПК.

Як це працює? Порівняно з генеративним AI та великими мовними моделями (LLM), навченими на величезних публічних даних, AI, який працюватиме на вашому ПК, є більш доступним на багатьох рівнях. Концепція легша для розуміння, а оскільки модель навчається на ваших даних без необхідності доступу до хмари, переваги стають більш привабливими для широкого кола користувачів.

У найближчому майбутньому світ AI PC включатиме персональних помічників і невеликі AI-моделі, які працюватимуть безпосередньо на вашому ПК, використовуючи ваші дані для надання персоналізованих, приватних і більш безпечних AI-покращень для повсякденних завдань — ведення протоколів зустрічей, організація ліги фентезі-футболу, автоматизація покращень для редагування фото та відео або складання ідеального маршруту для сімейної зустрічі з урахуванням часу прибуття та від’їзду кожного.

## **Створення потоків генерації коду на AIPC**

***Note*** ：Якщо ви ще не завершили встановлення середовища, будь ласка, відвідайте [Lab 0 -Installations](./01.Installations.md)

1. Відкрийте розширення Prompt flow у Visual Studio Code та створіть порожній проект потоку

![create](../../../../../../../../../imgs/02/vscodeext/pf_create.png)

2. Додайте параметри Inputs та Outputs і додайте Python-код як новий потік

![flow](../../../../../../../../../imgs/02/vscodeext/pf_flow.png)

Ви можете орієнтуватися на цю структуру (flow.dag.yaml) для побудови вашого потоку

```yaml

inputs:
  question:
    type: string
    default: how to write Bubble Algorithm
outputs:
  answer:
    type: string
    reference: ${Chat_With_Phi3.output}
nodes:
- name: Chat_With_Phi3
  type: python
  source:
    type: code
    path: Chat_With_Phi3.py
  inputs:
    question: ${inputs.question}


```

3. Додайте код у ***Chat_With_Phi3.py***

```python


from promptflow.core import tool

# import torch
from transformers import AutoTokenizer, pipeline,TextStreamer
import intel_npu_acceleration_library as npu_lib

import warnings

import asyncio
import platform

class Phi3CodeAgent:
    
    model = None
    tokenizer = None
    text_streamer = None
    
    model_id = "microsoft/Phi-3-mini-4k-instruct"

    @staticmethod
    def init_phi3():
        
        if Phi3CodeAgent.model is None or Phi3CodeAgent.tokenizer is None or Phi3CodeAgent.text_streamer is None:
            Phi3CodeAgent.model = npu_lib.NPUModelForCausalLM.from_pretrained(
                                    Phi3CodeAgent.model_id,
                                    torch_dtype="auto",
                                    dtype=npu_lib.int4,
                                    trust_remote_code=True
                                )
            Phi3CodeAgent.tokenizer = AutoTokenizer.from_pretrained(Phi3CodeAgent.model_id)
            Phi3CodeAgent.text_streamer = TextStreamer(Phi3CodeAgent.tokenizer, skip_prompt=True)

    

    @staticmethod
    def chat_with_phi3(prompt):
        
        Phi3CodeAgent.init_phi3()

        messages = "<|system|>You are a AI Python coding assistant. Please help me to generate code in Python.The answer only genertated Python code, but any comments and instructions do not need to be generated<|end|><|user|>" + prompt +"<|end|><|assistant|>"



        generation_args = {
            "max_new_tokens": 1024,
            "return_full_text": False,
            "temperature": 0.3,
            "do_sample": False,
            "streamer": Phi3CodeAgent.text_streamer,
        }

        pipe = pipeline(
            "text-generation",
            model=Phi3CodeAgent.model,
            tokenizer=Phi3CodeAgent.tokenizer,
            # **generation_args
        )

        result = ''

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            response = pipe(messages, **generation_args)
            result =response[0]['generated_text']
            return result


@tool
def my_python_tool(question: str) -> str:
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    return Phi3CodeAgent.chat_with_phi3(question)


```

4. Ви можете протестувати потік через Debug або Run, щоб перевірити, чи коректно генерується код

![RUN](../../../../../../../../../imgs/02/vscodeext/pf_run.png)

5. Запустіть потік як API для розробки у терміналі

```

pf flow serve --source ./ --port 8080 --host localhost   

```

Ви можете протестувати його у Postman / Thunder Client

### **Note**

1. Перший запуск займає багато часу. Рекомендується завантажити модель phi-3 через Hugging face CLI.

2. Враховуючи обмежену обчислювальну потужність Intel NPU, рекомендується використовувати Phi-3-mini-4k-instruct.

3. Ми використовуємо Intel NPU Acceleration для квантизації INT4, але якщо ви повторно запускаєте сервіс, потрібно видалити папки cache та nc_workshop.

## **Ресурси**

1. Вивчайте Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Вивчайте Intel NPU Acceleration [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Приклад коду, завантажте [Local NPU Agent Sample Code](../../../../../../../../../code/07.Lab/01/AIPC)

**Відмова від відповідальності**:  
Цей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ рідною мовою слід вважати авторитетним джерелом. Для критично важливої інформації рекомендується звертатися до професійного людського перекладу. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникли внаслідок використання цього перекладу.