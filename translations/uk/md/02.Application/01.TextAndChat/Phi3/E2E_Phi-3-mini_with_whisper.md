<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7f72d7981ed3640865700f51ae407da4",
  "translation_date": "2026-01-14T16:16:15+00:00",
  "source_file": "md/02.Application/01.TextAndChat/Phi3/E2E_Phi-3-mini_with_whisper.md",
  "language_code": "uk"
}
-->
# Інтерактивний Phi 3 Mini 4K Instruct Chatbot з Whisper

## Огляд

Інтерактивний Phi 3 Mini 4K Instruct Chatbot — це інструмент, який дозволяє користувачам взаємодіяти з демо Microsoft Phi 3 Mini 4K instruct за допомогою текстового або аудіо вводу. Чатбот можна використовувати для різноманітних завдань, таких як переклад, оновлення погоди та загальне збирання інформації.

### Початок роботи

Щоб скористатися цим чатботом, просто дотримуйтесь інструкцій:

1. Відкрийте новий [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb)
2. У головному вікні блокноту ви побачите інтерфейс чатбокса з текстовим полем для вводу і кнопкою "Send".
3. Щоб скористатися текстовим чатботом, просто введіть ваше повідомлення у текстове поле і натисніть кнопку "Send". Чатбот відповість аудіофайлом, який можна прослухати прямо в блокноті.

**Примітка**: Цей інструмент потребує GPU і доступу до моделей Microsoft Phi-3 та OpenAI Whisper, які використовуються для розпізнавання мовлення та перекладу.

### Вимоги до GPU

Для запуску цього демо вам потрібно 12 Гб пам’яті GPU.

Вимоги до пам’яті для запуску демо **Microsoft-Phi-3-Mini-4K instruct** на GPU залежать від кількох факторів, таких як розмір вхідних даних (аудіо або текст), мова для перекладу, швидкість моделі і доступна пам’ять на GPU.

Загалом, модель Whisper призначена для роботи на GPU. Рекомендований мінімум пам’яті GPU для запуску моделі Whisper становить 8 Гб, але модель може обробляти більшу кількість пам’яті за потреби.

Важливо зазначити, що робота з великою кількістю даних або високим обсягом запитів до моделі може вимагати більше пам’яті GPU та/або спричинити проблеми з продуктивністю. Рекомендується тестувати ваш випадок використання з різними конфігураціями та моніторити використання пам’яті для визначення оптимальних налаштувань під ваші конкретні потреби.

## E2E Приклад для Інтерактивного Phi 3 Mini 4K Instruct Chatbot з Whisper

Jupyter блокнот під назвою [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) демонструє, як використовувати демо Microsoft Phi 3 Mini 4K instruct для генерації тексту з аудіо або письмового вводу. Блокнот визначає кілька функцій:

1. `tts_file_name(text)`: ця функція генерує ім’я файлу на основі вхідного тексту для збереження згенерованого аудіофайлу.
1. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: ця функція використовує Edge TTS API для генерації аудіофайлу зі списку частин вхідного тексту. Вхідні параметри — список частин, швидкість мовлення, ім’я голосу і шлях до збереження згенерованого аудіофайлу.
1. `talk(input_text)`: ця функція генерує аудіофайл за допомогою Edge TTS API і зберігає його з випадковим іменем у директорії /content/audio. Вхідним параметром є текст для перетворення в мовлення.
1. `run_text_prompt(message, chat_history)`: ця функція використовує демо Microsoft Phi 3 Mini 4K instruct для генерації аудіофайлу з введеного повідомлення та додає його до історії чату.
1. `run_audio_prompt(audio, chat_history)`: ця функція перетворює аудіофайл у текст за допомогою API моделі Whisper і передає його у функцію `run_text_prompt()`.
1. Код запускає додаток Gradio, який дозволяє користувачам взаємодіяти з демо Phi 3 Mini 4K instruct, вводячи повідомлення або завантажуючи аудіофайли. Вихід відображається як текстове повідомлення у додатку.

## Усунення несправностей

Встановлення драйверів Cuda GPU

1. Переконайтеся, що ваші Linux-застосунки оновлені

    ```bash
    sudo apt update
    ```

1. Встановіть драйвери Cuda

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. Зареєструйте місце розташування драйвера cuda

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. Перевірка розміру пам’яті Nvidia GPU (потрібно 12 Гб пам’яті GPU)

    ```bash
    nvidia-smi
    ```

1. Очищення кешу: Якщо ви використовуєте PyTorch, можна викликати torch.cuda.empty_cache(), щоб звільнити всю невикористану кешовану пам’ять, щоб її могли використовувати інші GPU-застосунки

    ```python
    torch.cuda.empty_cache() 
    ```

1. Перевірка Nvidia Cuda

    ```bash
    nvcc --version
    ```

1. Виконайте наступні дії для створення токена Hugging Face.

    - Перейдіть на сторінку [Hugging Face Token Settings](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo).
    - Оберіть **New token**.
    - Введіть назву проекту (**Name**), яку хочете використовувати.
    - Оберіть тип (**Type**) — **Write**.

> [!NOTE]
>
> Якщо ви зіткнулися з такою помилкою:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> Щоб це виправити, введіть у терміналі наступну команду.
>
> ```bash
> sudo ldconfig
> ```

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, враховуйте, що автоматичний переклад може містити помилки або неточності. Оригінальний документ рідною мовою слід вважати авторитетним джерелом. Для важливої інформації рекомендується звертатися до професійного людського перекладу. Ми не несемо відповідальності за будь-які непорозуміння чи неправильні тлумачення, що виникли внаслідок використання цього перекладу.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->