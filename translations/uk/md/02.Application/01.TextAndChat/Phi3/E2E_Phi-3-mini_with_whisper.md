<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "006e8cf75211d3297f24e1b22e38955f",
  "translation_date": "2025-07-09T19:12:41+00:00",
  "source_file": "md/02.Application/01.TextAndChat/Phi3/E2E_Phi-3-mini_with_whisper.md",
  "language_code": "uk"
}
-->
# Інтерактивний Phi 3 Mini 4K Instruct Chatbot з Whisper

## Огляд

Інтерактивний Phi 3 Mini 4K Instruct Chatbot — це інструмент, який дозволяє користувачам взаємодіяти з демонстрацією Microsoft Phi 3 Mini 4K instruct за допомогою текстового або аудіо вводу. Чатбот можна використовувати для різних завдань, таких як переклад, оновлення погоди та загальний збір інформації.

### Початок роботи

Щоб скористатися цим чатботом, просто виконайте наступні кроки:

1. Відкрийте новий [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb)
2. У головному вікні ноутбука ви побачите інтерфейс чатбокса з полем для введення тексту та кнопкою «Send».
3. Щоб використовувати текстовий чатбот, просто введіть своє повідомлення у поле для тексту та натисніть кнопку «Send». Чатбот відповість аудіофайлом, який можна відтворити безпосередньо в ноутбуці.

**Note**: Цей інструмент вимагає наявності GPU та доступу до моделей Microsoft Phi-3 і OpenAI Whisper, які використовуються для розпізнавання мови та перекладу.

### Вимоги до GPU

Для запуску цієї демонстрації вам потрібно 12 ГБ пам’яті GPU.

Вимоги до пам’яті для запуску демонстрації **Microsoft-Phi-3-Mini-4K instruct** на GPU залежать від кількох факторів, таких як розмір вхідних даних (аудіо або текст), мова перекладу, швидкість моделі та доступна пам’ять на GPU.

Загалом, модель Whisper розроблена для роботи на GPU. Рекомендований мінімум пам’яті GPU для запуску Whisper — 8 ГБ, але вона може працювати з більшим обсягом пам’яті за потреби.

Важливо зазначити, що обробка великої кількості даних або високий обсяг запитів до моделі може вимагати більше пам’яті GPU і/або призводити до проблем з продуктивністю. Рекомендується тестувати ваш випадок використання з різними конфігураціями та контролювати використання пам’яті, щоб визначити оптимальні налаштування для ваших конкретних потреб.

## Приклад E2E для Інтерактивного Phi 3 Mini 4K Instruct Chatbot з Whisper

Jupyter ноутбук під назвою [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) демонструє, як використовувати Microsoft Phi 3 Mini 4K instruct Demo для генерації тексту з аудіо або текстового вводу. У ноутбуці визначено кілька функцій:

1. `tts_file_name(text)`: ця функція генерує ім’я файлу на основі вхідного тексту для збереження згенерованого аудіофайлу.
1. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: ця функція використовує Edge TTS API для створення аудіофайлу зі списку фрагментів тексту. Вхідні параметри — список фрагментів, швидкість мовлення, ім’я голосу та шлях для збереження згенерованого аудіофайлу.
1. `talk(input_text)`: ця функція генерує аудіофайл за допомогою Edge TTS API і зберігає його під випадковим ім’ям у директорії /content/audio. Вхідний параметр — текст, який потрібно перетворити на мову.
1. `run_text_prompt(message, chat_history)`: ця функція використовує демонстрацію Microsoft Phi 3 Mini 4K instruct для генерації аудіофайлу з вхідного повідомлення та додає його до історії чату.
1. `run_audio_prompt(audio, chat_history)`: ця функція конвертує аудіофайл у текст за допомогою Whisper model API і передає його у функцію `run_text_prompt()`.
1. Код запускає додаток Gradio, який дозволяє користувачам взаємодіяти з демонстрацією Phi 3 Mini 4K instruct, вводячи повідомлення або завантажуючи аудіофайли. Вивід відображається у вигляді текстового повідомлення в додатку.

## Вирішення проблем

Встановлення драйверів Cuda GPU

1. Переконайтеся, що ваші Linux-додатки оновлені

    ```bash
    sudo apt update
    ```

1. Встановіть драйвери Cuda

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. Зареєструйте розташування драйвера cuda

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. Перевірка розміру пам’яті Nvidia GPU (потрібно 12 ГБ пам’яті GPU)

    ```bash
    nvidia-smi
    ```

1. Очищення кешу: якщо ви використовуєте PyTorch, можна викликати torch.cuda.empty_cache(), щоб звільнити всю невикористану кешовану пам’ять для використання іншими GPU-додатками

    ```python
    torch.cuda.empty_cache() 
    ```

1. Перевірка Nvidia Cuda

    ```bash
    nvcc --version
    ```

1. Виконайте наступні дії, щоб створити токен Hugging Face.

    - Перейдіть на [сторінку налаштувань токенів Hugging Face](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo).
    - Виберіть **New token**.
    - Введіть назву проекту, яку хочете використовувати.
    - Виберіть **Type** як **Write**.

> **Note**
>
> Якщо ви зіткнулися з такою помилкою:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> Щоб вирішити цю проблему, введіть наступну команду у вашому терміналі.
>
> ```bash
> sudo ldconfig
> ```

**Відмова від відповідальності**:  
Цей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ рідною мовою слід вважати авторитетним джерелом. Для критично важливої інформації рекомендується звертатися до професійного людського перекладу. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникли внаслідок використання цього перекладу.