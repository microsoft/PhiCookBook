<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c2bc0950f44919ac75a88c1a871680c2",
  "translation_date": "2025-07-09T18:56:11+00:00",
  "source_file": "md/03.FineTuning/Finetuning_VSCodeaitoolkit.md",
  "language_code": "uk"
}
-->
## Ласкаво просимо до AI Toolkit для VS Code

[AI Toolkit для VS Code](https://github.com/microsoft/vscode-ai-toolkit/tree/main) об’єднує різні моделі з Azure AI Studio Catalog та інших каталогів, таких як Hugging Face. Цей набір інструментів спрощує типові завдання розробки для створення AI-додатків з використанням генеративних AI-інструментів і моделей завдяки:
- Початку роботи з пошуком моделей та ігровою площадкою.
- Тонкому налаштуванню моделей і виконанню висновків із використанням локальних обчислювальних ресурсів.
- Віддаленому тонкому налаштуванню і виконанню висновків із використанням ресурсів Azure.

[Встановити AI Toolkit для VSCode](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio)

![AIToolkit FineTuning](../../../../imgs/03/intro/Aitoolkit.png)


**[Private Preview]** Одноразове розгортання Azure Container Apps для запуску тонкого налаштування моделей і виконання висновків у хмарі.

Тепер перейдемо до розробки вашого AI-додатку:

- [Ласкаво просимо до AI Toolkit для VS Code](../../../../md/03.FineTuning)
- [Локальна розробка](../../../../md/03.FineTuning)
  - [Підготовка](../../../../md/03.FineTuning)
  - [Активація Conda](../../../../md/03.FineTuning)
  - [Тільки тонке налаштування базової моделі](../../../../md/03.FineTuning)
  - [Тонке налаштування моделі та виконання висновків](../../../../md/03.FineTuning)
  - [Тонке налаштування моделі](../../../../md/03.FineTuning)
  - [Microsoft Olive](../../../../md/03.FineTuning)
  - [Приклади та ресурси для тонкого налаштування](../../../../md/03.FineTuning)
- [**\[Private Preview\]** Віддалена розробка](../../../../md/03.FineTuning)
  - [Вимоги](../../../../md/03.FineTuning)
  - [Налаштування віддаленого проекту](../../../../md/03.FineTuning)
  - [Розгортання ресурсів Azure](../../../../md/03.FineTuning)
  - [\[Опційно\] Додати токен Huggingface до секретів Azure Container App](../../../../md/03.FineTuning)
  - [Запуск тонкого налаштування](../../../../md/03.FineTuning)
  - [Розгортання кінцевої точки для висновків](../../../../md/03.FineTuning)
  - [Деплой кінцевої точки для висновків](../../../../md/03.FineTuning)
  - [Розширене використання](../../../../md/03.FineTuning)

## Локальна розробка
### Підготовка

1. Переконайтеся, що на хості встановлено драйвер NVIDIA.
2. Запустіть `huggingface-cli login`, якщо ви використовуєте HF для роботи з датасетами.
3. Пояснення налаштувань ключа `Olive` для всього, що впливає на використання пам’яті.

### Активація Conda
Оскільки ми використовуємо середовище WSL, яке є спільним, потрібно вручну активувати середовище conda. Після цього кроку можна запускати тонке налаштування або виконання висновків.

```bash
conda activate [conda-env-name] 
```

### Тільки тонке налаштування базової моделі
Щоб просто спробувати базову модель без тонкого налаштування, після активації conda виконайте цю команду.

```bash
cd inference

# Web browser interface allows to adjust a few parameters like max new token length, temperature and so on.
# User has to manually open the link (e.g. http://0.0.0.0:7860) in a browser after gradio initiates the connections.
python gradio_chat.py --baseonly
```

### Тонке налаштування моделі та виконання висновків

Після відкриття робочого простору в контейнері розробника відкрийте термінал (за замовчуванням шлях — корінь проекту), потім виконайте команду нижче для тонкого налаштування LLM на вибраному датасеті.

```bash
python finetuning/invoke_olive.py 
```

Контрольні точки та фінальна модель будуть збережені у папці `models`.

Далі виконайте висновки з тонко налаштованою моделлю через чати у `консолі`, `веб-браузері` або `prompt flow`.

```bash
cd inference

# Console interface.
python console_chat.py

# Web browser interface allows to adjust a few parameters like max new token length, temperature and so on.
# User has to manually open the link (e.g. http://127.0.0.1:7860) in a browser after gradio initiates the connections.
python gradio_chat.py
```

Щоб використовувати `prompt flow` у VS Code, зверніться до цього [швидкого старту](https://microsoft.github.io/promptflow/how-to-guides/quick-start.html).

### Тонке налаштування моделі

Далі завантажте відповідну модель залежно від наявності GPU на вашому пристрої.

Щоб розпочати локальну сесію тонкого налаштування з використанням QLoRA, оберіть модель для тонкого налаштування з нашого каталогу.
| Платформа(и) | Наявність GPU | Назва моделі | Розмір (ГБ) |
|---------|---------|--------|--------|
| Windows | Так | Phi-3-mini-4k-**directml**-int4-awq-block-128-onnx | 2.13GB |
| Linux | Так | Phi-3-mini-4k-**cuda**-int4-onnx | 2.30GB |
| Windows<br>Linux | Ні | Phi-3-mini-4k-**cpu**-int4-rtn-block-32-acc-level-4-onnx | 2.72GB |

**_Примітка_** Для завантаження моделей не потрібен обліковий запис Azure.

Модель Phi3-mini (int4) має розмір приблизно 2-3 ГБ. Залежно від швидкості вашого інтернету, завантаження може зайняти кілька хвилин.

Почніть з вибору назви проекту та його розташування.
Далі оберіть модель із каталогу моделей. Вам буде запропоновано завантажити шаблон проекту. Потім можна натиснути "Configure Project" для налаштування різних параметрів.

### Microsoft Olive

Ми використовуємо [Olive](https://microsoft.github.io/Olive/why-olive.html) для запуску тонкого налаштування QLoRA на моделі PyTorch з нашого каталогу. Всі налаштування за замовчуванням оптимізовані для локального запуску процесу тонкого налаштування з ефективним використанням пам’яті, але їх можна підлаштувати під ваші потреби.

### Приклади та ресурси для тонкого налаштування

- [Посібник для початку роботи з тонким налаштуванням](https://learn.microsoft.com/windows/ai/toolkit/toolkit-fine-tune)
- [Тонке налаштування з використанням датасету HuggingFace](https://github.com/microsoft/vscode-ai-toolkit/blob/main/archive/walkthrough-hf-dataset.md)
- [Тонке налаштування з простим датасетом](https://github.com/microsoft/vscode-ai-toolkit/blob/main/archive/walkthrough-simple-dataset.md)

## **[Private Preview]** Віддалена розробка

### Вимоги

1. Щоб запускати тонке налаштування моделей у вашому віддаленому середовищі Azure Container App, переконайтеся, що у вашій підписці достатньо GPU-ресурсів. Подайте [запит у службу підтримки](https://azure.microsoft.com/support/create-ticket/) для отримання необхідної потужності для вашого додатку. [Дізнатися більше про GPU-ресурси](https://learn.microsoft.com/azure/container-apps/workload-profiles-overview)
2. Якщо ви використовуєте приватний датасет на HuggingFace, переконайтеся, що у вас є [обліковий запис HuggingFace](https://huggingface.co/?WT.mc_id=aiml-137032-kinfeylo) та [створений токен доступу](https://huggingface.co/docs/hub/security-tokens?WT.mc_id=aiml-137032-kinfeylo)
3. Увімкніть прапорець функції Remote Fine-tuning and Inference у AI Toolkit для VS Code
   1. Відкрийте налаштування VS Code, вибравши *File -> Preferences -> Settings*.
   2. Перейдіть до *Extensions* і виберіть *AI Toolkit*.
   3. Увімкніть опцію *"Enable Remote Fine-tuning And Inference"*.
   4. Перезапустіть VS Code для застосування змін.

- [Віддалене тонке налаштування](https://github.com/microsoft/vscode-ai-toolkit/blob/main/archive/remote-finetuning.md)

### Налаштування віддаленого проекту
1. Виконайте команду `AI Toolkit: Focus on Resource View` через палітру команд.
2. Перейдіть до *Model Fine-tuning*, щоб отримати доступ до каталогу моделей. Призначте ім’я проекту та виберіть його розташування на вашому комп’ютері. Потім натисніть кнопку *"Configure Project"*.
3. Налаштування проекту
    1. Уникайте увімкнення опції *"Fine-tune locally"*.
    2. З’являться налаштування Olive з попередньо встановленими значеннями за замовчуванням. Відкоригуйте та заповніть ці параметри за потребою.
    3. Перейдіть до *Generate Project*. Цей етап використовує WSL і передбачає створення нового середовища Conda, готуючись до майбутніх оновлень із підтримкою Dev Containers.
4. Натисніть *"Relaunch Window In Workspace"*, щоб відкрити ваш віддалений проект розробки.

> **Примітка:** Проект наразі працює або локально, або віддалено в AI Toolkit для VS Code. Якщо під час створення проекту ви обрали *"Fine-tune locally"*, він працюватиме виключно в WSL без можливості віддаленої розробки. Якщо ж не увімкнути *"Fine-tune locally"*, проект буде обмежений віддаленим середовищем Azure Container App.

### Розгортання ресурсів Azure
Щоб почати, потрібно розгорнути ресурси Azure для віддаленого тонкого налаштування. Для цього виконайте команду `AI Toolkit: Provision Azure Container Apps job for fine-tuning` через палітру команд.

Слідкуйте за процесом розгортання за посиланням, яке відображається в каналі виводу.

### [Опційно] Додати токен Huggingface до секретів Azure Container App
Якщо ви використовуєте приватний датасет HuggingFace, встановіть ваш токен HuggingFace як змінну середовища, щоб уникнути необхідності ручного входу в Hugging Face Hub.
Це можна зробити за допомогою команди `AI Toolkit: Add Azure Container Apps Job secret for fine-tuning`. У цій команді можна вказати ім’я секрету як [`HF_TOKEN`](https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables#hftoken) і використати ваш токен Hugging Face як значення секрету.

### Запуск тонкого налаштування
Щоб розпочати віддалене тонке налаштування, виконайте команду `AI Toolkit: Run fine-tuning`.

Щоб переглянути системні та консольні логи, ви можете відвідати портал Azure за посиланням у панелі виводу (додаткові кроки див. у [Перегляд і запит логів в Azure](https://aka.ms/ai-toolkit/remote-provision#view-and-query-logs-on-azure)). Або переглянути логи консолі безпосередньо у панелі виводу VSCode, виконавши команду `AI Toolkit: Show the running fine-tuning job streaming logs`.
> **Примітка:** Завдання може бути в черзі через нестачу ресурсів. Якщо логи не відображаються, виконайте команду `AI Toolkit: Show the running fine-tuning job streaming logs`, зачекайте деякий час і повторіть команду для повторного підключення до потокового логу.

Під час цього процесу для тонкого налаштування буде використано QLoRA, який створить адаптери LoRA для моделі, що використовуються під час виконання висновків.
Результати тонкого налаштування зберігатимуться в Azure Files.

### Розгортання кінцевої точки для висновків
Після навчання адаптерів у віддаленому середовищі використовуйте простий додаток Gradio для взаємодії з моделлю.
Подібно до процесу тонкого налаштування, потрібно налаштувати ресурси Azure для віддаленого виконання висновків, виконавши команду `AI Toolkit: Provision Azure Container Apps for inference` через палітру команд.

За замовчуванням підписка та група ресурсів для висновків мають співпадати з тими, що використовуються для тонкого налаштування. Виконання висновків використовуватиме те саме середовище Azure Container App і матиме доступ до моделі та адаптера моделі, збережених в Azure Files, які були створені під час етапу тонкого налаштування.

### Деплой кінцевої точки для висновків
Якщо ви хочете змінити код для висновків або перезавантажити модель для висновків, виконайте команду `AI Toolkit: Deploy for inference`. Це синхронізує ваш останній код з Azure Container App і перезапустить репліку.

Після успішного розгортання ви можете отримати доступ до API висновків, натиснувши кнопку "*Go to Inference Endpoint*" у сповіщенні VSCode. Або веб-адресу API можна знайти у `ACA_APP_ENDPOINT` у файлі `./infra/inference.config.json` та у панелі виводу. Тепер ви готові оцінювати модель за допомогою цієї кінцевої точки.

### Розширене використання
Для отримання додаткової інформації про віддалену розробку з AI Toolkit зверніться до документації [Тонке налаштування моделей віддалено](https://aka.ms/ai-toolkit/remote-provision) та [Виконання висновків з тонко налаштованою моделлю](https://aka.ms/ai-toolkit/remote-inference).

**Відмова від відповідальності**:  
Цей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ рідною мовою слід вважати авторитетним джерелом. Для критично важливої інформації рекомендується звертатися до професійного людського перекладу. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникли внаслідок використання цього перекладу.