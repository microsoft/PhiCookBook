<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-07-16T18:13:20+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "uk"
}
-->
# Початок роботи з Phi-3 локально

Цей посібник допоможе вам налаштувати локальне середовище для запуску моделі Phi-3 за допомогою Ollama. Ви можете запускати модель кількома способами, зокрема через GitHub Codespaces, VS Code Dev Containers або у вашому локальному середовищі.

## Налаштування середовища

### GitHub Codespaces

Ви можете запустити цей шаблон віртуально, використовуючи GitHub Codespaces. Кнопка відкриє веб-версію VS Code у вашому браузері:

1. Відкрийте шаблон (це може зайняти кілька хвилин):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Відкрийте термінал

### VS Code Dev Containers

⚠️ Цей варіант працюватиме лише, якщо Docker Desktop має виділено щонайменше 16 ГБ оперативної пам’яті. Якщо у вас менше 16 ГБ, ви можете спробувати [варіант з GitHub Codespaces](../../../../../md/01.Introduction/01) або [налаштувати локально](../../../../../md/01.Introduction/01).

Пов’язаний варіант — VS Code Dev Containers, який відкриє проєкт у вашому локальному VS Code за допомогою [розширення Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Запустіть Docker Desktop (встановіть, якщо ще не встановлено)
2. Відкрийте проєкт:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. У вікні VS Code, що відкриється, коли з’являться файли проєкту (це може зайняти кілька хвилин), відкрийте термінал.
4. Продовжуйте з [кроками розгортання](../../../../../md/01.Introduction/01)

### Локальне середовище

1. Переконайтеся, що встановлені такі інструменти:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Тестування моделі

1. Попросіть Ollama завантажити та запустити модель phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Завантаження моделі займе кілька хвилин.

2. Коли вивід покаже "success", ви можете надіслати повідомлення цій моделі через командний рядок.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Через кілька секунд ви побачите потік відповіді від моделі.

4. Щоб дізнатися про різні техніки роботи з мовними моделями, відкрийте Python-ноутбук [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) і виконайте кожну клітинку. Якщо ви використовували модель, відмінну від 'phi3:mini', змініть `MODEL_NAME` у першій клітинці.

5. Щоб поспілкуватися з моделлю phi3:mini з Python, відкрийте файл [chat.py](../../../../../code/01.Introduce/chat.py) і запустіть його. За потреби ви можете змінити `MODEL_NAME` у верхній частині файлу, а також модифікувати системне повідомлення або додати приклади few-shot.

**Відмова від відповідальності**:  
Цей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ рідною мовою слід вважати авторитетним джерелом. Для критично важливої інформації рекомендується звертатися до професійного людського перекладу. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникли внаслідок використання цього перекладу.