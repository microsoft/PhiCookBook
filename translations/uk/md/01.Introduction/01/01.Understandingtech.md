<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:49:14+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "uk"
}
-->
# Ключові згадані технології

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) — низькорівневий API для апаратно-прискореного машинного навчання, побудований на базі DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) — платформа паралельних обчислень і модель API, розроблена Nvidia, що дозволяє виконувати загального призначення обробку на графічних процесорах (GPU).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) — відкритий формат для представлення моделей машинного навчання, що забезпечує сумісність між різними ML-фреймворками.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) — формат для представлення та оновлення моделей машинного навчання, особливо корисний для невеликих мовних моделей, які ефективно працюють на CPU з 4-8-бітною квантизацією.

## DirectML

DirectML — це низькорівневий API, який забезпечує апаратне прискорення машинного навчання. Він побудований на базі DirectX 12 для використання GPU-прискорення і є незалежним від виробника, тобто не вимагає змін у коді для роботи з різними GPU. Переважно використовується для навчання моделей і виконання висновків на GPU.

Щодо підтримки апаратного забезпечення, DirectML розроблений для роботи з широким спектром GPU, включно з інтегрованими та дискретними GPU AMD, інтегрованими GPU Intel і дискретними GPU NVIDIA. Це частина платформи Windows AI і підтримується на Windows 10 та 11, що дозволяє навчати моделі та виконувати висновки на будь-якому пристрої з Windows.

Були оновлення та нові можливості, пов’язані з DirectML, наприклад підтримка до 150 операторів ONNX і використання як ONNX runtime, так і WinML. За підтримкою стоять провідні виробники апаратного забезпечення (IHVs), кожен з яких реалізує різні метакоманди.

## CUDA

CUDA (Compute Unified Device Architecture) — це платформа паралельних обчислень і модель API, створена Nvidia. Вона дозволяє розробникам використовувати графічний процесор з підтримкою CUDA для загального призначення обробки — підхід, відомий як GPGPU (General-Purpose computing on Graphics Processing Units). CUDA є ключовим фактором прискорення GPU від Nvidia і широко застосовується в різних сферах, включно з машинним навчанням, науковими обчисленнями та обробкою відео.

Підтримка апаратного забезпечення для CUDA обмежена GPU Nvidia, оскільки це пропрієтарна технологія Nvidia. Кожна архітектура підтримує певні версії CUDA toolkit, який надає необхідні бібліотеки та інструменти для розробки і запуску CUDA-додатків.

## ONNX

ONNX (Open Neural Network Exchange) — відкритий формат для представлення моделей машинного навчання. Він визначає розширювану модель обчислювального графа, а також вбудовані оператори і стандартні типи даних. ONNX дозволяє розробникам переносити моделі між різними ML-фреймворками, забезпечуючи сумісність і спрощуючи створення та розгортання AI-додатків.

Phi3 mini може працювати з ONNX Runtime на CPU і GPU на різних пристроях, включно з серверними платформами, Windows, Linux, Mac-десктопами та мобільними CPU.  
Оптимізовані конфігурації, які ми додали:

- ONNX-моделі для int4 DML: квантизовані до int4 за допомогою AWQ  
- ONNX-модель для fp16 CUDA  
- ONNX-модель для int4 CUDA: квантизована до int4 за допомогою RTN  
- ONNX-модель для int4 CPU і Mobile: квантизована до int4 за допомогою RTN  

## Llama.cpp

Llama.cpp — це відкрита бібліотека, написана на C++. Вона виконує висновки на різних великих мовних моделях (LLM), включно з Llama. Розроблена разом із бібліотекою ggml (загального призначення для тензорів), llama.cpp прагне забезпечити швидший висновок і менше споживання пам’яті порівняно з оригінальною реалізацією на Python. Підтримує апаратну оптимізацію, квантизацію та пропонує простий API і приклади. Якщо вас цікавить ефективний висновок LLM, llama.cpp варто вивчити, оскільки Phi3 може запускати Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) — це формат для представлення та оновлення моделей машинного навчання. Він особливо корисний для невеликих мовних моделей (SLM), які ефективно працюють на CPU з 4-8-бітною квантизацією. GGUF підходить для швидкого прототипування та запуску моделей на периферійних пристроях або в пакетних завданнях, таких як CI/CD пайплайни.

**Відмова від відповідальності**:  
Цей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ рідною мовою слід вважати авторитетним джерелом. Для критично важливої інформації рекомендується звертатися до професійного людського перекладу. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникли внаслідок використання цього перекладу.