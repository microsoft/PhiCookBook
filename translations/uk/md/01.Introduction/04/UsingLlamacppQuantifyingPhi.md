<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "462bddc47427d8785f3c9fd817b346fe",
  "translation_date": "2025-07-16T22:13:13+00:00",
  "source_file": "md/01.Introduction/04/UsingLlamacppQuantifyingPhi.md",
  "language_code": "uk"
}
-->
# **Квантизація сімейства Phi за допомогою llama.cpp**

## **Що таке llama.cpp**

llama.cpp — це бібліотека з відкритим кодом, переважно написана на C++, яка виконує інференс на різних великих мовних моделях (LLM), таких як Llama. Її основна мета — забезпечити передову продуктивність інференсу LLM на широкому спектрі апаратного забезпечення з мінімальними налаштуваннями. Крім того, для цієї бібліотеки доступні Python-обгортки, які пропонують високорівневий API для доповнення тексту та сумісний з OpenAI веб-сервер.

Головна мета llama.cpp — надати можливість запуску інференсу LLM з мінімальними налаштуваннями та передовою продуктивністю на різному апаратному забезпеченні — локально та в хмарі.

- Проста реалізація на C/C++ без залежностей
- Apple silicon підтримується на рівні першокласного користувача — оптимізовано через ARM NEON, Accelerate та Metal frameworks
- Підтримка AVX, AVX2 та AVX512 для архітектур x86
- Квантизація цілими числами 1.5-біт, 2-біт, 3-біт, 4-біт, 5-біт, 6-біт та 8-біт для швидшого інференсу та зменшення використання пам’яті
- Користувацькі CUDA ядра для запуску LLM на GPU NVIDIA (підтримка AMD GPU через HIP)
- Підтримка бекендів Vulkan та SYCL
- Гібридний інференс CPU+GPU для часткового прискорення моделей, більших за загальну ємність VRAM

## **Квантизація Phi-3.5 за допомогою llama.cpp**

Модель Phi-3.5-Instruct можна квантизувати за допомогою llama.cpp, але Phi-3.5-Vision та Phi-3.5-MoE поки що не підтримуються. Формат, у який конвертує llama.cpp — gguf, який також є найпоширенішим форматом квантизації.

На Hugging Face є велика кількість моделей у квантизованому форматі GGUF. AI Foundry, Ollama та LlamaEdge використовують llama.cpp, тому моделі GGUF також часто застосовуються.

### **Що таке GGUF**

GGUF — це бінарний формат, оптимізований для швидкого завантаження та збереження моделей, що робить його дуже ефективним для інференсу. GGUF розроблений для використання з GGML та іншими виконавцями. GGUF створив @ggerganov, який також є розробником llama.cpp — популярного фреймворку для інференсу LLM на C/C++. Моделі, спочатку розроблені у фреймворках на кшталт PyTorch, можна конвертувати у формат GGUF для використання з цими рушіями.

### **ONNX проти GGUF**

ONNX — це традиційний формат машинного/глибинного навчання, який добре підтримується в різних AI-фреймворках і має широкі сценарії використання на edge-пристроях. Щодо GGUF, він базується на llama.cpp і фактично з’явився в епоху GenAI. Обидва мають схожі сфери застосування. Якщо вам потрібна краща продуктивність на вбудованому обладнанні та в прикладних шарах, ONNX може бути вашим вибором. Якщо ви використовуєте похідний фреймворк і технології llama.cpp, тоді GGUF може бути кращим варіантом.

### **Квантизація Phi-3.5-Instruct за допомогою llama.cpp**

**1. Налаштування середовища**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. Квантизація**

Конвертація Phi-3.5-Instruct у FP16 GGUF за допомогою llama.cpp


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Квантизація Phi-3.5 у INT4


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. Тестування**

Встановлення llama-cpp-python


```bash

pip install llama-cpp-python -U

```

***Примітка*** 

Якщо ви використовуєте Apple Silicon, будь ласка, встановіть llama-cpp-python таким чином


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

Тестування 


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **Ресурси**

1. Дізнайтеся більше про llama.cpp [https://github.com/ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)
2. Дізнайтеся більше про onnxruntime [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)
3. Дізнайтеся більше про GGUF [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**Відмова від відповідальності**:  
Цей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ рідною мовою слід вважати авторитетним джерелом. Для критично важливої інформації рекомендується звертатися до професійного людського перекладу. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникли внаслідок використання цього перекладу.