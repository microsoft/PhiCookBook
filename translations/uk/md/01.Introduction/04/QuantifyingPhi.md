# **Квантифікація родини Phi**

Квантифікація моделі — це процес відображення параметрів (таких як ваги та значення активації) у моделі нейронної мережі з великого діапазону значень (зазвичай неперервного) до меншого скінченного діапазону значень. Ця технологія може зменшити розмір і обчислювальну складність моделі та підвищити ефективність роботи моделі в середовищах з обмеженими ресурсами, таких як мобільні пристрої або вбудовані системи. Квантифікація моделі досягає стиснення за рахунок зменшення точності параметрів, але водночас вводить певні втрати точності. Тому у процесі квантифікації необхідно балансувати розмір моделі, обчислювальну складність і точність. Поширені методи квантифікації включають квантифікацію з фіксованою комою, квантифікацію з рухомою комою тощо. Ви можете обирати відповідну стратегію квантифікації залежно від конкретного сценарію та потреб.

Ми прагнемо розгорнути модель GenAI на периферійних пристроях і дозволити більшій кількості пристроїв брати участь у сценаріях GenAI, таких як мобільні пристрої, AI PC/Copilot+PC та традиційні IoT-пристрої. Через модель з квантифікацією ми можемо розгорнути її на різних периферійних пристроях залежно від конкретного пристрою. У поєднанні з фреймворком прискорення моделі та моделлю квантифікації, що надаються виробниками апаратного забезпечення, ми можемо побудувати кращі прикладні сценарії SLM.

У сценарії квантифікації ми маємо різні точності (INT4, INT8, FP16, FP32). Нижче наведено пояснення поширених точностей квантифікації.

### **INT4**

INT4 — це радикальний метод квантифікації, який квантифікує ваги та значення активації моделі у 4-бітові цілі числа. Квантифікація INT4 зазвичай призводить до більшої втрати точності через менший діапазон представлення та нижчу точність. Проте, порівняно з квантифікацією INT8, INT4 дозволяє ще більше знизити вимоги до збереження даних та обчислювальну складність моделі. Слід зауважити, що у практичних застосунках квантифікація INT4 використовується досить рідко через занадто низьку точність, яка може спричинити суттєве погіршення продуктивності моделі. Крім того, не всі апаратні засоби підтримують операції INT4, тому сумісність апаратного забезпечення слід враховувати при виборі методу квантифікації.

### **INT8**

Квантифікація INT8 — це процес перетворення вагів і активацій моделі з чисел з плаваючою крапкою у 8-бітові цілі числа. Хоча числовий діапазон, представлений цілими числами INT8, менший і менш точний, це може суттєво знизити вимоги до збереження і обчислень. У квантифікації INT8 ваги та значення активації проходять процес квантифікації, який включає масштабування та зсув, щоб максимально зберегти оригінальну інформацію у форматі з плаваючою крапкою. Під час виконання інференсу ці квантизовані значення будуть деквантизовані назад у числа з плаваючою крапкою для обчислень, а потім знову квантизовані в INT8 для наступного кроку. Цей метод забезпечує достатню точність для більшості застосунків, зберігаючи при цьому високу обчислювальну ефективність.

### **FP16**

Формат FP16, тобто 16-бітові числа з плаваючою крапкою (float16), зменшує обсяг пам’яті вдвічі порівняно з 32-бітовими числами з плаваючою крапкою (float32), що має значні переваги в масштабних завданнях глибокого навчання. Формат FP16 дозволяє завантажувати більші моделі або обробляти більше даних у межах тих же обмежень пам’яті GPU. Оскільки сучасне апаратне забезпечення GPU продовжує підтримувати операції FP16, використання формату FP16 може також покращувати швидкість обчислень. Однак формат FP16 має і свої недоліки — нижчу точність, що може у деяких випадках призводити до чисельної нестабільності або втрати точності.

### **FP32**

Формат FP32 забезпечує вищу точність і може точно представляти широкий діапазон значень. У сценаріях, де виконуються складні математичні операції або потрібні високоточні результати, перевагу надають формату FP32. Однак висока точність означає більше використання пам’яті та довший час обчислень. Для масштабних моделей глибокого навчання, особливо якщо багато параметрів моделі та величезний обсяг даних, формат FP32 може призвести до нестачі пам’яті GPU або зниження швидкості інференсу.

На мобільних пристроях чи IoT-пристроях ми можемо конвертувати моделі Phi-3.x до INT4, у той час як AI PC / Copilot PC може використовувати вищу точність, таку як INT8, FP16, FP32.

На даний момент різні виробники апаратного забезпечення мають свої фреймворки для підтримки генеративних моделей, такі як OpenVINO від Intel, QNN від Qualcomm, MLX від Apple та CUDA від Nvidia, які у поєднанні з квантифікацією моделі дозволяють виконувати локальне розгортання.

З точки зору технологій, після квантифікації ми маємо різну підтримку форматів, таких як PyTorch / TensorFlow, GGUF і ONNX. Я зробив порівняння форматів і сценарії застосування між GGUF та ONNX. Тут я рекомендую формат квантифікації ONNX, який має хорошу підтримку від фреймворку моделі до апаратного забезпечення. У цій главі ми зосередимось на ONNX Runtime для GenAI, OpenVINO та Apple MLX для виконання квантифікації моделі (якщо у вас є кращі методи, ви також можете запропонувати їх через PR).

**У цій главі розглядається**

1. [Квантифікація Phi-3.5 / 4 за допомогою llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Квантифікація Phi-3.5 / 4 за допомогою розширень Generative AI для onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Квантифікація Phi-3.5 / 4 за допомогою Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Квантифікація Phi-3.5 / 4 за допомогою Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Відмова від відповідальності**:  
Цей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, просимо враховувати, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ рідною мовою слід вважати офіційним джерелом. Для критично важливої інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння чи неправильне тлумачення, що виникли внаслідок використання цього перекладу.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->