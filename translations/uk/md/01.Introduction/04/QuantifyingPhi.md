<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-07-16T21:51:45+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "uk"
}
-->
# **Квантифікація родини Phi**

Квантифікація моделі — це процес відображення параметрів (таких як ваги та значення активації) у нейронній мережі з великого діапазону значень (зазвичай неперервного) у менший скінченний діапазон. Ця технологія дозволяє зменшити розмір і обчислювальну складність моделі, а також підвищити ефективність її роботи в умовах обмежених ресурсів, наприклад, на мобільних пристроях або вбудованих системах. Квантифікація моделі досягає стиснення за рахунок зниження точності параметрів, але при цьому виникає певна втрата точності. Тому в процесі квантифікації необхідно балансувати між розміром моделі, обчислювальною складністю та точністю. Поширені методи квантифікації включають квантифікацію з фіксованою точкою, квантифікацію з плаваючою точкою тощо. Ви можете обрати відповідну стратегію квантифікації залежно від конкретного сценарію та потреб.

Ми прагнемо розгорнути модель GenAI на периферійних пристроях і дозволити більшій кількості пристроїв працювати в сценаріях GenAI, таких як мобільні пристрої, AI PC/Copilot+PC та традиційні IoT-пристрої. Завдяки квантифікованій моделі ми можемо розгортати її на різних периферійних пристроях залежно від типу пристрою. У поєднанні з фреймворком прискорення моделей і квантифікованими моделями, які надають виробники апаратного забезпечення, ми можемо створювати кращі сценарії застосування SLM.

У сценаріях квантифікації ми маємо різні рівні точності (INT4, INT8, FP16, FP32). Нижче наведено пояснення найбільш поширених точностей квантифікації.

### **INT4**

Квантифікація INT4 — це радикальний метод, який квантифікує ваги та значення активації моделі у 4-бітні цілі числа. Квантифікація INT4 зазвичай призводить до більшої втрати точності через менший діапазон представлення та нижчу точність. Однак порівняно з INT8, INT4 дозволяє ще більше зменшити вимоги до зберігання та обчислювальну складність моделі. Варто зазначити, що квантифікація INT4 досить рідкісна у практичних застосуваннях, оскільки надто низька точність може суттєво погіршити продуктивність моделі. Крім того, не всі апаратні засоби підтримують операції INT4, тому при виборі методу квантифікації слід враховувати сумісність з апаратним забезпеченням.

### **INT8**

Квантифікація INT8 — це процес перетворення ваг і активацій моделі з чисел з плаваючою точкою у 8-бітні цілі числа. Хоча числовий діапазон INT8 менший і менш точний, це значно знижує вимоги до зберігання та обчислень. Під час квантифікації INT8 ваги та значення активації проходять процес масштабування та зсуву, щоб максимально зберегти інформацію оригінальних чисел з плаваючою точкою. Під час інференсу ці квантифіковані значення деквантифікуються назад у числа з плаваючою точкою для обчислень, а потім знову квантифікуються в INT8 для наступного кроку. Цей метод забезпечує достатню точність у більшості застосувань при високій обчислювальній ефективності.

### **FP16**

Формат FP16, тобто 16-бітні числа з плаваючою точкою (float16), зменшує обсяг пам’яті вдвічі порівняно з 32-бітними числами з плаваючою точкою (float32), що має значні переваги у великих глибоких нейронних мережах. Формат FP16 дозволяє завантажувати більші моделі або обробляти більше даних у межах обмежень пам’яті GPU. Оскільки сучасне апаратне забезпечення GPU все більше підтримує операції FP16, використання цього формату може також покращити швидкість обчислень. Проте FP16 має й свої недоліки — нижчу точність, що іноді може призводити до числової нестабільності або втрати точності.

### **FP32**

Формат FP32 забезпечує вищу точність і може точно представляти широкий діапазон значень. У випадках, коли виконуються складні математичні операції або потрібні високоточні результати, перевагу надають формату FP32. Однак висока точність означає більші витрати пам’яті та довший час обчислень. Для великих моделей глибокого навчання, особливо коли параметрів моделі багато і обсяг даних великий, формат FP32 може спричинити нестачу пам’яті GPU або зниження швидкості інференсу.

На мобільних або IoT-пристроях ми можемо конвертувати моделі Phi-3.x у INT4, тоді як AI PC / Copilot PC можуть використовувати вищу точність, таку як INT8, FP16, FP32.

Наразі різні виробники апаратного забезпечення мають власні фреймворки для підтримки генеративних моделей, такі як Intel OpenVINO, Qualcomm QNN, Apple MLX, Nvidia CUDA тощо, які у поєднанні з квантифікацією моделей дозволяють виконувати локальне розгортання.

З технічної точки зору, після квантифікації ми підтримуємо різні формати, такі як PyTorch / Tensorflow, GGUF та ONNX. Я провів порівняння форматів і сценаріїв застосування між GGUF та ONNX. Тут я рекомендую формат квантифікації ONNX, який має хорошу підтримку від фреймворку моделі до апаратного забезпечення. У цій главі ми зосередимося на ONNX Runtime для GenAI, OpenVINO та Apple MLX для виконання квантифікації моделей (якщо у вас є кращі ідеї, ви можете запропонувати їх, подавши PR).

**Ця глава включає**

1. [Квантифікація Phi-3.5 / 4 за допомогою llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Квантифікація Phi-3.5 / 4 за допомогою розширень Generative AI для onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Квантифікація Phi-3.5 / 4 за допомогою Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Квантифікація Phi-3.5 / 4 за допомогою Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

**Відмова від відповідальності**:  
Цей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ рідною мовою слід вважати авторитетним джерелом. Для критично важливої інформації рекомендується звертатися до професійного людського перекладу. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникли внаслідок використання цього перекладу.