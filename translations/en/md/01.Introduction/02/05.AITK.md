<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4951d458c0b60c02cd1e751b40903877",
  "translation_date": "2025-07-16T19:20:07+00:00",
  "source_file": "md/01.Introduction/02/05.AITK.md",
  "language_code": "en"
}
-->
# Phi Family in AITK

[AI Toolkit for VS Code](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio) makes generative AI app development easier by combining advanced AI development tools and models from the Azure AI Foundry Catalog and other sources like Hugging Face. You can browse AI models powered by GitHub Models and Azure AI Foundry Model Catalogs, download them locally or remotely, fine-tune, test, and integrate them into your applications.

The AI Toolkit Preview runs locally. Whether you use local inference or fine-tuning depends on the model you choose; some may require a GPU such as an NVIDIA CUDA GPU. You can also run GitHub Models directly with AITK.

## Getting Started

[Learn more about installing the Windows Subsystem for Linux](https://learn.microsoft.com/windows/wsl/install?WT.mc_id=aiml-137032-kinfeylo)

and [how to change the default distribution](https://learn.microsoft.com/windows/wsl/install#change-the-default-linux-distribution-installed).

[AI Toolkit GitHub Repo](https://github.com/microsoft/vscode-ai-toolkit/)

- Windows, Linux, macOS
  
- For fine-tuning on both Windows and Linux, an Nvidia GPU is required. Additionally, **Windows** needs the Windows Subsystem for Linux with Ubuntu 18.04 or later. [Learn more about installing the Windows Subsystem for Linux](https://learn.microsoft.com/windows/wsl/install) and [changing the default distribution](https://learn.microsoft.com/windows/wsl/install#change-the-default-linux-distribution-installed).

### Install AI Toolkit

AI Toolkit is delivered as a [Visual Studio Code Extension](https://code.visualstudio.com/docs/setup/additional-components#_vs-code-extensions), so first install [VS Code](https://code.visualstudio.com/docs/setup/windows?WT.mc_id=aiml-137032-kinfeylo), then download AI Toolkit from the [VS Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio).  
The [AI Toolkit is available in the Visual Studio Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio) and can be installed like any other VS Code extension.

If you’re new to installing VS Code extensions, follow these steps:

### Sign In

1. In the Activity Bar in VS Code, select **Extensions**  
2. In the Extensions Search bar, type "AI Toolkit"  
3. Select "AI Toolkit for Visual Studio Code"  
4. Click **Install**

Now you’re ready to use the extension!

You will be asked to sign in to GitHub—click "Allow" to continue. You will be redirected to the GitHub sign-in page.

Please sign in and follow the prompts. After successful sign-in, you will be redirected back to VS Code.

Once installed, the AI Toolkit icon will appear in your Activity Bar.

Let’s explore the available features!

### Available Actions

The AI Toolkit’s main sidebar is organized into:

- **Models**  
- **Resources**  
- **Playground**  
- **Fine-tuning**  
- **Evaluation**

These are found in the Resources section. To get started, select **Model Catalog**.

### Download a model from the catalog

When you open AI Toolkit from the VS Code sidebar, you can choose from the following options:

![AI toolkit model catalog](../../../../../translated_images/en/AItoolkitmodel_catalog.7a7be6a7d8468d31.png)

- Find a supported model in the **Model Catalog** and download it locally  
- Test model inference in the **Model Playground**  
- Fine-tune models locally or remotely in **Model Fine-tuning**  
- Deploy fine-tuned models to the cloud via the AI Toolkit command palette  
- Evaluate models

> [!NOTE]
>
> **GPU vs CPU**
>
> Model cards display the model size, platform, and accelerator type (CPU, GPU). For best performance on **Windows devices with at least one GPU**, choose model versions optimized specifically for Windows.
>
> This ensures the model is optimized for the DirectML accelerator.
>
> Model names follow the format:
>
> - `{model_name}-{accelerator}-{quantization}-{format}`.
>
>To check if your Windows device has a GPU, open **Task Manager** and go to the **Performance** tab. GPUs will be listed as "GPU 0", "GPU 1", etc.

### Run the model in the playground

After setting all parameters, click **Generate Project**.

Once your model has downloaded, click **Load in Playground** on the model card in the catalog to:

- Start the model download  
- Install all prerequisites and dependencies  
- Create a VS Code workspace

![Load model in playground](../../../../../translated_images/en/AItoolkitload_model_into_playground.dcef5355b1653b52.png)

### Use the REST API in your application

The AI Toolkit includes a local REST API web server **on port 5272** that uses the [OpenAI chat completions format](https://platform.openai.com/docs/api-reference/chat/create).

This lets you test your application locally without relying on a cloud AI model service. For example, the following JSON file shows how to configure the request body:

```json
{
    "model": "Phi-4",
    "messages": [
        {
            "role": "user",
            "content": "what is the golden ratio?"
        }
    ],
    "temperature": 0.7,
    "top_p": 1,
    "top_k": 10,
    "max_tokens": 100,
    "stream": true
}
```

You can test the REST API using tools like [Postman](https://www.postman.com/) or the CURL utility:

```bash
curl -vX POST http://127.0.0.1:5272/v1/chat/completions -H 'Content-Type: application/json' -d @body.json
```

### Using the OpenAI client library for Python

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://127.0.0.1:5272/v1/", 
    api_key="x" # required for the API but not used
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "what is the golden ratio?",
        }
    ],
    model="Phi-4",
)

print(chat_completion.choices[0].message.content)
```

### Using Azure OpenAI client library for .NET

Add the [Azure OpenAI client library for .NET](https://www.nuget.org/packages/Azure.AI.OpenAI/) to your project via NuGet:

```bash
dotnet add {project_name} package Azure.AI.OpenAI --version 1.0.0-beta.17
```

Add a C# file named **OverridePolicy.cs** to your project and paste the following code:

```csharp
// OverridePolicy.cs
using Azure.Core.Pipeline;
using Azure.Core;

internal partial class OverrideRequestUriPolicy(Uri overrideUri)
    : HttpPipelineSynchronousPolicy
{
    private readonly Uri _overrideUri = overrideUri;

    public override void OnSendingRequest(HttpMessage message)
    {
        message.Request.Uri.Reset(_overrideUri);
    }
}
```

Then, paste the following code into your **Program.cs** file:

```csharp
// Program.cs
using Azure.AI.OpenAI;

Uri localhostUri = new("http://localhost:5272/v1/chat/completions");

OpenAIClientOptions clientOptions = new();
clientOptions.AddPolicy(
    new OverrideRequestUriPolicy(localhostUri),
    Azure.Core.HttpPipelinePosition.BeforeTransport);
OpenAIClient client = new(openAIApiKey: "unused", clientOptions);

ChatCompletionsOptions options = new()
{
    DeploymentName = "Phi-4",
    Messages =
    {
        new ChatRequestSystemMessage("You are a helpful assistant. Be brief and succinct."),
        new ChatRequestUserMessage("What is the golden ratio?"),
    }
};

StreamingResponse<StreamingChatCompletionsUpdate> streamingChatResponse
    = await client.GetChatCompletionsStreamingAsync(options);

await foreach (StreamingChatCompletionsUpdate chatChunk in streamingChatResponse)
{
    Console.Write(chatChunk.ContentUpdate);
}
```


## Fine Tuning with AI Toolkit

- Start with model discovery and playground  
- Fine-tune and run inference using local computing resources  
- Fine-tune and run inference remotely using Azure resources

[Fine Tuning with AI Toolkit](../../03.FineTuning/Finetuning_VSCodeaitoolkit.md)

## AI Toolkit Q&A Resources

For common issues and solutions, please visit our [Q&A page](https://github.com/microsoft/vscode-ai-toolkit/blob/main/archive/QA.md)

**Disclaimer**:  
This document has been translated using the AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator). While we strive for accuracy, please be aware that automated translations may contain errors or inaccuracies. The original document in its native language should be considered the authoritative source. For critical information, professional human translation is recommended. We are not liable for any misunderstandings or misinterpretations arising from the use of this translation.