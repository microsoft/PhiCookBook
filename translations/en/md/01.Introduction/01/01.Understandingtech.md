<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:39:43+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "en"
}
-->
# Key technologies mentioned include

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - a low-level API for hardware-accelerated machine learning built on top of DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - a parallel computing platform and application programming interface (API) model developed by Nvidia, enabling general-purpose processing on graphics processing units (GPUs).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - an open format designed to represent machine learning models that provides interoperability between different ML frameworks.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - a format used for representing and updating machine learning models, particularly useful for smaller language models that can run effectively on CPUs with 4-8bit quantization.

## DirectML

DirectML is a low-level API that enables hardware-accelerated machine learning. It’s built on top of DirectX 12 to leverage GPU acceleration and is vendor-agnostic, meaning it works across different GPU vendors without requiring code changes. It’s mainly used for model training and inference workloads on GPUs.

Regarding hardware support, DirectML is designed to work with a wide range of GPUs, including AMD integrated and discrete GPUs, Intel integrated GPUs, and NVIDIA discrete GPUs. It’s part of the Windows AI Platform and is supported on Windows 10 & 11, allowing model training and inference on any Windows device.

There have been updates and new opportunities related to DirectML, such as support for up to 150 ONNX operators and usage by both the ONNX runtime and WinML. It’s backed by major Integrated Hardware Vendors (IHVs), each implementing various metacommands.

## CUDA

CUDA, which stands for Compute Unified Device Architecture, is a parallel computing platform and application programming interface (API) model created by Nvidia. It allows software developers to use a CUDA-enabled graphics processing unit (GPU) for general-purpose processing – an approach known as GPGPU (General-Purpose computing on Graphics Processing Units). CUDA is a key driver of Nvidia’s GPU acceleration and is widely used in fields like machine learning, scientific computing, and video processing.

Hardware support for CUDA is specific to Nvidia’s GPUs, as it is proprietary technology developed by Nvidia. Each GPU architecture supports specific versions of the CUDA toolkit, which provides the necessary libraries and tools for developers to build and run CUDA applications.

## ONNX

ONNX (Open Neural Network Exchange) is an open format designed to represent machine learning models. It defines an extensible computation graph model, along with built-in operators and standard data types. ONNX enables developers to transfer models between different ML frameworks, promoting interoperability and simplifying AI application development and deployment.

Phi3 mini can run with ONNX Runtime on CPU and GPU across various devices, including server platforms, Windows, Linux, Mac desktops, and mobile CPUs.  
The optimized configurations we have added are:

- ONNX models for int4 DML: Quantized to int4 via AWQ  
- ONNX model for fp16 CUDA  
- ONNX model for int4 CUDA: Quantized to int4 via RTN  
- ONNX model for int4 CPU and Mobile: Quantized to int4 via RTN

## Llama.cpp

Llama.cpp is an open-source software library written in C++. It performs inference on various Large Language Models (LLMs), including Llama. Developed alongside the ggml library (a general-purpose tensor library), llama.cpp aims to provide faster inference and lower memory usage compared to the original Python implementation. It supports hardware optimization, quantization, and offers a simple API and examples. If you’re interested in efficient LLM inference, llama.cpp is worth exploring as Phi3 can run Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) is a format used for representing and updating machine learning models. It is especially useful for smaller language models (SLMs) that can run efficiently on CPUs with 4-8bit quantization. GGUF is ideal for rapid prototyping and running models on edge devices or in batch jobs like CI/CD pipelines.

**Disclaimer**:  
This document has been translated using the AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator). While we strive for accuracy, please be aware that automated translations may contain errors or inaccuracies. The original document in its native language should be considered the authoritative source. For critical information, professional human translation is recommended. We are not liable for any misunderstandings or misinterpretations arising from the use of this translation.