<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "bd049872f37c3079c87d4fe17109cea0",
  "translation_date": "2025-07-16T18:13:31+00:00",
  "source_file": "md/01.Introduction/01/01.Guidance.md",
  "language_code": "en"
}
-->
### Guidance-AI and Phi Models as a Service (MaaS)
We are integrating [Guidance](https://github.com/guidance-ai/guidance) into the Phi-3.5-mini serverless endpoint offering in Azure AI Foundry to make outputs more predictable by defining structures tailored to specific applications. With Guidance, you can avoid costly retries and, for example, limit the model to choose from predefined lists (such as medical codes), restrict outputs to exact quotes from the given context, or enforce any regex pattern. Guidance guides the model token by token during inference, reducing cost and latency by 30-50%, making it a unique and valuable addition to the [Phi-3-mini serverless endpoint](https://aka.ms/try-phi3.5mini).

## [**Guidance-AI**](https://github.com/guidance-ai/guidance) is a framework designed to help developers build and deploy AI models efficiently. It focuses on providing tools and best practices for creating robust AI applications.

When combined with **Phi Models as a Service (MaaS)**, it delivers a powerful solution for deploying small language models (SLMs) that are both cost-effective and high-performing.

**Guidance-AI** is a programming framework that helps developers control and steer large language models (LLMs) more precisely. It enables structured outputs, reducing latency and cost compared to traditional prompting or fine-tuning approaches.

### Key Features of Guidance-AI:
- **Efficient Control**: Allows developers to guide how the language model generates text, ensuring outputs are high-quality and relevant.
- **Cost and Latency Reduction**: Optimizes the generation process to be faster and more affordable.
- **Flexible Integration**: Compatible with various backends, including Transformers, llama.cpp, AzureAI, VertexAI, and OpenAI.
- **Rich Output Structures**: Supports complex output formats like conditionals, loops, and tool usage, making it easier to produce clear and parsable results.
- **Compatibility**: Enables running a single Guidance program across multiple backends, enhancing flexibility and ease of use.

### Example Use Cases:
- **Constrained Generation**: Guiding model output using regular expressions and context-free grammars.
- **Tool Integration**: Seamlessly combining control and generation, such as incorporating a calculator within a text generation task.

For more details and examples, visit the [Guidance-AI GitHub repository](https://github.com/guidance-ai/guidance).

[Check out the Phi-3.5 Sample](../../../../../code/01.Introduce/guidance.ipynb)

### Key Features of Phi Models:
1. **Cost-Effective**: Designed to be affordable while delivering strong performance.
2. **Low Latency**: Suitable for real-time applications that require quick responses.
3. **Flexibility**: Can be deployed across various environments, including cloud, edge, and offline.
4. **Customization**: Models can be fine-tuned with domain-specific data to improve results.
5. **Security and Compliance**: Built following Microsoftâ€™s AI principles, ensuring accountability, transparency, fairness, reliability, safety, privacy, and inclusiveness.

### Phi Models as a Service (MaaS):
Phi models are accessible through a pay-as-you-go billing system via inference APIs, making it easy to integrate them into your applications without large upfront investments.

### Getting Started with Phi-3:
To begin using Phi models, explore the [Azure AI model catalog](https://ai.azure.com/explore/models) or the [GitHub Marketplace Models](https://github.com/marketplace/models), which offer prebuilt and customizable models. You can also use tools like [Azure AI Foundry](https://ai.azure.com) to develop and deploy your AI applications.

### Resources
[Sample Notebook on getting started with Guidance](../../../../../code/01.Introduce/guidance.ipynb)

**Disclaimer**:  
This document has been translated using the AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator). While we strive for accuracy, please be aware that automated translations may contain errors or inaccuracies. The original document in its native language should be considered the authoritative source. For critical information, professional human translation is recommended. We are not liable for any misunderstandings or misinterpretations arising from the use of this translation.