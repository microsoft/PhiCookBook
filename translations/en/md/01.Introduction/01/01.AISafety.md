<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c8273672cc57df2be675407a1383aaf0",
  "translation_date": "2025-07-09T19:42:29+00:00",
  "source_file": "md/01.Introduction/01/01.AISafety.md",
  "language_code": "en"
}
-->
# AI safety for Phi models  
The Phi family of models was developed following the [Microsoft Responsible AI Standard](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFl), a company-wide set of requirements based on six principles: accountability, transparency, fairness, reliability and safety, privacy and security, and inclusiveness, which make up [Microsoftâ€™s Responsible AI principles](https://www.microsoft.com/ai/responsible-ai).

Like previous Phi models, a comprehensive safety evaluation and post-training safety approach was used, with extra measures to address the multilingual capabilities of this release. Our approach to safety training and evaluations, including testing across multiple languages and risk categories, is detailed in the [Phi Safety Post-Training Paper](https://arxiv.org/abs/2407.13833). While the Phi models benefit from this approach, developers should follow responsible AI best practices, including identifying, measuring, and mitigating risks related to their specific use case and cultural and linguistic context.

## Best Practices

Like other models, the Phi family can sometimes behave in ways that are unfair, unreliable, or offensive.

Some limitations of SLM and LLM to be aware of include:

- **Quality of Service:** The Phi models are primarily trained on English text. Performance in languages other than English will be lower. English dialects with less representation in the training data may perform worse than standard American English.  
- **Representation of Harms & Perpetuation of Stereotypes:** These models may over- or under-represent certain groups, erase representation of some groups, or reinforce negative or demeaning stereotypes. Despite safety post-training, these issues may persist due to uneven representation of groups or the presence of negative stereotypes in training data that reflect real-world patterns and societal biases.  
- **Inappropriate or Offensive Content:** The models may generate other types of inappropriate or offensive content, making them unsuitable for sensitive contexts without additional, use-case-specific mitigations.  
- **Information Reliability:** Language models can produce nonsensical or fabricated content that may sound plausible but is inaccurate or outdated.  
- **Limited Scope for Code:** Most Phi-3 training data is based on Python and common packages like "typing, math, random, collections, datetime, itertools". If the model generates Python scripts using other packages or scripts in other languages, users should manually verify all API calls.

Developers should follow responsible AI best practices and ensure their specific use case complies with relevant laws and regulations (e.g., privacy, trade).

## Responsible AI Considerations

Like other language models, the Phi series can sometimes behave in ways that are unfair, unreliable, or offensive. Key limitations to keep in mind include:

**Quality of Service:** The Phi models are mainly trained on English text. Performance in other languages will be lower. English dialects with less representation in the training data may perform worse than standard American English.

**Representation of Harms & Perpetuation of Stereotypes:** These models may over- or under-represent certain groups, erase representation of some groups, or reinforce negative or demeaning stereotypes. Despite safety post-training, these issues may remain due to uneven group representation or the presence of negative stereotypes in training data reflecting real-world patterns and societal biases.

**Inappropriate or Offensive Content:** The models may generate inappropriate or offensive content, making them unsuitable for sensitive contexts without additional, use-case-specific mitigations.  
**Information Reliability:** Language models can produce nonsensical or fabricated content that may sound reasonable but is inaccurate or outdated.

**Limited Scope for Code:** Most Phi-3 training data is based on Python and common packages like "typing, math, random, collections, datetime, itertools". If the model generates Python scripts using other packages or scripts in other languages, users should manually verify all API calls.

Developers should follow responsible AI best practices and ensure their use case complies with relevant laws and regulations (e.g., privacy, trade). Important considerations include:

**Allocation:** Models may not be suitable for scenarios that could significantly impact legal status or the allocation of resources or life opportunities (e.g., housing, employment, credit) without further evaluation and additional debiasing techniques.

**High-Risk Scenarios:** Developers should evaluate whether models are appropriate for high-risk scenarios where unfair, unreliable, or offensive outputs could cause serious harm or costs. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (e.g., legal or health advice). Additional safeguards should be implemented at the application level based on the deployment context.

**Misinformation:** Models may produce inaccurate information. Developers should follow transparency best practices and inform users they are interacting with an AI system. At the application level, developers can implement feedback mechanisms and pipelines to ground responses in use-case-specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).

**Generation of Harmful Content:** Developers should evaluate outputs in context and use available safety classifiers or custom solutions suited to their use case.

**Misuse:** Other forms of misuse such as fraud, spam, or malware generation may be possible. Developers should ensure their applications comply with applicable laws and regulations.

### Finetuning and AI Content Safety

After fine-tuning a model, we strongly recommend using [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) to monitor generated content, identify and block potential risks, threats, and quality issues.

![Phi3AISafety](../../../../../imgs/01/01/01.phi3aisafety.png)

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) supports both text and image content. It can be deployed in the cloud, disconnected containers, and on edge or embedded devices.

## Overview of Azure AI Content Safety

Azure AI Content Safety is not a one-size-fits-all solution; it can be customized to align with specific business policies. Its multilingual models also allow it to understand multiple languages simultaneously.

![AIContentSafety](../../../../../imgs/01/01/01.AIcontentsafety.png)

- **Azure AI Content Safety**  
- **Microsoft Developer**  
- **5 videos**

The Azure AI Content Safety service detects harmful user-generated and AI-generated content in applications and services. It includes text and image APIs that help detect harmful or inappropriate material.

[AI Content Safety Playlist](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

**Disclaimer**:  
This document has been translated using the AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator). While we strive for accuracy, please be aware that automated translations may contain errors or inaccuracies. The original document in its native language should be considered the authoritative source. For critical information, professional human translation is recommended. We are not liable for any misunderstandings or misinterpretations arising from the use of this translation.