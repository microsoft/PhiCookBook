<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a54cd3d65b6963e4e8ce21e143c3ab04",
  "translation_date": "2025-07-09T20:04:31+00:00",
  "source_file": "md/01.Introduction/03/Remote_Interence.md",
  "language_code": "en"
}
-->
# Remote Inferencing with the fine-tuned model

After training the adapters in the remote environment, use a simple Gradio app to interact with the model.

![Fine-tune complete](../../../../../imgs/03/RemoteServer/log-finetuning-res.png)

### Provision Azure Resources
Set up the Azure Resources for remote inference by running the `AI Toolkit: Provision Azure Container Apps for inference` command from the command palette. During this process, you will be prompted to select your Azure Subscription and resource group.  
![Provision Inference Resource](../../../../../imgs/03/RemoteServer/command-provision-inference.png)
   
By default, the subscription and resource group for inference should match those used for fine-tuning. The inference will use the same Azure Container App Environment and access the model and model adapter stored in Azure Files, which were created during the fine-tuning step.

## Using AI Toolkit 

### Deployment for Inference  
If you want to update the inference code or reload the inference model, run the `AI Toolkit: Deploy for inference` command. This will sync your latest code with ACA and restart the replica.

![Deploy for inference](../../../../../imgs/01/03/RemoteServer/command-deploy.png)

Once deployment completes successfully, the model is ready for evaluation through this endpoint.

### Accessing the Inference API

You can access the inference API by clicking the "*Go to Inference Endpoint*" button shown in the VSCode notification. Alternatively, the web API endpoint is available under `ACA_APP_ENDPOINT` in `./infra/inference.config.json` and in the output panel.

![App Endpoint](../../../../../imgs/01/03/RemoteServer/notification-deploy.png)

> **Note:** It may take a few minutes for the inference endpoint to become fully operational.

## Inference Components Included in the Template
 
| Folder | Contents |
| ------ |--------- |
| `infra` | Contains all necessary configurations for remote operations. |
| `infra/provision/inference.parameters.json` | Holds parameters for the bicep templates, used to provision Azure resources for inference. |
| `infra/provision/inference.bicep` | Contains templates for provisioning Azure resources for inference. |
| `infra/inference.config.json` | Configuration file generated by the `AI Toolkit: Provision Azure Container Apps for inference` command. It serves as input for other remote command palettes. |

### Using AI Toolkit to configure Azure Resource Provision
Configure the [AI Toolkit](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio)

Run the `Provision Azure Container Apps for inference` command.

You can find configuration parameters in the `./infra/provision/inference.parameters.json` file. Details are as follows:
| Parameter | Description |
| --------- |------------ |
| `defaultCommands` | Commands to start the web API. |
| `maximumInstanceCount` | Sets the maximum number of GPU instances. |
| `location` | Location where Azure resources will be provisioned. Defaults to the location of the selected resource group. |
| `storageAccountName`, `fileShareName`, `acaEnvironmentName`, `acaEnvironmentStorageName`, `acaAppName`, `acaLogAnalyticsName` | These parameters name the Azure resources for provisioning. By default, they match the fine-tuning resource names. You can enter new, unused names to create custom resources, or specify existing Azure resource names if you want to reuse them. For more details, see [Using existing Azure Resources](../../../../../md/01.Introduction/03). |

### Using Existing Azure Resources

By default, inference provisioning uses the same Azure Container App Environment, Storage Account, Azure File Share, and Azure Log Analytics as the fine-tuning step. A separate Azure Container App is created specifically for the inference API.

If you customized Azure resources during fine-tuning or want to use your own existing Azure resources for inference, specify their names in the `./infra/inference.parameters.json` file. Then run the `AI Toolkit: Provision Azure Container Apps for inference` command from the command palette. This will update any specified resources and create any missing ones.

For example, if you have an existing Azure container environment, your `./infra/finetuning.parameters.json` might look like this:

```json
{
    "$schema": "https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#",
    "contentVersion": "1.0.0.0",
    "parameters": {
      ...
      "acaEnvironmentName": {
        "value": "<your-aca-env-name>"
      },
      "acaEnvironmentStorageName": {
        "value": null
      },
      ...
    }
  }
```

### Manual Provision  
If you prefer to manually set up Azure resources, you can use the bicep files provided in the `./infra/provision` folder. If you have already configured all Azure resources without using the AI Toolkit command palette, simply enter the resource names in the `inference.config.json` file.

For example:

```json
{
  "SUBSCRIPTION_ID": "<your-subscription-id>",
  "RESOURCE_GROUP_NAME": "<your-resource-group-name>",
  "STORAGE_ACCOUNT_NAME": "<your-storage-account-name>",
  "FILE_SHARE_NAME": "<your-file-share-name>",
  "ACA_APP_NAME": "<your-aca-name>",
  "ACA_APP_ENDPOINT": "<your-aca-endpoint>"
}
```

**Disclaimer**:  
This document has been translated using the AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator). While we strive for accuracy, please be aware that automated translations may contain errors or inaccuracies. The original document in its native language should be considered the authoritative source. For critical information, professional human translation is recommended. We are not liable for any misunderstandings or misinterpretations arising from the use of this translation.