<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7b08e277df2a9307f861ae54bc30c772",
  "translation_date": "2025-12-22T00:04:54+00:00",
  "source_file": "md/01.Introduction/02/06.NVIDIA.md",
  "language_code": "pcm"
}
-->
## Phi Family for NVIDIA NIM

NVIDIA NIM na set of easy-to-use microservices wey dem design to accelerate deployment of generative AI models across the cloud, data center, and workstations. NIMs dey categorized by model family and on a per model basis. For example, NVIDIA NIM for large language models (LLMs) bring the power of state-of-the-art LLMs to enterprise applications, providing unmatched natural language processing and understanding capabilities.

NIM dey make am easy for IT and DevOps teams to self-host large language models (LLMs) in their own managed environments while still providing developers with industry standard APIs wey allow dem build powerful copilots, chatbots, and AI assistants wey fit transform their business. Leveraging NVIDIAâ€™s cutting-edge GPU acceleration and scalable deployment, NIM dey offer the fastest path to inference with unparalleled performance.

You fit use NIVIDIA NIM do inference for Phi Family Models

![nim](../../../../../translated_images/Phi-NIM.09bebb743387ee4a.pcm.png)

### **Samples - Phi-3-Vision for NVIDIA NIM**


Imagine say you get one image (`demo.png`) an you want make e generate Python code wey go process this image an save a new version of am (`phi-3-vision.jpg`). 

The code above dey automate this process by:

1. Setting up the environment and necessary configurations.
2. Creating a prompt wey instruct the model to generate the required Python code.
3. Sending the prompt to the model and collecting the generated code.
4. Extracting and running the generated code.
5. Displaying the original and processed images.

This approach dey leverage the power of AI to automate image processing tasks, making am easier and faster to achieve your goals. 

[Sample Code Solution](../../../code/06.E2E/E2E_Nvidia_NIM_Phi3_Vision.ipynb)

Let's break down wetin the entire code dey do step by step:

1. **Install Di Package Wey Dem Need**:
    ```python
    !pip install langchain_nvidia_ai_endpoints -U
    ```
    Dis command dey install the `langchain_nvidia_ai_endpoints` package, make sure say na the latest version.

2. **Import Di Necessary Modules**:
    ```python
    from langchain_nvidia_ai_endpoints import ChatNVIDIA
    import getpass
    import os
    import base64
    ```
    These imports dey bring in the necessary modules for interacting with the NVIDIA AI endpoints, handling passwords securely, interacting with the operating system, and encoding/decoding data in base64 format.

3. **Set Up Di API Key**:
    ```python
    if not os.getenv("NVIDIA_API_KEY"):
        os.environ["NVIDIA_API_KEY"] = getpass.getpass("Enter your NVIDIA API key: ")
    ```
    Dis code dey check if the `NVIDIA_API_KEY` environment variable is set. If no, e go prompt the user to enter their API key securely.

4. **Define Di Model and Image Path**:
    ```python
    model = 'microsoft/phi-3-vision-128k-instruct'
    chat = ChatNVIDIA(model=model)
    img_path = './imgs/demo.png'
    ```
    This sets the model to be used, creates an instance of `ChatNVIDIA` with the specified model, and defines the path to the image file.

5. **Create Di Text Prompt**:
    ```python
    text = "Please create Python code for image, and use plt to save the new picture under imgs/ and name it phi-3-vision.jpg."
    ```
    This defines a text prompt instructing the model to generate Python code for processing an image.

6. **Encode Di Image for Base64**:
    ```python
    with open(img_path, "rb") as f:
        image_b64 = base64.b64encode(f.read()).decode()
    image = f'<img src="data:image/png;base64,{image_b64}" />'
    ```
    This code reads the image file, encodes it in base64, and creates an HTML image tag with the encoded data.

7. **Combine Di Text an Image into Prompt**:
    ```python
    prompt = f"{text} {image}"
    ```
    This combines the text prompt and the HTML image tag into a single string.

8. **Generate Code Using ChatNVIDIA**:
    ```python
    code = ""
    for chunk in chat.stream(prompt):
        print(chunk.content, end="")
        code += chunk.content
    ```
    This code sends the prompt to the `ChatNVIDIA` model and collects the generated code in chunks, printing and appending each chunk to the `code` string.

9. **Extract Di Python Code from Di Generated Content**:
    ```python
    begin = code.index('```python') + 9
    code = code[begin:]
    end = code.index('```')
    code = code[:end]
    ```
    This extracts the actual Python code from the generated content by removing the markdown formatting.

10. **Run Di Generated Code**:
    ```python
    import subprocess
    result = subprocess.run(["python", "-c", code], capture_output=True)
    ```
    This runs the extracted Python code as a subprocess and captures its output.

11. **Display Di Images**:
    ```python
    from IPython.display import Image, display
    display(Image(filename='./imgs/phi-3-vision.jpg'))
    display(Image(filename='./imgs/demo.png'))
    ```
    These lines display the images using the `IPython.display` module.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
Abeg note:
Dem don use AI translation service wey dem dey call Co-op Translator (https://github.com/Azure/co-op-translator) take translate dis document. Even though we dey try make everything correct, make you sabi say automatic translation fit get errors or mistakes. The original document for e original language na the correct/main source. If na important information, e better make professional human translator check am. We no go responsible for any misunderstanding or wrong interpretation wey fit come from using this translation.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->