<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-12-21T23:01:29+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "pcm"
}
-->
# Di main technologies wey dem mention include

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - na low-level API wey dey enable hardware-accelerated machine learning wey dem build on top of DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - na parallel computing platform and application programming interface (API) model wey Nvidia develop, e dey enable general-purpose processing for graphics processing units (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - na open format wey dem design to represent machine learning models wey dey provide interoperability between different ML frameworks.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - na format wey dem dey use represent and update machine learning models, especially good for smaller language models wey fit run well for CPUs wit 4-8bit quantization.

## DirectML

DirectML na low-level API wey dey enable hardware-accelerated machine learning. Dem build am on top of DirectX 12 make e fit use GPU acceleration and e vendor-agnostic, meaning say you no need change code to make am work for different GPU vendors. Dem dey mainly use am for model training and inferencing workloads for GPUs.

As for hardware support, DirectML design make e fit work wit plenty GPUs, including AMD integrated and discrete GPUs, Intel integrated GPUs, and NVIDIA discrete GPUs. E dey part of the Windows AI Platform and e dey supported on Windows 10 & 11, so you fit do model training and inferencing for any Windows device.

Dem don get updates and opportunities wey concern DirectML, like support for up to 150 ONNX operators and the way dem dey use am both for the ONNX runtime and WinML. Big Integrated Hardware Vendors (IHVs) back am, and each of dem dey implement different metacommands.

## CUDA

CUDA, wey na short for Compute Unified Device Architecture, na parallel computing platform and application programming interface (API) model wey Nvidia create. E dey allow software developers to use CUDA-enabled graphics processing unit (GPU) for general purpose processing — dis approach dem dey call GPGPU (General-Purpose computing on Graphics Processing Units). CUDA na one key thing wey dey enable Nvidia GPU acceleration and e dey widely use for many fields, including machine learning, scientific computing, and video processing.

Hardware support for CUDA dey specific to Nvidia GPUs, because na proprietary technology wey Nvidia develop. Each architecture go support specific versions of the CUDA toolkit, wey get the libraries and tools developers need to build and run CUDA applications.

## ONNX

ONNX (Open Neural Network Exchange) na open format wey dem design to represent machine learning models. E provide definition for an extensible computation graph model, plus definitions of built-in operators and standard data types. ONNX dey allow developers move models between different ML frameworks, make interoperability easier and help dem deploy AI applications faster.

Phi3 mini fit run with ONNX Runtime on CPU and GPU across devices, including server platforms, Windows, Linux and Mac desktops, and mobile CPUs.
The optimized configurations we have added are

- ONNX models for int4 DML: Quantized to int4 via AWQ
- ONNX model for fp16 CUDA
- ONNX model for int4 CUDA: Quantized to int4 via RTN
- ONNX model for int4 CPU and Mobile: Quantized to int4 via RTN

## Llama.cpp

Llama.cpp na open-source software library wey dem write for C++. E dey perform inference on different Large Language Models (LLMs), including Llama. Dem develop am alongside the ggml library (a general-purpose tensor library), and llama.cpp reason be say make inference fast pass and make memory use small pass the original Python implementation. E support hardware optimization, quantization, and e get simple API plus examples3. If you dey interested for efficient LLM inference, llama.cpp worth to check as Phi3 fit run Llama.cpp. 

## GGUF

GGUF (Generic Graph Update Format) na format wey dem dey use represent and update machine learning models. E dey especially useful for smaller language models (SLMs) wey fit run well for CPUs wit 4-8bit quantization. GGUF good for rapid prototyping and to run models for edge devices or for batch jobs like CI/CD pipelines.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
Disclaimer:
Dis document don translate wit AI translation service (Co-op Translator — https://github.com/Azure/co-op-translator). Even though we dey try make everything correct, abeg note say automatic translation fit get mistakes or no too correct parts. Di original document for im own language na di proper source wey you suppose trust. If na important matter, e better make professional human translator do am. We no go responsible for any misunderstanding or wrong interpretation wey fit come from using dis translation.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->