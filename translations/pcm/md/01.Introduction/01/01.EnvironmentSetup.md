<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-12-21T23:16:45+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "pcm"
}
-->
# How to run Phi-3 for your local machine

This guide go help you set up your local environment make you fit run the Phi-3 model using Ollama. You fit run the model for different ways, including using GitHub Codespaces, VS Code Dev Containers, or your local environment.

## How to set up di environment

### GitHub Codespaces

You fit run this template virtually by using GitHub Codespaces. Di button go open one web-based VS Code instance for your browser:

1. Open the template (this fit take several minutes):

    [![Open am for GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Open one terminal window

### VS Code Dev Containers

⚠️ Dis option go only work if your Docker Desktop get at least 16 GB of RAM. If you no get 16 GB of RAM, you fit try the [GitHub Codespaces option](../../../../../md/01.Introduction/01) or [set it up locally](../../../../../md/01.Introduction/01).

Another option na VS Code Dev Containers, wey go open the project for your local VS Code using the [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Start Docker Desktop (install am if you never install am)
2. Open the project:

    [![Open am for Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. For the VS Code window wey open, once the project files show up (this fit take several minutes), open one terminal window.
4. Continue with the [deployment steps](../../../../../md/01.Introduction/01)

### Local Environment

1. Make sure say you don install the following tools:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Test the model

1. Tell Ollama make e download and run di phi3:mini model:

    ```shell
    ollama run phi3:mini
    ```

    E go take few minutes to download di model.

2. Once you see "success" in the output, you fit send message to that model from the prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. After few seconds, you go see a response stream from di model.

4. To learn about different techniques used with language models, open the Python notebook [ollama.ipynb](../../../code/01.Introduce/ollama.ipynb) and run each cell . If you use a model other than 'phi3:mini', change the `MODEL_NAME` in the first cell.

5. To yarn with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You fit change the `MODEL_NAME` at the top of the file as needed, and you fit also modify the system message or add few-shot examples if you want.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
Disclaimer:
Dis document don translate by AI translation service Co-op Translator (https://github.com/Azure/co-op-translator). Even though we dey try make am correct, make you sabi say automatic translations fit get errors or inaccuracies. The original document for im original language suppose be the authoritative source. If na critical information, make you use professional human translation. We no dey liable for any misunderstandings or wrong interpretations wey fit come from this translation.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->