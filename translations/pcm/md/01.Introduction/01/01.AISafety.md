<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c8273672cc57df2be675407a1383aaf0",
  "translation_date": "2025-12-21T23:23:32+00:00",
  "source_file": "md/01.Introduction/01/01.AISafety.md",
  "language_code": "pcm"
}
-->
# AI safety for Phi models
Di Phi family of models dem develop make dem follow di [Microsoft Responsible AI Standard](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFl), wey na company-wide set of requirements based on dis six principles: accountability, transparency, fairness, reliability and safety, privacy and security, and inclusiveness wey form [Microsoftâ€™s Responsible AI principles](https://www.microsoft.com/ai/responsible-ai). 

Like di previous Phi models, dem use plenti different ways take evaluate safety and do post-training safety steps, plus dem add extra steps to cover di multi-lingual abilities wey this release get. Our way wey we take do safety training and evaluations wey include testing for many languages and risk categories dey explain for di [Phi Safety Post-Training Paper](https://arxiv.org/abs/2407.13833). Even though di Phi models dey benefit from dis approach, developers suppose still apply responsible AI best practices, like to map, measure, and reduce risks wey relate to their particular use case and cultural and language context.

## Best Practices

Like oda models, di Phi family fit behave in ways wey no fair, no reliable, or fit offend people.

Some of di limiting behaviors of SLM and LLM wey you suppose sabi include:

- **Quality of Service:** Di Phi models dem train mostly on English text. Languages wey no be English go dey perform worse. English language varieties wey no too dey represent for di training data fit perform worse pass standard American English.
- **Representation of Harms & Perpetuation of Stereotypes:** These models fit over- or under-represent groups of people, fit make some groups disappear from representation, or fit reinforce disrespectful or negative stereotypes. Even after safety post-training, these limits fit still dey because different groups no too represent well for di training data or because plenti examples of negative stereotypes dey di training data wey reflect real-world patterns and societal biases.
- **Inappropriate or Offensive Content:** These models fit produce other kain inappropriate or offensive content, wey fit make am no good to deploy for sensitive contexts without extra mitigations wey cleanly fit di use case.
Information Reliability: Language models fit generate nonsensical content or make up content wey fit sound reasonable but no correct or don old.
- **Limited Scope for Code:** Majority of Phi-3 training data base for Python and dem dey use common packages like "typing, math, random, collections, datetime, itertools". If di model generate Python scripts wey use oda packages or scripts for oda languages, we strongly recommend sey users make sure dem manually verify all API uses.

Developers suppose apply responsible AI best practices and dem responsible make sure sey any specific use case comply with relevant laws and regulations (ex: privacy, trade, etc.). 

## Responsible AI Considerations

Like oda language models, di Phi series models fit behave in ways wey no fair, no reliable, or fit offend people. Some of di limiting behaviors wey you suppose sabi include:

**Quality of Service:** Di Phi models dem train mostly on English text. Languages wey no be English go dey perform worse. English language varieties wey no too dey represent for di training data fit perform worse pass standard American English.

**Representation of Harms & Perpetuation of Stereotypes:** These models fit over- or under-represent groups of people, fit make some groups disappear from representation, or fit reinforce disrespectful or negative stereotypes. Even after safety post-training, these limits fit still dey because different groups no too represent well for di training data or because plenti examples of negative stereotypes dey di training data wey reflect real-world patterns and societal biases.

**Inappropriate or Offensive Content:** These models fit produce other kain inappropriate or offensive content, wey fit make am no good to deploy for sensitive contexts without extra mitigations wey cleanly fit di use case.
Information Reliability: Language models fit generate nonsensical content or make up content wey fit sound reasonable but no correct or don old.

**Limited Scope for Code:** Majority of Phi-3 training data base for Python and use common packages like "typing, math, random, collections, datetime, itertools". If di model generate Python scripts wey use oda packages or scripts for oda languages, we strongly recommend sey users make sure dem manually verify all API uses.

Developers suppose apply responsible AI best practices and dem responsible make sure sey any specific use case comply with relevant laws and regulations (ex: privacy, trade, etc.). Important areas wey dem suppose consider include:

**Allocation:** Models no too suitable for scenarios wey fit get big impact for person legal status or wey fit affect how resources or life opportunities dem dey divide (example: housing, employment, credit, etc.) without further assessments and extra debiasing techniques.

**High-Risk Scenarios:** Developers suppose check if e good to use models for high-risk scenarios wey unfair, unreliable, or offensive outputs fit cost plenti or cause harm. Dis include to dey give advice for sensitive or expert domains wey accuracy and reliability matter (example: legal or health advice). Additional safeguards suppose dey for application level based on how and where you deploy am.

**Misinformation:** Models fit produce wrong information. Developers suppose follow transparency best practices and tell end-users say dem dey interact with AI system. For application level, developers fit build feedback mechanisms and pipelines to ground responses with use-case specific, contextual information, technique wey dem dey call Retrieval Augmented Generation (RAG).

**Generation of Harmful Content:** Developers suppose check outputs based on di context and use available safety classifiers or custom solutions wey fit di use case.

**Misuse:** Oda kinds of misuse like fraud, spam, or malware production fit possible, and developers suppose make sure their applications no dey violate applicable laws and regulations.

### Finetuning and AI Content Safety

After you fine-tune model, we strongly recommend sey you use [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) measures to monitor di content wey di models generate, identify and block potential risks, threats, and quality wahala.

![Phi3AISafety](../../../../../translated_images/01.phi3aisafety.c0d7fc42f5a5c405.pcm.png)

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) support both text and image content. E fit deploy for cloud, disconnected containers, and on edge/embedded devices.

## Overview of Azure AI Content Safety

Azure AI Content Safety no be one-size-fits-all solution; you fit customize am make e match business policies. Also, im multi-lingual models make am fit understand many languages at di same time.

![AIContentSafety](../../../../../translated_images/01.AIcontentsafety.a288819b8ce8da1a.pcm.png)

- **Azure AI Content Safety**
- **Microsoft Developer**
- **5 videos**

Azure AI Content Safety service dey detect harmful user-generated and AI-generated content for applications and services. E get text and image APIs wey allow you detect harmful or inappropriate material.

[AI Content Safety Playlist](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
Abeg note:
Dis dokument na AI translate use [Co-op Translator](https://github.com/Azure/co-op-translator). Even though we dey try make am correct, make you sabi say automatic translation fit get errors or wrong parts. The original dokument for im original language suppose be the correct/main source. If na important info, make you use professional human translator. We no dey responsible for any misunderstanding or wrong interpretation wey fit happen because of this translation.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->