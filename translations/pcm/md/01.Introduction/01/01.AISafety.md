# AI safety for Phi models
Di Phi family of models dem develop based on di [Microsoft Responsible AI Standard](https://www.microsoft.com/ai/principles-and-approach#responsible-ai-standard), wey na company-wide set of requirements wey base on six principles: accountability, transparency, fairness, reliability and safety, privacy and security, plus inclusiveness wey form [Microsoftâ€™s Responsible AI principles](https://www.microsoft.com/ai/responsible-ai). 

Like di Phi models wey come before, dem use multi-faceted safety evaluation plus safety post-training approach, plus some extra measures to take care of di multi-lingual capabilities wey dis release get. Our way to safety training and evaluations wey include testing for plenty languages and different risk categories dey inside di [Phi Safety Post-Training Paper](https://arxiv.org/abs/2407.13833). Though di Phi models benefit from dis approach, developers suppose still apply responsible AI best practices, like mapping, measuring, and reducing risks wey relate to their specific use case plus di cultural and linguistic environment.

## Best Practices

Like other models, di Phi family fit behave for way wey no be fair, no reliable, or fit offend people.

Some limiting behaviours for SLM and LLM wey you for sabi na:

- **Quality of Service:** Di Phi models dem train mainly on English text. Languages outside English go get worse performance. English language versions wey no too get plenty place for di training data fit perform worse pass normal American English.
- **Representation of Harms & Perpetuation of Stereotypes:** Dis models fit show some people group pass wetin e supposed, or even remove some group representation, or support bad or negative stereotypes. Even with safety post-training, dis kain problem still fit dey because different group get different levels of representation or because di training data get plenty examples of negative stereotypes wey reflect wetin dey happen for real life and society bias.
- **Inappropriate or Offensive Content:** Dis models fit produce other kind inappropriate or offensive content, wey for sometimes no good to use am for sensitive things without extra steps wey concern the exact use case.
- **Information Reliability:** Language models fit generate nonsense or make up story wey fit sound okay but no correct or e don old.
- **Limited Scope for Code:** Most of di Phi-3 training data base on Python and e use common packages like "typing, math, random, collections, datetime, itertools". If di model generate Python scripts wey use other packages or code for different languages, we strongly advise make users check all API use by hand.

Developers suppose apply responsible AI best practices and dem go responsible to make sure say their specific use case follow relevant laws and regulations (example: privacy, trade, etc.). 

## Responsible AI Considerations

Like other language models, di Phi series models fit behave in ways wey no fair, no reliable, or fit offend people. Some limiting behaviours to sabi na:

**Quality of Service:** Di Phi models dem train mainly on English text. Languages outside English go get worse performance. English language versions wey no get plenty space for di training data fit perform worse pass normal American English.

**Representation of Harms & Perpetuation of Stereotypes:** Dis models fit show some group of people pass wetin e suppose, or erase representation of some groups, or support bad or negative stereotypes. Even with safety post-training, dis kain problem still fit dey because different group get different levels of representation or because di training data get stories of negative stereotypes wey reflect real life patterns and society bias.

**Inappropriate or Offensive Content:** Dis models fit produce other kind inappropriate or offensive content, wey fit make am no good to use for sensitive matters without extra mitigation steps wey relate to di use case.
**Information Reliability:** Language models fit create nonsense content or make up story wey fit sound okay but no correct or old news.

**Limited Scope for Code:** Most of di Phi-3 training data base on Python and use packages wey common like "typing, math, random, collections, datetime, itertools". If di model generate Python scripts wey use other packages or scripts for other languages, we dey strongly recommend say users check all API uses manually.

Developers suppose apply responsible AI best practices and dem responsible to make sure say their specific use case follow all laws and regulations (like privacy, trade, etc.). Important things to consider include:

**Allocation:** Models no too suitable for cases wey fit get big impact on legal status or how resources or life chances dem divide (example: housing, work, credit, etc.) without more checks and more debiasing methods.

**High-Risk Scenarios:** Developers suppose check if models fit for high-risk cases wey unfair, unreliable, or offensive output fit cost plenty money or cause harm. This one include advice for sensitive or expert matters wey accuracy and reliability na top priority (example: legal or health advice). Extra safeguards suppose dey at app level based on how dem deploy am.

**Misinformation:** Models fit provide wrong information. Developers suppose follow transparency best practices and tell end users say dem dey interact with AI system. For app level, developers fit build feedback ways and pipelines to base answer on use-case specific, proper information, wey dem dey call Retrieval Augmented Generation (RAG).

**Generation of Harmful Content:** Developers suppose check output based on context and use existing safety classifiers or custom solutions wey fit their use case.

**Misuse:** Other kinds of misuse like fraud, spam, or malware production fit happen, so developers suppose make sure their apps no break any laws and regulations.

### Finetuning and AI Content Safety

After you fine-tune model, we dey advise say you use [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) steps to watch di content wey model generate, detect and block wahala, threats, and quality issues.

![Phi3AISafety](../../../../../translated_images/01.phi3aisafety.c0d7fc42f5a5c405.pcm.png)

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) fit handle text and image content. You fit deploy am for cloud, disconnected containers, and edge/embedded devices.

## Overview of Azure AI Content Safety

Azure AI Content Safety no be one-size-fits-all solution; you fit customize am make e follow your business policies. E get multi-lingual models too, wey fit understand plenty languages at the same time.

![AIContentSafety](../../../../../translated_images/01.AIcontentsafety.a288819b8ce8da1a.pcm.png)

- **Azure AI Content Safety**
- **Microsoft Developer**
- **5 videos**

Azure AI Content Safety service dey detect harmful user-generated and AI-generated content for apps and services. E get text and image APIs wey allow you detect harmful or inappropriate matter.

[AI Content Safety Playlist](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Disclaimer**:  
Dis dokiment don translate wit AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator). Even tho we dey try make am correct, make you sabi say machine translation fit get some error or mistake. Di original dokiment wey dem write for dia own language na di correct one. If na important info you want, better make human professional translate am. We no go take responsibility for any wrong understanding or wrong meaning wey fit show because of dis translation.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->