<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f1ff728038c4f554b660a36b76cbdd6e",
  "translation_date": "2025-12-22T00:38:35+00:00",
  "source_file": "md/01.Introduction/03/overview.md",
  "language_code": "pcm"
}
-->
For di konteks of Phi-3-mini, inference na di process wey dey use di model to make predictions or generate outputs based on input data. Make I give you more details about Phi-3-mini and im inference capabilities.

Phi-3-mini na part of di Phi-3 series of models wey Microsoft release. Dem models design to redefine wetin possible with Small Language Models (SLMs). 

Here be some key points about Phi-3-mini and im inference capabilities:

## **Phi-3-mini Overview:**
- Phi-3-mini get parameter size of 3.8 billion.
- E fit run no be only for traditional computing devices but e fit run for edge devices like mobile devices and IoT devices.
- Di release of Phi-3-mini dey enable individuals and enterprises to deploy SLMs for different hardware devices, especially for resource-constrained environments.
- E cover different model formats, including di traditional PyTorch format, di quantized version of di gguf format, and di ONNX-based quantized version.

## **Accessing Phi-3-mini:**
To access Phi-3-mini, you fit use [Semantic Kernel](https://github.com/microsoft/SemanticKernelCookBook?WT.mc_id=aiml-138114-kinfeylo) in a Copilot application. Semantic Kernel normally dey compatible with Azure OpenAI Service, open-source models on Hugging Face, and local models.
You fit also use [Ollama](https://ollama.com) or [LlamaEdge](https://llamaedge.com) to call quantized models. Ollama allow individual users to call different quantized models, while LlamaEdge dey provide cross-platform availability for GGUF models.

## **Quantized Models:**
Plenty users prefer to use quantized models for local inference. For example, you fit directly run Ollama run Phi-3 or configure am offline using a Modelfile. Di Modelfile dey specify di GGUF file path and di prompt format.

## **Generative AI Possibilities:**
When you combine SLMs like Phi-3-mini e open new possibilities for generative AI. Inference na just di first step; these models fit dey used for different tasks for resource-constrained, latency-bound, and cost-constrained scenarios.

## **Unlocking Generative AI with Phi-3-mini: A Guide to Inference and Deployment** 
Learn how to use Semantic Kernel, Ollama/LlamaEdge, and ONNX Runtime to access and infer Phi-3-mini models, and explore di possibilities of generative AI in different application scenarios.

**Features**
Inference phi3-mini model in:

- [Semantic Kernel](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/semantickernel?WT.mc_id=aiml-138114-kinfeylo)
- [Ollama](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ollama?WT.mc_id=aiml-138114-kinfeylo)
- [LlamaEdge WASM](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm?WT.mc_id=aiml-138114-kinfeylo)
- [ONNX Runtime](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/onnx?WT.mc_id=aiml-138114-kinfeylo)
- [iOS](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ios?WT.mc_id=aiml-138114-kinfeylo)

For summary, Phi-3-mini dey allow developers to explore different model formats and leverage generative AI for different application scenarios.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
Disclaimer:
Dis dokument don translate wit AI translation service Co-op Translator (https://github.com/Azure/co-op-translator). Even tho we dey try make am correct, abeg sabi say automated translations fit get mistakes or wrong parts. Di original dokument for im original language na di authority. For important mata, na professional human translator you suppose use. We no go take any blame for any misunderstanding or wrong interpretation wey fit arise from dis translation.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->