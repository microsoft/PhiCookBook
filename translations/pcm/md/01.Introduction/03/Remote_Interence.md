<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a54cd3d65b6963e4e8ce21e143c3ab04",
  "translation_date": "2025-12-22T00:15:47+00:00",
  "source_file": "md/01.Introduction/03/Remote_Interence.md",
  "language_code": "pcm"
}
-->
# Remote Inferencing wit di fine-tuned model

After dem don train di adapters for di remote environment, use one simple Gradio application to interact wit di model.

![Fine-tune don complete](../../../../../translated_images/log-finetuning-res.7b92254e7e822c7f.pcm.png)

### Provision Azure Resources
You need set up di Azure Resources for remote inference by executing the `AI Toolkit: Provision Azure Container Apps for inference` from the command palette. During dis setup, dem go ask you to select your Azure Subscription and resource group.  
![Provision Inference Resource](../../../../../translated_images/command-provision-inference.467afc8d351642fc.pcm.png)
   
By default, di subscription and di resource group for inference suppose match dem wey dem use for fine-tuning. Di inference go use di same Azure Container App Environment and go access di model and model adapter wey dey stored in Azure Files, wey dem generate during di fine-tuning step. 

## Using AI Toolkit 

### Deployment for Inference  
If you wan revise di inference code or reload di inference model, abeg execute di `AI Toolkit: Deploy for inference` command. Dis go synchronize your latest code wit ACA and restart di replica.  

![Deploy for inference](../../../../../translated_images/command-deploy.9adb4e310dd0b0ae.pcm.png)

After deployment don complete successful, di model don ready for evaluation using dis endpoint.

### Accessing di Inference API

You fit access di inference API by clicking on di "*Go to Inference Endpoint*" button wey display for di VSCode notification. Alternatively, di web API endpoint fit dey found under `ACA_APP_ENDPOINT` in `./infra/inference.config.json` and for di output panel.

![App Endpoint](../../../../../translated_images/notification-deploy.446e480a44b1be58.pcm.png)

> **Nota:** Di inference endpoint fit need small minutes make e become fully operational.

## Inference Components Included in the Template
 
| Folder | Contents |
| ------ |--------- |
| `infra` | Get all di necessary configurations for remote operations. |
| `infra/provision/inference.parameters.json` | E hold parameters for di bicep templates, wey dem dey use for provisioning Azure resources for inference. |
| `infra/provision/inference.bicep` | E get templates for provisioning Azure resources for inference. |
| `infra/inference.config.json` |Di configuration file, generated by di `AI Toolkit: Provision Azure Container Apps for inference` command. E dey used as input for oda remote command palettes. |

### Using AI Toolkit to configuring Azure Resource Provision
Set up di [AI Toolkit](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio)

Provision Azure Container Apps for inference` command.

You fit find configuration parameters in `./infra/provision/inference.parameters.json` file. Here are the details:
| Parameter | Description |
| --------- |------------ |
| `defaultCommands` | These na di commands to initiate a web API. |
| `maximumInstanceCount` | Dis parameter set di maximum capacity of GPU instances. |
| `location` | Dis na di location wey Azure resources go be provisioned. Di default value na di same as di chosen resource group's location. |
| `storageAccountName`, `fileShareName` `acaEnvironmentName`, `acaEnvironmentStorageName`, `acaAppName`,  `acaLogAnalyticsName` | These parameters dey used to name di Azure resources for provision. By default, dem go be di same as di fine-tuning resource name. You fit input a new, unused resource name to create your own custom-named resources, or you fit input di name of an existing Azure resource if you prefer use dat one. For details, refer to di section [Using existing Azure Resources](../../../../../md/01.Introduction/03). |

### Using Existing Azure Resources

By default, di inference provision dey use di same Azure Container App Environment, Storage Account, Azure File Share, and Azure Log Analytics wey dem use for fine-tuning. Separate Azure Container App dey created only for di inference API. 

If you don customize di Azure resources during di fine-tuning step or you want to use your own existing Azure resources for inference, specify dia names in di `./infra/inference.parameters.json` file. Then, run di `AI Toolkit: Provision Azure Container Apps for inference` command from di command palette. Dis go update any specified resources and create any wey dey missing.

For example, if you get existing Azure container environment, your `./infra/finetuning.parameters.json` go look like this:

```json
{
    "$schema": "https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#",
    "contentVersion": "1.0.0.0",
    "parameters": {
      ...
      "acaEnvironmentName": {
        "value": "<your-aca-env-name>"
      },
      "acaEnvironmentStorageName": {
        "value": null
      },
      ...
    }
  }
```

### Manual Provision  
If you prefer to manually configure di Azure resources, you fit use di provided bicep files in di `./infra/provision` folders. If you don already set up and configure all di Azure resources without using di AI Toolkit command palette, you fit simply enter di resource names in di `inference.config.json` file.

For example:

```json
{
  "SUBSCRIPTION_ID": "<your-subscription-id>",
  "RESOURCE_GROUP_NAME": "<your-resource-group-name>",
  "STORAGE_ACCOUNT_NAME": "<your-storage-account-name>",
  "FILE_SHARE_NAME": "<your-file-share-name>",
  "ACA_APP_NAME": "<your-aca-name>",
  "ACA_APP_ENDPOINT": "<your-aca-endpoint>"
}
```

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
Abeg note:
Dis document na AI translation wey [Co-op Translator](https://github.com/Azure/co-op-translator) do. Even though we dey try make am correct, automated translations fit get mistakes or wrong meanings. Di original document for im own language na di official source wey you suppose trust. For critical information, make una use professional human translator. We no dey liable for any misunderstanding or misinterpretation wey fit come from dis translation.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->