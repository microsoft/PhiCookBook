<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "bc29f7fe7fc16bed6932733eac8c81b8",
  "translation_date": "2025-12-21T19:42:34+00:00",
  "source_file": "md/02.Application/02.Code/Phi3/VSCodeExt/HOL/AIPC/02.PromptflowWithNPU.md",
  "language_code": "pcm"
}
-->
# **Lab 2 -  Run Prompt flow wit Phi-3-mini for AIPC**

## **Wetin be Prompt flow**

Prompt flow na set of development tools wey dem design to streamline di end-to-end development cycle of LLM-based AI applications, from ideation, prototyping, testing, evaluation reach production deployment and monitoring. E dey make prompt engineering much easier and e dey enable you build LLM apps wey get production quality.

Wit prompt flow, you go fit:

- Create flows wey link LLMs, prompts, Python code and oda tools together inside an executable workflow.

- Debug and iterate your flows, especially di interaction wit LLMs, with ease.

- Evaluate your flows, calculate quality and performance metrics wit bigger datasets.

- Integrate di testing and evaluation into your CI/CD system to make sure say your flow get quality.

- Deploy your flows to di serving platform wey you choose or integrate into your app’s code base easily.

- (Optional but highly recommended) Collaborate wit your team by using di cloud version of Prompt flow for Azure AI.

## **Wetin be AIPC**

An AI PC get CPU, GPU and NPU, each get specific AI acceleration capabilities. NPU, wey be neural processing unit, na special accelerator wey dey handle artificial intelligence (AI) and machine learning (ML) tasks right for your PC instead of sending data make dem process am for cloud. GPU and CPU fit also process these workloads, but di NPU dey especially good for low-power AI calculations. Di AI PC represent one big change for how our computers dey operate. E no be solution for problem wey never exist before. Instead, e promise to be big improvement for everyday PC usage.

So how e dey work? Compared to generative AI and di massive large language models (LLMs) wey dem train on plenty public data, di AI wey go run for your PC dey more accessible for almost every level. Di concept easy to understand, and because e dey trained on your data, without need to access di cloud, di benefits dey more immediately appealing to more people.

For di near-term, di AI PC world go involve personal assistants and smaller AI models wey dey run directly on your PC, using your data to give personal, private, more secure AI enhancements for things wey you dey do everyday – take meeting minutes, organize fantasy football league, automate enhancements for photo and video editing, or lay out di perfect itinerary for family reunion based on everybody arrival and departure times.


## **Building generation code flows for AIPC**

***Note*** ：If you never complete di environment installation, abeg visit [Lab 0 -Installations](./01.Installations.md)

1. Open di Prompt flow Extension for Visual Studio Code and create an empty flow project

![make](../../../../../../../../../translated_images/pf_create.bde888dc83502eba.pcm.png)

2. Add Inputs and Outputs parameters and put Python Code as new flow

![flow](../../../../../../../../../translated_images/pf_flow.520824c0969f2a94.pcm.png)


You fit refer to dis structure (flow.dag.yaml) to construct your flow

```yaml

inputs:
  question:
    type: string
    default: how to write Bubble Algorithm
outputs:
  answer:
    type: string
    reference: ${Chat_With_Phi3.output}
nodes:
- name: Chat_With_Phi3
  type: python
  source:
    type: code
    path: Chat_With_Phi3.py
  inputs:
    question: ${inputs.question}


```

3. Add Code inside ***Chat_With_Phi3.py***


```python


from promptflow.core import tool

# dey import torch
from transformers import AutoTokenizer, pipeline,TextStreamer
import intel_npu_acceleration_library as npu_lib

import warnings

import asyncio
import platform

class Phi3CodeAgent:
    
    model = None
    tokenizer = None
    text_streamer = None
    
    model_id = "microsoft/Phi-3-mini-4k-instruct"

    @staticmethod
    def init_phi3():
        
        if Phi3CodeAgent.model is None or Phi3CodeAgent.tokenizer is None or Phi3CodeAgent.text_streamer is None:
            Phi3CodeAgent.model = npu_lib.NPUModelForCausalLM.from_pretrained(
                                    Phi3CodeAgent.model_id,
                                    torch_dtype="auto",
                                    dtype=npu_lib.int4,
                                    trust_remote_code=True
                                )
            Phi3CodeAgent.tokenizer = AutoTokenizer.from_pretrained(Phi3CodeAgent.model_id)
            Phi3CodeAgent.text_streamer = TextStreamer(Phi3CodeAgent.tokenizer, skip_prompt=True)

    

    @staticmethod
    def chat_with_phi3(prompt):
        
        Phi3CodeAgent.init_phi3()

        messages = "<|system|>You are a AI Python coding assistant. Please help me to generate code in Python.The answer only genertated Python code, but any comments and instructions do not need to be generated<|end|><|user|>" + prompt +"<|end|><|assistant|>"



        generation_args = {
            "max_new_tokens": 1024,
            "return_full_text": False,
            "temperature": 0.3,
            "do_sample": False,
            "streamer": Phi3CodeAgent.text_streamer,
        }

        pipe = pipeline(
            "text-generation",
            model=Phi3CodeAgent.model,
            tokenizer=Phi3CodeAgent.tokenizer,
            # **generation_args
        )

        result = ''

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            response = pipe(messages, **generation_args)
            result =response[0]['generated_text']
            return result


@tool
def my_python_tool(question: str) -> str:
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    return Phi3CodeAgent.chat_with_phi3(question)


```

4. You fit test di flow from Debug or Run to check if di generation code dey okay or not 

![Run](../../../../../../../../../translated_images/pf_run.4239e8a0b420a582.pcm.png)

5. Run di flow as development API for terminal

```

pf flow serve --source ./ --port 8080 --host localhost   

```

You fit test am in Postman / Thunder Client


### **Note**

1. Di first run go take long time. E dey recommended make you download di phi-3 model from Hugging Face CLI.

2. Considering say Intel NPU get limited computing power, e dey recommended to use Phi-3-mini-4k-instruct

3. We dey use Intel NPU Acceleration to quantize INT4 conversion, but if you re-run di service, you go need to delete di cache and nc_workshop folders.



## **Resources**

1. Learn Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Learn Intel NPU Acceleration [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Sample Code, download [Local NPU Agent Sample Code](../../../../../../../../../code/07.Lab/01/AIPC)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
Abeg note:
Dis document don translate with AI translation service Co-op Translator (https://github.com/Azure/co-op-translator). Even though we dey try make am correct, abeg sabi say automatic translation fit get mistakes or no too accurate. De original document for im own language na di official source you suppose rely on. If na important information, make you use professional human translator. We no go responsible for any misunderstanding or wrong interpretation wey fit come from dis translation.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->