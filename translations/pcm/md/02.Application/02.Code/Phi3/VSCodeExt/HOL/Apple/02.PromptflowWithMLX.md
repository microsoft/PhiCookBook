# **Lab 2 -  Run Prompt flow with Phi-3-mini in AIPC**

## **Wetin be Prompt flow**

Prompt flow na suite of development tools wey dem design to make end-to-end development cycle for LLM-based AI applications easy — from ideation, prototyping, testing, evaluation reach production deployment and monitoring. E dey make prompt engineering way easier and e go allow you build LLM apps wey get production quality.

With prompt flow, you go fit:

- Create flows wey dey link LLMs, prompts, Python code and oda tools together for one executable workflow.

- Debug and iterate your flows, especially di interaction wit LLMs, without wahala.

- Evaluate your flows, calculate quality and performance metrics wit bigger datasets.

- Integrate the testing and evaluation into your CI/CD system to make sure say your flow get steady quality.

- Deploy your flows to di serving platform wey you choose or integrate am into your app’s code base easily.

- (Optional but highly recommended) Collaborate wit your team by using di cloud version of Prompt flow for Azure AI.



## **Building generation code flows on Apple Silicon**

***Note*** ：If you never finish di environment installation, abeg visit [Lab 0 -Installations](./01.Installations.md)

1. Open the Prompt flow Extension for Visual Studio Code and create an empty flow project

![make](../../../../../../../../../translated_images/pf_create.bde888dc83502eba.pcm.png)

2. Add Inputs and Outputs parameters and Add Python Code as new flow

![flow](../../../../../../../../../translated_images/pf_flow.520824c0969f2a94.pcm.png)


You fit refer to this structure (flow.dag.yaml) to construct your flow

```yaml

inputs:
  prompt:
    type: string
    default: Write python code for Fibonacci serie. Please use markdown as output
outputs:
  result:
    type: string
    reference: ${gen_code_by_phi3.output}
nodes:
- name: gen_code_by_phi3
  type: python
  source:
    type: code
    path: gen_code_by_phi3.py
  inputs:
    prompt: ${inputs.prompt}


```

3. Quantify phi-3-mini

We dey try run SLM better for local devices. Normally, we dey quantify di model (INT4, FP16, FP32)


```bash

python -m mlx_lm.convert --hf-path microsoft/Phi-3-mini-4k-instruct

```

**Note:** default folder na mlx_model 

4. Add Code for ***Chat_With_Phi3.py***


```python


from promptflow import tool

from mlx_lm import load, generate


# Di inputs section go change based on di arguments wey dey di tool function after you save di code
# If you add types to di arguments and di return value, e go help di system show di types correct
# Abeg update di function name/signature as e need
@tool
def my_python_tool(prompt: str) -> str:

    model_id = './mlx_model_phi3_mini'

    model, tokenizer = load(model_id)

    # <|user|>\nAbeg write Python code wey go do Fibonacci serie. Make you use markdown for di output<|end|>\n<|assistant|>

    response = generate(model, tokenizer, prompt="<|user|>\n" + prompt  + "<|end|>\n<|assistant|>", max_tokens=2048, verbose=True)

    return response


```

4. You fit test di flow from Debug or Run to check if di generation code dey okay or no

![RUN](../../../../../../../../../translated_images/pf_run.4239e8a0b420a582.pcm.png)

5. Run flow as development API inside terminal

```

pf flow serve --source ./ --port 8080 --host localhost   

```

You fit test am for Postman / Thunder Client


### **Note**

1. The first run go take time. E good make you download di phi-3 model from Hugging face CLI.

2. Considering say Intel NPU get limited computing power, e better make you use Phi-3-mini-4k-instruct

3. We dey use Intel NPU Acceleration to quantize INT4 conversion, but if you run di service again, you go need delete di cache and nc_workshop folders.



## **Resources**

1. Learn Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Learn Intel NPU Acceleration [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Sample Code, download [Local NPU Agent Sample Code](../../../../../../../../../code/07.Lab/01/AIPC/local-npu-agent)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
Disclaimer:
Dis dokument don translate wit AI translation service Co-op Translator (https://github.com/Azure/co-op-translator). Even tho we dey try make am correct, abeg note say automatic translation fit get errors or no too accurate. Di original dokument for im original language suppose be di main/authoritative source. If na important information, e better make professional human translator check am. We no dey liable for any misunderstanding or wrong interpretation wey fit result from this translation.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->