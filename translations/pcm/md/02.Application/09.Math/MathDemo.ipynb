{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e7ac41",
   "metadata": {},
   "source": [
    "## Import Di Libraries Wey Dem Need\n",
    "Import PyTorch an transformers libraries wey you need to load an use di Phi-4 model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6381136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6caaf7",
   "metadata": {},
   "source": [
    "## Set di random seed\n",
    "\n",
    "Set di random seed make results dem dey the same across different runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f9d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ce6d9",
   "metadata": {},
   "source": [
    "## Make we load Phi-4-mini-flash-reasoning model and tokenizer\n",
    "\n",
    "Make we load di Microsoft Phi-4-mini-flash-reasoningmodel and di tokenizer wey match am from Hugging Face. Di model go load on CUDA so inference go dey faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d981b896",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"microsoft/Phi-4-mini-flash-reasoning\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "   model_id,\n",
    "   device_map=\"cuda\",\n",
    "   torch_dtype=\"auto\",\n",
    "   trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8789ea8",
   "metadata": {},
   "source": [
    "## Make di Input Message\n",
    "\n",
    "Make one conversation message wey get one quadratic equation math problem, and format am using di chat template for di model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1841fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\n",
    "   \"role\": \"user\",\n",
    "   \"content\": \"How to solve 3*x^2+4*x+5=1?\"\n",
    "}]   \n",
    "inputs = tokenizer.apply_chat_template(\n",
    "   messages,\n",
    "   add_generation_prompt=True,\n",
    "   return_dict=True,\n",
    "   return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b87bf8",
   "metadata": {},
   "source": [
    "## Generate Response\n",
    "Make one response from the model by using parameters wey dem specify like temperature and top_p to control how random the output go be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728f2de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "   **inputs.to(model.device),\n",
    "   max_new_tokens=32768,\n",
    "   temperature=0.6,\n",
    "   top_p=0.95,\n",
    "   do_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5088f0d",
   "metadata": {},
   "source": [
    "## Decode di Output make e be Text\n",
    "Turn di generated token sequences back to text wey pipo fit read, make e no include di original input tokens so e go show only di model response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f67c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\nDisclaimer:\nDis document don translate with AI translation service [Co-op Translator] (https://github.com/Azure/co-op-translator). We dey try make am correct, but make you sabi sey automated/machine translations fit get mistakes or no too accurate. Di original document for im original language na im be di authoritative/correct source. If na important information, make you use professional human translator. We no go take responsibility for any misunderstanding or wrong interpretation wey fit result from using this translation.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  },
  "coopTranslator": {
   "original_hash": "93d4d22a7a9ec7e7e668474526477813",
   "translation_date": "2025-12-22T07:16:05+00:00",
   "source_file": "md/02.Application/09.Math/MathDemo.ipynb",
   "language_code": "pcm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}