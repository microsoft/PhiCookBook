{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Phi 3 Mini 4K Instruct Chatbot wit Whisper\n",
    "\n",
    "### Introdukshon:\n",
    "Di Interactive Phi 3 Mini 4K Instruct Chatbot na tool wey allow users to interact wit di Microsoft Phi 3 Mini 4K instruct demo wit text or audio input. Di chatbot fit dey used for different kain tasks, like translate, give weather updates, and gather general information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[Make your Huggingface Access Token](https://huggingface.co/settings/tokens)\n",
    "\n",
    "Make a new token \n",
    "Give am a new name \n",
    "Select write permission dem\n",
    "copy the token and save am in a safe place\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di following Python code dey perform two main tins: e dey import the `os` module and e dey set one environment variable.\n",
    "\n",
    "1. How e dey import the `os` module:\n",
    "   - Di `os` module for Python dey give way to interact wit di operating system. E dey allow you make plenty operating systemâ€“related waka, like access environment variables, work wit files and directories, and so on.\n",
    "   - For this code, dem import di `os` module wit di `import` statement. Dis statement make di functionality of di `os` module available for use for di current Python script.\n",
    "\n",
    "2. Setting an environment variable:\n",
    "   - Environment variable na value wey programs wey dey run for di operating system fit access. Na way to store configuration settings or oda information wey plenty programs fit use.\n",
    "   - For this code, dem dey set new environment variable wit di `os.environ` dictionary. Di key for di dictionary na `'HF_TOKEN'`, and di value dem dey assign from di `HUGGINGFACE_TOKEN` variable.\n",
    "   - Di `HUGGINGFACE_TOKEN` variable dey defined just above dis code snippet, and dem assign am string value `\"hf_**************\"` using di `#@param` syntax. Dis syntax dey often used for Jupyter notebooks to allow user input and parameter configuration directly inside di notebook interface.\n",
    "   - By setting di `'HF_TOKEN'` environment variable, oda parts of di program or oda programs wey dey run for di same operating system fit access am.\n",
    "\n",
    "Overall, dis code dey import di `os` module and e dey set environment variable wey dem name `'HF_TOKEN'` wit di value wey `HUGGINGFACE_TOKEN` variable provide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dis code snippet dey define a function wey dem call clear_output wey dey used to clear the output of the current cell in Jupyter Notebook or IPython. Make we break down the code make we understand wetin e dey do:\n",
    "\n",
    "The function clear_output takes one parameter called wait, which is a boolean value. By default, wait is set to False. Dis parameter dey determine whether the function suppose wait until new output show to replace the existing output before e clear am.\n",
    "\n",
    "The function itsef dey used to clear the output of the current cell. In Jupyter Notebook or IPython, when a cell produce output, like printed text or graphical plots, that output dey show below the cell. The clear_output function dey allow you to clear that output.\n",
    "\n",
    "The implementation of the function no dey provided for the code snippet, as the ellipsis (...) show. The ellipsis represent placeholder for the actual code wey go perform the clearing of the output. The implementation fit involve interaction with the Jupyter Notebook or IPython API to remove the existing output from the cell.\n",
    "\n",
    "Overall, dis function dey provide one convenient way to clear the output of the current cell in Jupyter Notebook or IPython, make e easier to manage and update the displayed output during interactive coding sessions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform text-to-speech (TTS) using the Edge TTS service. Let's go through the relevant function implementations one by one:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Dis function dey take one input value and dey calculate the rate string for the TTS voice. The input value mean how fast you want the speech, where a value of 1 represent normal speed. The function go calculate the rate string by subtracting 1 from the input value, multiplying am by 100, and then determining the sign based on whether the input value dey greater than or equal to 1. The function go return the rate string for the format \"{sign}{rate}\".\n",
    "\n",
    "2.`make_chunks(input_text, language)`: Dis function dey take an input text and a language as parameters. E go split the input text into chunks based on language-specific rules. For this implementation, if the language is \"English\", the function go split the text at each period (\".\") and remove any leading or trailing whitespace. E go then append a period to each chunk and return the filtered list of chunks.\n",
    "\n",
    "3. `tts_file_name(text)`: Dis function dey generate a file name for the TTS audio file based on the input text. E go perform several transformations on the text: remove a trailing period (if present), convert the text to lowercase, strip leading and trailing whitespace, and replace spaces with underscores. E go then truncate the text to a maximum of 25 characters (if e longer) or use the full text if e empty. Finally, e go generate a random string using the [`uuid`] module and combine am with the truncated text to create the file name in the format \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Dis function dey merge multiple audio files into a single audio file. E go take a list of audio file paths and an output path as parameters. The function go initialize an empty `AudioSegment` object called [`merged_audio`]. E go then iterate through each audio file path, load the audio file using the `AudioSegment.from_file()` method from the `pydub` library, and append the current audio file to the [`merged_audio`] object. Finally, e go export the merged audio to the specified output path in the MP3 format.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path): This function performs the TTS operation using the Edge TTS service. E go take a list of text chunks, the speed of the speech, the voice name, and the save path as parameters. If the number of chunks is greater than 1, the function go create a directory for storing the individual chunk audio files. E go then iterate through each chunk, construct an Edge TTS command using the `calculate_rate_string()' function, the voice name, and the chunk text, and execute the command using the `os.system()` function. If the command execution succeed, e go append the path of the generated audio file to a list. After processing all the chunks, e go merge the individual audio files using the `merge_audio_files()` function and save the merged audio to the specified save path. If na only one chunk, e go directly generate the Edge TTS command and save the audio to the save path. Finally, e go return the save path of the generated audio file.\n",
    "\n",
    "6. `random_audio_name_generate()`: Dis function dey generate a random audio file name using the [`uuid`] module. E go generate a random UUID, convert am to a string, take the first 8 characters, append the \".mp3\" extension, and return the random audio file name.\n",
    "\n",
    "7. `talk(input_text)`: Dis function na the main entry point for performing the TTS operation. E go take an input text as a parameter. E first go check the length of the input text to determine if na long sentence (greater than or equal to 600 characters). Based on the length and the value of the `translate_text_flag` variable, e go determine the language and generate the list of text chunks using the `make_chunks()` function. E go then generate a save path for the audio file using the `random_audio_name_generate()` function. Finally, e go call the `edge_free_tts()` function to perform the TTS operation and return the save path of the generated audio file.\n",
    "\n",
    "Overall, these functions dey work together to split the input text into chunks, generate a file name for the audio file, perform the TTS operation using the Edge TTS service, and merge the individual audio files into a single audio file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na implementation of two functions: convert_to_text and run_text_prompt, as well as di declaration of two classes: str and Audio.\n",
    "\n",
    "Di convert_to_text function dey take an audio_path as input and e dey transcribe di audio to text using one model wey dem dey call whisper_model. Di function first go check if di gpu flag set to True. If e set, di whisper_model go dey used with som parameters like word_timestamps=True, fp16=True, language='English', and task='translate'. If di gpu flag False, di whisper_model go dey used with fp16=False. Di transcription wey come out go den save inside one file wey dem name 'scan.txt' and e go return as di text.\n",
    "\n",
    "Di run_text_prompt function dey take one message and one chat_history as input. E dey use di phi_demo function to generate response from chatbot based on di input message. Di response wey dem generate go den pass enter di talk function, wey go convert di response to audio file and return di file path. Di Audio class dey used to display and play di audio file. Di audio dey display using di display function from di IPython.display module, and di Audio object dey created with autoplay=True parameter, so di audio go start to play automatically. Di chat_history go update with di input message and di generated response, and dem go return empty string and di updated chat_history.\n",
    "\n",
    "Di str class na built-in class for Python wey represent one sequence of characters. E get plenty methods to manipulate and work with strings, like capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill, and more. Dem methods make you fit do things like search, replace, format, and manipulate strings.\n",
    "\n",
    "Di Audio class na custom class wey represent one audio object. Dem dey use am to create audio player for di Jupyter Notebook environment. Di class dey accept different parameters like data, filename, url, embed, rate, autoplay, and normalize. Di data parameter fit be numpy array, list of samples, one string wey represent filename or URL, or raw PCM data. Di filename parameter dey used to show local file to load di audio data from, and di url parameter dey used to show URL to download di audio data from. Di embed parameter dey determine whether di audio data suppose embed using data URI or make dem reference am from di original source. Di rate parameter dey specify di sampling rate of di audio data. Di autoplay parameter dey determine whether di audio go start to play automatically. Di normalize parameter dey specify whether di audio data go normalize (rescale) to di maximum possible range. Di Audio class still get methods like reload to reload di audio data from file or URL, and attributes like src_attr, autoplay_attr, and element_id_attr to fetch di corresponding attributes for di audio element for HTML.\n",
    "\n",
    "Overall, these functions and classes dey used to transcribe audio to text, generate audio responses from one chatbot, and display and play audio for di Jupyter Notebook environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\nDisclaimer:\nDis document don translate wit AI translation service Co-op Translator (https://github.com/Azure/co-op-translator). Even though we dey try make am correct, abeg sabi say automatic translations fit get mistakes or be inaccurate. Di original document for im native language na di official source. If na critical matter, make person wey sabi do professional human translation handle am. We no dey responsible for any misunderstanding or wrong interpretation wey fit come from using dis translation.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-12-22T04:59:45+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "pcm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}