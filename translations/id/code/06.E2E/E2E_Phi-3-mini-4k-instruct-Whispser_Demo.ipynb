{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot Interaktif Phi 3 Mini 4K Instruct dengan Whisper\n",
    "\n",
    "### Pendahuluan:\n",
    "Chatbot Interaktif Phi 3 Mini 4K Instruct adalah alat yang memungkinkan pengguna berinteraksi dengan demo Microsoft Phi 3 Mini 4K Instruct menggunakan input teks atau audio. Chatbot ini dapat digunakan untuk berbagai tugas, seperti penerjemahan, pembaruan cuaca, dan pengumpulan informasi umum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "Buat Token Akses Huggingface Anda\n",
    "\n",
    "Buat token baru  \n",
    "Berikan nama baru  \n",
    "Pilih izin tulis  \n",
    "Salin token dan simpan di tempat yang aman\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python berikut melakukan dua tugas utama: mengimpor modul `os` dan menetapkan variabel lingkungan.\n",
    "\n",
    "1. Mengimpor modul `os`:\n",
    "   - Modul `os` dalam Python menyediakan cara untuk berinteraksi dengan sistem operasi. Modul ini memungkinkan Anda melakukan berbagai tugas terkait sistem operasi, seperti mengakses variabel lingkungan, bekerja dengan file dan direktori, dll.\n",
    "   - Dalam kode ini, modul `os` diimpor menggunakan pernyataan `import`. Pernyataan ini membuat fungsi dari modul `os` tersedia untuk digunakan dalam skrip Python saat ini.\n",
    "\n",
    "2. Menetapkan variabel lingkungan:\n",
    "   - Variabel lingkungan adalah nilai yang dapat diakses oleh program yang berjalan di sistem operasi. Ini adalah cara untuk menyimpan pengaturan konfigurasi atau informasi lain yang dapat digunakan oleh beberapa program.\n",
    "   - Dalam kode ini, variabel lingkungan baru ditetapkan menggunakan kamus `os.environ`. Kunci dari kamus adalah `'HF_TOKEN'`, dan nilainya diberikan dari variabel `HUGGINGFACE_TOKEN`.\n",
    "   - Variabel `HUGGINGFACE_TOKEN` didefinisikan tepat di atas cuplikan kode ini, dan diberi nilai string `\"hf_**************\"` menggunakan sintaks `#@param`. Sintaks ini sering digunakan dalam Jupyter notebook untuk memungkinkan input pengguna dan konfigurasi parameter langsung di antarmuka notebook.\n",
    "   - Dengan menetapkan variabel lingkungan `'HF_TOKEN'`, variabel ini dapat diakses oleh bagian lain dari program atau program lain yang berjalan di sistem operasi yang sama.\n",
    "\n",
    "Secara keseluruhan, kode ini mengimpor modul `os` dan menetapkan variabel lingkungan bernama `'HF_TOKEN'` dengan nilai yang diberikan dalam variabel `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuplikan kode ini mendefinisikan sebuah fungsi bernama clear_output yang digunakan untuk menghapus output dari sel saat ini di Jupyter Notebook atau IPython. Mari kita uraikan kode ini dan pahami fungsinya:\n",
    "\n",
    "Fungsi clear_output memiliki satu parameter bernama wait, yang merupakan nilai boolean. Secara default, wait diatur ke False. Parameter ini menentukan apakah fungsi harus menunggu hingga output baru tersedia untuk menggantikan output yang ada sebelum menghapusnya.\n",
    "\n",
    "Fungsi ini sendiri digunakan untuk menghapus output dari sel saat ini. Di Jupyter Notebook atau IPython, ketika sebuah sel menghasilkan output, seperti teks yang dicetak atau grafik, output tersebut ditampilkan di bawah sel. Fungsi clear_output memungkinkan Anda untuk menghapus output tersebut.\n",
    "\n",
    "Implementasi fungsi ini tidak disediakan dalam cuplikan kode, seperti yang ditunjukkan oleh tanda elipsis (...). Tanda elipsis mewakili placeholder untuk kode aktual yang melakukan penghapusan output. Implementasi fungsi ini mungkin melibatkan interaksi dengan API Jupyter Notebook atau IPython untuk menghapus output yang ada dari sel.\n",
    "\n",
    "Secara keseluruhan, fungsi ini menyediakan cara yang praktis untuk menghapus output dari sel saat ini di Jupyter Notebook atau IPython, sehingga mempermudah pengelolaan dan pembaruan output yang ditampilkan selama sesi pengkodean interaktif.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan text-to-speech (TTS) menggunakan layanan Edge TTS. Mari kita bahas implementasi fungsi yang relevan satu per satu:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Fungsi ini menerima nilai input dan menghitung string kecepatan untuk suara TTS. Nilai input mewakili kecepatan bicara yang diinginkan, di mana nilai 1 mewakili kecepatan normal. Fungsi ini menghitung string kecepatan dengan mengurangi 1 dari nilai input, mengalikannya dengan 100, dan kemudian menentukan tanda berdasarkan apakah nilai input lebih besar atau sama dengan 1. Fungsi ini mengembalikan string kecepatan dalam format \"{sign}{rate}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: Fungsi ini menerima teks input dan bahasa sebagai parameter. Fungsi ini membagi teks input menjadi beberapa bagian berdasarkan aturan spesifik bahasa. Dalam implementasi ini, jika bahasanya adalah \"English\", fungsi ini membagi teks di setiap titik (\".\") dan menghapus spasi di awal atau akhir. Kemudian, fungsi ini menambahkan titik ke setiap bagian dan mengembalikan daftar bagian yang telah difilter.\n",
    "\n",
    "3. `tts_file_name(text)`: Fungsi ini menghasilkan nama file untuk file audio TTS berdasarkan teks input. Fungsi ini melakukan beberapa transformasi pada teks: menghapus titik di akhir (jika ada), mengubah teks menjadi huruf kecil, menghapus spasi di awal dan akhir, serta mengganti spasi dengan garis bawah. Kemudian, fungsi ini memotong teks hingga maksimal 25 karakter (jika lebih panjang) atau menggunakan teks penuh jika kosong. Akhirnya, fungsi ini menghasilkan string acak menggunakan modul [`uuid`] dan menggabungkannya dengan teks yang telah dipotong untuk membuat nama file dalam format \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Fungsi ini menggabungkan beberapa file audio menjadi satu file audio. Fungsi ini menerima daftar jalur file audio dan jalur output sebagai parameter. Fungsi ini menginisialisasi objek kosong `AudioSegment` yang disebut [`merged_audio`]. Kemudian, fungsi ini melakukan iterasi melalui setiap jalur file audio, memuat file audio menggunakan metode `AudioSegment.from_file()` dari pustaka `pydub`, dan menambahkan file audio saat ini ke objek [`merged_audio`]. Akhirnya, fungsi ini mengekspor audio yang telah digabungkan ke jalur output yang ditentukan dalam format MP3.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: Fungsi ini melakukan operasi TTS menggunakan layanan Edge TTS. Fungsi ini menerima daftar bagian teks, kecepatan bicara, nama suara, dan jalur penyimpanan sebagai parameter. Jika jumlah bagian lebih dari 1, fungsi ini membuat direktori untuk menyimpan file audio bagian individu. Kemudian, fungsi ini melakukan iterasi melalui setiap bagian, menyusun perintah Edge TTS menggunakan fungsi `calculate_rate_string()`, nama suara, dan teks bagian, lalu menjalankan perintah menggunakan fungsi `os.system()`. Jika eksekusi perintah berhasil, fungsi ini menambahkan jalur file audio yang dihasilkan ke daftar. Setelah memproses semua bagian, fungsi ini menggabungkan file audio individu menggunakan fungsi `merge_audio_files()` dan menyimpan audio yang telah digabungkan ke jalur penyimpanan yang ditentukan. Jika hanya ada satu bagian, fungsi ini langsung menghasilkan perintah Edge TTS dan menyimpan audio ke jalur penyimpanan. Akhirnya, fungsi ini mengembalikan jalur penyimpanan file audio yang dihasilkan.\n",
    "\n",
    "6. `random_audio_name_generate()`: Fungsi ini menghasilkan nama file audio acak menggunakan modul [`uuid`]. Fungsi ini menghasilkan UUID acak, mengonversinya menjadi string, mengambil 8 karakter pertama, menambahkan ekstensi \".mp3\", dan mengembalikan nama file audio acak.\n",
    "\n",
    "7. `talk(input_text)`: Fungsi ini adalah titik masuk utama untuk melakukan operasi TTS. Fungsi ini menerima teks input sebagai parameter. Fungsi ini pertama-tama memeriksa panjang teks input untuk menentukan apakah itu kalimat panjang (lebih besar atau sama dengan 600 karakter). Berdasarkan panjang dan nilai variabel `translate_text_flag`, fungsi ini menentukan bahasa dan menghasilkan daftar bagian teks menggunakan fungsi `make_chunks()`. Kemudian, fungsi ini menghasilkan jalur penyimpanan untuk file audio menggunakan fungsi `random_audio_name_generate()`. Akhirnya, fungsi ini memanggil fungsi `edge_free_tts()` untuk melakukan operasi TTS dan mengembalikan jalur penyimpanan file audio yang dihasilkan.\n",
    "\n",
    "Secara keseluruhan, fungsi-fungsi ini bekerja sama untuk membagi teks input menjadi bagian-bagian, menghasilkan nama file untuk file audio, melakukan operasi TTS menggunakan layanan Edge TTS, dan menggabungkan file audio individu menjadi satu file audio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementasi dua fungsi: convert_to_text dan run_text_prompt, serta deklarasi dua kelas: str dan Audio.\n",
    "\n",
    "Fungsi convert_to_text menerima audio_path sebagai input dan mentranskripsi audio menjadi teks menggunakan model bernama whisper_model. Fungsi ini pertama-tama memeriksa apakah flag gpu disetel ke True. Jika iya, whisper_model digunakan dengan parameter tertentu seperti word_timestamps=True, fp16=True, language='English', dan task='translate'. Jika flag gpu adalah False, whisper_model digunakan dengan fp16=False. Hasil transkripsi kemudian disimpan ke file bernama 'scan.txt' dan dikembalikan sebagai teks.\n",
    "\n",
    "Fungsi run_text_prompt menerima message dan chat_history sebagai input. Fungsi ini menggunakan phi_demo untuk menghasilkan respons dari chatbot berdasarkan pesan yang diberikan. Respons yang dihasilkan kemudian diteruskan ke fungsi talk, yang mengubah respons tersebut menjadi file audio dan mengembalikan path file tersebut. Kelas Audio digunakan untuk menampilkan dan memutar file audio. Audio ditampilkan menggunakan fungsi display dari modul IPython.display, dan objek Audio dibuat dengan parameter autoplay=True, sehingga audio mulai diputar secara otomatis. chat_history diperbarui dengan pesan input dan respons yang dihasilkan, lalu string kosong dan chat_history yang diperbarui dikembalikan.\n",
    "\n",
    "Kelas str adalah kelas bawaan di Python yang merepresentasikan urutan karakter. Kelas ini menyediakan berbagai metode untuk memanipulasi dan bekerja dengan string, seperti capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill, dan lainnya. Metode-metode ini memungkinkan Anda melakukan operasi seperti pencarian, penggantian, format, dan manipulasi string.\n",
    "\n",
    "Kelas Audio adalah kelas kustom yang merepresentasikan objek audio. Kelas ini digunakan untuk membuat pemutar audio di lingkungan Jupyter Notebook. Kelas ini menerima berbagai parameter seperti data, filename, url, embed, rate, autoplay, dan normalize. Parameter data dapat berupa array numpy, daftar sampel, string yang merepresentasikan nama file atau URL, atau data PCM mentah. Parameter filename digunakan untuk menentukan file lokal yang akan dimuat data audionya, dan parameter url digunakan untuk menentukan URL untuk mengunduh data audio. Parameter embed menentukan apakah data audio harus disematkan menggunakan data URI atau direferensikan dari sumber aslinya. Parameter rate menentukan tingkat sampling data audio. Parameter autoplay menentukan apakah audio harus mulai diputar secara otomatis. Parameter normalize menentukan apakah data audio harus dinormalisasi (diskalakan ulang) ke rentang maksimum yang mungkin. Kelas Audio juga menyediakan metode seperti reload untuk memuat ulang data audio dari file atau URL, serta atribut seperti src_attr, autoplay_attr, dan element_id_attr untuk mengambil atribut yang sesuai untuk elemen audio dalam HTML.\n",
    "\n",
    "Secara keseluruhan, fungsi dan kelas ini digunakan untuk mentranskripsi audio menjadi teks, menghasilkan respons audio dari chatbot, serta menampilkan dan memutar audio di lingkungan Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Penafian**:  \nDokumen ini telah diterjemahkan menggunakan layanan penerjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Meskipun kami berusaha untuk memberikan hasil yang akurat, harap diketahui bahwa terjemahan otomatis mungkin mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang otoritatif. Untuk informasi yang bersifat kritis, disarankan menggunakan jasa penerjemahan profesional oleh manusia. Kami tidak bertanggung jawab atas kesalahpahaman atau penafsiran yang keliru yang timbul dari penggunaan terjemahan ini.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T23:19:01+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "id"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}