<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:27:52+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "id"
}
-->
# Teknologi kunci yang disebutkan meliputi

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - API tingkat rendah untuk machine learning yang dipercepat perangkat keras, dibangun di atas DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - platform komputasi paralel dan model antarmuka pemrograman aplikasi (API) yang dikembangkan oleh Nvidia, memungkinkan pemrosesan tujuan umum pada unit pemrosesan grafis (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - format terbuka yang dirancang untuk merepresentasikan model machine learning yang menyediakan interoperabilitas antar berbagai kerangka kerja ML.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - format yang digunakan untuk merepresentasikan dan memperbarui model machine learning, sangat berguna untuk model bahasa kecil yang dapat berjalan efektif pada CPU dengan kuantisasi 4-8bit.

## DirectML

DirectML adalah API tingkat rendah yang memungkinkan machine learning dipercepat perangkat keras. Dibangun di atas DirectX 12 untuk memanfaatkan akselerasi GPU dan bersifat vendor-agnostik, artinya tidak memerlukan perubahan kode agar dapat bekerja di berbagai vendor GPU. DirectML terutama digunakan untuk pelatihan model dan beban kerja inferensi pada GPU.

Mengenai dukungan perangkat keras, DirectML dirancang untuk bekerja dengan berbagai GPU, termasuk GPU terintegrasi dan diskrit AMD, GPU terintegrasi Intel, dan GPU diskrit NVIDIA. Ini merupakan bagian dari Windows AI Platform dan didukung di Windows 10 & 11, memungkinkan pelatihan model dan inferensi di perangkat Windows mana pun.

Telah ada pembaruan dan peluang terkait DirectML, seperti dukungan hingga 150 operator ONNX dan digunakan oleh runtime ONNX serta WinML. DirectML didukung oleh vendor perangkat keras terintegrasi utama (IHV), yang masing-masing mengimplementasikan berbagai metaperintah.

## CUDA

CUDA, singkatan dari Compute Unified Device Architecture, adalah platform komputasi paralel dan model antarmuka pemrograman aplikasi (API) yang dibuat oleh Nvidia. CUDA memungkinkan pengembang perangkat lunak menggunakan GPU yang mendukung CUDA untuk pemrosesan tujuan umum â€“ pendekatan yang disebut GPGPU (General-Purpose computing on Graphics Processing Units). CUDA merupakan kunci akselerasi GPU Nvidia dan banyak digunakan di berbagai bidang, termasuk machine learning, komputasi ilmiah, dan pemrosesan video.

Dukungan perangkat keras untuk CUDA khusus pada GPU Nvidia, karena ini adalah teknologi kepemilikan yang dikembangkan oleh Nvidia. Setiap arsitektur mendukung versi tertentu dari toolkit CUDA, yang menyediakan pustaka dan alat yang diperlukan bagi pengembang untuk membangun dan menjalankan aplikasi CUDA.

## ONNX

ONNX (Open Neural Network Exchange) adalah format terbuka yang dirancang untuk merepresentasikan model machine learning. ONNX menyediakan definisi model grafik komputasi yang dapat diperluas, serta definisi operator bawaan dan tipe data standar. ONNX memungkinkan pengembang memindahkan model antar berbagai kerangka kerja ML, mendukung interoperabilitas dan mempermudah pembuatan serta penyebaran aplikasi AI.

Phi3 mini dapat berjalan dengan ONNX Runtime pada CPU dan GPU di berbagai perangkat, termasuk platform server, desktop Windows, Linux dan Mac, serta CPU mobile.
Konfigurasi yang telah kami optimalkan adalah

- Model ONNX untuk int4 DML: Dikuantisasi ke int4 melalui AWQ
- Model ONNX untuk fp16 CUDA
- Model ONNX untuk int4 CUDA: Dikuantisasi ke int4 melalui RTN
- Model ONNX untuk int4 CPU dan Mobile: Dikuantisasi ke int4 melalui RTN

## Llama.cpp

Llama.cpp adalah pustaka perangkat lunak sumber terbuka yang ditulis dalam C++. Ini melakukan inferensi pada berbagai Large Language Models (LLM), termasuk Llama. Dikembangkan bersamaan dengan pustaka ggml (pustaka tensor tujuan umum), llama.cpp bertujuan memberikan inferensi lebih cepat dan penggunaan memori lebih rendah dibandingkan implementasi Python asli. Llama.cpp mendukung optimasi perangkat keras, kuantisasi, dan menyediakan API sederhana serta contoh-contoh. Jika Anda tertarik pada inferensi LLM yang efisien, llama.cpp layak untuk dijelajahi karena Phi3 dapat menjalankan Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) adalah format yang digunakan untuk merepresentasikan dan memperbarui model machine learning. Format ini sangat berguna untuk model bahasa kecil (SLM) yang dapat berjalan efektif pada CPU dengan kuantisasi 4-8bit. GGUF bermanfaat untuk prototipe cepat dan menjalankan model pada perangkat edge atau dalam pekerjaan batch seperti pipeline CI/CD.

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan layanan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Meskipun kami berusaha untuk akurasi, harap diperhatikan bahwa terjemahan otomatis mungkin mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang sahih. Untuk informasi yang penting, disarankan menggunakan terjemahan profesional oleh manusia. Kami tidak bertanggung jawab atas kesalahpahaman atau salah tafsir yang timbul dari penggunaan terjemahan ini.