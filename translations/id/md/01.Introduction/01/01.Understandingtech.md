<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:46:25+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "id"
}
-->
# Teknologi utama yang disebutkan meliputi

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - API tingkat rendah untuk pembelajaran mesin yang dipercepat perangkat keras yang dibangun di atas DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - platform komputasi paralel dan model antarmuka pemrograman aplikasi (API) yang dikembangkan oleh Nvidia, memungkinkan pemrosesan tujuan umum pada unit pemrosesan grafis (GPU).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - format terbuka yang dirancang untuk merepresentasikan model pembelajaran mesin yang menyediakan interoperabilitas antar berbagai kerangka kerja ML.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - format yang digunakan untuk merepresentasikan dan memperbarui model pembelajaran mesin, sangat berguna untuk model bahasa kecil yang dapat berjalan efektif di CPU dengan kuantisasi 4-8bit.

## DirectML

DirectML adalah API tingkat rendah yang memungkinkan pembelajaran mesin dipercepat oleh perangkat keras. API ini dibangun di atas DirectX 12 untuk memanfaatkan akselerasi GPU dan bersifat vendor-agnostik, artinya tidak memerlukan perubahan kode agar dapat bekerja di berbagai vendor GPU. DirectML terutama digunakan untuk pelatihan model dan beban kerja inferensi pada GPU.

Mengenai dukungan perangkat keras, DirectML dirancang untuk bekerja dengan berbagai GPU, termasuk GPU terintegrasi dan diskrit AMD, GPU terintegrasi Intel, serta GPU diskrit NVIDIA. Ini merupakan bagian dari Windows AI Platform dan didukung di Windows 10 & 11, memungkinkan pelatihan model dan inferensi di perangkat Windows manapun.

Telah ada pembaruan dan peluang terkait DirectML, seperti dukungan hingga 150 operator ONNX dan digunakan oleh runtime ONNX serta WinML. DirectML didukung oleh vendor perangkat keras terintegrasi utama (IHV), yang masing-masing mengimplementasikan berbagai metaperintah.

## CUDA

CUDA, singkatan dari Compute Unified Device Architecture, adalah platform komputasi paralel dan model antarmuka pemrograman aplikasi (API) yang dibuat oleh Nvidia. CUDA memungkinkan pengembang perangkat lunak menggunakan unit pemrosesan grafis (GPU) yang mendukung CUDA untuk pemrosesan tujuan umum â€” pendekatan yang dikenal sebagai GPGPU (General-Purpose computing on Graphics Processing Units). CUDA adalah kunci akselerasi GPU Nvidia dan banyak digunakan di berbagai bidang, termasuk pembelajaran mesin, komputasi ilmiah, dan pemrosesan video.

Dukungan perangkat keras untuk CUDA khusus pada GPU Nvidia, karena ini adalah teknologi kepemilikan yang dikembangkan oleh Nvidia. Setiap arsitektur mendukung versi tertentu dari toolkit CUDA, yang menyediakan perpustakaan dan alat yang diperlukan bagi pengembang untuk membangun dan menjalankan aplikasi CUDA.

## ONNX

ONNX (Open Neural Network Exchange) adalah format terbuka yang dirancang untuk merepresentasikan model pembelajaran mesin. ONNX menyediakan definisi model grafik komputasi yang dapat diperluas, serta definisi operator bawaan dan tipe data standar. ONNX memungkinkan pengembang memindahkan model antar kerangka kerja ML yang berbeda, mendukung interoperabilitas dan mempermudah pembuatan serta penerapan aplikasi AI.

Phi3 mini dapat berjalan dengan ONNX Runtime pada CPU dan GPU di berbagai perangkat, termasuk platform server, desktop Windows, Linux, dan Mac, serta CPU mobile.  
Konfigurasi yang telah kami optimalkan meliputi:

- Model ONNX untuk int4 DML: Dikuantisasi ke int4 melalui AWQ  
- Model ONNX untuk fp16 CUDA  
- Model ONNX untuk int4 CUDA: Dikuantisasi ke int4 melalui RTN  
- Model ONNX untuk int4 CPU dan Mobile: Dikuantisasi ke int4 melalui RTN  

## Llama.cpp

Llama.cpp adalah perpustakaan perangkat lunak sumber terbuka yang ditulis dalam C++. Perpustakaan ini melakukan inferensi pada berbagai Large Language Models (LLM), termasuk Llama. Dikembangkan bersamaan dengan perpustakaan ggml (perpustakaan tensor serbaguna), llama.cpp bertujuan memberikan inferensi yang lebih cepat dan penggunaan memori yang lebih rendah dibandingkan implementasi Python aslinya. Llama.cpp mendukung optimasi perangkat keras, kuantisasi, serta menyediakan API sederhana dan contoh penggunaan. Jika Anda tertarik pada inferensi LLM yang efisien, llama.cpp layak untuk dijelajahi karena Phi3 dapat menjalankan Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) adalah format yang digunakan untuk merepresentasikan dan memperbarui model pembelajaran mesin. Format ini sangat berguna untuk model bahasa kecil (SLM) yang dapat berjalan efektif di CPU dengan kuantisasi 4-8bit. GGUF bermanfaat untuk prototipe cepat dan menjalankan model di perangkat edge atau dalam pekerjaan batch seperti pipeline CI/CD.

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan layanan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Meskipun kami berupaya untuk mencapai akurasi, harap diperhatikan bahwa terjemahan otomatis mungkin mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang sahih. Untuk informasi penting, disarankan menggunakan terjemahan profesional oleh manusia. Kami tidak bertanggung jawab atas kesalahpahaman atau penafsiran yang keliru yang timbul dari penggunaan terjemahan ini.