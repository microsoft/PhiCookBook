<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "bc29f7fe7fc16bed6932733eac8c81b8",
  "translation_date": "2025-07-17T04:00:13+00:00",
  "source_file": "md/02.Application/02.Code/Phi3/VSCodeExt/HOL/AIPC/02.PromptflowWithNPU.md",
  "language_code": "id"
}
-->
# **Lab 2 - Menjalankan Prompt flow dengan Phi-3-mini di AIPC**

## **Apa itu Prompt flow**

Prompt flow adalah rangkaian alat pengembangan yang dirancang untuk mempermudah siklus pengembangan aplikasi AI berbasis LLM secara menyeluruh, mulai dari ide, prototipe, pengujian, evaluasi hingga penerapan produksi dan pemantauan. Ini membuat rekayasa prompt menjadi jauh lebih mudah dan memungkinkan Anda membangun aplikasi LLM dengan kualitas produksi.

Dengan prompt flow, Anda dapat:

- Membuat alur yang menghubungkan LLM, prompt, kode Python, dan alat lain dalam sebuah workflow yang dapat dijalankan.

- Debug dan iterasi alur Anda, terutama interaksi dengan LLM dengan mudah.

- Mengevaluasi alur Anda, menghitung metrik kualitas dan performa dengan dataset yang lebih besar.

- Mengintegrasikan pengujian dan evaluasi ke dalam sistem CI/CD Anda untuk memastikan kualitas alur.

- Menerapkan alur Anda ke platform penyajian yang Anda pilih atau mengintegrasikannya ke basis kode aplikasi Anda dengan mudah.

- (Opsional tapi sangat disarankan) Berkolaborasi dengan tim Anda dengan memanfaatkan versi cloud dari Prompt flow di Azure AI.

## **Apa itu AIPC**

AI PC memiliki CPU, GPU, dan NPU, masing-masing dengan kemampuan akselerasi AI khusus. NPU, atau neural processing unit, adalah akselerator khusus yang menangani tugas kecerdasan buatan (AI) dan pembelajaran mesin (ML) langsung di PC Anda tanpa harus mengirim data untuk diproses di cloud. GPU dan CPU juga dapat memproses beban kerja ini, tetapi NPU sangat unggul dalam perhitungan AI dengan daya rendah. AI PC mewakili perubahan mendasar dalam cara komputer kita beroperasi. Ini bukan solusi untuk masalah yang sebelumnya tidak ada. Sebaliknya, ini menjanjikan peningkatan besar untuk penggunaan PC sehari-hari.

Lalu bagaimana cara kerjanya? Dibandingkan dengan AI generatif dan model bahasa besar (LLM) yang dilatih dengan banyak data publik, AI yang berjalan di PC Anda lebih mudah diakses di hampir semua level. Konsepnya lebih mudah dipahami, dan karena dilatih dengan data Anda sendiri tanpa perlu mengakses cloud, manfaatnya lebih langsung terasa bagi khalayak yang lebih luas.

Dalam jangka pendek, dunia AI PC melibatkan asisten pribadi dan model AI yang lebih kecil yang berjalan langsung di PC Anda, menggunakan data Anda untuk menawarkan peningkatan AI yang personal, privat, dan lebih aman untuk hal-hal yang sudah Anda lakukan setiap hari – mencatat hasil rapat, mengatur liga fantasy football, mengotomatisasi peningkatan untuk pengeditan foto dan video, atau menyusun jadwal perjalanan sempurna untuk reuni keluarga berdasarkan waktu kedatangan dan keberangkatan semua orang.

## **Membangun alur kode generasi di AIPC**

***Note*** ：Jika Anda belum menyelesaikan instalasi lingkungan, silakan kunjungi [Lab 0 -Installations](./01.Installations.md)

1. Buka Ekstensi Prompt flow di Visual Studio Code dan buat proyek alur kosong

![create](../../../../../../../../../translated_images/pf_create.bde888dc83502eba.id.png)

2. Tambahkan parameter Inputs dan Outputs serta Tambahkan Kode Python sebagai alur baru

![flow](../../../../../../../../../translated_images/pf_flow.520824c0969f2a94.id.png)

Anda dapat merujuk pada struktur ini (flow.dag.yaml) untuk membangun alur Anda

```yaml

inputs:
  question:
    type: string
    default: how to write Bubble Algorithm
outputs:
  answer:
    type: string
    reference: ${Chat_With_Phi3.output}
nodes:
- name: Chat_With_Phi3
  type: python
  source:
    type: code
    path: Chat_With_Phi3.py
  inputs:
    question: ${inputs.question}


```

3. Tambahkan Kode di ***Chat_With_Phi3.py***

```python


from promptflow.core import tool

# import torch
from transformers import AutoTokenizer, pipeline,TextStreamer
import intel_npu_acceleration_library as npu_lib

import warnings

import asyncio
import platform

class Phi3CodeAgent:
    
    model = None
    tokenizer = None
    text_streamer = None
    
    model_id = "microsoft/Phi-3-mini-4k-instruct"

    @staticmethod
    def init_phi3():
        
        if Phi3CodeAgent.model is None or Phi3CodeAgent.tokenizer is None or Phi3CodeAgent.text_streamer is None:
            Phi3CodeAgent.model = npu_lib.NPUModelForCausalLM.from_pretrained(
                                    Phi3CodeAgent.model_id,
                                    torch_dtype="auto",
                                    dtype=npu_lib.int4,
                                    trust_remote_code=True
                                )
            Phi3CodeAgent.tokenizer = AutoTokenizer.from_pretrained(Phi3CodeAgent.model_id)
            Phi3CodeAgent.text_streamer = TextStreamer(Phi3CodeAgent.tokenizer, skip_prompt=True)

    

    @staticmethod
    def chat_with_phi3(prompt):
        
        Phi3CodeAgent.init_phi3()

        messages = "<|system|>You are a AI Python coding assistant. Please help me to generate code in Python.The answer only genertated Python code, but any comments and instructions do not need to be generated<|end|><|user|>" + prompt +"<|end|><|assistant|>"



        generation_args = {
            "max_new_tokens": 1024,
            "return_full_text": False,
            "temperature": 0.3,
            "do_sample": False,
            "streamer": Phi3CodeAgent.text_streamer,
        }

        pipe = pipeline(
            "text-generation",
            model=Phi3CodeAgent.model,
            tokenizer=Phi3CodeAgent.tokenizer,
            # **generation_args
        )

        result = ''

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            response = pipe(messages, **generation_args)
            result =response[0]['generated_text']
            return result


@tool
def my_python_tool(question: str) -> str:
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    return Phi3CodeAgent.chat_with_phi3(question)


```

4. Anda dapat menguji alur dari Debug atau Run untuk memeriksa apakah kode generasi berjalan dengan baik atau tidak

![RUN](../../../../../../../../../translated_images/pf_run.4239e8a0b420a582.id.png)

5. Jalankan alur sebagai API pengembangan di terminal

```

pf flow serve --source ./ --port 8080 --host localhost   

```

Anda dapat mengujinya di Postman / Thunder Client

### **Note**

1. Jalankan pertama kali akan memakan waktu lama. Disarankan untuk mengunduh model phi-3 dari Hugging face CLI.

2. Mengingat keterbatasan daya komputasi Intel NPU, disarankan menggunakan Phi-3-mini-4k-instruct

3. Kami menggunakan Intel NPU Acceleration untuk konversi kuantisasi INT4, tetapi jika Anda menjalankan ulang layanan, Anda perlu menghapus folder cache dan nc_workshop.

## **Sumber Daya**

1. Pelajari Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Pelajari Intel NPU Acceleration [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Kode Contoh, unduh [Local NPU Agent Sample Code](../../../../../../../../../code/07.Lab/01/AIPC)

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan layanan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Meskipun kami berupaya untuk akurasi, harap diketahui bahwa terjemahan otomatis mungkin mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang sah. Untuk informasi penting, disarankan menggunakan terjemahan profesional oleh manusia. Kami tidak bertanggung jawab atas kesalahpahaman atau penafsiran yang salah yang timbul dari penggunaan terjemahan ini.