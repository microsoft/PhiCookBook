{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot interactif Phi 3 Mini 4K Instruct avec Whisper\n",
    "\n",
    "### Introduction :\n",
    "Le chatbot interactif Phi 3 Mini 4K Instruct est un outil qui permet aux utilisateurs d'interagir avec la démonstration Microsoft Phi 3 Mini 4K Instruct en utilisant une entrée textuelle ou audio. Le chatbot peut être utilisé pour diverses tâches, telles que la traduction, les mises à jour météo et la collecte d'informations générales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[Créer votre jeton d'accès Huggingface](https://huggingface.co/settings/tokens)\n",
    "\n",
    "Créer un nouveau jeton  \n",
    "Donner un nouveau nom  \n",
    "Sélectionner les permissions d'écriture  \n",
    "Copier le jeton et le conserver dans un endroit sûr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code Python suivant effectue deux tâches principales : importer le module `os` et définir une variable d'environnement.\n",
    "\n",
    "1. Importation du module `os` :\n",
    "   - Le module `os` en Python offre un moyen d'interagir avec le système d'exploitation. Il permet d'effectuer diverses tâches liées au système, comme accéder aux variables d'environnement, manipuler des fichiers et des répertoires, etc.\n",
    "   - Dans ce code, le module `os` est importé à l'aide de l'instruction `import`. Cette instruction rend les fonctionnalités du module `os` disponibles pour une utilisation dans le script Python actuel.\n",
    "\n",
    "2. Définition d'une variable d'environnement :\n",
    "   - Une variable d'environnement est une valeur accessible par les programmes exécutés sur le système d'exploitation. Elle permet de stocker des paramètres de configuration ou d'autres informations pouvant être utilisées par plusieurs programmes.\n",
    "   - Dans ce code, une nouvelle variable d'environnement est définie en utilisant le dictionnaire `os.environ`. La clé du dictionnaire est `'HF_TOKEN'`, et la valeur est assignée à partir de la variable `HUGGINGFACE_TOKEN`.\n",
    "   - La variable `HUGGINGFACE_TOKEN` est définie juste au-dessus de cet extrait de code, et elle reçoit une valeur de type chaîne de caractères `\"hf_**************\"` à l'aide de la syntaxe `#@param`. Cette syntaxe est souvent utilisée dans les notebooks Jupyter pour permettre la saisie utilisateur et la configuration des paramètres directement dans l'interface du notebook.\n",
    "   - En définissant la variable d'environnement `'HF_TOKEN'`, celle-ci peut être accessible par d'autres parties du programme ou par d'autres programmes exécutés sur le même système d'exploitation.\n",
    "\n",
    "En résumé, ce code importe le module `os` et définit une variable d'environnement nommée `'HF_TOKEN'` avec la valeur fournie dans la variable `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce snippet de code définit une fonction appelée clear_output, utilisée pour effacer le contenu affiché dans la cellule actuelle d'un Jupyter Notebook ou d'IPython. Décortiquons le code et comprenons son fonctionnement :\n",
    "\n",
    "La fonction clear_output prend un paramètre appelé wait, qui est une valeur booléenne. Par défaut, wait est défini sur False. Ce paramètre détermine si la fonction doit attendre qu'une nouvelle sortie soit disponible pour remplacer l'ancienne avant de l'effacer.\n",
    "\n",
    "La fonction elle-même est utilisée pour effacer le contenu affiché dans la cellule actuelle. Dans un Jupyter Notebook ou IPython, lorsqu'une cellule produit une sortie, comme du texte imprimé ou des graphiques, cette sortie est affichée sous la cellule. La fonction clear_output permet de supprimer cette sortie.\n",
    "\n",
    "L'implémentation de la fonction n'est pas fournie dans le snippet de code, comme indiqué par les points de suspension (...). Ces points de suspension représentent un espace réservé pour le code réel qui effectue l'effacement de la sortie. L'implémentation de la fonction pourrait impliquer une interaction avec l'API de Jupyter Notebook ou d'IPython pour supprimer la sortie existante de la cellule.\n",
    "\n",
    "En résumé, cette fonction offre un moyen pratique d'effacer le contenu affiché dans la cellule actuelle d'un Jupyter Notebook ou d'IPython, facilitant ainsi la gestion et la mise à jour des sorties affichées lors de sessions de codage interactives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effectuer la synthèse vocale (TTS) en utilisant le service Edge TTS. Passons en revue les implémentations des fonctions pertinentes une par une :\n",
    "\n",
    "1. `calculate_rate_string(input_value)` : Cette fonction prend une valeur en entrée et calcule la chaîne de taux pour la voix TTS. La valeur d'entrée représente la vitesse souhaitée de la parole, où une valeur de 1 représente la vitesse normale. La fonction calcule la chaîne de taux en soustrayant 1 de la valeur d'entrée, en la multipliant par 100, puis en déterminant le signe en fonction de si la valeur d'entrée est supérieure ou égale à 1. La fonction retourne la chaîne de taux au format \"{signe}{taux}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)` : Cette fonction prend un texte en entrée et une langue comme paramètres. Elle divise le texte en morceaux en fonction des règles spécifiques à la langue. Dans cette implémentation, si la langue est \"English\", la fonction divise le texte à chaque point (\".\") et supprime les espaces en début ou en fin de chaîne. Elle ajoute ensuite un point à chaque morceau et retourne la liste filtrée des morceaux.\n",
    "\n",
    "3. `tts_file_name(text)` : Cette fonction génère un nom de fichier pour le fichier audio TTS basé sur le texte en entrée. Elle effectue plusieurs transformations sur le texte : suppression d'un point final (si présent), conversion du texte en minuscules, suppression des espaces en début et en fin de chaîne, et remplacement des espaces par des underscores. Elle tronque ensuite le texte à un maximum de 25 caractères (si plus long) ou utilise le texte complet s'il est vide. Enfin, elle génère une chaîne aléatoire en utilisant le module [`uuid`] et la combine avec le texte tronqué pour créer le nom de fichier au format \"/content/edge_tts_voice/{texte_tronqué}_{chaîne_aléatoire}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)` : Cette fonction fusionne plusieurs fichiers audio en un seul fichier audio. Elle prend une liste de chemins de fichiers audio et un chemin de sortie comme paramètres. La fonction initialise un objet `AudioSegment` vide appelé [`merged_audio`]. Elle parcourt ensuite chaque chemin de fichier audio, charge le fichier audio en utilisant la méthode `AudioSegment.from_file()` de la bibliothèque `pydub`, et ajoute le fichier audio actuel à l'objet [`merged_audio`]. Enfin, elle exporte l'audio fusionné vers le chemin de sortie spécifié au format MP3.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)` : Cette fonction effectue l'opération TTS en utilisant le service Edge TTS. Elle prend une liste de morceaux de texte, la vitesse de la parole, le nom de la voix et le chemin de sauvegarde comme paramètres. Si le nombre de morceaux est supérieur à 1, la fonction crée un répertoire pour stocker les fichiers audio des morceaux individuels. Elle parcourt ensuite chaque morceau, construit une commande Edge TTS en utilisant la fonction `calculate_rate_string()`, le nom de la voix et le texte du morceau, et exécute la commande en utilisant la fonction `os.system()`. Si l'exécution de la commande réussit, elle ajoute le chemin du fichier audio généré à une liste. Après avoir traité tous les morceaux, elle fusionne les fichiers audio individuels en utilisant la fonction `merge_audio_files()` et sauvegarde l'audio fusionné au chemin de sauvegarde spécifié. S'il n'y a qu'un seul morceau, elle génère directement la commande Edge TTS et sauvegarde l'audio au chemin de sauvegarde. Enfin, elle retourne le chemin de sauvegarde du fichier audio généré.\n",
    "\n",
    "6. `random_audio_name_generate()` : Cette fonction génère un nom de fichier audio aléatoire en utilisant le module [`uuid`]. Elle génère un UUID aléatoire, le convertit en chaîne, prend les huit premiers caractères, ajoute l'extension \".mp3\", et retourne le nom de fichier audio aléatoire.\n",
    "\n",
    "7. `talk(input_text)` : Cette fonction est le point d'entrée principal pour effectuer l'opération TTS. Elle prend un texte en entrée comme paramètre. Elle vérifie d'abord la longueur du texte en entrée pour déterminer s'il s'agit d'une phrase longue (600 caractères ou plus). En fonction de la longueur et de la valeur de la variable `translate_text_flag`, elle détermine la langue et génère la liste des morceaux de texte en utilisant la fonction `make_chunks()`. Elle génère ensuite un chemin de sauvegarde pour le fichier audio en utilisant la fonction `random_audio_name_generate()`. Enfin, elle appelle la fonction `edge_free_tts()` pour effectuer l'opération TTS et retourne le chemin de sauvegarde du fichier audio généré.\n",
    "\n",
    "Dans l'ensemble, ces fonctions travaillent ensemble pour diviser le texte en morceaux, générer un nom de fichier pour le fichier audio, effectuer l'opération TTS en utilisant le service Edge TTS, et fusionner les fichiers audio individuels en un seul fichier audio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mise en œuvre de deux fonctions : convert_to_text et run_text_prompt, ainsi que la déclaration de deux classes : str et Audio.\n",
    "\n",
    "La fonction convert_to_text prend un chemin audio (audio_path) en entrée et transcrit l'audio en texte à l'aide d'un modèle appelé whisper_model. La fonction commence par vérifier si le drapeau gpu est défini sur True. Si c'est le cas, le whisper_model est utilisé avec certains paramètres tels que word_timestamps=True, fp16=True, language='English', et task='translate'. Si le drapeau gpu est False, le whisper_model est utilisé avec fp16=False. La transcription obtenue est ensuite sauvegardée dans un fichier nommé 'scan.txt' et retournée sous forme de texte.\n",
    "\n",
    "La fonction run_text_prompt prend un message et un historique de conversation (chat_history) en entrée. Elle utilise la fonction phi_demo pour générer une réponse d'un chatbot basée sur le message d'entrée. La réponse générée est ensuite passée à la fonction talk, qui convertit la réponse en un fichier audio et retourne le chemin du fichier. La classe Audio est utilisée pour afficher et lire le fichier audio. L'audio est affiché à l'aide de la fonction display du module IPython.display, et l'objet Audio est créé avec le paramètre autoplay=True, ce qui permet à l'audio de démarrer automatiquement. L'historique de conversation est mis à jour avec le message d'entrée et la réponse générée, et une chaîne vide ainsi que l'historique de conversation mis à jour sont retournés.\n",
    "\n",
    "La classe str est une classe intégrée de Python qui représente une séquence de caractères. Elle fournit diverses méthodes pour manipuler et travailler avec des chaînes de caractères, telles que capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill, et bien d'autres. Ces méthodes permettent d'effectuer des opérations comme la recherche, le remplacement, le formatage et la manipulation de chaînes de caractères.\n",
    "\n",
    "La classe Audio est une classe personnalisée qui représente un objet audio. Elle est utilisée pour créer un lecteur audio dans l'environnement Jupyter Notebook. La classe accepte divers paramètres tels que data, filename, url, embed, rate, autoplay, et normalize. Le paramètre data peut être un tableau numpy, une liste d'échantillons, une chaîne représentant un nom de fichier ou une URL, ou des données PCM brutes. Le paramètre filename est utilisé pour spécifier un fichier local à partir duquel charger les données audio, et le paramètre url est utilisé pour spécifier une URL à partir de laquelle télécharger les données audio. Le paramètre embed détermine si les données audio doivent être intégrées à l'aide d'un URI de données ou référencées à partir de la source originale. Le paramètre rate spécifie la fréquence d'échantillonnage des données audio. Le paramètre autoplay détermine si l'audio doit commencer à jouer automatiquement. Le paramètre normalize spécifie si les données audio doivent être normalisées (recalibrées) à la plage maximale possible. La classe Audio fournit également des méthodes comme reload pour recharger les données audio à partir d'un fichier ou d'une URL, et des attributs comme src_attr, autoplay_attr, et element_id_attr pour récupérer les attributs correspondants pour l'élément audio en HTML.\n",
    "\n",
    "En résumé, ces fonctions et classes sont utilisées pour transcrire l'audio en texte, générer des réponses audio à partir d'un chatbot, et afficher et lire des fichiers audio dans l'environnement Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Avertissement** :  \nCe document a été traduit à l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, il est recommandé de recourir à une traduction professionnelle réalisée par un humain. Nous déclinons toute responsabilité en cas de malentendus ou d'interprétations erronées résultant de l'utilisation de cette traduction.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T16:34:58+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "fr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}