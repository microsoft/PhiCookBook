<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4164123a700fecd535d850f09506d72a",
  "translation_date": "2025-03-27T03:39:24+00:00",
  "source_file": "code\\03.Finetuning\\olive-ort-example\\README.md",
  "language_code": "fr"
}
-->
# Ajuster Phi3 avec Olive

Dans cet exemple, vous utiliserez Olive pour :

1. Ajuster un adaptateur LoRA afin de classifier des phrases en Tristesse, Joie, Peur, Surprise.
1. Fusionner les poids de l'adaptateur avec le mod√®le de base.
1. Optimiser et quantifier le mod√®le dans `int4`.

Nous vous montrerons √©galement comment effectuer une inf√©rence avec le mod√®le ajust√© en utilisant l'API Generate d'ONNX Runtime (ORT).

> **‚ö†Ô∏è Pour l'ajustement, vous devrez disposer d'un GPU adapt√© - par exemple, un A10, V100, A100.**

## üíæ Installation

Cr√©ez un nouvel environnement virtuel Python (par exemple, en utilisant `conda`) :

```bash
conda create -n olive-ai python=3.11
conda activate olive-ai
```

Ensuite, installez Olive et les d√©pendances n√©cessaires pour un flux de travail d'ajustement :

```bash
cd Phi-3CookBook/code/04.Finetuning/olive-ort-example
pip install olive-ai[gpu]
pip install -r requirements.txt
```

## üß™ Ajuster Phi3 avec Olive

Le [fichier de configuration Olive](../../../../../code/03.Finetuning/olive-ort-example/phrase-classification.json) contient un *flux de travail* avec les *√©tapes* suivantes :

Phi3 -> LoRA -> MergeAdapterWeights -> ModelBuilder

√Ä un niveau √©lev√©, ce flux de travail effectuera les actions suivantes :

1. Ajuster Phi3 (pendant 150 √©tapes, que vous pouvez modifier) en utilisant les donn√©es du fichier [dataset/data-classification.json](../../../../../code/03.Finetuning/olive-ort-example/dataset/dataset-classification.json).
1. Fusionner les poids de l'adaptateur LoRA avec le mod√®le de base. Cela produira un artefact de mod√®le unique au format ONNX.
1. Model Builder optimisera le mod√®le pour l'ex√©cution avec ONNX runtime *et* le quantifiera dans `int4`.

Pour ex√©cuter le flux de travail, lancez :

```bash
olive run --config phrase-classification.json
```

Une fois Olive termin√©, votre mod√®le Phi3 ajust√© et optimis√© `int4` sera disponible dans : `code/04.Finetuning/olive-ort-example/models/lora-merge-mb/gpu-cuda_model`.

## üßë‚Äçüíª Int√©grer Phi3 ajust√© dans votre application 

Pour ex√©cuter l'application :

```bash
python app/app.py --phrase "cricket is a wonderful sport!" --model-path models/lora-merge-mb/gpu-cuda_model
```

La r√©ponse devrait √™tre une classification en un mot de la phrase (Tristesse/Joie/Peur/Surprise).

**Clause de non-responsabilit√©** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de faire appel √† une traduction humaine professionnelle. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.