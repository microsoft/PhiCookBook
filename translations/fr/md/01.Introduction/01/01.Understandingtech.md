<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:39:53+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "fr"
}
-->
# Technologies clés mentionnées

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - une API bas niveau pour l’apprentissage automatique accéléré par matériel, construite sur DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - une plateforme de calcul parallèle et un modèle d’interface de programmation d’applications (API) développé par Nvidia, permettant le traitement généraliste sur les unités de traitement graphique (GPU).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - un format ouvert conçu pour représenter les modèles d’apprentissage automatique, offrant une interopérabilité entre différents frameworks ML.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - un format utilisé pour représenter et mettre à jour les modèles d’apprentissage automatique, particulièrement utile pour les petits modèles de langage pouvant fonctionner efficacement sur CPU avec une quantification 4-8 bits.

## DirectML

DirectML est une API bas niveau qui permet l’accélération matérielle de l’apprentissage automatique. Elle est construite sur DirectX 12 pour exploiter l’accélération GPU et est indépendante du fournisseur, ce qui signifie qu’elle ne nécessite pas de modifications de code pour fonctionner avec différents fabricants de GPU. Elle est principalement utilisée pour l’entraînement et l’inférence de modèles sur GPU.

Concernant la compatibilité matérielle, DirectML est conçu pour fonctionner avec une large gamme de GPU, incluant les GPU intégrés et discrets AMD, les GPU intégrés Intel, ainsi que les GPU discrets NVIDIA. Il fait partie de la plateforme Windows AI et est supporté sur Windows 10 et 11, permettant l’entraînement et l’inférence de modèles sur n’importe quel appareil Windows.

Des mises à jour et opportunités liées à DirectML ont été apportées, comme le support de jusqu’à 150 opérateurs ONNX et son utilisation par le runtime ONNX ainsi que WinML. Il est soutenu par les principaux fournisseurs de matériel intégré (IHVs), chacun implémentant divers métacommandes.

## CUDA

CUDA, qui signifie Compute Unified Device Architecture, est une plateforme de calcul parallèle et un modèle d’API créé par Nvidia. Elle permet aux développeurs d’utiliser un GPU compatible CUDA pour des traitements à usage général – une approche appelée GPGPU (General-Purpose computing on Graphics Processing Units). CUDA est un élément clé de l’accélération GPU de Nvidia et est largement utilisée dans divers domaines, notamment l’apprentissage automatique, le calcul scientifique et le traitement vidéo.

Le support matériel de CUDA est spécifique aux GPU Nvidia, car il s’agit d’une technologie propriétaire développée par Nvidia. Chaque architecture supporte des versions spécifiques du toolkit CUDA, qui fournit les bibliothèques et outils nécessaires aux développeurs pour créer et exécuter des applications CUDA.

## ONNX

ONNX (Open Neural Network Exchange) est un format ouvert conçu pour représenter les modèles d’apprentissage automatique. Il fournit une définition d’un modèle de graphe de calcul extensible, ainsi que des définitions d’opérateurs intégrés et de types de données standards. ONNX permet aux développeurs de transférer des modèles entre différents frameworks ML, facilitant l’interopérabilité et simplifiant la création et le déploiement d’applications IA.

Phi3 mini peut fonctionner avec ONNX Runtime sur CPU et GPU sur différents appareils, y compris les plateformes serveurs, Windows, Linux, Mac desktop et CPU mobiles.  
Les configurations optimisées que nous avons ajoutées sont :

- Modèles ONNX pour int4 DML : quantifiés en int4 via AWQ  
- Modèle ONNX pour fp16 CUDA  
- Modèle ONNX pour int4 CUDA : quantifié en int4 via RTN  
- Modèle ONNX pour int4 CPU et Mobile : quantifié en int4 via RTN  

## Llama.cpp

Llama.cpp est une bibliothèque logicielle open source écrite en C++. Elle réalise l’inférence sur divers grands modèles de langage (LLM), y compris Llama. Développée parallèlement à la bibliothèque ggml (une bibliothèque tensorielle polyvalente), llama.cpp vise à offrir une inférence plus rapide et une consommation mémoire réduite par rapport à l’implémentation Python originale. Elle prend en charge l’optimisation matérielle, la quantification, et propose une API simple ainsi que des exemples. Si vous vous intéressez à l’inférence efficace de LLM, llama.cpp mérite d’être exploré, d’autant plus que Phi3 peut exécuter Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) est un format utilisé pour représenter et mettre à jour les modèles d’apprentissage automatique. Il est particulièrement adapté aux petits modèles de langage (SLM) qui peuvent fonctionner efficacement sur CPU avec une quantification 4-8 bits. GGUF est utile pour le prototypage rapide et l’exécution de modèles sur des appareils en périphérie ou dans des tâches par lots comme les pipelines CI/CD.

**Avertissement** :  
Ce document a été traduit à l’aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d’assurer l’exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d’origine doit être considéré comme la source faisant foi. Pour les informations critiques, une traduction professionnelle réalisée par un humain est recommandée. Nous déclinons toute responsabilité en cas de malentendus ou de mauvaises interprétations résultant de l’utilisation de cette traduction.