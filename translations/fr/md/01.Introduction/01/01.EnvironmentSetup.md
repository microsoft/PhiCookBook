<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-07-16T18:06:15+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "fr"
}
-->
# Commencer avec Phi-3 en local

Ce guide vous aidera à configurer votre environnement local pour exécuter le modèle Phi-3 avec Ollama. Vous pouvez lancer le modèle de plusieurs façons, notamment en utilisant GitHub Codespaces, les conteneurs de développement VS Code, ou votre environnement local.

## Configuration de l’environnement

### GitHub Codespaces

Vous pouvez exécuter ce modèle virtuellement en utilisant GitHub Codespaces. Le bouton ouvrira une instance VS Code basée sur le web dans votre navigateur :

1. Ouvrez le modèle (cela peut prendre plusieurs minutes) :

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Ouvrez une fenêtre de terminal

### Conteneurs de développement VS Code

⚠️ Cette option ne fonctionnera que si Docker Desktop dispose d’au moins 16 Go de RAM. Si vous avez moins de 16 Go de RAM, vous pouvez essayer l’[option GitHub Codespaces](../../../../../md/01.Introduction/01) ou [configurer localement](../../../../../md/01.Introduction/01).

Une option liée est les conteneurs de développement VS Code, qui ouvriront le projet dans votre VS Code local en utilisant l’[extension Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) :

1. Lancez Docker Desktop (installez-le si ce n’est pas déjà fait)
2. Ouvrez le projet :

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Dans la fenêtre VS Code qui s’ouvre, une fois que les fichiers du projet apparaissent (cela peut prendre plusieurs minutes), ouvrez une fenêtre de terminal.
4. Continuez avec les [étapes de déploiement](../../../../../md/01.Introduction/01)

### Environnement local

1. Assurez-vous que les outils suivants sont installés :

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Tester le modèle

1. Demandez à Ollama de télécharger et d’exécuter le modèle phi3:mini :

    ```shell
    ollama run phi3:mini
    ```

    Le téléchargement du modèle prendra quelques minutes.

2. Une fois que vous voyez "success" dans la sortie, vous pouvez envoyer un message à ce modèle depuis l’invite.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Après quelques secondes, vous devriez voir un flux de réponse provenant du modèle.

4. Pour découvrir différentes techniques utilisées avec les modèles de langage, ouvrez le notebook Python [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) et exécutez chaque cellule. Si vous avez utilisé un modèle autre que 'phi3:mini', modifiez la variable `MODEL_NAME` dans la première cellule.

5. Pour converser avec le modèle phi3:mini depuis Python, ouvrez le fichier Python [chat.py](../../../../../code/01.Introduce/chat.py) et exécutez-le. Vous pouvez modifier la variable `MODEL_NAME` en haut du fichier selon vos besoins, et vous pouvez aussi ajuster le message système ou ajouter des exemples few-shot si vous le souhaitez.

**Avertissement** :  
Ce document a été traduit à l’aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d’assurer l’exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d’origine doit être considéré comme la source faisant foi. Pour les informations critiques, une traduction professionnelle réalisée par un humain est recommandée. Nous déclinons toute responsabilité en cas de malentendus ou de mauvaises interprétations résultant de l’utilisation de cette traduction.