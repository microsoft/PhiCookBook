<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "839ccc4b3886ef10cfd4e64977f5792d",
  "translation_date": "2026-01-04T06:26:45+00:00",
  "source_file": "md/01.Introduction/01/01.AISafety.md",
  "language_code": "fr"
}
-->
# Sécurité de l'IA pour les modèles Phi
La famille de modèles Phi a été développée conformément à la [Norme Responsible AI de Microsoft](https://www.microsoft.com/ai/principles-and-approach#responsible-ai-standard), qui est un ensemble d'exigences à l'échelle de l'entreprise basé sur les six principes suivants : responsabilité, transparence, équité, fiabilité et sécurité, confidentialité et sécurité, et inclusion qui forment les [principes Responsible AI de Microsoft](https://www.microsoft.com/ai/responsible-ai). 

Comme pour les modèles Phi précédents, une approche multi-facettes d'évaluation de la sécurité et d'ajustement post-entraînement a été adoptée, avec des mesures supplémentaires prises pour tenir compte des capacités multilingues de cette version. Notre approche en matière de formation à la sécurité et d'évaluations, y compris les tests à travers plusieurs langues et catégories de risques, est décrite dans le [Phi Safety Post-Training Paper](https://arxiv.org/abs/2407.13833). Bien que les modèles Phi bénéficient de cette approche, les développeurs doivent appliquer les meilleures pratiques en matière d'IA responsable, notamment cartographier, mesurer et atténuer les risques associés à leur cas d'utilisation spécifique et à leur contexte culturel et linguistique.

## Bonnes pratiques

Comme d'autres modèles, la famille Phi peut potentiellement se comporter de manière injuste, peu fiable ou offensante.

Certains des comportements limitants de SLM et LLM auxquels il faut prêter attention incluent :

- **Qualité de service :** Les modèles Phi sont principalement entraînés sur des textes en anglais. Les langues autres que l'anglais auront des performances moindres. Les variétés de la langue anglaise moins représentées dans les données d'entraînement pourraient présenter de moins bonnes performances que l'anglais américain standard.
- **Représentation des préjudices et perpétuation des stéréotypes :** Ces modèles peuvent sur- ou sous-représenter des groupes de personnes, effacer la représentation de certains groupes, ou renforcer des stéréotypes dégradants ou négatifs. Malgré l'ajustement post-entraînement de sécurité, ces limites peuvent encore être présentes en raison de niveaux de représentation différents des différents groupes ou de la prévalence d'exemples de stéréotypes négatifs dans les données d'entraînement qui reflètent des modèles réels et des biais sociétaux.
- **Contenu inapproprié ou offensant :** Ces modèles peuvent produire d'autres types de contenus inappropriés ou offensants, ce qui peut rendre leur déploiement inapproprié dans des contextes sensibles sans atténuations supplémentaires spécifiques au cas d'utilisation.
Fiabilité de l'information : Les modèles de langage peuvent générer du contenu non-sensique ou fabriquer des informations qui peuvent paraître plausibles mais qui sont inexactes ou obsolètes.
- **Portée limitée pour le code :** La majorité des données d'entraînement de Phi-3 sont basées sur Python et utilisent des packages courants tels que "typing, math, random, collections, datetime, itertools". Si le modèle génère des scripts Python qui utilisent d'autres packages ou des scripts dans d'autres langages, nous recommandons fortement aux utilisateurs de vérifier manuellement toutes les utilisations d'API.

Les développeurs doivent appliquer les meilleures pratiques en matière d'IA responsable et sont responsables de s'assurer qu'un cas d'utilisation spécifique est conforme aux lois et réglementations pertinentes (par ex. confidentialité, commerce, etc.). 

## Considérations sur l'IA responsable

Comme d'autres modèles de langage, la série Phi peut potentiellement se comporter de manière injuste, peu fiable ou offensante. Parmi les comportements limitants à connaître, on trouve :

**Qualité de service :** Les modèles Phi sont principalement entraînés sur des textes en anglais. Les langues autres que l'anglais auront des performances moindres. Les variétés de la langue anglaise moins représentées dans les données d'entraînement pourraient présenter de moins bonnes performances que l'anglais américain standard.

**Représentation des préjudices et perpétuation des stéréotypes :** Ces modèles peuvent sur- ou sous-représenter des groupes de personnes, effacer la représentation de certains groupes, ou renforcer des stéréotypes dégradants ou négatifs. Malgré l'ajustement post-entraînement de sécurité, ces limites peuvent encore être présentes en raison de niveaux de représentation différents des différents groupes ou de la prévalence d'exemples de stéréotypes négatifs dans les données d'entraînement qui reflètent des modèles réels et des biais sociétaux.

**Contenu inapproprié ou offensant :** Ces modèles peuvent produire d'autres types de contenus inappropriés ou offensants, ce qui peut rendre leur déploiement inapproprié dans des contextes sensibles sans atténuations supplémentaires spécifiques au cas d'utilisation.
Fiabilité de l'information : Les modèles de langage peuvent générer du contenu non-sensique ou fabriquer des informations qui peuvent paraître plausibles mais qui sont inexactes ou obsolètes.

**Portée limitée pour le code :** La majorité des données d'entraînement de Phi-3 sont basées sur Python et utilisent des packages courants tels que "typing, math, random, collections, datetime, itertools". Si le modèle génère des scripts Python qui utilisent d'autres packages ou des scripts dans d'autres langages, nous recommandons fortement aux utilisateurs de vérifier manuellement toutes les utilisations d'API.

Les développeurs doivent appliquer les meilleures pratiques en matière d'IA responsable et sont responsables de s'assurer qu'un cas d'utilisation spécifique est conforme aux lois et réglementations pertinentes (par ex. confidentialité, commerce, etc.). Les domaines importants à considérer incluent :

**Allocation :** Les modèles peuvent ne pas convenir à des scénarios susceptibles d'avoir un impact conséquent sur le statut juridique ou l'allocation de ressources ou d'opportunités de vie (ex : logement, emploi, crédit, etc.) sans évaluations supplémentaires et techniques de dé-biaisage additionnelles.

**Scénarios à haut risque :** Les développeurs doivent évaluer la pertinence d'utiliser des modèles dans des scénarios à haut risque où des sorties injustes, peu fiables ou offensantes pourraient être extrêmement coûteuses ou entraîner des préjudices. Cela inclut la fourniture de conseils dans des domaines sensibles ou d'expertise où la précision et la fiabilité sont critiques (ex : conseils juridiques ou médicaux). Des garanties supplémentaires doivent être mises en œuvre au niveau de l'application en fonction du contexte de déploiement.

**Désinformation :** Les modèles peuvent produire des informations inexactes. Les développeurs doivent suivre les meilleures pratiques de transparence et informer les utilisateurs finaux qu'ils interagissent avec un système d'IA. Au niveau de l'application, les développeurs peuvent mettre en place des mécanismes de retour et des pipelines pour ancrer les réponses dans des informations contextuelles spécifiques au cas d'utilisation, une technique connue sous le nom de Retrieval Augmented Generation (RAG).

**Génération de contenu nuisible :** Les développeurs doivent évaluer les sorties en fonction de leur contexte et utiliser des classificateurs de sécurité disponibles ou des solutions personnalisées appropriées pour leur cas d'utilisation.

**Mésusage :** D'autres formes d'utilisation abusive telles que la fraude, le spam ou la production de logiciels malveillants peuvent être possibles, et les développeurs doivent s'assurer que leurs applications ne violent pas les lois et réglementations applicables.

### Ajustement fin et sécurité du contenu IA

Après l'ajustement fin d'un modèle, nous recommandons fortement d'exploiter les mesures [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) pour surveiller le contenu généré par les modèles, identifier et bloquer les risques, menaces et problèmes de qualité potentiels.

![Sécurité IA Phi3](../../../../../translated_images/01.phi3aisafety.c0d7fc42f5a5c405.fr.png)

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) prend en charge à la fois le contenu textuel et visuel. Il peut être déployé dans le cloud, dans des conteneurs déconnectés et sur des appareils embarqués/edge.

![Sécurité du contenu IA](../../../../../translated_images/01.AIcontentsafety.a288819b8ce8da1a.fr.png)

- **Azure AI Content Safety**
- **Microsoft Developer**
- **5 vidéos**

Le service Azure AI Content Safety détecte le contenu nuisible généré par les utilisateurs et par l'IA dans les applications et services. Il inclut des API texte et image qui vous permettent de détecter du matériel nuisible ou inapproprié.

[Playlist Sécurité du contenu IA](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
Clause de non-responsabilité :
Ce document a été traduit à l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour les informations critiques, il est recommandé de recourir à une traduction professionnelle réalisée par un traducteur humain. Nous déclinons toute responsabilité en cas de malentendus ou de mauvaises interprétations résultant de l'utilisation de cette traduction.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->