<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3139a6a82f357a9f90f1fe51c4caf65a",
  "translation_date": "2025-03-27T08:29:01+00:00",
  "source_file": "md\\01.Introduction\\04\\UsingIntelOpenVINOQuantifyingPhi.md",
  "language_code": "fr"
}
-->
# **Quantification de Phi-3.5 avec Intel OpenVINO**

Intel est le fabricant de CPU le plus traditionnel avec de nombreux utilisateurs. Avec l'essor de l'apprentissage automatique et du deep learning, Intel s'est √©galement lanc√© dans la comp√©tition pour l'acc√©l√©ration de l'IA. Pour l'inf√©rence des mod√®les, Intel utilise non seulement des GPU et des CPU, mais √©galement des NPU.

Nous esp√©rons d√©ployer la famille Phi-3.x c√¥t√© terminal, en aspirant √† devenir la partie la plus importante des PC AI et des PC Copilot. Le chargement du mod√®le c√¥t√© terminal d√©pend de la coop√©ration entre diff√©rents fabricants de mat√©riel. Ce chapitre se concentre principalement sur le sc√©nario d'application d'Intel OpenVINO en tant que mod√®le quantifi√©.

## **Qu'est-ce qu'OpenVINO**

OpenVINO est une bo√Æte √† outils open-source pour optimiser et d√©ployer des mod√®les de deep learning du cloud √† la p√©riph√©rie. Elle acc√©l√®re l'inf√©rence de deep learning dans divers cas d'utilisation, comme l'IA g√©n√©rative, la vid√©o, l'audio et le langage, avec des mod√®les issus de frameworks populaires comme PyTorch, TensorFlow, ONNX, et bien d'autres. Convertissez et optimisez des mod√®les, puis d√©ployez-les sur un m√©lange de mat√©riels Intel¬Æ et d'environnements, sur site ou sur appareil, dans le navigateur ou dans le cloud.

D√©sormais, avec OpenVINO, vous pouvez rapidement quantifier le mod√®le GenAI sur du mat√©riel Intel et acc√©l√©rer l'inf√©rence du mod√®le.

OpenVINO prend d√©sormais en charge la conversion quantifi√©e de Phi-3.5-Vision et Phi-3.5-Instruct.

### **Configuration de l'environnement**

Veuillez vous assurer que les d√©pendances d'environnement suivantes sont install√©es, voici le fichier requirement.txt :

```txt

--extra-index-url https://download.pytorch.org/whl/cpu
optimum-intel>=1.18.2
nncf>=2.11.0
openvino>=2024.3.0
transformers>=4.40
openvino-genai>=2024.3.0.0

```

### **Quantification de Phi-3.5-Instruct avec OpenVINO**

Dans le terminal, ex√©cutez ce script :

```bash


export llm_model_id = "microsoft/Phi-3.5-mini-instruct"

export llm_model_path = "your save quantizing Phi-3.5-instruct location"

optimum-cli export openvino --model {llm_model_id} --task text-generation-with-past --weight-format int4 --group-size 128 --ratio 0.6  --sym  --trust-remote-code {llm_model_path}


```

### **Quantification de Phi-3.5-Vision avec OpenVINO**

Ex√©cutez ce script dans Python ou Jupyter Lab :

```python

import requests
from pathlib import Path
from ov_phi3_vision import convert_phi3_model
import nncf

if not Path("ov_phi3_vision.py").exists():
    r = requests.get(url="https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/phi-3-vision/ov_phi3_vision.py")
    open("ov_phi3_vision.py", "w").write(r.text)


if not Path("gradio_helper.py").exists():
    r = requests.get(url="https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/phi-3-vision/gradio_helper.py")
    open("gradio_helper.py", "w").write(r.text)

if not Path("notebook_utils.py").exists():
    r = requests.get(url="https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py")
    open("notebook_utils.py", "w").write(r.text)



model_id = "microsoft/Phi-3.5-vision-instruct"
out_dir = Path("../model/phi-3.5-vision-128k-instruct-ov")
compression_configuration = {
    "mode": nncf.CompressWeightsMode.INT4_SYM,
    "group_size": 64,
    "ratio": 0.6,
}
if not out_dir.exists():
    convert_phi3_model(model_id, out_dir, compression_configuration)

```

### **ü§ñ Exemples pour Phi-3.5 avec Intel OpenVINO**

| Laboratoires | Pr√©sentation | Acc√©der |
| -------- | ------- |  ------- |
| üöÄ Lab-Pr√©sentation de Phi-3.5 Instruct  | Apprenez √† utiliser Phi-3.5 Instruct sur votre PC AI    |  [Acc√©der](../../../../../code/09.UpdateSamples/Aug/intel-phi35-instruct-zh.ipynb)    |
| üöÄ Lab-Pr√©sentation de Phi-3.5 Vision (image) | Apprenez √† utiliser Phi-3.5 Vision pour analyser des images sur votre PC AI      |  [Acc√©der](../../../../../code/09.UpdateSamples/Aug/intel-phi35-vision-img.ipynb)    |
| üöÄ Lab-Pr√©sentation de Phi-3.5 Vision (vid√©o)   | Apprenez √† utiliser Phi-3.5 Vision pour analyser des vid√©os sur votre PC AI    |  [Acc√©der](../../../../../code/09.UpdateSamples/Aug/intel-phi35-vision-video.ipynb)    |

## **Ressources**

1. En savoir plus sur Intel OpenVINO [https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html)

2. D√©p√¥t GitHub d'Intel OpenVINO [https://github.com/openvinotoolkit/openvino.genai](https://github.com/openvinotoolkit/openvino.genai)

**Avertissement** :  
Ce document a √©t√© traduit en utilisant le service de traduction automatis√©e [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction professionnelle r√©alis√©e par un humain. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.