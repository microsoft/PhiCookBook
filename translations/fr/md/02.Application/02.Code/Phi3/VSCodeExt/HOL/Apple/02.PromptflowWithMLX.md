<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3dbbf568625b1ee04b354c2dc81d3248",
  "translation_date": "2025-03-27T12:16:50+00:00",
  "source_file": "md\\02.Application\\02.Code\\Phi3\\VSCodeExt\\HOL\\Apple\\02.PromptflowWithMLX.md",
  "language_code": "fr"
}
-->
# **Lab 2 - Exécuter Prompt flow avec Phi-3-mini dans AIPC**

## **Qu'est-ce que Prompt flow**

Prompt flow est un ensemble d'outils de développement conçu pour simplifier le cycle complet de développement des applications d'IA basées sur des LLM, allant de l'idéation, au prototypage, en passant par les tests, l'évaluation, jusqu'au déploiement en production et la surveillance. Il facilite grandement l'ingénierie des prompts et permet de construire des applications LLM de qualité production.

Avec Prompt flow, vous pourrez :

- Créer des flux qui relient des LLM, des prompts, du code Python et d'autres outils dans un workflow exécutable.

- Déboguer et itérer vos flux, notamment les interactions avec les LLM, facilement.

- Évaluer vos flux, calculer des métriques de qualité et de performance avec des ensembles de données plus larges.

- Intégrer les tests et l'évaluation dans votre système CI/CD pour garantir la qualité de vos flux.

- Déployer vos flux sur la plateforme de service de votre choix ou les intégrer facilement dans le code de votre application.

- (Optionnel mais fortement recommandé) Collaborer avec votre équipe en utilisant la version cloud de Prompt flow sur Azure AI.



## **Construire des flux de génération de code sur Apple Silicon**

***Note*** : Si vous n’avez pas encore terminé l’installation de l’environnement, veuillez visiter [Lab 0 - Installations](./01.Installations.md)

1. Ouvrez l'extension Prompt flow dans Visual Studio Code et créez un projet de flux vide.

![create](../../../../../../../../../translated_images/pf_create.d6172d8277a78a7fa82cd6ff727ed44e037fa78b662f1f62d5963f36d712d229.fr.png)

2. Ajoutez des paramètres d'Entrées et de Sorties et ajoutez du code Python comme nouveau flux.

![flow](../../../../../../../../../translated_images/pf_flow.d5646a323fb7f444c0b98b4521057a592325c583e7ba18bc31500bc0415e9ef3.fr.png)


Vous pouvez vous référer à cette structure (flow.dag.yaml) pour construire votre flux.

```yaml

inputs:
  prompt:
    type: string
    default: Write python code for Fibonacci serie. Please use markdown as output
outputs:
  result:
    type: string
    reference: ${gen_code_by_phi3.output}
nodes:
- name: gen_code_by_phi3
  type: python
  source:
    type: code
    path: gen_code_by_phi3.py
  inputs:
    prompt: ${inputs.prompt}


```

3. Quantifier phi-3-mini

Nous souhaitons mieux exécuter le SLM sur des appareils locaux. En général, nous quantifions le modèle (INT4, FP16, FP32).

```bash

python -m mlx_lm.convert --hf-path microsoft/Phi-3-mini-4k-instruct

```

**Remarque :** le dossier par défaut est mlx_model.

4. Ajoutez du code dans ***Chat_With_Phi3.py***

```python


from promptflow import tool

from mlx_lm import load, generate


# The inputs section will change based on the arguments of the tool function, after you save the code
# Adding type to arguments and return value will help the system show the types properly
# Please update the function name/signature per need
@tool
def my_python_tool(prompt: str) -> str:

    model_id = './mlx_model_phi3_mini'

    model, tokenizer = load(model_id)

    # <|user|>\nWrite python code for Fibonacci serie. Please use markdown as output<|end|>\n<|assistant|>

    response = generate(model, tokenizer, prompt="<|user|>\n" + prompt  + "<|end|>\n<|assistant|>", max_tokens=2048, verbose=True)

    return response


```

4. Vous pouvez tester le flux à partir de Debug ou Run pour vérifier si le code généré est correct ou non.

![RUN](../../../../../../../../../translated_images/pf_run.d918637dc00f61e9bdeec37d4cc9646f77d270ac9203bcce13569f3157202b6e.fr.png)

5. Exécutez le flux comme API de développement dans le terminal.

```

pf flow serve --source ./ --port 8080 --host localhost   

```

Vous pouvez le tester dans Postman / Thunder Client.


### **Remarque**

1. La première exécution prend beaucoup de temps. Il est recommandé de télécharger le modèle phi-3 à partir de Hugging Face CLI.

2. Compte tenu de la puissance de calcul limitée du NPU Intel, il est recommandé d'utiliser Phi-3-mini-4k-instruct.

3. Nous utilisons l'accélération NPU Intel pour quantifier la conversion INT4, mais si vous relancez le service, vous devez supprimer les dossiers cache et nc_workshop.



## **Ressources**

1. Apprenez Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Apprenez l'accélération NPU Intel [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Code d'exemple, téléchargez [Local NPU Agent Sample Code](../../../../../../../../../code/07.Lab/01/AIPC/local-npu-agent)

**Avertissement** :  
Ce document a été traduit à l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous fassions de notre mieux pour garantir l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, il est recommandé de recourir à une traduction humaine professionnelle. Nous déclinons toute responsabilité en cas de malentendus ou d'interprétations erronées découlant de l'utilisation de cette traduction.