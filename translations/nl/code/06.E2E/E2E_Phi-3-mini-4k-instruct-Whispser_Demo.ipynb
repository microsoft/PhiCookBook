{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactieve Phi 3 Mini 4K Instruct Chatbot met Whisper\n",
    "\n",
    "### Introductie:\n",
    "De Interactieve Phi 3 Mini 4K Instruct Chatbot is een hulpmiddel waarmee gebruikers kunnen communiceren met de Microsoft Phi 3 Mini 4K instruct-demo via tekst- of audio-invoer. De chatbot kan worden gebruikt voor verschillende taken, zoals vertalingen, weerupdates en het verzamelen van algemene informatie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "Maak je Huggingface-toegangstoken aan\n",
    "\n",
    "Maak een nieuw token aan  \n",
    "Geef een nieuwe naam op  \n",
    "Selecteer schrijfrechten  \n",
    "Kopieer het token en bewaar het op een veilige plek\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De volgende Python-code voert twee hoofdtaken uit: het importeren van de `os`-module en het instellen van een omgevingsvariabele.\n",
    "\n",
    "1. Importeren van de `os`-module:\n",
    "   - De `os`-module in Python biedt een manier om te communiceren met het besturingssysteem. Hiermee kun je verschillende taken uitvoeren die gerelateerd zijn aan het besturingssysteem, zoals toegang krijgen tot omgevingsvariabelen, werken met bestanden en mappen, enzovoort.\n",
    "   - In deze code wordt de `os`-module geïmporteerd met behulp van de `import`-instructie. Deze instructie maakt de functionaliteit van de `os`-module beschikbaar voor gebruik in het huidige Python-script.\n",
    "\n",
    "2. Instellen van een omgevingsvariabele:\n",
    "   - Een omgevingsvariabele is een waarde die toegankelijk is voor programma's die op het besturingssysteem draaien. Het is een manier om configuratie-instellingen of andere informatie op te slaan die door meerdere programma's kan worden gebruikt.\n",
    "   - In deze code wordt een nieuwe omgevingsvariabele ingesteld met behulp van de `os.environ`-dictionary. De sleutel van de dictionary is `'HF_TOKEN'`, en de waarde wordt toegewezen vanuit de variabele `HUGGINGFACE_TOKEN`.\n",
    "   - De variabele `HUGGINGFACE_TOKEN` wordt net boven dit codefragment gedefinieerd en krijgt een stringwaarde `\"hf_**************\"` toegewezen met behulp van de `#@param`-syntax. Deze syntax wordt vaak gebruikt in Jupyter-notebooks om gebruikersinvoer en parameterconfiguratie rechtstreeks in de notebookinterface mogelijk te maken.\n",
    "   - Door de omgevingsvariabele `'HF_TOKEN'` in te stellen, kan deze worden benaderd door andere delen van het programma of andere programma's die op hetzelfde besturingssysteem draaien.\n",
    "\n",
    "Samenvattend importeert deze code de `os`-module en stelt een omgevingsvariabele genaamd `'HF_TOKEN'` in met de waarde die is opgegeven in de variabele `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze codefragment definieert een functie genaamd clear_output, die wordt gebruikt om de uitvoer van de huidige cel in Jupyter Notebook of IPython te wissen. Laten we de code opsplitsen en de functionaliteit ervan begrijpen:\n",
    "\n",
    "De functie clear_output neemt één parameter genaamd wait, die een booleaanse waarde is. Standaard is wait ingesteld op False. Deze parameter bepaalt of de functie moet wachten totdat nieuwe uitvoer beschikbaar is om de bestaande uitvoer te vervangen voordat deze wordt gewist.\n",
    "\n",
    "De functie zelf wordt gebruikt om de uitvoer van de huidige cel te wissen. In Jupyter Notebook of IPython, wanneer een cel uitvoer genereert, zoals afgedrukte tekst of grafische plots, wordt die uitvoer onder de cel weergegeven. De clear_output-functie stelt je in staat om die uitvoer te wissen.\n",
    "\n",
    "De implementatie van de functie wordt niet verstrekt in het codefragment, zoals aangegeven door de ellipsis (...). De ellipsis vertegenwoordigt een placeholder voor de daadwerkelijke code die het wissen van de uitvoer uitvoert. De implementatie van de functie kan het gebruik van de Jupyter Notebook- of IPython-API omvatten om de bestaande uitvoer uit de cel te verwijderen.\n",
    "\n",
    "Over het algemeen biedt deze functie een handige manier om de uitvoer van de huidige cel in Jupyter Notebook of IPython te wissen, waardoor het eenvoudiger wordt om de weergegeven uitvoer te beheren en bij te werken tijdens interactieve coderingssessies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voer tekst-naar-spraak (TTS) uit met behulp van de Edge TTS-service. Laten we de relevante functie-implementaties één voor één doornemen:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Deze functie neemt een invoerwaarde en berekent de snelheidsstring voor de TTS-stem. De invoerwaarde vertegenwoordigt de gewenste snelheid van de spraak, waarbij een waarde van 1 de normale snelheid aangeeft. De functie berekent de snelheidsstring door 1 af te trekken van de invoerwaarde, deze te vermenigvuldigen met 100, en vervolgens het teken te bepalen op basis van of de invoerwaarde groter dan of gelijk aan 1 is. De functie retourneert de snelheidsstring in het formaat \"{sign}{rate}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: Deze functie neemt een invoertekst en een taal als parameters. Het splitst de invoertekst in stukken op basis van taal-specifieke regels. In deze implementatie, als de taal \"Engels\" is, splitst de functie de tekst bij elke punt (\".\") en verwijdert eventuele leidende of afsluitende spaties. Vervolgens voegt het een punt toe aan elk stuk en retourneert de gefilterde lijst van stukken.\n",
    "\n",
    "3. `tts_file_name(text)`: Deze functie genereert een bestandsnaam voor het TTS-audiobestand op basis van de invoertekst. Het voert verschillende transformaties uit op de tekst: het verwijderen van een afsluitende punt (indien aanwezig), het omzetten van de tekst naar kleine letters, het verwijderen van leidende en afsluitende spaties, en het vervangen van spaties door onderstrepingstekens. Vervolgens wordt de tekst ingekort tot maximaal 25 tekens (indien langer) of gebruikt de volledige tekst als deze leeg is. Ten slotte genereert het een willekeurige string met behulp van de [`uuid`] module en combineert deze met de ingekorte tekst om de bestandsnaam te maken in het formaat \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Deze functie voegt meerdere audiobestanden samen tot één audiobestand. Het neemt een lijst van audiobestandslocaties en een uitvoerlocatie als parameters. De functie initialiseert een lege `AudioSegment`-object genaamd [`merged_audio`]. Vervolgens doorloopt het elke audiobestandslocatie, laadt het audiobestand met behulp van de methode `AudioSegment.from_file()` van de `pydub`-bibliotheek, en voegt het huidige audiobestand toe aan het [`merged_audio`] object. Ten slotte exporteert het de samengevoegde audio naar de opgegeven uitvoerlocatie in MP3-formaat.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: Deze functie voert de TTS-operatie uit met behulp van de Edge TTS-service. Het neemt een lijst van tekststukken, de snelheid van de spraak, de stemnaam en de opslaglocatie als parameters. Als het aantal stukken groter is dan 1, maakt de functie een map aan voor het opslaan van de individuele audiobestanden van de stukken. Vervolgens doorloopt het elk stuk, construeert een Edge TTS-commando met behulp van de functie `calculate_rate_string()`, de stemnaam en de tekst van het stuk, en voert het commando uit met behulp van de functie `os.system()`. Als de uitvoering van het commando succesvol is, voegt het het pad van het gegenereerde audiobestand toe aan een lijst. Na het verwerken van alle stukken, voegt het de individuele audiobestanden samen met behulp van de functie `merge_audio_files()` en slaat de samengevoegde audio op in de opgegeven opslaglocatie. Als er slechts één stuk is, genereert het direct het Edge TTS-commando en slaat de audio op in de opslaglocatie. Ten slotte retourneert het de opslaglocatie van het gegenereerde audiobestand.\n",
    "\n",
    "6. `random_audio_name_generate()`: Deze functie genereert een willekeurige audiobestandsnaam met behulp van de [`uuid`] module. Het genereert een willekeurige UUID, zet deze om naar een string, neemt de eerste 8 tekens, voegt de \".mp3\"-extensie toe, en retourneert de willekeurige audiobestandsnaam.\n",
    "\n",
    "7. `talk(input_text)`: Deze functie is het belangrijkste toegangspunt voor het uitvoeren van de TTS-operatie. Het neemt een invoertekst als parameter. Het controleert eerst de lengte van de invoertekst om te bepalen of het een lange zin is (groter dan of gelijk aan 600 tekens). Op basis van de lengte en de waarde van de variabele `translate_text_flag` bepaalt het de taal en genereert het de lijst van tekststukken met behulp van de functie `make_chunks()`. Vervolgens genereert het een opslaglocatie voor het audiobestand met behulp van de functie `random_audio_name_generate()`. Ten slotte roept het de functie `edge_free_tts()` aan om de TTS-operatie uit te voeren en retourneert het de opslaglocatie van het gegenereerde audiobestand.\n",
    "\n",
    "Samenvattend werken deze functies samen om de invoertekst op te splitsen in stukken, een bestandsnaam te genereren voor het audiobestand, de TTS-operatie uit te voeren met behulp van de Edge TTS-service, en de individuele audiobestanden samen te voegen tot één audiobestand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De implementatie van twee functies: convert_to_text en run_text_prompt, evenals de declaratie van twee klassen: str en Audio.\n",
    "\n",
    "De functie convert_to_text neemt een audio_path als invoer en transcribeert de audio naar tekst met behulp van een model genaamd whisper_model. De functie controleert eerst of de gpu-vlag is ingesteld op True. Als dat het geval is, wordt het whisper_model gebruikt met bepaalde parameters zoals word_timestamps=True, fp16=True, language='English', en task='translate'. Als de gpu-vlag False is, wordt het whisper_model gebruikt met fp16=False. De resulterende transcriptie wordt vervolgens opgeslagen in een bestand genaamd 'scan.txt' en geretourneerd als tekst.\n",
    "\n",
    "De functie run_text_prompt neemt een message en een chat_history als invoer. Het gebruikt de phi_demo functie om een reactie van een chatbot te genereren op basis van het ingevoerde bericht. De gegenereerde reactie wordt vervolgens doorgegeven aan de talk functie, die de reactie omzet in een audiobestand en het bestandspad retourneert. De Audio-klasse wordt gebruikt om het audiobestand weer te geven en af te spelen. De audio wordt weergegeven met behulp van de display functie uit de IPython.display module, en het Audio-object wordt gemaakt met de parameter autoplay=True, zodat de audio automatisch begint te spelen. De chat_history wordt bijgewerkt met het ingevoerde bericht en de gegenereerde reactie, en een lege string en de bijgewerkte chat_history worden geretourneerd.\n",
    "\n",
    "De str-klasse is een ingebouwde klasse in Python die een reeks tekens vertegenwoordigt. Het biedt verschillende methoden voor het manipuleren en werken met strings, zoals capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill, en meer. Deze methoden stellen je in staat om bewerkingen uit te voeren zoals zoeken, vervangen, formatteren en manipuleren van strings.\n",
    "\n",
    "De Audio-klasse is een aangepaste klasse die een audio-object vertegenwoordigt. Het wordt gebruikt om een audiospeler te maken in de Jupyter Notebook-omgeving. De klasse accepteert verschillende parameters zoals data, filename, url, embed, rate, autoplay, en normalize. De data-parameter kan een numpy-array zijn, een lijst van samples, een string die een bestandsnaam of URL vertegenwoordigt, of ruwe PCM-data. De filename-parameter wordt gebruikt om een lokaal bestand op te geven om de audiodata uit te laden, en de url-parameter wordt gebruikt om een URL op te geven om de audiodata te downloaden. De embed-parameter bepaalt of de audiodata moet worden ingesloten met behulp van een data-URI of moet worden verwezen vanuit de oorspronkelijke bron. De rate-parameter specificeert de bemonsteringssnelheid van de audiodata. De autoplay-parameter bepaalt of de audio automatisch moet beginnen met afspelen. De normalize-parameter specificeert of de audiodata moet worden genormaliseerd (herschaald) naar het maximale mogelijke bereik. De Audio-klasse biedt ook methoden zoals reload om de audiodata opnieuw te laden vanuit een bestand of URL, en attributen zoals src_attr, autoplay_attr, en element_id_attr om de bijbehorende attributen voor het audio-element in HTML op te halen.\n",
    "\n",
    "Over het algemeen worden deze functies en klassen gebruikt om audio naar tekst te transcriberen, audio-reacties van een chatbot te genereren, en audio weer te geven en af te spelen in de Jupyter Notebook-omgeving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:  \nDit document is vertaald met behulp van de AI-vertalingsservice [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we ons best doen voor nauwkeurigheid, dient u zich ervan bewust te zijn dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in zijn oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor cruciale informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T23:17:16+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "nl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}