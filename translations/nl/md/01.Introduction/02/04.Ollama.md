<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0b38834693bb497f96bf53f0d941f9a1",
  "translation_date": "2025-05-09T09:18:24+00:00",
  "source_file": "md/01.Introduction/02/04.Ollama.md",
  "language_code": "nl"
}
-->
## Phi Family in Ollama

[Ollama](https://ollama.com) maakt het voor meer mensen mogelijk om open source LLM of SLM rechtstreeks te implementeren via eenvoudige scripts, en kan ook API’s bouwen om lokale Copilot-toepassingen te ondersteunen.

## **1. Installatie**

Ollama ondersteunt draaien op Windows, macOS en Linux. Je kunt Ollama installeren via deze link ([https://ollama.com/download](https://ollama.com/download)). Na een succesvolle installatie kun je direct via een terminalvenster het Ollama-script gebruiken om Phi-3 aan te roepen. Je kunt alle [beschikbare bibliotheken in Ollama](https://ollama.com/library) bekijken. Als je deze repository opent in een Codespace, is Ollama al geïnstalleerd.

```bash

ollama run phi4

```

> [!NOTE]
> Het model wordt eerst gedownload wanneer je het voor het eerst draait. Natuurlijk kun je ook direct het gedownloade Phi-4 model specificeren. We gebruiken WSL als voorbeeld om het commando uit te voeren. Nadat het model succesvol is gedownload, kun je direct via de terminal interactie aangaan.

![run](../../../../../translated_images/ollama_run.b0be611de61f3bb3b42e22205cedf6714b0335ba9288e71d985bf9024f3c20f5.nl.png)

## **2. Roep de phi-4 API aan vanuit Ollama**

Als je de Phi-4 API wilt aanroepen die door Ollama wordt gegenereerd, kun je dit commando in de terminal gebruiken om de Ollama-server te starten.

```bash

ollama serve

```

> [!NOTE]
> Als je MacOS of Linux gebruikt, kan het zijn dat je de volgende foutmelding krijgt: **"Error: listen tcp 127.0.0.1:11434: bind: address already in use"**. Deze fout kan optreden bij het uitvoeren van het commando. Je kunt deze fout negeren, want meestal betekent het dat de server al draait, of je kunt Ollama stoppen en opnieuw starten:

**macOS**

```bash

brew services restart ollama

```

**Linux**

```bash

sudo systemctl stop ollama

```

Ollama ondersteunt twee API’s: generate en chat. Je kunt de model-API die Ollama biedt aanroepen afhankelijk van je behoeften, door verzoeken te sturen naar de lokale service die draait op poort 11434.

**Chat**

```bash

curl http://127.0.0.1:11434/api/chat -d '{
  "model": "phi3",
  "messages": [
    {
      "role": "system",
      "content": "Your are a python developer."
    },
    {
      "role": "user",
      "content": "Help me generate a bubble algorithm"
    }
  ],
  "stream": false
  
}'

This is the result in Postman

![Screenshot of JSON results for generate request](../../../../../translated_images/ollama_gen.bd58ab69d4004826e8cd31e17a3c59840df127b0a30ac9bb38325ac58c74caa5.nl.png)

## Additional Resources

Check the list of available models in Ollama in [their library](https://ollama.com/library).

Pull your model from the Ollama server using this command

```bash  
ollama pull phi4  
```

Run the model using this command

```bash  
ollama run phi4  
```

***Note:*** Visit this link [https://github.com/ollama/ollama/blob/main/docs/api.md](https://github.com/ollama/ollama/blob/main/docs/api.md) to learn more

## Calling Ollama from Python

You can use `requests` or `urllib3` to make requests to the local server endpoints used above. However, a popular way to use Ollama in Python is via the [openai](https://pypi.org/project/openai/) SDK, since Ollama provides OpenAI-compatible server endpoints as well.

Here is an example for phi3-mini:

```python  
import openai

client = openai.OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="nokeyneeded",
)

response = client.chat.completions.create(
    model="phi4",
    temperature=0.7,
    n=1,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a haiku about a hungry cat"},
    ],
)

print("Response:")
print(response.choices[0].message.content)  
```

## Calling Ollama from JavaScript 

```javascript  
// Voorbeeld van een bestand samenvatten met Phi-4  
script({  
    model: "ollama:phi4",  
    title: "Samenvatten met Phi-4",  
    system: ["system"],  
})  

// Voorbeeld van samenvatten  
const file = def("FILE", env.files)  
$`Summarize ${file} in a single paragraph.`  
```

## Calling Ollama from C#

Create a new C# Console application and add the following NuGet package:

```bash  
dotnet add package Microsoft.SemanticKernel --version 1.34.0  
```

Then replace this code in the `Program.cs` file

```csharp  
using Microsoft.SemanticKernel;  
using Microsoft.SemanticKernel.ChatCompletion;  

// voeg chat completion service toe met behulp van de lokale ollama server endpoint  
#pragma warning disable SKEXP0001, SKEXP0003, SKEXP0010, SKEXP0011, SKEXP0050, SKEXP0052  
builder.AddOpenAIChatCompletion(  
    modelId: "phi4",  
    endpoint: new Uri("http://localhost:11434/"),  
    apiKey: "non required");  

// roep een eenvoudige prompt aan bij de chat service  
string prompt = "Write a joke about kittens";  
var response = await kernel.InvokePromptAsync(prompt);  
Console.WriteLine(response.GetValue<string>());  
```

Run the app with the command:

```bash  
dotnet run  

**Disclaimer**:  
Dit document is vertaald met behulp van de AI-vertalingsdienst [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u er rekening mee te houden dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in de oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor cruciale informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.