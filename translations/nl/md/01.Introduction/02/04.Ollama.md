<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2aa35f3c8b437fd5dc9995d53909d495",
  "translation_date": "2025-12-21T12:26:19+00:00",
  "source_file": "md/01.Introduction/02/04.Ollama.md",
  "language_code": "nl"
}
-->
## Phi-familie in Ollama


[Ollama](https://ollama.com) stelt meer mensen in staat om open source LLM of SLM rechtstreeks te implementeren via eenvoudige scripts, en kan ook API's bouwen om lokale Copilot-toepassingsscenario's te ondersteunen.

## **1. Installatie**

Ollama ondersteunt uitvoering op Windows, macOS en Linux. Je kunt Ollama installeren via deze link ([https://ollama.com/download](https://ollama.com/download)). Na een succesvolle installatie kun je direct het Ollama-script gebruiken om Phi-3 via een terminalvenster aan te roepen. Je kunt alle [beschikbare bibliotheken in Ollama](https://ollama.com/library) bekijken. Als je deze repository in een Codespace opent, heeft deze Ollama al geÃ¯nstalleerd.

```bash

ollama run phi4

```

> [!NOTE]
> Het model wordt eerst gedownload wanneer je het voor het eerst uitvoert. Uiteraard kun je ook direct het gedownloade Phi-4-model opgeven. We nemen WSL als voorbeeld om het commando uit te voeren. Nadat het model succesvol is gedownload, kun je direct in de terminal interacteren.

![uitvoeren](../../../../../translated_images/nl/ollama_run.e9755172b162b381.png)

## **2. De phi-4 API vanuit Ollama aanroepen**

Als je de door Ollama gegenereerde Phi-4 API wilt aanroepen, kun je dit commando in de terminal gebruiken om de Ollama-server te starten.

```bash

ollama serve

```

> [!NOTE]
> Als je macOS of Linux gebruikt, merk op dat je de volgende fout kunt tegenkomen **"Error: listen tcp 127.0.0.1:11434: bind: address already in use"**. Je kunt deze fout krijgen bij het uitvoeren van het commando. Je kunt die fout negeren, aangezien dit doorgaans aangeeft dat de server al draait, of je kunt Ollama stoppen en opnieuw starten:

**macOS**

```bash

brew services restart ollama

```

**Linux**

```bash

sudo systemctl stop ollama

```

Ollama ondersteunt twee API's: generate en chat. Je kunt de door Ollama geleverde model-API naar behoefte aanroepen door verzoeken te sturen naar de lokale service die draait op poort 11434.

**Chat**

```bash

curl http://127.0.0.1:11434/api/chat -d '{
  "model": "phi3",
  "messages": [
    {
      "role": "system",
      "content": "Your are a python developer."
    },
    {
      "role": "user",
      "content": "Help me generate a bubble algorithm"
    }
  ],
  "stream": false
  
}'
```

Dit is het resultaat in Postman

![Schermafbeelding van JSON-resultaten voor generate-aanvraag](../../../../../translated_images/nl/ollama_gen.bda5d4e715366cc9.png)

## Aanvullende bronnen

Bekijk de lijst met beschikbare modellen in Ollama in [de bibliotheek](https://ollama.com/library).

Haal je model van de Ollama-server met dit commando

```bash
ollama pull phi4
```

Voer het model uit met dit commando

```bash
ollama run phi4
```

***Opmerking:*** Bezoek deze link [https://github.com/ollama/ollama/blob/main/docs/api.md](https://github.com/ollama/ollama/blob/main/docs/api.md) om meer te leren

## Ollama aanroepen vanuit Python

Je kunt `requests` of `urllib3` gebruiken om verzoeken te doen naar de lokale serverendpoints die hierboven zijn gebruikt. Een populaire manier om Ollama in Python te gebruiken is echter via de [openai](https://pypi.org/project/openai/) SDK, aangezien Ollama ook OpenAI-compatibele serverendpoints biedt.

Hier is een voorbeeld voor phi3-mini:

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="nokeyneeded",
)

response = client.chat.completions.create(
    model="phi4",
    temperature=0.7,
    n=1,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a haiku about a hungry cat"},
    ],
)

print("Response:")
print(response.choices[0].message.content)
```

## Ollama aanroepen vanuit JavaScript 

```javascript
// Voorbeeld van het samenvatten van een bestand met Phi-4
script({
    model: "ollama:phi4",
    title: "Summarize with Phi-4",
    system: ["system"],
})

// Voorbeeld van het samenvatten
const file = def("FILE", env.files)
$`Summarize ${file} in a single paragraph.`
```

## Ollama aanroepen vanuit C#

Maak een nieuwe C# Console-toepassing en voeg het volgende NuGet-pakket toe:

```bash
dotnet add package Microsoft.SemanticKernel --version 1.34.0
```

Vervang vervolgens deze code in het bestand `Program.cs`

```csharp
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.ChatCompletion;

// add chat completion service using the local ollama server endpoint
#pragma warning disable SKEXP0001, SKEXP0003, SKEXP0010, SKEXP0011, SKEXP0050, SKEXP0052
builder.AddOpenAIChatCompletion(
    modelId: "phi4",
    endpoint: new Uri("http://localhost:11434/"),
    apiKey: "non required");

// invoke a simple prompt to the chat service
string prompt = "Write a joke about kittens";
var response = await kernel.InvokePromptAsync(prompt);
Console.WriteLine(response.GetValue<string>());
```

Voer de app uit met het commando:

```bash
dotnet run
```

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Vrijwaring**:
Dit document is vertaald met behulp van de AI-vertalingsdienst [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we naar nauwkeurigheid streven, moet u er rekening mee houden dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het oorspronkelijke document in de oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor kritieke informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor eventuele misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->