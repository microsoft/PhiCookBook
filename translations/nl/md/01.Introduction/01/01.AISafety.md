<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c8273672cc57df2be675407a1383aaf0",
  "translation_date": "2025-07-16T17:50:05+00:00",
  "source_file": "md/01.Introduction/01/01.AISafety.md",
  "language_code": "nl"
}
-->
# AI-veiligheid voor Phi-modellen  
De Phi-familie van modellen is ontwikkeld volgens de [Microsoft Responsible AI Standard](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFl), een bedrijf brede set eisen gebaseerd op de volgende zes principes: verantwoordelijkheid, transparantie, eerlijkheid, betrouwbaarheid en veiligheid, privacy en beveiliging, en inclusiviteit, die samen [Microsoft’s Responsible AI principes](https://www.microsoft.com/ai/responsible-ai) vormen.

Net als bij de eerdere Phi-modellen is een veelzijdige veiligheidsbeoordeling en een veiligheidsgerichte natraining toegepast, met extra maatregelen om rekening te houden met de meertalige mogelijkheden van deze release. Onze aanpak voor veiligheidstraining en evaluaties, inclusief testen in meerdere talen en risicocategorieën, wordt beschreven in het [Phi Safety Post-Training Paper](https://arxiv.org/abs/2407.13833). Hoewel de Phi-modellen profiteren van deze aanpak, dienen ontwikkelaars verantwoordelijke AI-best practices toe te passen, waaronder het in kaart brengen, meten en beperken van risico’s die samenhangen met hun specifieke gebruikssituatie en culturele en taalkundige context.

## Best Practices

Net als andere modellen kunnen de Phi-modellen zich mogelijk op manieren gedragen die oneerlijk, onbetrouwbaar of aanstootgevend zijn.

Enkele van de beperkende gedragingen van SLM en LLM waar je je bewust van moet zijn, zijn onder andere:

- **Kwaliteit van de Dienst:** De Phi-modellen zijn voornamelijk getraind op Engelse teksten. Talen anders dan Engels zullen slechter presteren. Engelse taalvarianten die minder vertegenwoordigd zijn in de trainingsdata kunnen slechter presteren dan standaard Amerikaans Engels.  
- **Weergave van Schade & Bestendiging van Stereotypen:** Deze modellen kunnen groepen mensen over- of ondervertegenwoordigen, de representatie van sommige groepen wissen, of vernederende of negatieve stereotypen versterken. Ondanks de veiligheidsgerichte natraining kunnen deze beperkingen nog steeds aanwezig zijn door verschillende niveaus van representatie van groepen of de prevalentie van voorbeelden van negatieve stereotypen in de trainingsdata die echte wereldpatronen en maatschappelijke vooroordelen weerspiegelen.  
- **Ongepaste of Aanstootgevende Inhoud:** Deze modellen kunnen andere soorten ongepaste of aanstootgevende inhoud genereren, waardoor het ongepast kan zijn ze in gevoelige contexten in te zetten zonder aanvullende mitigaties die specifiek zijn voor het gebruiksscenario.  
- **Betrouwbaarheid van Informatie:** Taalmodellen kunnen onzinnige inhoud genereren of informatie verzinnen die misschien redelijk klinkt, maar onjuist of verouderd is.  
- **Beperkte Toepassingsscope voor Code:** Het merendeel van de Phi-3 trainingsdata is gebaseerd op Python en gebruikt gangbare pakketten zoals "typing, math, random, collections, datetime, itertools". Als het model Python-scripts genereert die andere pakketten gebruiken of scripts in andere talen, raden we gebruikers sterk aan om alle API-gebruik handmatig te verifiëren.

Ontwikkelaars dienen verantwoordelijke AI-best practices toe te passen en zijn verantwoordelijk voor het waarborgen dat een specifiek gebruiksscenario voldoet aan relevante wetten en regelgeving (bijv. privacy, handel, enz.).

## Overwegingen voor Verantwoorde AI

Net als andere taalmodellen kunnen de Phi-serie modellen zich mogelijk op manieren gedragen die oneerlijk, onbetrouwbaar of aanstootgevend zijn. Enkele van de beperkende gedragingen waar je op moet letten zijn:

**Kwaliteit van de Dienst:** De Phi-modellen zijn voornamelijk getraind op Engelse teksten. Talen anders dan Engels zullen slechter presteren. Engelse taalvarianten die minder vertegenwoordigd zijn in de trainingsdata kunnen slechter presteren dan standaard Amerikaans Engels.

**Weergave van Schade & Bestendiging van Stereotypen:** Deze modellen kunnen groepen mensen over- of ondervertegenwoordigen, de representatie van sommige groepen wissen, of vernederende of negatieve stereotypen versterken. Ondanks de veiligheidsgerichte natraining kunnen deze beperkingen nog steeds aanwezig zijn door verschillende niveaus van representatie van groepen of de prevalentie van voorbeelden van negatieve stereotypen in de trainingsdata die echte wereldpatronen en maatschappelijke vooroordelen weerspiegelen.

**Ongepaste of Aanstootgevende Inhoud:** Deze modellen kunnen andere soorten ongepaste of aanstootgevende inhoud genereren, waardoor het ongepast kan zijn ze in gevoelige contexten in te zetten zonder aanvullende mitigaties die specifiek zijn voor het gebruiksscenario.  
**Betrouwbaarheid van Informatie:** Taalmodellen kunnen onzinnige inhoud genereren of informatie verzinnen die misschien redelijk klinkt, maar onjuist of verouderd is.

**Beperkte Toepassingsscope voor Code:** Het merendeel van de Phi-3 trainingsdata is gebaseerd op Python en gebruikt gangbare pakketten zoals "typing, math, random, collections, datetime, itertools". Als het model Python-scripts genereert die andere pakketten gebruiken of scripts in andere talen, raden we gebruikers sterk aan om alle API-gebruik handmatig te verifiëren.

Ontwikkelaars dienen verantwoordelijke AI-best practices toe te passen en zijn verantwoordelijk voor het waarborgen dat een specifiek gebruiksscenario voldoet aan relevante wetten en regelgeving (bijv. privacy, handel, enz.). Belangrijke aandachtspunten zijn:

**Toewijzing:** Modellen zijn mogelijk niet geschikt voor scenario’s die een ingrijpende impact kunnen hebben op juridische status of de toewijzing van middelen of levensmogelijkheden (bijv. huisvesting, werk, krediet, enz.) zonder verdere beoordelingen en aanvullende technieken om vooroordelen te verminderen.

**Hoog-Risico Scenario’s:** Ontwikkelaars moeten de geschiktheid van het gebruik van modellen in hoog-risico scenario’s beoordelen, waarbij oneerlijke, onbetrouwbare of aanstootgevende uitkomsten extreem kostbaar kunnen zijn of schade kunnen veroorzaken. Dit omvat het geven van advies in gevoelige of expertgebieden waar nauwkeurigheid en betrouwbaarheid cruciaal zijn (bijv. juridisch of medisch advies). Extra waarborgen moeten op applicatieniveau worden geïmplementeerd, afgestemd op de context van de inzet.

**Desinformatie:** Modellen kunnen onjuiste informatie produceren. Ontwikkelaars dienen transparantie best practices te volgen en eindgebruikers te informeren dat ze met een AI-systeem communiceren. Op applicatieniveau kunnen ontwikkelaars feedbackmechanismen en pipelines bouwen om antwoorden te baseren op gebruiksspecifieke, contextuele informatie, een techniek die bekend staat als Retrieval Augmented Generation (RAG).

**Generatie van Schadelijke Inhoud:** Ontwikkelaars moeten outputs beoordelen in hun context en beschikbare veiligheidsclassificaties of maatwerkoplossingen gebruiken die geschikt zijn voor hun gebruiksscenario.

**Misbruik:** Andere vormen van misbruik zoals fraude, spam of malwareproductie zijn mogelijk, en ontwikkelaars moeten ervoor zorgen dat hun applicaties geen toepasselijke wetten en regels overtreden.

### Fijn afstemmen en AI-inhoudsveiligheid

Na het fijn afstemmen van een model raden we sterk aan om gebruik te maken van [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) maatregelen om de door de modellen gegenereerde inhoud te monitoren, potentiële risico’s, bedreigingen en kwaliteitsproblemen te identificeren en te blokkeren.

![Phi3AISafety](../../../../../translated_images/01.phi3aisafety.c0d7fc42f5a5c40507c5e8be556615b8377a63b8764865d057d4faac3757a478.nl.png)

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) ondersteunt zowel tekst- als beeldinhoud. Het kan worden ingezet in de cloud, in losgekoppelde containers en op edge/embedded apparaten.

## Overzicht van Azure AI Content Safety

Azure AI Content Safety is geen kant-en-klare oplossing; het kan worden aangepast aan de specifieke beleidsregels van bedrijven. Daarnaast maken de meertalige modellen het mogelijk om meerdere talen tegelijk te begrijpen.

![AIContentSafety](../../../../../translated_images/01.AIcontentsafety.a288819b8ce8da1a56cf708aff010a541799d002ae7ae84bb819b19ab8950591.nl.png)

- **Azure AI Content Safety**  
- **Microsoft Developer**  
- **5 video’s**

De Azure AI Content Safety-service detecteert schadelijke door gebruikers en AI gegenereerde inhoud in applicaties en diensten. Het bevat tekst- en beeld-API’s waarmee je schadelijk of ongepast materiaal kunt detecteren.

[AI Content Safety Playlist](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

**Disclaimer**:  
Dit document is vertaald met behulp van de AI-vertalingsdienst [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u er rekening mee te houden dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in de oorspronkelijke taal moet als de gezaghebbende bron worden beschouwd. Voor cruciale informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor eventuele misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.