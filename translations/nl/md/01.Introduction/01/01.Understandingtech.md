<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:25:58+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "nl"
}
-->
# Belangrijke genoemde technologieën zijn

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - een laag-niveau API voor hardwareversnelde machine learning, gebouwd bovenop DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - een platform voor parallel rekenen en een API-model ontwikkeld door Nvidia, waarmee algemene verwerking mogelijk is op grafische verwerkingseenheden (GPU's).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - een open formaat ontworpen om machine learning modellen te representeren en interoperabiliteit tussen verschillende ML-frameworks te bieden.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - een formaat voor het representeren en updaten van machine learning modellen, vooral handig voor kleinere taalmodellen die efficiënt draaien op CPU’s met 4-8bit kwantisatie.

## DirectML

DirectML is een laag-niveau API die hardwareversnelde machine learning mogelijk maakt. Het is gebouwd bovenop DirectX 12 om GPU-versnelling te benutten en is leverancier-onafhankelijk, wat betekent dat er geen codewijzigingen nodig zijn om het op verschillende GPU-leveranciers te laten werken. Het wordt voornamelijk gebruikt voor modeltraining en inferentie op GPU’s.

Wat betreft hardware-ondersteuning is DirectML ontworpen om te werken met een breed scala aan GPU’s, waaronder AMD geïntegreerde en discrete GPU’s, Intel geïntegreerde GPU’s en NVIDIA discrete GPU’s. Het maakt deel uit van het Windows AI Platform en wordt ondersteund op Windows 10 & 11, waardoor modeltraining en inferentie op elk Windows-apparaat mogelijk is.

Er zijn updates en kansen rondom DirectML geweest, zoals ondersteuning voor tot wel 150 ONNX-operatoren en het gebruik door zowel de ONNX runtime als WinML. Het wordt ondersteund door grote Integrated Hardware Vendors (IHVs), die elk verschillende metacommands implementeren.

## CUDA

CUDA, wat staat voor Compute Unified Device Architecture, is een platform voor parallel rekenen en een API-model ontwikkeld door Nvidia. Het stelt softwareontwikkelaars in staat een CUDA-compatibele grafische verwerkingseenheid (GPU) te gebruiken voor algemene verwerking – een aanpak die GPGPU (General-Purpose computing on Graphics Processing Units) wordt genoemd. CUDA is een belangrijke motor achter Nvidia’s GPU-versnelling en wordt veel gebruikt in diverse gebieden, waaronder machine learning, wetenschappelijk rekenen en videobewerking.

De hardware-ondersteuning voor CUDA is specifiek voor Nvidia GPU’s, omdat het een propriëtaire technologie is ontwikkeld door Nvidia. Elke architectuur ondersteunt specifieke versies van de CUDA toolkit, die de benodigde bibliotheken en tools biedt voor ontwikkelaars om CUDA-toepassingen te bouwen en uit te voeren.

## ONNX

ONNX (Open Neural Network Exchange) is een open formaat ontworpen om machine learning modellen te representeren. Het biedt een definitie van een uitbreidbaar computationeel grafiekmodel, evenals definities van ingebouwde operatoren en standaarddatatypes. ONNX maakt het mogelijk voor ontwikkelaars om modellen tussen verschillende ML-frameworks te verplaatsen, wat interoperabiliteit bevordert en het eenvoudiger maakt AI-toepassingen te creëren en uit te rollen.

Phi3 mini kan draaien met ONNX Runtime op CPU en GPU over verschillende apparaten, waaronder serverplatforms, Windows, Linux en Mac desktops, en mobiele CPU’s.  
De geoptimaliseerde configuraties die we hebben toegevoegd zijn

- ONNX modellen voor int4 DML: gekwantiseerd naar int4 via AWQ  
- ONNX model voor fp16 CUDA  
- ONNX model voor int4 CUDA: gekwantiseerd naar int4 via RTN  
- ONNX model voor int4 CPU en mobiel: gekwantiseerd naar int4 via RTN

## Llama.cpp

Llama.cpp is een open-source softwarebibliotheek geschreven in C++. Het voert inferentie uit op diverse Large Language Models (LLM’s), waaronder Llama. Ontwikkeld samen met de ggml bibliotheek (een algemene tensorbibliotheek), streeft llama.cpp naar snellere inferentie en lager geheugenverbruik vergeleken met de originele Python-implementatie. Het ondersteunt hardware-optimalisatie, kwantisatie en biedt een eenvoudige API en voorbeelden3. Als je geïnteresseerd bent in efficiënte LLM-inferentie, is llama.cpp het waard om te verkennen, aangezien Phi3 Llama.cpp kan draaien.

## GGUF

GGUF (Generic Graph Update Format) is een formaat voor het representeren en updaten van machine learning modellen. Het is vooral nuttig voor kleinere taalmodellen (SLM’s) die effectief kunnen draaien op CPU’s met 4-8bit kwantisatie. GGUF is voordelig voor snelle prototyping en het draaien van modellen op edge-apparaten of in batchtaken zoals CI/CD pipelines.

**Disclaimer**:  
Dit document is vertaald met behulp van de AI-vertalingsdienst [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u er rekening mee te houden dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in de oorspronkelijke taal geldt als de gezaghebbende bron. Voor belangrijke informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor eventuele misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.