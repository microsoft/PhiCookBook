<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:45:51+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "nl"
}
-->
# Belangrijke genoemde technologieën zijn

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - een low-level API voor hardwareversnelde machine learning, gebouwd bovenop DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - een parallel computing platform en API-model ontwikkeld door Nvidia, waarmee algemene verwerking mogelijk is op grafische verwerkingseenheden (GPU's).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - een open formaat ontworpen om machine learning modellen te representeren en interoperabiliteit tussen verschillende ML-frameworks te bieden.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - een formaat voor het representeren en bijwerken van machine learning modellen, vooral handig voor kleinere taalmodellen die effectief kunnen draaien op CPU's met 4-8bit quantisatie.

## DirectML

DirectML is een low-level API die hardwareversnelde machine learning mogelijk maakt. Het is gebouwd bovenop DirectX 12 om GPU-versnelling te benutten en is vendor-onafhankelijk, wat betekent dat er geen codewijzigingen nodig zijn om op verschillende GPU-merken te werken. Het wordt voornamelijk gebruikt voor modeltraining en inferentie op GPU's.

Wat betreft hardware-ondersteuning is DirectML ontworpen om te werken met een breed scala aan GPU's, waaronder AMD geïntegreerde en discrete GPU's, Intel geïntegreerde GPU's en NVIDIA discrete GPU's. Het maakt deel uit van het Windows AI Platform en wordt ondersteund op Windows 10 & 11, waardoor modeltraining en inferentie mogelijk zijn op elk Windows-apparaat.

Er zijn updates en mogelijkheden rondom DirectML geweest, zoals ondersteuning voor tot wel 150 ONNX-operatoren en het gebruik door zowel de ONNX runtime als WinML. Het wordt ondersteund door grote Integrated Hardware Vendors (IHV's), die elk verschillende metacommands implementeren.

## CUDA

CUDA, wat staat voor Compute Unified Device Architecture, is een parallel computing platform en API-model ontwikkeld door Nvidia. Het stelt softwareontwikkelaars in staat om een CUDA-compatibele grafische verwerkingseenheid (GPU) te gebruiken voor algemene verwerking – een aanpak die GPGPU (General-Purpose computing on Graphics Processing Units) wordt genoemd. CUDA is een belangrijke motor achter Nvidia’s GPU-versnelling en wordt veel gebruikt in diverse vakgebieden, waaronder machine learning, wetenschappelijk rekenen en videobewerking.

De hardware-ondersteuning voor CUDA is specifiek voor Nvidia GPU's, aangezien het een propriëtaire technologie is ontwikkeld door Nvidia. Elke architectuur ondersteunt specifieke versies van de CUDA toolkit, die de benodigde bibliotheken en tools biedt voor ontwikkelaars om CUDA-applicaties te bouwen en uit te voeren.

## ONNX

ONNX (Open Neural Network Exchange) is een open formaat ontworpen om machine learning modellen te representeren. Het biedt een definitie van een uitbreidbaar computationeel grafiekmodel, evenals definities van ingebouwde operatoren en standaard datatypes. ONNX maakt het mogelijk om modellen tussen verschillende ML-frameworks te verplaatsen, wat interoperabiliteit bevordert en het eenvoudiger maakt om AI-toepassingen te creëren en te implementeren.

Phi3 mini kan draaien met ONNX Runtime op CPU en GPU op verschillende apparaten, waaronder serverplatforms, Windows, Linux en Mac desktops, en mobiele CPU's.  
De geoptimaliseerde configuraties die we hebben toegevoegd zijn

- ONNX modellen voor int4 DML: Gequantiseerd naar int4 via AWQ  
- ONNX model voor fp16 CUDA  
- ONNX model voor int4 CUDA: Gequantiseerd naar int4 via RTN  
- ONNX model voor int4 CPU en Mobile: Gequantiseerd naar int4 via RTN  

## Llama.cpp

Llama.cpp is een open-source softwarebibliotheek geschreven in C++. Het voert inferentie uit op verschillende Large Language Models (LLM's), waaronder Llama. Ontwikkeld naast de ggml-bibliotheek (een algemene tensorbibliotheek), streeft llama.cpp ernaar snellere inferentie en lager geheugenverbruik te bieden vergeleken met de originele Python-implementatie. Het ondersteunt hardware-optimalisatie, quantisatie en biedt een eenvoudige API en voorbeelden. Als je geïnteresseerd bent in efficiënte LLM-inferentie, is llama.cpp zeker het verkennen waard, aangezien Phi3 Llama.cpp kan draaien.

## GGUF

GGUF (Generic Graph Update Format) is een formaat dat wordt gebruikt voor het representeren en bijwerken van machine learning modellen. Het is vooral nuttig voor kleinere taalmodellen (SLM's) die effectief kunnen draaien op CPU's met 4-8bit quantisatie. GGUF is handig voor snelle prototyping en het draaien van modellen op edge-apparaten of in batchtaken zoals CI/CD-pijplijnen.

**Disclaimer**:  
Dit document is vertaald met behulp van de AI-vertalingsdienst [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u er rekening mee te houden dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in de oorspronkelijke taal moet als de gezaghebbende bron worden beschouwd. Voor cruciale informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor eventuele misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.