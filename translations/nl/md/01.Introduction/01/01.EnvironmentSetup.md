<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-09T07:12:14+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "nl"
}
-->
# Aan de slag met Phi-3 lokaal

Deze gids helpt je bij het opzetten van je lokale omgeving om het Phi-3 model te draaien met Ollama. Je kunt het model op verschillende manieren gebruiken, waaronder via GitHub Codespaces, VS Code Dev Containers of je eigen lokale omgeving.

## Omgevingsinstelling

### GitHub Codespaces

Je kunt deze template virtueel draaien met GitHub Codespaces. De knop opent een webgebaseerde VS Code in je browser:

1. Open de template (dit kan enkele minuten duren):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Open een terminalvenster

### VS Code Dev Containers

⚠️ Deze optie werkt alleen als Docker Desktop minimaal 16 GB RAM toegewezen heeft. Heb je minder dan 16 GB RAM, probeer dan de [GitHub Codespaces optie](../../../../../md/01.Introduction/01) of [zet het lokaal op](../../../../../md/01.Introduction/01).

Een gerelateerde optie is VS Code Dev Containers, waarmee je het project opent in je lokale VS Code met behulp van de [Dev Containers extensie](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Start Docker Desktop (installeer het indien nog niet geïnstalleerd)
2. Open het project:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. In het geopende VS Code venster, zodra de projectbestanden geladen zijn (dit kan enkele minuten duren), open je een terminalvenster.
4. Ga verder met de [deployment stappen](../../../../../md/01.Introduction/01)

### Lokale omgeving

1. Zorg dat de volgende tools geïnstalleerd zijn:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Test het model

1. Vraag Ollama om het phi3:mini model te downloaden en te starten:

    ```shell
    ollama run phi3:mini
    ```

    Dit kan enkele minuten duren om het model te downloaden.

2. Zodra je "success" in de output ziet, kun je een bericht naar dat model sturen vanuit de prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Na een paar seconden zou je een responsstroom van het model moeten zien.

4. Om meer te leren over verschillende technieken die met taalmodellen worden gebruikt, open je de Python notebook [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) en voer je elke cel uit. Als je een ander model dan 'phi3:mini' hebt gebruikt, pas dan de `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` bovenaan het bestand aan, en je kunt ook het systeembericht aanpassen of few-shot voorbeelden toevoegen indien gewenst.

**Disclaimer**:  
Dit document is vertaald met behulp van de AI-vertalingsdienst [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u er rekening mee te houden dat automatische vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in de oorspronkelijke taal geldt als de gezaghebbende bron. Voor cruciale informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor eventuele misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.