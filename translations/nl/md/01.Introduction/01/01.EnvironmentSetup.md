<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-07-16T18:10:48+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "nl"
}
-->
# Aan de slag met Phi-3 lokaal

Deze gids helpt je bij het opzetten van je lokale omgeving om het Phi-3 model te draaien met Ollama. Je kunt het model op verschillende manieren gebruiken, waaronder via GitHub Codespaces, VS Code Dev Containers of je eigen lokale omgeving.

## Omgevingsinstelling

### GitHub Codespaces

Je kunt deze template virtueel draaien met GitHub Codespaces. De knop opent een webgebaseerde VS Code-omgeving in je browser:

1. Open de template (dit kan enkele minuten duren):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Open een terminalvenster

### VS Code Dev Containers

⚠️ Deze optie werkt alleen als je Docker Desktop minimaal 16 GB RAM heeft toegewezen. Heb je minder dan 16 GB RAM, probeer dan de [GitHub Codespaces optie](../../../../../md/01.Introduction/01) of [zet het lokaal op](../../../../../md/01.Introduction/01).

Een gerelateerde optie is VS Code Dev Containers, waarmee je het project lokaal in VS Code opent met de [Dev Containers extensie](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Start Docker Desktop (installeer het als je het nog niet hebt)
2. Open het project:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. In het VS Code-venster dat opent, zodra de projectbestanden geladen zijn (dit kan enkele minuten duren), open je een terminalvenster.
4. Ga verder met de [deploy-stappen](../../../../../md/01.Introduction/01)

### Lokale omgeving

1. Zorg dat de volgende tools geïnstalleerd zijn:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Test het model

1. Vraag Ollama om het phi3:mini model te downloaden en te starten:

    ```shell
    ollama run phi3:mini
    ```

    Dit duurt een paar minuten om het model te downloaden.

2. Zodra je "success" in de output ziet, kun je een bericht naar dat model sturen via de prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Na enkele seconden zou je een responsstroom van het model moeten zien.

4. Om meer te leren over verschillende technieken met taalmodellen, open je de Python-notebook [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) en voer je elke cel uit. Als je een ander model dan 'phi3:mini' gebruikt, pas dan de `MODEL_NAME` aan in de eerste cel.

5. Om een gesprek te voeren met het phi3:mini model vanuit Python, open je het Python-bestand [chat.py](../../../../../code/01.Introduce/chat.py) en voer je het uit. Je kunt de `MODEL_NAME` bovenaan het bestand aanpassen indien nodig, en je kunt ook het systeembericht wijzigen of few-shot voorbeelden toevoegen als je dat wilt.

**Disclaimer**:  
Dit document is vertaald met behulp van de AI-vertalingsdienst [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u er rekening mee te houden dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in de oorspronkelijke taal moet als de gezaghebbende bron worden beschouwd. Voor cruciale informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor eventuele misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.