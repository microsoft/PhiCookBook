<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f737bf207e1691cdc654535c48dd2df4",
  "translation_date": "2025-04-03T07:33:08+00:00",
  "source_file": "md\\02.Application\\01.TextAndChat\\Phi3\\E2E_Phi-3-mini_with_whisper.md",
  "language_code": "ur"
}
-->
# انٹرایکٹو Phi 3 Mini 4K انسٹرکٹ چیٹ بوٹ ود وِسپَر

## جائزہ

انٹرایکٹو Phi 3 Mini 4K انسٹرکٹ چیٹ بوٹ ایک ایسا ٹول ہے جو صارفین کو مائیکروسافٹ Phi 3 Mini 4K انسٹرکٹ ڈیمو کے ساتھ ٹیکسٹ یا آڈیو ان پٹ کے ذریعے بات چیت کرنے کی اجازت دیتا ہے۔ یہ چیٹ بوٹ مختلف کاموں کے لیے استعمال کیا جا سکتا ہے، جیسے ترجمہ، موسم کی تازہ ترین معلومات، اور عمومی معلومات حاصل کرنا۔

### شروعات کیسے کریں

اس چیٹ بوٹ کو استعمال کرنے کے لیے درج ذیل ہدایات پر عمل کریں:

1. ایک نیا [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) کھولیں۔
2. نوٹ بک کے مرکزی ونڈو میں آپ کو ایک چیٹ باکس انٹرفیس نظر آئے گا جس میں ایک ٹیکسٹ ان پٹ باکس اور ایک "Send" بٹن ہوگا۔
3. ٹیکسٹ پر مبنی چیٹ بوٹ استعمال کرنے کے لیے، اپنے پیغام کو ٹیکسٹ ان پٹ باکس میں ٹائپ کریں اور "Send" بٹن پر کلک کریں۔ چیٹ بوٹ آپ کو ایک آڈیو فائل کے ساتھ جواب دے گا جسے نوٹ بک کے اندر ہی چلایا جا سکتا ہے۔

**نوٹ**: اس ٹول کے لیے ایک GPU اور مائیکروسافٹ Phi-3 اور OpenAI Whisper ماڈلز تک رسائی کی ضرورت ہے، جو تقریر کی شناخت اور ترجمہ کے لیے استعمال ہوتا ہے۔

### GPU کی ضروریات

اس ڈیمو کو چلانے کے لیے آپ کو 12GB GPU میموری کی ضرورت ہوگی۔

**Microsoft-Phi-3-Mini-4K انسٹرکٹ** ڈیمو کو GPU پر چلانے کے لیے میموری کی ضروریات کئی عوامل پر منحصر ہوں گی، جیسے ان پٹ ڈیٹا (آڈیو یا ٹیکسٹ) کا سائز، ترجمے کے لیے استعمال ہونے والی زبان، ماڈل کی رفتار، اور GPU پر دستیاب میموری۔

عام طور پر، Whisper ماڈل کو GPU پر چلانے کے لیے ڈیزائن کیا گیا ہے۔ Whisper ماڈل کو چلانے کے لیے تجویز کردہ کم از کم GPU میموری 8GB ہے، لیکن اگر ضرورت ہو تو یہ زیادہ میموری کو بھی ہینڈل کر سکتا ہے۔

یہ اہم ہے کہ ماڈل پر زیادہ مقدار میں ڈیٹا یا درخواستوں کا زیادہ حجم چلانے کے لیے زیادہ GPU میموری کی ضرورت ہو سکتی ہے اور/یا کارکردگی کے مسائل پیدا ہو سکتے ہیں۔ اپنی ضروریات کے لیے بہترین ترتیبات کا تعین کرنے کے لیے مختلف کنفیگریشنز کے ساتھ اپنے کیس کا ٹیسٹ کریں اور میموری کے استعمال کی نگرانی کریں۔

## انٹرایکٹو Phi 3 Mini 4K انسٹرکٹ چیٹ بوٹ ود وِسپَر کے لیے E2E سیمپل

جیوپیٹر نوٹ بک [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) دکھاتی ہے کہ مائیکروسافٹ Phi 3 Mini 4K انسٹرکٹ ڈیمو کو آڈیو یا ٹیکسٹ ان پٹ سے ٹیکسٹ جنریٹ کرنے کے لیے کیسے استعمال کیا جائے۔ نوٹ بک کئی فنکشنز کو ڈیفائن کرتی ہے:

1. `tts_file_name(text)`: یہ فنکشن ان پٹ ٹیکسٹ کی بنیاد پر آڈیو فائل کو محفوظ کرنے کے لیے ایک فائل کا نام جنریٹ کرتا ہے۔
2. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: یہ فنکشن Edge TTS API کا استعمال کرتے ہوئے ان پٹ ٹیکسٹ کے چنکس کی فہرست سے آڈیو فائل جنریٹ کرتا ہے۔ ان پٹ پیرامیٹرز میں چنکس کی فہرست، تقریر کی رفتار، آواز کا نام، اور آؤٹ پٹ پاتھ شامل ہیں۔
3. `talk(input_text)`: یہ فنکشن Edge TTS API کا استعمال کرتے ہوئے ایک آڈیو فائل جنریٹ کرتا ہے اور اسے /content/audio ڈائریکٹری میں ایک رینڈم فائل نام پر محفوظ کرتا ہے۔ ان پٹ پیرامیٹر وہ ٹیکسٹ ہے جسے تقریر میں تبدیل کرنا ہے۔
4. `run_text_prompt(message, chat_history)`: یہ فنکشن مائیکروسافٹ Phi 3 Mini 4K انسٹرکٹ ڈیمو کا استعمال کرتے ہوئے میسج ان پٹ سے ایک آڈیو فائل جنریٹ کرتا ہے اور اسے چیٹ ہسٹری میں شامل کرتا ہے۔
5. `run_audio_prompt(audio, chat_history)`: یہ فنکشن Whisper ماڈل API کا استعمال کرتے ہوئے آڈیو فائل کو ٹیکسٹ میں تبدیل کرتا ہے اور اسے `run_text_prompt()` فنکشن کو بھیجتا ہے۔
6. کوڈ ایک Gradio ایپ لانچ کرتا ہے جو صارفین کو Phi 3 Mini 4K انسٹرکٹ ڈیمو کے ساتھ بات چیت کرنے کی اجازت دیتا ہے، چاہے وہ میسجز ٹائپ کریں یا آڈیو فائلز اپلوڈ کریں۔ آؤٹ پٹ ایپ کے اندر ایک ٹیکسٹ میسج کی صورت میں دکھایا جاتا ہے۔

## خرابیوں کا پتہ لگانا

Cuda GPU ڈرائیورز انسٹال کرنا

1. یقینی بنائیں کہ آپ کی لینکس ایپلی کیشنز اپ ٹو ڈیٹ ہیں۔

    ```bash
    sudo apt update
    ```

2. Cuda ڈرائیورز انسٹال کریں۔

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

3. Cuda ڈرائیور کی لوکیشن رجسٹر کریں۔

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

4. Nvidia GPU میموری سائز چیک کریں (ضروری 12GB GPU میموری)

    ```bash
    nvidia-smi
    ```

5. کیش خالی کریں: اگر آپ PyTorch استعمال کر رہے ہیں، تو torch.cuda.empty_cache() کال کر کے تمام غیر استعمال شدہ کیشڈ میموری کو ریلیز کریں تاکہ یہ دیگر GPU ایپلی کیشنز کے لیے استعمال ہو سکے۔

    ```python
    torch.cuda.empty_cache() 
    ```

6. Nvidia Cuda چیک کریں۔

    ```bash
    nvcc --version
    ```

7. Hugging Face ٹوکن بنانے کے لیے درج ذیل کام انجام دیں۔

    - [Hugging Face Token Settings صفحہ](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo) پر جائیں۔
    - **New token** منتخب کریں۔
    - پروجیکٹ کا **Name** درج کریں جو آپ استعمال کرنا چاہتے ہیں۔
    - **Type** کو **Write** پر منتخب کریں۔

> **نوٹ**
>
> اگر آپ کو درج ذیل ایرر کا سامنا ہو:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> اسے حل کرنے کے لیے، اپنے ٹرمینل میں درج ذیل کمانڈ ٹائپ کریں۔
>
> ```bash
> sudo ldconfig
> ```

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ اگرچہ ہم درستگی کے لیے کوشاں ہیں، براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا خامیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے لیے ہم ذمہ دار نہیں ہیں۔