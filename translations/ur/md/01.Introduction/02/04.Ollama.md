<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2aa35f3c8b437fd5dc9995d53909d495",
  "translation_date": "2025-12-21T10:16:01+00:00",
  "source_file": "md/01.Introduction/02/04.Ollama.md",
  "language_code": "ur"
}
-->
## اولاما میں Phi خاندان


[Ollama](https://ollama.com) زیادہ لوگوں کو سادہ اسکرپٹس کے ذریعے اوپن سورس LLM یا SLM براہِ راست ڈپلوائے کرنے کی اجازت دیتا ہے، اور مقامی Copilot ایپلیکیشن کے منظرناموں میں مدد کے لیے APIs بھی بنا سکتا ہے۔

## **1. تنصیب**

Ollama ونڈوز، macOS، اور لینکس پر چلانے کی حمایت کرتا ہے۔ آپ Ollama کو اس لنک کے ذریعے انسٹال کر سکتے ہیں ([https://ollama.com/download](https://ollama.com/download)). کامیاب تنصیب کے بعد، آپ ٹرمینل ونڈو کے ذریعے براہِ راست Ollama اسکرپٹ استعمال کرکے Phi-3 کو کال کر سکتے ہیں۔ آپ تمام [Ollama میں دستیاب لائبریریز](https://ollama.com/library) دیکھ سکتے ہیں۔ اگر آپ اس ریپوزیٹری کو Codespace میں کھولتے ہیں، تو اس میں پہلے ہی Ollama نصب ہوگا۔

```bash

ollama run phi4

```

> [!NOTE]
> ماڈل پہلے بار چلانے پر ڈاؤن لوڈ ہوگا۔ یقیناً، آپ براہِ راست ڈاؤن لوڈ کردہ Phi-4 ماڈل کا بھی مخصوص کر سکتے ہیں۔ ہم کمانڈ چلانے کی مثال کے طور پر WSL لیتے ہیں۔ ماڈل کامیابی سے ڈاؤن لوڈ ہونے کے بعد، آپ براہِ راست ٹرمینل پر انٹریکٹ کر سکتے ہیں۔

![چلائیں](../../../../../translated_images/ollama_run.e9755172b162b381359f8dc8ad0eb1499e13266d833afaf29c47e928d6d7abc5.ur.png)

## **2. Ollama سے phi-4 API کال کرنا**

اگر آپ Ollama کے ذریعے تیار کردہ Phi-4 API کو کال کرنا چاہتے ہیں، تو آپ Ollama سرور کو شروع کرنے کے لیے ٹرمینل میں یہ کمانڈ استعمال کر سکتے ہیں۔

```bash

ollama serve

```

> [!NOTE]
> اگر آپ MacOS یا Linux چلا رہے ہیں، تو براہِ کرم نوٹ کریں کہ آپ کو مندرجہ ذیل ایرر کا سامنا ہو سکتا ہے **"Error: listen tcp 127.0.0.1:11434: bind: address already in use"** آپ کو یہ ایرر کمانڈ چلانے کے دوران مل سکتا ہے۔ آپ اس ایرر کو نظرانداز کر سکتے ہیں کیونکہ یہ عام طور پر بتاتا ہے کہ سرور پہلے سے چل رہا ہے، یا آپ Ollama کو روک کر دوبارہ شروع کر سکتے ہیں:

**macOS**

```bash

brew services restart ollama

```

**Linux**

```bash

sudo systemctl stop ollama

```

Ollama دو APIs کو سپورٹ کرتا ہے: generate اور chat۔ آپ اپنی ضرورت کے مطابق Ollama کے فراہم کردہ ماڈل API کو پورٹ 11434 پر چلنے والی مقامی سروس کو ریکویسٹ بھیج کر کال کر سکتے ہیں۔

**چیٹ**

```bash

curl http://127.0.0.1:11434/api/chat -d '{
  "model": "phi3",
  "messages": [
    {
      "role": "system",
      "content": "Your are a python developer."
    },
    {
      "role": "user",
      "content": "Help me generate a bubble algorithm"
    }
  ],
  "stream": false
  
}'
```

یہ Postman میں نتیجہ ہے

![generate ریکویسٹ کے لیے JSON نتائج کا اسکرین شاٹ](../../../../../translated_images/ollama_gen.bda5d4e715366cc9c1cae2956e30bfd55b07b22ca782ef69e680100a9a1fd563.ur.png)

## اضافی وسائل

Ollama میں دستیاب ماڈلز کی فہرست ان کی [لائبریری](https://ollama.com/library) میں دیکھیں۔

اپنا ماڈل Ollama سرور سے اس کمانڈ کے ذریعے کھینچیں

```bash
ollama pull phi4
```

اس کمانڈ کا استعمال کرتے ہوئے ماڈل چلائیں

```bash
ollama run phi4
```

***نوٹ:*** مزید معلومات کے لیے اس لنک پر جائیں [https://github.com/ollama/ollama/blob/main/docs/api.md](https://github.com/ollama/ollama/blob/main/docs/api.md)

## Python سے Ollama کو کال کرنا

آپ اوپر استعمال ہونے والے مقامی سرور endpoints پر ریکویسٹ بھیجنے کے لیے `requests` یا `urllib3` استعمال کر سکتے ہیں۔ تاہم، Python میں Ollama استعمال کرنے کا ایک مقبول طریقہ [openai](https://pypi.org/project/openai/) SDK کے ذریعے ہے، کیونکہ Ollama OpenAI-مطابق سرور endpoints بھی فراہم کرتا ہے۔

یہاں phi3-mini کی مثال ہے:

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="nokeyneeded",
)

response = client.chat.completions.create(
    model="phi4",
    temperature=0.7,
    n=1,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a haiku about a hungry cat"},
    ],
)

print("Response:")
print(response.choices[0].message.content)
```

## JavaScript سے Ollama کو کال کرنا 

```javascript
// Phi-4 کے ساتھ ایک فائل کا خلاصہ کرنے کی مثال
script({
    model: "ollama:phi4",
    title: "Summarize with Phi-4",
    system: ["system"],
})

// خلاصہ کرنے کی مثال
const file = def("FILE", env.files)
$`Summarize ${file} in a single paragraph.`
```

## C# سے Ollama کو کال کرنا

ایک نیا C# Console اپلیکیشن بنائیں اور درج ذیل NuGet پیکیج شامل کریں:

```bash
dotnet add package Microsoft.SemanticKernel --version 1.34.0
```

پھر `Program.cs` فائل میں اس کوڈ کو تبدیل کریں

```csharp
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.ChatCompletion;

// add chat completion service using the local ollama server endpoint
#pragma warning disable SKEXP0001, SKEXP0003, SKEXP0010, SKEXP0011, SKEXP0050, SKEXP0052
builder.AddOpenAIChatCompletion(
    modelId: "phi4",
    endpoint: new Uri("http://localhost:11434/"),
    apiKey: "non required");

// invoke a simple prompt to the chat service
string prompt = "Write a joke about kittens";
var response = await kernel.InvokePromptAsync(prompt);
Console.WriteLine(response.GetValue<string>());
```

ایپ کو درج ذیل کمانڈ سے چلائیں:

```bash
dotnet run
```

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
دستبرداری:
اس دستاویز کا ترجمہ AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کے ذریعے کیا گیا ہے۔ اگرچہ ہم درستگی کے لیے کوشاں ہیں، براہِ کرم نوٹ کریں کہ خودکار ترجموں میں غلطیاں یا عدم درستیاں ہو سکتی ہیں۔ اصل دستاویز اپنی مادری زبان میں معتبر ماخذ سمجھی جانی چاہیے۔ اہم معلومات کے لیے پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے لیے ہم ذمہ دار نہیں ہیں۔
<!-- CO-OP TRANSLATOR DISCLAIMER END -->