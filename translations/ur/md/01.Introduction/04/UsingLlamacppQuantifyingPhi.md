<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a7aaeb42235207ba74581473b305581",
  "translation_date": "2025-04-03T07:04:20+00:00",
  "source_file": "md\\01.Introduction\\04\\UsingLlamacppQuantifyingPhi.md",
  "language_code": "ur"
}
-->
# **llama.cpp کے ذریعے Phi فیملی کو کوانٹائز کرنا**

## **llama.cpp کیا ہے؟**

llama.cpp ایک اوپن سورس سافٹ ویئر لائبریری ہے جو بنیادی طور پر C++ میں لکھی گئی ہے اور مختلف بڑے لینگویج ماڈلز (LLMs) جیسے Llama پر انفرنس انجام دیتی ہے۔ اس کا بنیادی مقصد مختلف ہارڈویئر پر بہترین کارکردگی کے ساتھ LLM انفرنس فراہم کرنا ہے، وہ بھی کم سے کم سیٹ اپ کے ساتھ۔ اس کے علاوہ، اس لائبریری کے لیے Python بائنڈنگز بھی دستیاب ہیں، جو ٹیکسٹ کمپلیشن کے لیے ہائی لیول API اور OpenAI کے موافق ویب سرور فراہم کرتی ہیں۔

llama.cpp کا بنیادی مقصد کم سے کم سیٹ اپ کے ساتھ مختلف ہارڈویئر پر بہترین کارکردگی کے ساتھ LLM انفرنس کو ممکن بنانا ہے، چاہے وہ مقامی طور پر ہو یا کلاؤڈ میں۔

- سادہ C/C++ امپلیمنٹیشن، بغیر کسی ڈپینڈنسی کے
- ایپل سلیکان کے لیے خصوصی سپورٹ - ARM NEON، Accelerate، اور Metal فریم ورک کے ذریعے بہتر بنایا گیا
- x86 آرکیٹیکچرز کے لیے AVX، AVX2، اور AVX512 سپورٹ
- 1.5-bit، 2-bit، 3-bit، 4-bit، 5-bit، 6-bit، اور 8-bit انٹیجر کوانٹائزیشن، تیز انفرنس اور کم میموری استعمال کے لیے
- NVIDIA GPUs پر LLMs چلانے کے لیے کسٹم CUDA کرنلز (AMD GPUs کے لیے HIP کے ذریعے سپورٹ)
- Vulkan اور SYCL بیک اینڈ سپورٹ
- CPU+GPU ہائبرڈ انفرنس، جو ماڈلز کو اس وقت بھی جزوی طور پر تیز کرتا ہے جب وہ VRAM کی کل صلاحیت سے زیادہ ہوں

## **Phi-3.5 کو llama.cpp کے ذریعے کوانٹائز کرنا**

Phi-3.5-Instruct ماڈل کو llama.cpp کے ذریعے کوانٹائز کیا جا سکتا ہے، لیکن Phi-3.5-Vision اور Phi-3.5-MoE ابھی تک سپورٹ نہیں کرتے۔ llama.cpp کے ذریعے کنورٹ کیا گیا فارمیٹ gguf ہے، جو سب سے زیادہ استعمال ہونے والا کوانٹائزیشن فارمیٹ بھی ہے۔

Hugging Face پر gguf فارمیٹ کے بہت سارے کوانٹائزڈ ماڈلز دستیاب ہیں۔ AI Foundry، Ollama، اور LlamaEdge، سب llama.cpp پر انحصار کرتے ہیں، اس لیے gguf ماڈلز اکثر استعمال کیے جاتے ہیں۔

### **GGUF کیا ہے؟**

GGUF ایک بائنری فارمیٹ ہے جو ماڈلز کو جلدی لوڈ اور محفوظ کرنے کے لیے بہتر بنایا گیا ہے، اور انفرنس کے لیے انتہائی مؤثر ہے۔ GGUF کو GGML اور دیگر ایکزیکیوٹرز کے ساتھ استعمال کے لیے ڈیزائن کیا گیا ہے۔ GGUF کو @ggerganov نے تیار کیا، جو llama.cpp کے ڈیولپر بھی ہیں، جو ایک مقبول C/C++ LLM انفرنس فریم ورک ہے۔ ایسے ماڈلز جو ابتدا میں PyTorch جیسے فریم ورک میں تیار کیے گئے ہوں، انہیں GGUF فارمیٹ میں تبدیل کیا جا سکتا ہے تاکہ ان انجنز کے ساتھ استعمال کیا جا سکے۔

### **ONNX بمقابلہ GGUF**

ONNX ایک روایتی مشین لرننگ/ڈیپ لرننگ فارمیٹ ہے، جو مختلف AI فریم ورکس میں اچھی طرح سپورٹ کیا جاتا ہے اور ایج ڈیوائسز میں اچھے استعمال کے مواقع فراہم کرتا ہے۔ GGUF، دوسری طرف، llama.cpp پر مبنی ہے اور اسے GenAI کے دور میں تیار کیا گیا۔ دونوں کے استعمال میں مماثلت ہے۔ اگر آپ کو ایمبیڈڈ ہارڈویئر اور ایپلیکیشن لیئرز میں بہتر کارکردگی چاہیے، تو ONNX آپ کا انتخاب ہو سکتا ہے۔ لیکن اگر آپ llama.cpp کے ڈیریویٹو فریم ورک اور ٹیکنالوجی استعمال کرتے ہیں، تو GGUF بہتر ہو سکتا ہے۔

### **Phi-3.5-Instruct کو llama.cpp کے ذریعے کوانٹائز کرنا**

**1. ماحول کی ترتیب**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. کوانٹائزیشن**

llama.cpp کا استعمال کرتے ہوئے Phi-3.5-Instruct کو FP16 GGUF میں تبدیل کریں


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Phi-3.5 کو INT4 میں کوانٹائز کرنا


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. ٹیسٹنگ**

llama-cpp-python انسٹال کریں


```bash

pip install llama-cpp-python -U

```

***نوٹ*** 

اگر آپ ایپل سلیکان استعمال کرتے ہیں، تو براہ کرم llama-cpp-python کو اس طرح انسٹال کریں


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

ٹیسٹنگ 


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **وسائل**

1. llama.cpp کے بارے میں مزید جانیں [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. GGUF کے بارے میں مزید جانیں [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**ڈس کلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ اگرچہ ہم درستگی کی پوری کوشش کرتے ہیں، براہ کرم یہ بات ذہن میں رکھیں کہ خودکار ترجمے میں غلطیاں یا خامیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔