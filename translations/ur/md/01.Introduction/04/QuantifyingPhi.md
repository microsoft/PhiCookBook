<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-07-16T21:42:22+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ur"
}
-->
# **Phi فیملی کی مقدار بندی**

ماڈل کی مقدار بندی سے مراد نیورل نیٹ ورک ماڈل کے پیرامیٹرز (جیسے وزن اور ایکٹیویشن ویلیوز) کو ایک بڑے ویلیو رینج (عام طور پر مسلسل ویلیو رینج) سے ایک چھوٹے محدود ویلیو رینج میں منتقل کرنے کا عمل ہے۔ یہ ٹیکنالوجی ماڈل کے سائز اور حسابی پیچیدگی کو کم کر سکتی ہے اور موبائل ڈیوائسز یا ایمبیڈڈ سسٹمز جیسے محدود وسائل والے ماحول میں ماڈل کی کارکردگی کو بہتر بنا سکتی ہے۔ ماڈل کی مقدار بندی پیرامیٹرز کی درستگی کو کم کر کے کمپریشن حاصل کرتی ہے، لیکن اس کے نتیجے میں کچھ حد تک درستگی کا نقصان بھی ہوتا ہے۔ لہٰذا مقدار بندی کے عمل میں ماڈل کے سائز، حسابی پیچیدگی، اور درستگی کے درمیان توازن قائم کرنا ضروری ہے۔ عام مقدار بندی کے طریقے میں فکسڈ پوائنٹ مقدار بندی، فلوٹنگ پوائنٹ مقدار بندی وغیرہ شامل ہیں۔ آپ مخصوص حالات اور ضروریات کے مطابق مناسب مقدار بندی کی حکمت عملی منتخب کر سکتے ہیں۔

ہم GenAI ماڈل کو ایج ڈیوائسز پر تعینات کرنا چاہتے ہیں تاکہ مزید ڈیوائسز GenAI کے منظرناموں میں شامل ہو سکیں، جیسے موبائل ڈیوائسز، AI PC/Copilot+PC، اور روایتی IoT ڈیوائسز۔ مقدار بندی شدہ ماڈل کے ذریعے، ہم اسے مختلف ایج ڈیوائسز پر ان کی نوعیت کے مطابق تعینات کر سکتے ہیں۔ ہارڈویئر بنانے والوں کی جانب سے فراہم کردہ ماڈل ایکسلریشن فریم ورک اور مقدار بندی ماڈل کے ساتھ مل کر، ہم بہتر SLM ایپلیکیشن منظرنامے تشکیل دے سکتے ہیں۔

مقدار بندی کے منظرنامے میں ہمارے پاس مختلف درستگیاں ہوتی ہیں (INT4, INT8, FP16, FP32)۔ ذیل میں عام استعمال ہونے والی مقدار بندی کی درستگیوں کی وضاحت دی گئی ہے۔

### **INT4**

INT4 مقدار بندی ایک انتہائی سخت مقدار بندی کا طریقہ ہے جو ماڈل کے وزن اور ایکٹیویشن ویلیوز کو 4-بٹ انٹیجرز میں تبدیل کرتا ہے۔ INT4 مقدار بندی عام طور پر کم درستگی کی وجہ سے زیادہ نقصان کا باعث بنتی ہے کیونکہ اس کی نمائندگی کی حد چھوٹی اور درستگی کم ہوتی ہے۔ تاہم، INT8 مقدار بندی کے مقابلے میں، INT4 مقدار بندی ماڈل کی اسٹوریج کی ضروریات اور حسابی پیچیدگی کو مزید کم کر سکتی ہے۔ یہ بات قابل ذکر ہے کہ عملی استعمال میں INT4 مقدار بندی نسبتاً کم ہوتی ہے کیونکہ بہت کم درستگی ماڈل کی کارکردگی میں نمایاں کمی کا سبب بن سکتی ہے۔ اس کے علاوہ، تمام ہارڈویئر INT4 آپریشنز کی حمایت نہیں کرتے، لہٰذا مقدار بندی کے طریقہ کار کے انتخاب میں ہارڈویئر کی مطابقت کو مدنظر رکھنا ضروری ہے۔

### **INT8**

INT8 مقدار بندی وہ عمل ہے جس میں ماڈل کے وزن اور ایکٹیویشنز کو فلوٹنگ پوائنٹ نمبرز سے 8-بٹ انٹیجرز میں تبدیل کیا جاتا ہے۔ اگرچہ INT8 انٹیجرز کی نمائندگی کی حد چھوٹی اور کم درست ہوتی ہے، یہ اسٹوریج اور حسابی ضروریات کو نمایاں طور پر کم کر سکتی ہے۔ INT8 مقدار بندی میں، ماڈل کے وزن اور ایکٹیویشن ویلیوز مقدار بندی کے عمل سے گزرتے ہیں، جس میں اسکیلنگ اور آفسیٹ شامل ہوتے ہیں تاکہ اصل فلوٹنگ پوائنٹ معلومات کو زیادہ سے زیادہ محفوظ رکھا جا سکے۔ انفرنس کے دوران، یہ مقدار بند ویلیوز دوبارہ فلوٹنگ پوائنٹ نمبرز میں تبدیل کی جاتی ہیں تاکہ حساب کیا جا سکے، اور پھر اگلے مرحلے کے لیے دوبارہ INT8 میں مقدار بند کی جاتی ہیں۔ یہ طریقہ زیادہ تر ایپلیکیشنز میں کافی درستگی فراہم کرتا ہے جبکہ اعلیٰ حسابی کارکردگی برقرار رکھتا ہے۔

### **FP16**

FP16 فارمیٹ، یعنی 16-بٹ فلوٹنگ پوائنٹ نمبرز (float16)، 32-بٹ فلوٹنگ پوائنٹ نمبرز (float32) کے مقابلے میں میموری کا نصف استعمال کرتا ہے، جو بڑے پیمانے پر ڈیپ لرننگ ایپلیکیشنز میں نمایاں فوائد رکھتا ہے۔ FP16 فارمیٹ کے ذریعے ایک ہی GPU میموری حدود میں بڑے ماڈلز لوڈ کرنا یا زیادہ ڈیٹا پروسیس کرنا ممکن ہوتا ہے۔ جدید GPU ہارڈویئر FP16 آپریشنز کی حمایت جاری رکھے ہوئے ہے، اس لیے FP16 فارمیٹ کا استعمال کمپیوٹنگ کی رفتار میں بھی بہتری لا سکتا ہے۔ تاہم، FP16 فارمیٹ کی اپنی محدودیاں بھی ہیں، یعنی کم درستگی، جو بعض صورتوں میں عددی عدم استحکام یا درستگی کے نقصان کا باعث بن سکتی ہے۔

### **FP32**

FP32 فارمیٹ زیادہ درستگی فراہم کرتا ہے اور وسیع رینج کی ویلیوز کو درست طریقے سے ظاہر کر سکتا ہے۔ جہاں پیچیدہ ریاضیاتی آپریشنز کیے جاتے ہیں یا اعلیٰ درستگی کے نتائج درکار ہوتے ہیں، وہاں FP32 فارمیٹ کو ترجیح دی جاتی ہے۔ تاہم، زیادہ درستگی کا مطلب زیادہ میموری کا استعمال اور طویل حسابی وقت بھی ہوتا ہے۔ بڑے پیمانے پر ڈیپ لرننگ ماڈلز کے لیے، خاص طور پر جب ماڈل کے بہت سے پیرامیٹرز اور بہت زیادہ ڈیٹا ہو، FP32 فارمیٹ GPU میموری کی کمی یا انفرنس کی رفتار میں کمی کا باعث بن سکتا ہے۔

موبائل ڈیوائسز یا IoT ڈیوائسز پر، ہم Phi-3.x ماڈلز کو INT4 میں تبدیل کر سکتے ہیں، جبکہ AI PC / Copilot PC پر زیادہ درستگی جیسے INT8، FP16، FP32 استعمال کی جا سکتی ہے۔

اس وقت مختلف ہارڈویئر بنانے والے جنریٹو ماڈلز کی حمایت کے لیے مختلف فریم ورکس فراہم کرتے ہیں، جیسے Intel کا OpenVINO، Qualcomm کا QNN، Apple کا MLX، اور Nvidia کا CUDA وغیرہ، جنہیں ماڈل کی مقدار بندی کے ساتھ مل کر لوکل تعیناتی مکمل کی جا سکتی ہے۔

ٹیکنالوجی کے لحاظ سے، مقدار بندی کے بعد ہمارے پاس مختلف فارمیٹ کی حمایت موجود ہے، جیسے PyTorch / Tensorflow فارمیٹ، GGUF، اور ONNX۔ میں نے GGUF اور ONNX کے درمیان فارمیٹ کا موازنہ اور ایپلیکیشن منظرنامے کیے ہیں۔ یہاں میں ONNX مقدار بندی فارمیٹ کی سفارش کرتا ہوں، جسے ماڈل فریم ورک سے لے کر ہارڈویئر تک اچھی حمایت حاصل ہے۔ اس باب میں، ہم GenAI کے لیے ONNX Runtime، OpenVINO، اور Apple MLX پر ماڈل کی مقدار بندی پر توجہ دیں گے (اگر آپ کے پاس بہتر طریقہ ہو تو آپ PR جمع کروا کر ہمیں دے سکتے ہیں)۔

**اس باب میں شامل ہیں**

1. [llama.cpp کے ذریعے Phi-3.5 / 4 کی مقدار بندی](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntime کے لیے Generative AI ایکسٹینشنز کے ذریعے Phi-3.5 / 4 کی مقدار بندی](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO کے ذریعے Phi-3.5 / 4 کی مقدار بندی](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX فریم ورک کے ذریعے Phi-3.5 / 4 کی مقدار بندی](./UsingAppleMLXQuantifyingPhi.md)

**دستخطی دستبرداری**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کے ذریعے ترجمہ کی گئی ہے۔ اگرچہ ہم درستگی کے لیے کوشاں ہیں، براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا عدم درستیاں ہو سکتی ہیں۔ اصل دستاویز اپنی مادری زبان میں ہی معتبر ماخذ سمجھی جانی چاہیے۔ اہم معلومات کے لیے پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کی ذمہ داری ہم پر عائد نہیں ہوتی۔