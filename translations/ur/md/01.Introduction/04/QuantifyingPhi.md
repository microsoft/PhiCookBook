<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-05T17:11:48+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ur"
}
-->
# **فی خاندان کی مقدار کا تعین**

ماڈل کا مقداری بنانا اس عمل کو کہتے ہیں جس میں نیورل نیٹ ورک ماڈل کے پیرامیٹرز (جیسے وزن اور ایکٹیویشن ویلیوز) کو ایک بڑے ویلیو رینج (عام طور پر ایک لگاتار ویلیو رینج) سے ایک چھوٹے محدود ویلیو رینج میں نقشہ بند کیا جاتا ہے۔ یہ ٹیکنالوجی ماڈل کے سائز اور کمپیوٹیشنل پیچیدگی کو کم کر سکتی ہے اور موبائل ڈیوائسز یا ایمبیڈڈ سسٹمز جیسے محدود وسائل والے ماحول میں ماڈل کی کارکردگی کو بہتر بنا سکتی ہے۔ ماڈل مقداری بنانا پیرامیٹرز کی درستگی کو کم کرکے کمپریشن حاصل کرتا ہے، لیکن یہ درستگی میں کچھ نقصان بھی لاتا ہے۔ لہٰذا، مقدار کے عمل میں ماڈل کے سائز، کمپیوٹیشنل پیچیدگی، اور درستگی کے درمیان توازن ضروری ہے۔ عام مقداری طریقے میں فکسڈ پوائنٹ مقداری، فلوٹنگ پوائنٹ مقداری وغیرہ شامل ہیں۔ آپ مخصوص منظر نامے اور ضرورت کے مطابق مناسب مقداری حکمت عملی کا انتخاب کر سکتے ہیں۔

ہم امید کرتے ہیں کہ جین اے آئی ماڈل کو ایج ڈیوائسز پر تعینات کریں گے اور مزید ڈیوائسز کو جین اے آئی کے منظرناموں میں لائیں گے، جیسے کہ موبائل ڈیوائسز، اے آئی پی سی/کوپائلٹ+پی سی، اور روایتی آئی او ٹی ڈیوائسز۔ مقداری ماڈل کے ذریعے، ہم اسے مختلف ڈیوائسز کی بنیاد پر مختلف ایج ڈیوائسز پر تعینات کر سکتے ہیں۔ ہارڈویئر مینوفیکچررز کی جانب سے فراہم کردہ ماڈل تیز کرنے کے فریم ورک اور مقداری ماڈل کے ساتھ مل کر، ہم بہتر ایس ایل ایم درخواست کے منظرنامے تشکیل دے سکتے ہیں۔

مقداری منظرنامے میں، ہمارے پاس مختلف درستگیاں (INT4, INT8, FP16, FP32) ہوتی ہیں۔ ذیل میں عام طور پر استعمال ہونے والی مقداری درستگیوں کی وضاحت ہے۔

### **INT4**

INT4 مقداری ایک سخت مقدار کا طریقہ ہے جو ماڈل کے وزن اور ایکٹیویشن ویلیوز کو 4-بٹ کے انٹیجرز میں مقداری بناتا ہے۔ INT4 مقداری عام طور پر درستگی میں زیادہ نقصان کا باعث بنتی ہے کیونکہ نمائندگی کی حد چھوٹی اور درستگی کم ہوتی ہے۔ تاہم، INT8 مقداری کے مقابلے میں، INT4 مقداری اسٹوریج کی ضروریات اور کمپیوٹیشنل پیچیدگی کو اور زیادہ کم کر سکتی ہے۔ یہ قابل غور ہے کہ INT4 مقداری عملی استعمال میں نسبتاً کم ہوتی ہے کیونکہ بہت کم درستگی ماڈل کی کارکردگی میں نمایاں کمی کا سبب بن سکتی ہے۔ علاوہ ازیں، تمام ہارڈویئر INT4 آپریشنز کی حمایت نہیں کرتے، لہٰذا مقداری طریقہ منتخب کرتے وقت ہارڈویئر کی مطابقت پر غور کرنا ضروری ہے۔

### **INT8**

INT8 مقداری عمل ہے جس میں ماڈل کے وزن اور ایکٹیویشن کو فلوٹنگ پوائنٹ نمبروں سے 8-بٹ انٹیجرز میں تبدیل کیا جاتا ہے۔ اگرچہ INT8 انٹیجرز کی نمائندگی کا عددی سلسلہ چھوٹا اور کم درست ہوتا ہے، یہ ذخیرہ اور کیلکولیشن کی ضروریات کو نمایاں طور پر کم کر سکتا ہے۔ INT8 مقداری میں، ماڈل کے وزن اور ایکٹیویشن ویلیوز مقداری عمل سے گزرتے ہیں، جن میں اسکیلنگ اور آفسیٹ شامل ہیں، تاکہ اصل فلوٹنگ پوائنٹ معلومات کو زیادہ سے زیادہ محفوظ رکھا جا سکے۔ استنباط کے دوران، یہ مقداری قیمتیں دوبارہ فلوٹنگ پوائنٹ نمبروں میں ڈی کوانٹائز کی جاتی ہیں اور پھر اگلے مرحلے کے لیے دوبارہ INT8 میں مقداری بنائی جاتی ہیں۔ یہ طریقہ زیادہ تر ایپلیکیشنز میں کافی درستگی فراہم کر سکتا ہے اور ساتھ ہی اعلی کمپیوٹیشنل کارکردگی برقرار رکھتا ہے۔

### **FP16**

FP16 فارمیٹ، یعنی 16-بٹ فلوٹنگ پوائنٹ نمبر (float16)، 32-بٹ فلوٹنگ پوائنٹ نمبروں (float32) کے مقابلے میں میموری کا نصف استعمال کرتا ہے، جو بڑے پیمانے پر ڈیپ لرننگ ایپلیکیشنز میں اہم فوائد رکھتا ہے۔ FP16 فارمیٹ بڑے ماڈلز لوڈ کرنے یا ایک ہی جی پی یو میموری کی حدود میں زیادہ ڈیٹا پروسیس کرنے کی اجازت دیتا ہے۔ چونکہ جدید GPU ہارڈویئر FP16 آپریشنز کی حمایت جاری رکھے ہوئے ہے، FP16 فارمیٹ استعمال کرنے سے کمپیوٹنگ کی رفتار میں بہتری بھی ممکن ہے۔ تاہم، FP16 فارمیٹ میں کم درستگی کی داخل ہے، جو بعض حالات میں عددی غیر استحکام یا درستگی کے نقصان کا باعث بن سکتی ہے۔

### **FP32**

FP32 فارمیٹ زیادہ درستگی فراہم کرتا ہے اور ویلیوز کی وسیع رینج کو درستگی سے ظاہر کر سکتا ہے۔ جہاں پیچیدہ ریاضیاتی عملیات کی جاتی ہیں یا اعلیٰ درستگی کے نتائج درکار ہوتے ہیں، وہاں FP32 فارمیٹ کو ترجیح دی جاتی ہے۔ تاہم، زیادہ درستگی کا مطلب زیادہ میموری استعمال اور طویل حساب کتاب کا وقت بھی ہوتا ہے۔ بڑے پیمانے پر ڈیپ لرننگ ماڈلز کے لیے، خاص طور پر جب بہت سے ماڈل پیرامیٹرز اور بہت زیادہ ڈیٹا موجود ہو، FP32 فارمیٹ کی وجہ سے GPU کی میموری ناکافی ہو سکتی ہے یا استنباط کی رفتار کم ہو سکتی ہے۔

موبائل ڈیوائسز یا آئی او ٹی ڈیوائسز پر، ہم Phi-3.x ماڈلز کو INT4 میں تبدیل کر سکتے ہیں، جبکہ AI PC / Copilot PC زیادہ درستگی جیسے INT8، FP16، FP32 استعمال کر سکتے ہیں۔

فی الحال، مختلف ہارڈویئر مینوفیکچررز کے پاس جینیریٹیو ماڈلز کو سپورٹ کرنے کے مختلف فریم ورک ہیں، جیسے Intel کا OpenVINO، Qualcomm کا QNN، Apple کا MLX، اور Nvidia کا CUDA وغیرہ، جو ماڈل مقداری کے ساتھ مل کر مقامی تعیناتی مکمل کرتے ہیں۔

ٹیکنالوجی کے لحاظ سے، مقداری کے بعد ہمارے پاس مختلف فارمیٹ سپورٹ ہوتی ہے، جیسے PyTorch / TensorFlow فارمیٹ، GGUF، اور ONNX۔ میں نے GGUF اور ONNX کے درمیان فارمیٹ موازنہ اور درخواست کے منظرنامے کیے ہیں۔ یہاں میں ONNX مقداری فارمیٹ کی سفارش کرتا ہوں، جس کو ماڈل فریم ورک سے لے کر ہارڈویئر تک اچھا سپورٹ حاصل ہے۔ اس باب میں، ہم GenAI کے لیے ONNX Runtime، OpenVINO، اور Apple MLX پر ماڈل مقداری پر توجہ دیں گے (اگر آپ کے پاس بہتر طریقہ ہے تو اسے PR جمع کروا کر ہمارے ساتھ شیئر کیا جا سکتا ہے)۔

**یہ باب شامل ہے**

1. [llama.cpp استعمال کرتے ہوئے Phi-3.5 / 4 کی مقداری](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntime کے لیے Generative AI توسیعات استعمال کرتے ہوئے Phi-3.5 / 4 کی مقداری](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO استعمال کرتے ہوئے Phi-3.5 / 4 کی مقداری](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX Framework استعمال کرتے ہوئے Phi-3.5 / 4 کی مقداری](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**دستخطی نوٹ**:  
اس دستاویز کا ترجمہ اے آئی ترجمہ خدمت [Co-op Translator](https://github.com/Azure/co-op-translator) کے ذریعے کیا گیا ہے۔ اگرچہ ہم درستگی کی کوشش کرتے ہیں، براہ کرم یہ یاد رکھیں کہ خودکار ترجمے میں غلطیاں یا عدم صحت شامل ہو سکتی ہیں۔ اصل دستاویز اپنی مادری زبان میں مستند ماخذ سمجھی جانی چاہیے۔ اہم معلومات کے لیے پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تعبیر کے ذمہ دار نہیں ہیں۔
<!-- CO-OP TRANSLATOR DISCLAIMER END -->