<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-05-07T14:49:03+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ur"
}
-->
# **کوانٹفائنگ فی فیملی**

ماڈل کوانٹائزیشن سے مراد ایک نیورل نیٹ ورک ماڈل میں پیرامیٹرز (جیسے وزن اور ایکٹیویشن ویلیوز) کو ایک بڑے ویلیو رینج (عام طور پر ایک مسلسل ویلیو رینج) سے ایک چھوٹے محدود ویلیو رینج میں منتقل کرنے کا عمل ہے۔ یہ ٹیکنالوجی ماڈل کے سائز اور کمپیوٹیشنل پیچیدگی کو کم کر سکتی ہے اور موبائل ڈیوائسز یا ایمبیڈڈ سسٹمز جیسے وسائل محدود ماحول میں ماڈل کی کارکردگی کو بہتر بنا سکتی ہے۔ ماڈل کوانٹائزیشن پیرامیٹرز کی پریسیژن کو کم کر کے کمپریشن حاصل کرتی ہے، لیکن اس سے پریسیژن میں کچھ نقصان بھی ہوتا ہے۔ لہٰذا، کوانٹائزیشن کے عمل میں ماڈل کے سائز، کمپیوٹیشنل پیچیدگی، اور پریسیژن کے درمیان توازن ضروری ہوتا ہے۔ عام کوانٹائزیشن طریقے میں فکسڈ پوائنٹ کوانٹائزیشن، فلوٹنگ پوائنٹ کوانٹائزیشن وغیرہ شامل ہیں۔ آپ مخصوص منظرنامے اور ضروریات کے مطابق مناسب کوانٹائزیشن حکمت عملی منتخب کر سکتے ہیں۔

ہم چاہتے ہیں کہ GenAI ماڈل کو ایج ڈیوائسز پر ڈیپلائے کریں اور مزید ڈیوائسز کو GenAI کے منظرناموں میں لائیں، جیسے موبائل ڈیوائسز، AI PC/Copilot+PC، اور روایتی IoT ڈیوائسز۔ کوانٹائزیشن ماڈل کے ذریعے، ہم اسے مختلف ایج ڈیوائسز پر مختلف ڈیوائسز کی بنیاد پر ڈیپلائے کر سکتے ہیں۔ ہارڈویئر مینوفیکچررز کی طرف سے فراہم کردہ ماڈل ایکسیلیریشن فریم ورک اور کوانٹائزیشن ماڈل کے ساتھ مل کر، ہم بہتر SLM ایپلیکیشن منظرنامے تشکیل دے سکتے ہیں۔

کوانٹائزیشن منظرنامے میں، ہمارے پاس مختلف پریسیژنز ہیں (INT4, INT8, FP16, FP32)۔ ذیل میں عام استعمال ہونے والی کوانٹائزیشن پریسیژنز کی وضاحت ہے۔

### **INT4**

INT4 کوانٹائزیشن ایک سخت کوانٹائزیشن طریقہ ہے جو ماڈل کے وزن اور ایکٹیویشن ویلیوز کو 4-بٹ انٹیجرز میں کوانٹائز کرتا ہے۔ INT4 کوانٹائزیشن عموماً کم نمائندگی رینج اور کم پریسیژن کی وجہ سے زیادہ پریسیژن نقصان کا باعث بنتی ہے۔ تاہم، INT8 کوانٹائزیشن کے مقابلے میں، INT4 کوانٹائزیشن ماڈل کی اسٹوریج ضروریات اور کمپیوٹیشنل پیچیدگی کو مزید کم کر سکتی ہے۔ یہ بات قابل ذکر ہے کہ عملی استعمال میں INT4 کوانٹائزیشن نسبتاً کم ہوتی ہے، کیونکہ بہت کم درستگی ماڈل کی کارکردگی میں نمایاں کمی کا سبب بن سکتی ہے۔ علاوہ ازیں، تمام ہارڈویئر INT4 آپریشنز کو سپورٹ نہیں کرتے، اس لیے کوانٹائزیشن طریقہ منتخب کرتے وقت ہارڈویئر کی مطابقت کو مدنظر رکھنا ضروری ہے۔

### **INT8**

INT8 کوانٹائزیشن ماڈل کے وزن اور ایکٹیویشنز کو فلوٹنگ پوائنٹ نمبرز سے 8-بٹ انٹیجرز میں تبدیل کرنے کا عمل ہے۔ اگرچہ INT8 انٹیجرز کی نمائندگی کی حد کم اور کم درست ہوتی ہے، یہ اسٹوریج اور حساب کتاب کی ضروریات کو نمایاں طور پر کم کر سکتا ہے۔ INT8 کوانٹائزیشن میں، ماڈل کے وزن اور ایکٹیویشن ویلیوز کو ایک کوانٹائزیشن عمل سے گزارا جاتا ہے، جس میں اسکیلنگ اور آفسیٹ شامل ہوتے ہیں تاکہ اصل فلوٹنگ پوائنٹ معلومات کو زیادہ سے زیادہ محفوظ رکھا جا سکے۔ انفرنس کے دوران، یہ کوانٹائزڈ ویلیوز دوبارہ فلوٹنگ پوائنٹ نمبرز میں ڈی کوانٹائز کی جاتی ہیں، پھر اگلے مرحلے کے لیے دوبارہ INT8 میں کوانٹائز کی جاتی ہیں۔ یہ طریقہ زیادہ تر ایپلیکیشنز میں کافی درستگی فراہم کرتا ہے جبکہ اعلیٰ کمپیوٹیشنل کارکردگی برقرار رکھتا ہے۔

### **FP16**

FP16 فارمیٹ، یعنی 16-بٹ فلوٹنگ پوائنٹ نمبرز (float16)، 32-بٹ فلوٹنگ پوائنٹ نمبرز (float32) کے مقابلے میں میموری کا نصف حصہ استعمال کرتا ہے، جو بڑے پیمانے پر ڈیپ لرننگ ایپلیکیشنز میں نمایاں فوائد رکھتا ہے۔ FP16 فارمیٹ بڑے ماڈلز کو لوڈ کرنے یا ایک ہی GPU میموری کی حدود میں زیادہ ڈیٹا پروسیس کرنے کی اجازت دیتا ہے۔ چونکہ جدید GPU ہارڈویئر FP16 آپریشنز کو سپورٹ کرتا رہتا ہے، FP16 فارمیٹ کا استعمال کمپیوٹنگ کی رفتار میں بھی بہتری لا سکتا ہے۔ تاہم، FP16 فارمیٹ کی اپنی محدودیاں بھی ہیں، جیسے کم پریسیژن، جو بعض حالات میں عددی عدم استحکام یا پریسیژن کے نقصان کا باعث بن سکتی ہے۔

### **FP32**

FP32 فارمیٹ اعلیٰ پریسیژن فراہم کرتا ہے اور وسیع رینج کی ویلیوز کو درست طور پر ظاہر کر سکتا ہے۔ ایسے منظرناموں میں جہاں پیچیدہ ریاضیاتی عملیات انجام دی جاتی ہیں یا اعلیٰ درستگی کے نتائج درکار ہوتے ہیں، FP32 فارمیٹ کو ترجیح دی جاتی ہے۔ تاہم، زیادہ درستگی کا مطلب زیادہ میموری کا استعمال اور لمبا حساب کتاب کا وقت بھی ہوتا ہے۔ بڑے پیمانے پر ڈیپ لرننگ ماڈلز کے لیے، خاص طور پر جب ماڈل پیرامیٹرز کی تعداد زیادہ ہو اور ڈیٹا کا حجم بہت بڑا ہو، FP32 فارمیٹ GPU میموری کی کمی یا انفرنس کی رفتار میں کمی کا باعث بن سکتا ہے۔

موبائل ڈیوائسز یا IoT ڈیوائسز پر، ہم Phi-3.x ماڈلز کو INT4 میں تبدیل کر سکتے ہیں، جبکہ AI PC / Copilot PC پر زیادہ پریسیژن جیسے INT8, FP16, FP32 استعمال کی جا سکتی ہے۔

فی الحال، مختلف ہارڈویئر مینوفیکچررز کے پاس جنریٹو ماڈلز کی سپورٹ کے لیے مختلف فریم ورکس ہیں، جیسے Intel کا OpenVINO، Qualcomm کا QNN، Apple کا MLX، اور Nvidia کا CUDA وغیرہ، جو ماڈل کوانٹائزیشن کے ساتھ مل کر مقامی ڈیپلائےمنٹ مکمل کرتے ہیں۔

ٹیکنالوجی کے لحاظ سے، کوانٹائزیشن کے بعد ہمارے پاس مختلف فارمیٹ سپورٹ ہے، جیسے PyTorch / Tensorflow فارمیٹ، GGUF، اور ONNX۔ میں نے GGUF اور ONNX کے درمیان فارمیٹ موازنہ اور اطلاقی منظرنامے کیے ہیں۔ یہاں میں ONNX کوانٹائزیشن فارمیٹ کی سفارش کرتا ہوں، جسے ماڈل فریم ورک سے لے کر ہارڈویئر تک اچھا سپورٹ حاصل ہے۔ اس باب میں، ہم GenAI کے لیے ONNX Runtime، OpenVINO، اور Apple MLX پر ماڈل کوانٹائزیشن پر توجہ دیں گے (اگر آپ کے پاس بہتر طریقہ ہو تو آپ PR جمع کروا کر ہمیں دے سکتے ہیں)۔

**اس باب میں شامل ہیں**

1. [Quantizing Phi-3.5 / 4 using llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Quantizing Phi-3.5 / 4 using Generative AI extensions for onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Quantizing Phi-3.5 / 4 using Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Quantizing Phi-3.5 / 4 using Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

**ڈسکلیمر**:  
اس دستاویز کا ترجمہ AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کے ذریعے کیا گیا ہے۔ اگرچہ ہم درستگی کی کوشش کرتے ہیں، براہ کرم اس بات سے آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا بے ضابطگیاں ہو سکتی ہیں۔ اصل دستاویز اپنی مادری زبان میں معتبر ماخذ سمجھی جانی چاہیے۔ اہم معلومات کے لیے پیشہ ورانہ انسانی ترجمہ تجویز کیا جاتا ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے لیے ذمہ دار نہیں ہیں۔