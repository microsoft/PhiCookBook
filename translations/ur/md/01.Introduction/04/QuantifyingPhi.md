<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "418c693c63cc0e817dc560558f730a7a",
  "translation_date": "2025-04-03T07:00:40+00:00",
  "source_file": "md\\01.Introduction\\04\\QuantifyingPhi.md",
  "language_code": "ur"
}
-->
# **فائی فیملی کی مقدار بندی**

ماڈل کی مقدار بندی سے مراد نیورل نیٹ ورک ماڈل کے پیرامیٹرز (جیسے وزن اور ایکٹیویشن ویلیوز) کو ایک بڑے ویلیو رینج (عام طور پر ایک مسلسل ویلیو رینج) سے ایک چھوٹے محدود ویلیو رینج میں میپ کرنا ہے۔ یہ ٹیکنالوجی ماڈل کے سائز اور کمپیوٹیشنل پیچیدگی کو کم کر سکتی ہے اور موبائل ڈیوائسز یا ایمبیڈڈ سسٹمز جیسے وسائل محدود ماحول میں ماڈل کی آپریٹنگ کارکردگی کو بہتر بنا سکتی ہے۔ مقدار بندی ماڈل کے پیرامیٹرز کی درستگی کو کم کرکے کمپریشن حاصل کرتی ہے، لیکن اس سے کچھ حد تک درستگی کا نقصان بھی ہوتا ہے۔ لہٰذا، مقدار بندی کے عمل میں ماڈل کے سائز، کمپیوٹیشنل پیچیدگی، اور درستگی کے درمیان توازن قائم کرنا ضروری ہے۔ عام مقدار بندی کے طریقوں میں فکسڈ پوائنٹ مقدار بندی، فلوٹنگ پوائنٹ مقدار بندی وغیرہ شامل ہیں۔ آپ خاص منظر نامے اور ضروریات کے مطابق مناسب مقدار بندی کی حکمت عملی کا انتخاب کر سکتے ہیں۔

ہم چاہتے ہیں کہ GenAI ماڈل کو ایج ڈیوائسز پر ڈیپلائی کریں اور زیادہ سے زیادہ ڈیوائسز کو GenAI کے منظرناموں میں داخل ہونے دیں، جیسے موبائل ڈیوائسز، AI پی سی/کوپائلٹ+پی سی، اور روایتی IoT ڈیوائسز۔ مقدار بندی ماڈل کے ذریعے، ہم اسے مختلف ایج ڈیوائسز پر مختلف ڈیوائسز کی بنیاد پر ڈیپلائی کر سکتے ہیں۔ ہارڈویئر مینوفیکچررز کے فراہم کردہ ماڈل ایکسیلریشن فریم ورک اور مقدار بندی ماڈل کے ساتھ، ہم بہتر SLM ایپلیکیشن منظرنامے بنا سکتے ہیں۔

مقدار بندی کے منظرنامے میں، ہمارے پاس مختلف درستگیاں ہیں (INT4، INT8، FP16، FP32)۔ ذیل میں عام طور پر استعمال ہونے والی مقدار بندی کی درستگیوں کی وضاحت دی گئی ہے۔

### **INT4**

INT4 مقدار بندی ایک انتہائی مقدار بندی کا طریقہ ہے جو ماڈل کے وزن اور ایکٹیویشن ویلیوز کو 4-بِٹ انٹیجرز میں مقدار بند کرتا ہے۔ INT4 مقدار بندی عام طور پر زیادہ درستگی کا نقصان کرتی ہے کیونکہ اس کی نمائندگی کی حد چھوٹی ہوتی ہے اور درستگی کم ہوتی ہے۔ تاہم، INT8 مقدار بندی کے مقابلے میں، INT4 مقدار بندی ماڈل کے اسٹوریج کی ضروریات اور کمپیوٹیشنل پیچیدگی کو مزید کم کر سکتی ہے۔ یہ بات قابل ذکر ہے کہ عملی ایپلیکیشنز میں INT4 مقدار بندی نسبتا کم ہوتی ہے، کیونکہ بہت کم درستگی ماڈل کی کارکردگی میں نمایاں کمی کا باعث بن سکتی ہے۔ اس کے علاوہ، تمام ہارڈویئر INT4 آپریشنز کی حمایت نہیں کرتے، لہٰذا مقدار بندی کا طریقہ منتخب کرتے وقت ہارڈویئر کی مطابقت کو مدنظر رکھنا ضروری ہے۔

### **INT8**

INT8 مقدار بندی ماڈل کے وزن اور ایکٹیویشنز کو فلوٹنگ پوائنٹ نمبرز سے 8-بِٹ انٹیجرز میں تبدیل کرنے کا عمل ہے۔ اگرچہ INT8 انٹیجرز کے ذریعے نمائندگی کی گئی عددی حد چھوٹی اور کم درست ہے، یہ اسٹوریج اور حساب کی ضروریات کو نمایاں طور پر کم کر سکتی ہے۔ INT8 مقدار بندی میں، ماڈل کے وزن اور ایکٹیویشن ویلیوز مقدار بندی کے عمل سے گزرتے ہیں، جس میں اسکیلنگ اور آفسیٹ شامل ہوتا ہے تاکہ اصل فلوٹنگ پوائنٹ معلومات کو زیادہ سے زیادہ محفوظ رکھا جا سکے۔ انفیرنس کے دوران، ان مقدار بند ویلیوز کو حساب کے لیے فلوٹنگ پوائنٹ نمبرز میں دوبارہ ڈی مقدار بند کیا جاتا ہے اور پھر اگلے مرحلے کے لیے INT8 میں مقدار بند کیا جاتا ہے۔ یہ طریقہ زیادہ تر ایپلیکیشنز میں کافی درستگی فراہم کر سکتا ہے جبکہ اعلی کمپیوٹیشنل کارکردگی کو برقرار رکھتا ہے۔

### **FP16**

FP16 فارمیٹ، یعنی 16-بِٹ فلوٹنگ پوائنٹ نمبرز (float16)، 32-بِٹ فلوٹنگ پوائنٹ نمبرز (float32) کے مقابلے میں میموری کے استعمال کو آدھا کر دیتا ہے، جو بڑے پیمانے پر ڈیپ لرننگ ایپلیکیشنز میں نمایاں فوائد فراہم کرتا ہے۔ FP16 فارمیٹ بڑے ماڈلز کو لوڈ کرنے یا ایک ہی GPU میموری کی حد کے اندر مزید ڈیٹا پروسیس کرنے کی اجازت دیتا ہے۔ چونکہ جدید GPU ہارڈویئر مسلسل FP16 آپریشنز کی حمایت کرتا ہے، FP16 فارمیٹ کا استعمال کمپیوٹنگ کی رفتار میں بھی بہتری لا سکتا ہے۔ تاہم، FP16 فارمیٹ کی اپنی اندرونی خرابیاں بھی ہیں، یعنی کم درستگی، جو بعض اوقات عددی عدم استحکام یا درستگی کے نقصان کا باعث بن سکتی ہے۔

### **FP32**

FP32 فارمیٹ زیادہ درستگی فراہم کرتا ہے اور ایک وسیع حد تک ویلیوز کو درست طریقے سے ظاہر کر سکتا ہے۔ ان منظرناموں میں جہاں پیچیدہ ریاضیاتی آپریشنز انجام دیے جاتے ہیں یا اعلیٰ درستگی کے نتائج کی ضرورت ہوتی ہے، FP32 فارمیٹ کو ترجیح دی جاتی ہے۔ تاہم، زیادہ درستگی کا مطلب زیادہ میموری کا استعمال اور طویل حسابی وقت بھی ہوتا ہے۔ بڑے پیمانے پر ڈیپ لرننگ ماڈلز کے لیے، خاص طور پر جب ماڈل پیرامیٹرز کی تعداد زیادہ ہو اور ڈیٹا کی مقدار بہت بڑی ہو، FP32 فارمیٹ GPU میموری کی کمی یا انفیرنس کی رفتار میں کمی کا باعث بن سکتا ہے۔

موبائل ڈیوائسز یا IoT ڈیوائسز پر، ہم Phi-3.x ماڈلز کو INT4 میں تبدیل کر سکتے ہیں، جبکہ AI PC / Copilot PC زیادہ درستگی جیسے INT8، FP16، FP32 استعمال کر سکتے ہیں۔

فی الحال، مختلف ہارڈویئر مینوفیکچررز مختلف فریم ورک فراہم کرتے ہیں جو جنریٹیو ماڈلز کی حمایت کرتے ہیں، جیسے Intel کا OpenVINO، Qualcomm کا QNN، Apple کا MLX، اور Nvidia کا CUDA وغیرہ۔ مقدار بندی ماڈل کے ساتھ مقامی ڈیپلائیمنٹ مکمل کیا جا سکتا ہے۔

ٹیکنالوجی کے لحاظ سے، مقدار بندی کے بعد ہمارے پاس مختلف فارمیٹ سپورٹ موجود ہے، جیسے PyTorch / Tensorflow فارمیٹ، GGUF، اور ONNX۔ میں نے GGUF اور ONNX کے درمیان فارمیٹ کا موازنہ اور ایپلیکیشن منظرنامے کیے ہیں۔ یہاں میں ONNX مقدار بندی فارمیٹ کی سفارش کرتا ہوں، جس میں ماڈل فریم ورک سے لے کر ہارڈویئر تک اچھی سپورٹ موجود ہے۔ اس باب میں، ہم ONNX Runtime for GenAI، OpenVINO، اور Apple MLX پر ماڈل کی مقدار بندی پر توجہ مرکوز کریں گے (اگر آپ کے پاس کوئی بہتر طریقہ ہے تو آپ ہمیں PR جمع کروا کر دے سکتے ہیں)۔

**اس باب میں شامل ہے**

1. [Phi-3.5 / 4 کو llama.cpp استعمال کرتے ہوئے مقدار بند کرنا](./UsingLlamacppQuantifyingPhi.md)

2. [Phi-3.5 / 4 کو Generative AI extensions for onnxruntime استعمال کرتے ہوئے مقدار بند کرنا](./UsingORTGenAIQuantifyingPhi.md)

3. [Phi-3.5 / 4 کو Intel OpenVINO استعمال کرتے ہوئے مقدار بند کرنا](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Phi-3.5 / 4 کو Apple MLX Framework استعمال کرتے ہوئے مقدار بند کرنا](./UsingAppleMLXQuantifyingPhi.md)

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کے ذریعے ترجمہ کی گئی ہے۔ ہم درستگی کو یقینی بنانے کی کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔