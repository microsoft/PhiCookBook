<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:41:02+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "ur"
}
-->
# اہم ٹیکنالوجیز جن کا ذکر کیا گیا ہے

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - ایک کم سطحی API جو ہارڈویئر کی مدد سے مشین لرننگ کو تیز کرتا ہے اور DirectX 12 پر مبنی ہے۔  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia کی طرف سے تیار کردہ ایک متوازی کمپیوٹنگ پلیٹ فارم اور API ماڈل، جو گرافکس پروسیسنگ یونٹس (GPUs) پر عمومی مقصد کی پروسیسنگ کی سہولت دیتا ہے۔  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - ایک کھلا فارمیٹ جو مشین لرننگ ماڈلز کی نمائندگی کے لیے بنایا گیا ہے اور مختلف ML فریم ورکس کے درمیان انٹرآپریبلٹی فراہم کرتا ہے۔  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - ایک فارمیٹ جو مشین لرننگ ماڈلز کی نمائندگی اور اپ ڈیٹ کے لیے استعمال ہوتا ہے، خاص طور پر چھوٹے زبان کے ماڈلز کے لیے جو CPUs پر 4-8 بٹ کوانٹائزیشن کے ساتھ مؤثر طریقے سے چل سکتے ہیں۔

## DirectML

DirectML ایک کم سطحی API ہے جو ہارڈویئر کی مدد سے مشین لرننگ کو تیز کرتا ہے۔ یہ DirectX 12 کے اوپر بنایا گیا ہے تاکہ GPU کی تیز رفتاری کا فائدہ اٹھایا جا سکے اور یہ وینڈر سے آزاد ہے، یعنی مختلف GPU وینڈرز پر کام کرنے کے لیے کوڈ میں تبدیلی کی ضرورت نہیں ہوتی۔ یہ بنیادی طور پر GPUs پر ماڈل کی تربیت اور انفرنس کے کاموں کے لیے استعمال ہوتا ہے۔

ہارڈویئر کی حمایت کے حوالے سے، DirectML مختلف GPUs کے ساتھ کام کرنے کے لیے ڈیزائن کیا گیا ہے، جن میں AMD کے انٹیگریٹڈ اور ڈسکریٹ GPUs، Intel کے انٹیگریٹڈ GPUs، اور NVIDIA کے ڈسکریٹ GPUs شامل ہیں۔ یہ Windows AI Platform کا حصہ ہے اور Windows 10 اور 11 پر سپورٹڈ ہے، جو کسی بھی Windows ڈیوائس پر ماڈل کی تربیت اور انفرنس کی اجازت دیتا ہے۔

DirectML سے متعلق اپ ڈیٹس اور مواقع بھی آئے ہیں، جیسے کہ 150 ONNX آپریٹرز کی سپورٹ اور ONNX runtime اور WinML دونوں کے ذریعے استعمال۔ یہ بڑے Integrated Hardware Vendors (IHVs) کی حمایت یافتہ ہے، جو مختلف میٹا کمانڈز کو نافذ کرتے ہیں۔

## CUDA

CUDA، جس کا مطلب Compute Unified Device Architecture ہے، Nvidia کی طرف سے تیار کردہ ایک متوازی کمپیوٹنگ پلیٹ فارم اور API ماڈل ہے۔ یہ سافٹ ویئر ڈویلپرز کو CUDA-فعال گرافکس پروسیسنگ یونٹ (GPU) کو عمومی مقصد کی پروسیسنگ کے لیے استعمال کرنے کی اجازت دیتا ہے — جسے GPGPU (General-Purpose computing on Graphics Processing Units) کہا جاتا ہے۔ CUDA Nvidia کے GPU کی تیز رفتاری کا ایک اہم ذریعہ ہے اور اسے مشین لرننگ، سائنسی کمپیوٹنگ، اور ویڈیو پروسیسنگ سمیت مختلف شعبوں میں وسیع پیمانے پر استعمال کیا جاتا ہے۔

CUDA کی ہارڈویئر سپورٹ خاص طور پر Nvidia کے GPUs کے لیے مخصوص ہے، کیونکہ یہ Nvidia کی ملکیتی ٹیکنالوجی ہے۔ ہر آرکیٹیکچر CUDA ٹول کٹ کے مخصوص ورژنز کو سپورٹ کرتا ہے، جو ڈویلپرز کو CUDA ایپلیکیشنز بنانے اور چلانے کے لیے ضروری لائبریریاں اور ٹولز فراہم کرتا ہے۔

## ONNX

ONNX (Open Neural Network Exchange) ایک کھلا فارمیٹ ہے جو مشین لرننگ ماڈلز کی نمائندگی کے لیے بنایا گیا ہے۔ یہ ایک توسیع پذیر کمپیوٹیشن گراف ماڈل کی تعریف فراہم کرتا ہے، ساتھ ہی بلٹ ان آپریٹرز اور معیاری ڈیٹا ٹائپس کی تعریفیں بھی دیتا ہے۔ ONNX ڈویلپرز کو مختلف ML فریم ورکس کے درمیان ماڈلز منتقل کرنے کی اجازت دیتا ہے، جس سے انٹرآپریبلٹی ممکن ہوتی ہے اور AI ایپلیکیشنز کی تخلیق اور تعیناتی آسان ہو جاتی ہے۔

Phi3 mini ONNX Runtime کے ساتھ CPU اور GPU پر مختلف ڈیوائسز پر چل سکتا ہے، جن میں سرور پلیٹ فارمز، Windows، Linux اور Mac ڈیسک ٹاپس، اور موبائل CPUs شامل ہیں۔  
ہم نے درج ذیل بہتر شدہ کنفیگریشنز شامل کی ہیں:

- int4 DML کے لیے ONNX ماڈلز: AWQ کے ذریعے int4 کوانٹائزڈ  
- fp16 CUDA کے لیے ONNX ماڈل  
- int4 CUDA کے لیے ONNX ماڈل: RTN کے ذریعے int4 کوانٹائزڈ  
- int4 CPU اور موبائل کے لیے ONNX ماڈل: RTN کے ذریعے int4 کوانٹائزڈ  

## Llama.cpp

Llama.cpp ایک اوپن سورس سافٹ ویئر لائبریری ہے جو C++ میں لکھی گئی ہے۔ یہ مختلف بڑے زبان کے ماڈلز (LLMs) پر انفرنس انجام دیتی ہے، جن میں Llama بھی شامل ہے۔ ggml لائبریری (ایک جنرل پرپز ٹینسر لائبریری) کے ساتھ مل کر تیار کی گئی، llama.cpp کا مقصد اصل Python امپلیمنٹیشن کے مقابلے میں تیز تر انفرنس اور کم میموری استعمال فراہم کرنا ہے۔ یہ ہارڈویئر آپٹیمائزیشن، کوانٹائزیشن کی حمایت کرتا ہے، اور ایک سادہ API اور مثالیں فراہم کرتا ہے۔ اگر آپ مؤثر LLM انفرنس میں دلچسپی رکھتے ہیں تو llama.cpp کو آزمانا فائدہ مند ہے کیونکہ Phi3 Llama.cpp چلا سکتا ہے۔

## GGUF

GGUF (Generic Graph Update Format) ایک فارمیٹ ہے جو مشین لرننگ ماڈلز کی نمائندگی اور اپ ڈیٹ کے لیے استعمال ہوتا ہے۔ یہ خاص طور پر چھوٹے زبان کے ماڈلز (SLMs) کے لیے مفید ہے جو CPUs پر 4-8 بٹ کوانٹائزیشن کے ساتھ مؤثر طریقے سے چل سکتے ہیں۔ GGUF تیز پروٹوٹائپنگ اور ایج ڈیوائسز یا CI/CD پائپ لائنز جیسے بیچ جابز میں ماڈلز چلانے کے لیے فائدہ مند ہے۔

**دستخطی نوٹ**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کے ذریعے ترجمہ کی گئی ہے۔ اگرچہ ہم درستگی کے لیے کوشاں ہیں، براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا عدم درستیاں ہو سکتی ہیں۔ اصل دستاویز اپنی مادری زبان میں معتبر ماخذ سمجھی جانی چاہیے۔ اہم معلومات کے لیے پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کی ذمہ داری ہم پر عائد نہیں ہوتی۔