<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "583e1ebd3884b47b43c883072eb8fa03",
  "translation_date": "2025-04-03T06:39:35+00:00",
  "source_file": "md\\01.Introduction\\01\\01.Understandingtech.md",
  "language_code": "ur"
}
-->
# ذکر کردہ اہم ٹیکنالوجیز

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12 پر مبنی ہارڈویئر-ایکسلیریٹڈ مشین لرننگ کے لیے ایک لو-لیول API۔
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia کی جانب سے تیار کردہ ایک متوازی کمپیوٹنگ پلیٹ فارم اور ایپلیکیشن پروگرامنگ انٹرفیس (API) ماڈل جو گرافکس پروسیسنگ یونٹس (GPUs) پر جنرل-پرپز پروسیسنگ کو ممکن بناتا ہے۔
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - مشین لرننگ ماڈلز کی نمائندگی کے لیے ایک اوپن فارمیٹ جو مختلف ML فریم ورک کے درمیان انٹروپریبلٹی فراہم کرتا ہے۔
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - مشین لرننگ ماڈلز کی نمائندگی اور اپڈیٹ کرنے کے لیے ایک فارمیٹ، خاص طور پر چھوٹے زبان ماڈلز کے لیے مفید جو CPUs پر 4-8bit کوانٹائزیشن کے ساتھ مؤثر طریقے سے چل سکتے ہیں۔

## DirectML

DirectML ایک لو-لیول API ہے جو ہارڈویئر-ایکسلیریٹڈ مشین لرننگ کو ممکن بناتا ہے۔ یہ DirectX 12 پر مبنی ہے تاکہ GPU ایکسیلیریشن کا استعمال کیا جا سکے اور یہ وینڈر-اگناسٹک ہے، یعنی مختلف GPU وینڈرز پر کام کرنے کے لیے کوڈ میں تبدیلی کی ضرورت نہیں ہوتی۔ یہ بنیادی طور پر GPUs پر ماڈل ٹریننگ اور انفیرنسنگ ورک لوڈز کے لیے استعمال ہوتا ہے۔

جہاں تک ہارڈویئر سپورٹ کی بات ہے، DirectML AMD انٹیگریٹڈ اور ڈسکریٹ GPUs، Intel انٹیگریٹڈ GPUs، اور NVIDIA ڈسکریٹ GPUs کے ساتھ کام کرنے کے لیے ڈیزائن کیا گیا ہے۔ یہ Windows AI Platform کا حصہ ہے اور Windows 10 اور 11 پر سپورٹڈ ہے، جس سے کسی بھی Windows ڈیوائس پر ماڈل ٹریننگ اور انفیرنسنگ ممکن ہو جاتی ہے۔

DirectML کے حوالے سے اپڈیٹس اور مواقع موجود ہیں، جیسے کہ 150 ONNX آپریٹرز تک سپورٹ کرنا اور ONNX رن ٹائم اور WinML کے ذریعے استعمال ہونا۔ یہ اہم Integrated Hardware Vendors (IHVs) کی حمایت یافتہ ہے، جو مختلف میٹاکمانڈز کو نافذ کرتے ہیں۔

## CUDA

CUDA، جس کا مطلب Compute Unified Device Architecture ہے، Nvidia کی جانب سے تیار کردہ ایک متوازی کمپیوٹنگ پلیٹ فارم اور ایپلیکیشن پروگرامنگ انٹرفیس (API) ماڈل ہے۔ یہ سافٹ ویئر ڈیولپرز کو CUDA-ان ایبلڈ گرافکس پروسیسنگ یونٹ (GPU) کو جنرل پرپز پروسیسنگ کے لیے استعمال کرنے کی اجازت دیتا ہے – جسے GPGPU (General-Purpose computing on Graphics Processing Units) کہا جاتا ہے۔ CUDA Nvidia کے GPU ایکسیلیریشن کا ایک اہم عنصر ہے اور مشین لرننگ، سائنسی کمپیوٹنگ، اور ویڈیو پروسیسنگ سمیت مختلف شعبوں میں وسیع پیمانے پر استعمال ہوتا ہے۔

CUDA کی ہارڈویئر سپورٹ Nvidia کے GPUs تک محدود ہے کیونکہ یہ Nvidia کی جانب سے تیار کردہ ایک پراپرائٹری ٹیکنالوجی ہے۔ ہر آرکیٹیکچر CUDA ٹول کٹ کے مخصوص ورژنز کو سپورٹ کرتا ہے، جو ڈیولپرز کو CUDA ایپلیکیشنز بنانے اور چلانے کے لیے ضروری لائبریریاں اور ٹولز فراہم کرتا ہے۔

## ONNX

ONNX (Open Neural Network Exchange) مشین لرننگ ماڈلز کی نمائندگی کے لیے ایک اوپن فارمیٹ ہے۔ یہ ایک ایکسٹینسیبل کمپیوٹیشن گراف ماڈل کی تعریف فراہم کرتا ہے، ساتھ ہی بلٹ-ان آپریٹرز اور اسٹینڈرڈ ڈیٹا ٹائپس کی تعریف بھی۔ ONNX ڈیولپرز کو مختلف ML فریم ورک کے درمیان ماڈلز منتقل کرنے کی اجازت دیتا ہے، انٹروپریبلٹی کو ممکن بناتا ہے اور AI ایپلیکیشنز کی تخلیق اور ڈیپلائمنٹ کو آسان بناتا ہے۔

Phi3 mini ONNX رن ٹائم کے ساتھ CPU اور GPU پر مختلف ڈیوائسز پر چل سکتا ہے، جن میں سرور پلیٹ فارمز، Windows، Linux اور Mac ڈیسک ٹاپس، اور موبائل CPUs شامل ہیں۔
ہم نے درج ذیل بہتر کنفیگریشنز شامل کی ہیں:

- ONNX ماڈلز کے لیے int4 DML: AWQ کے ذریعے int4 پر کوانٹائز کیا گیا
- fp16 CUDA کے لیے ONNX ماڈل
- int4 CUDA کے لیے ONNX ماڈل: RTN کے ذریعے int4 پر کوانٹائز کیا گیا
- int4 CPU اور موبائل کے لیے ONNX ماڈل: RTN کے ذریعے int4 پر کوانٹائز کیا گیا

## Llama.cpp

Llama.cpp ایک اوپن سورس سافٹ ویئر لائبریری ہے جو C++ میں لکھی گئی ہے۔ یہ مختلف بڑے زبان ماڈلز (LLMs) پر انفیرنس کرتا ہے، بشمول Llama۔ ggml لائبریری (ایک جنرل-پرپز ٹینسر لائبریری) کے ساتھ تیار کردہ، llama.cpp کا مقصد اصل Python امپلیمینٹیشن کے مقابلے میں تیز تر انفیرنس اور کم میموری استعمال فراہم کرنا ہے۔ یہ ہارڈویئر آپٹیمائزیشن، کوانٹائزیشن، اور ایک سادہ API اور مثالیں پیش کرتا ہے۔ اگر آپ مؤثر LLM انفیرنس میں دلچسپی رکھتے ہیں، تو llama.cpp کو ضرور دیکھیں کیونکہ Phi3 Llama.cpp چلا سکتا ہے۔

## GGUF

GGUF (Generic Graph Update Format) مشین لرننگ ماڈلز کی نمائندگی اور اپڈیٹ کرنے کے لیے ایک فارمیٹ ہے۔ یہ خاص طور پر چھوٹے زبان ماڈلز (SLMs) کے لیے مفید ہے جو CPUs پر 4-8bit کوانٹائزیشن کے ساتھ مؤثر طریقے سے چل سکتے ہیں۔ GGUF تیز پروٹوٹائپنگ اور ایج ڈیوائسز یا CI/CD پائپ لائنز جیسے بیچ جابز پر ماڈلز چلانے کے لیے فائدہ مند ہے۔

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا خامیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والے کسی بھی غلط فہمی یا غلط تشریح کے لیے ہم ذمہ دار نہیں ہیں۔