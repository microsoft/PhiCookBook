<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-07T14:59:02+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "ur"
}
-->
# شامل کردہ اہم ٹیکنالوجیز

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - ایک کم سطحی API جو DirectX 12 کے اوپر بنایا گیا ہے اور ہارڈویئر کی مدد سے مشین لرننگ کو تیز کرتا ہے۔
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia کی طرف سے تیار کردہ ایک متوازی کمپیوٹنگ پلیٹ فارم اور API ماڈل جو گرافکس پروسیسنگ یونٹس (GPUs) پر عمومی پروسیسنگ کی اجازت دیتا ہے۔
3. [ONNX](https://onnx.ai/) (اوپن نیورل نیٹ ورک ایکسچینج) - ایک کھلا فارمیٹ جو مشین لرننگ ماڈلز کی نمائندگی کے لیے ڈیزائن کیا گیا ہے اور مختلف ML فریم ورکس کے درمیان انٹرآپریبلٹی فراہم کرتا ہے۔
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (جنرل گراف اپڈیٹ فارمیٹ) - ایک ایسا فارمیٹ جو مشین لرننگ ماڈلز کی نمائندگی اور اپڈیٹ کے لیے استعمال ہوتا ہے، خاص طور پر چھوٹے زبان کے ماڈلز کے لیے جو 4-8 بٹ کوانٹائزیشن کے ساتھ CPUs پر مؤثر طریقے سے چل سکتے ہیں۔

## DirectML

DirectML ایک کم سطحی API ہے جو ہارڈویئر کی مدد سے مشین لرننگ کو ممکن بناتا ہے۔ یہ DirectX 12 کے اوپر بنایا گیا ہے تاکہ GPU کی تیز رفتاری کا فائدہ اٹھایا جا سکے اور یہ وینڈر سے آزاد ہے، یعنی مختلف GPU وینڈرز پر کام کرنے کے لیے کوڈ میں تبدیلی کی ضرورت نہیں ہوتی۔ یہ بنیادی طور پر GPU پر ماڈل کی ٹریننگ اور انفیرنس کے کاموں کے لیے استعمال ہوتا ہے۔

ہارڈویئر سپورٹ کے حوالے سے، DirectML مختلف GPUs کے ساتھ کام کرنے کے لیے ڈیزائن کیا گیا ہے، جن میں AMD کے انٹیگریٹڈ اور ڈسکریٹ GPUs، Intel کے انٹیگریٹڈ GPUs، اور NVIDIA کے ڈسکریٹ GPUs شامل ہیں۔ یہ Windows AI پلیٹ فارم کا حصہ ہے اور Windows 10 اور 11 پر سپورٹڈ ہے، جو کسی بھی Windows ڈیوائس پر ماڈل کی ٹریننگ اور انفیرنس کی اجازت دیتا ہے۔

DirectML کے حوالے سے اپڈیٹس اور مواقع بھی آئے ہیں، جیسے 150 تک ONNX آپریٹرز کی سپورٹ اور ONNX runtime اور WinML دونوں کی طرف سے استعمال۔ یہ بڑے Integrated Hardware Vendors (IHVs) کی حمایت یافتہ ہے، جو مختلف میٹاکمانڈز کو نافذ کرتے ہیں۔

## CUDA

CUDA، جس کا مطلب Compute Unified Device Architecture ہے، Nvidia کی طرف سے تیار کردہ ایک متوازی کمپیوٹنگ پلیٹ فارم اور API ماڈل ہے۔ یہ سافٹ ویئر ڈویلپرز کو اجازت دیتا ہے کہ وہ CUDA-فعال GPU کا استعمال کرتے ہوئے عمومی مقصد کی پروسیسنگ کریں — جسے GPGPU (گرافکس پروسیسنگ یونٹس پر جنرل پرپز کمپیوٹنگ) کہا جاتا ہے۔ CUDA Nvidia کی GPU تیز رفتاری کا ایک کلیدی عنصر ہے اور مختلف میدانوں میں وسیع پیمانے پر استعمال ہوتا ہے، جن میں مشین لرننگ، سائنسی کمپیوٹنگ، اور ویڈیو پروسیسنگ شامل ہیں۔

CUDA کی ہارڈویئر سپورٹ Nvidia کے GPUs تک محدود ہے کیونکہ یہ Nvidia کی ملکیتی ٹیکنالوجی ہے۔ ہر آرکیٹیکچر CUDA ٹول کٹ کے مخصوص ورژنز کو سپورٹ کرتا ہے، جو ڈویلپرز کو CUDA ایپلیکیشنز بنانے اور چلانے کے لیے ضروری لائبریریز اور ٹولز فراہم کرتا ہے۔

## ONNX

ONNX (اوپن نیورل نیٹ ورک ایکسچینج) ایک کھلا فارمیٹ ہے جو مشین لرننگ ماڈلز کی نمائندگی کے لیے ڈیزائن کیا گیا ہے۔ یہ ایک توسیع پذیر کمپیوٹیشن گراف ماڈل کی تعریف فراہم کرتا ہے، نیز بلٹ ان آپریٹرز اور معیاری ڈیٹا ٹائپس کی تعریفیں بھی۔ ONNX ڈویلپرز کو مختلف ML فریم ورکس کے درمیان ماڈلز منتقل کرنے کی اجازت دیتا ہے، جو انٹرآپریبلٹی کو ممکن بناتا ہے اور AI ایپلیکیشنز بنانے اور تعینات کرنے کو آسان بناتا ہے۔

Phi3 mini ONNX Runtime کے ساتھ CPU اور GPU پر مختلف ڈیوائسز پر چل سکتا ہے، جن میں سرور پلیٹ فارمز، Windows، Linux اور Mac ڈیسک ٹاپس، اور موبائل CPUs شامل ہیں۔
ہم نے درج ذیل بہتر شدہ کنفیگریشنز شامل کی ہیں:

- int4 DML کے لیے ONNX ماڈلز: AWQ کے ذریعے int4 میں کوانٹائز کیے گئے
- fp16 CUDA کے لیے ONNX ماڈل
- int4 CUDA کے لیے ONNX ماڈل: RTN کے ذریعے int4 میں کوانٹائز کیا گیا
- int4 CPU اور موبائل کے لیے ONNX ماڈل: RTN کے ذریعے int4 میں کوانٹائز کیا گیا

## Llama.cpp

Llama.cpp ایک اوپن سورس سافٹ ویئر لائبریری ہے جو C++ میں لکھی گئی ہے۔ یہ مختلف بڑے زبان کے ماڈلز (LLMs) پر انفیرنس انجام دیتی ہے، جن میں Llama بھی شامل ہے۔ ggml لائبریری (ایک جنرل پرپز ٹینسر لائبریری) کے ساتھ تیار کی گئی، llama.cpp اصل Python امپلیمینٹیشن کے مقابلے میں تیز تر انفیرنس اور کم میموری استعمال فراہم کرنے کا مقصد رکھتی ہے۔ یہ ہارڈویئر کی اصلاح، کوانٹائزیشن کو سپورٹ کرتی ہے، اور ایک سادہ API اور مثالیں فراہم کرتی ہے۔ اگر آپ مؤثر LLM انفیرنس میں دلچسپی رکھتے ہیں تو llama.cpp کو آزمانا فائدہ مند ہے کیونکہ Phi3 Llama.cpp چلا سکتا ہے۔

## GGUF

GGUF (جنرل گراف اپڈیٹ فارمیٹ) ایک ایسا فارمیٹ ہے جو مشین لرننگ ماڈلز کی نمائندگی اور اپڈیٹ کے لیے استعمال ہوتا ہے۔ یہ خاص طور پر چھوٹے زبان کے ماڈلز (SLMs) کے لیے مفید ہے جو 4-8 بٹ کوانٹائزیشن کے ساتھ CPUs پر مؤثر طریقے سے چل سکتے ہیں۔ GGUF تیز پروٹو ٹائپنگ اور ایج ڈیوائسز یا بیچ جابز جیسے CI/CD پائپ لائنز میں ماڈلز چلانے کے لیے فائدہ مند ہے۔

**دستبرداری**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کے ذریعے ترجمہ کی گئی ہے۔ اگرچہ ہم درستگی کے لیے کوشاں ہیں، براہ کرم آگاہ رہیں کہ خودکار ترجموں میں غلطیاں یا عدم صحت ہو سکتی ہے۔ اصل دستاویز اپنی مادری زبان میں مستند ماخذ سمجھی جانی چاہیے۔ اہم معلومات کے لیے پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کی ذمہ داری ہم پر عائد نہیں ہوتی۔