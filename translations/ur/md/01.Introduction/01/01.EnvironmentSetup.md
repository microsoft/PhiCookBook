<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-07T15:00:18+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "ur"
}
-->
# Phi-3 کے ساتھ مقامی طور پر شروع کریں

یہ گائیڈ آپ کی مدد کرے گا کہ آپ اپنا لوکل ماحول سیٹ اپ کریں تاکہ Ollama کے ذریعے Phi-3 ماڈل چلایا جا سکے۔ آپ ماڈل کو مختلف طریقوں سے چلا سکتے ہیں، جن میں GitHub Codespaces، VS Code Dev Containers، یا آپ کا لوکل ماحول شامل ہیں۔

## ماحول کی ترتیب

### GitHub Codespaces

آپ اس ٹیمپلیٹ کو GitHub Codespaces استعمال کرتے ہوئے ورچوئلی چلا سکتے ہیں۔ بٹن آپ کے براؤزر میں ویب بیسڈ VS Code انسٹینس کھولے گا:

1. ٹیمپلیٹ کھولیں (اس میں چند منٹ لگ سکتے ہیں):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. ٹرمینل ونڈو کھولیں

### VS Code Dev Containers

⚠️ یہ آپشن صرف تب کام کرے گا جب آپ کے Docker Desktop کو کم از کم 16 GB RAM مختص کی گئی ہو۔ اگر آپ کے پاس 16 GB سے کم RAM ہے، تو آپ [GitHub Codespaces آپشن](../../../../../md/01.Introduction/01) آزما سکتے ہیں یا [لوکل طور پر سیٹ اپ](../../../../../md/01.Introduction/01) کر سکتے ہیں۔

ایک متعلقہ آپشن VS Code Dev Containers ہے، جو آپ کے لوکل VS Code میں پروجیکٹ کھولے گا [Dev Containers ایکسٹینشن](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) استعمال کرتے ہوئے:

1. Docker Desktop شروع کریں (اگر پہلے انسٹال نہیں کیا تو انسٹال کریں)
2. پروجیکٹ کھولیں:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. جب VS Code کی ونڈو کھلے اور پروجیکٹ کی فائلیں ظاہر ہوں (اس میں چند منٹ لگ سکتے ہیں)، تو ٹرمینل ونڈو کھولیں۔
4. [تعیناتی کے مراحل](../../../../../md/01.Introduction/01) کے ساتھ جاری رکھیں

### لوکل ماحول

1. یقینی بنائیں کہ درج ذیل ٹولز انسٹال ہیں:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## ماڈل کا ٹیسٹ کریں

1. Ollama سے کہیں کہ phi3:mini ماڈل ڈاؤن لوڈ اور چلائے:

    ```shell
    ollama run phi3:mini
    ```

    ماڈل ڈاؤن لوڈ کرنے میں چند منٹ لگیں گے۔

2. جب آپ آؤٹ پٹ میں "success" دیکھیں، تو آپ پرامپٹ سے اس ماڈل کو پیغام بھیج سکتے ہیں۔

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. چند سیکنڈ کے بعد، آپ ماڈل سے جواب کا سلسلہ دیکھ سکیں گے۔

4. زبان کے ماڈلز کے ساتھ استعمال ہونے والی مختلف تکنیکوں کے بارے میں جاننے کے لیے، Python نوٹ بک [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) کھولیں اور ہر سیل چلائیں۔ اگر آپ نے 'phi3:mini' کے علاوہ کوئی ماڈل استعمال کیا ہے، تو فائل کے اوپر `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` کو ضرورت کے مطابق تبدیل کریں، اور آپ سسٹم میسج میں ترمیم کر سکتے ہیں یا چاہیں تو few-shot مثالیں بھی شامل کر سکتے ہیں۔

**دستخط**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کے ذریعے ترجمہ کی گئی ہے۔ اگرچہ ہم درستگی کے لیے کوشاں ہیں، براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا بے دقتیاں ہو سکتی ہیں۔ اصل دستاویز اپنی مادری زبان میں ہی مستند ماخذ سمجھی جائے۔ اہم معلومات کے لیے پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کی ذمہ داری ہم پر نہیں ہوگی۔