<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-07-16T18:07:07+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "ur"
}
-->
# Phi-3 کے ساتھ مقامی طور پر شروع کریں

یہ رہنما آپ کی مدد کرے گا کہ آپ اپنے مقامی ماحول کو Ollama کے ذریعے Phi-3 ماڈل چلانے کے لیے ترتیب دیں۔ آپ ماڈل کو مختلف طریقوں سے چلا سکتے ہیں، جن میں GitHub Codespaces، VS Code Dev Containers، یا آپ کا مقامی ماحول شامل ہے۔

## ماحول کی ترتیب

### GitHub Codespaces

آپ اس ٹیمپلیٹ کو ورچوئلی GitHub Codespaces کے ذریعے چلا سکتے ہیں۔ بٹن آپ کے براؤزر میں ویب بیسڈ VS Code کا ایک انسٹینس کھولے گا:

1. ٹیمپلیٹ کھولیں (اس میں چند منٹ لگ سکتے ہیں):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. ایک ٹرمینل ونڈو کھولیں

### VS Code Dev Containers

⚠️ یہ آپشن صرف اس صورت میں کام کرے گا جب آپ کے Docker Desktop کو کم از کم 16 GB RAM مختص کی گئی ہو۔ اگر آپ کے پاس 16 GB سے کم RAM ہے، تو آپ [GitHub Codespaces آپشن](../../../../../md/01.Introduction/01) آزما سکتے ہیں یا [مقامی طور پر اسے ترتیب دے سکتے ہیں](../../../../../md/01.Introduction/01)۔

ایک متعلقہ آپشن VS Code Dev Containers ہے، جو آپ کے مقامی VS Code میں پروجیکٹ کو [Dev Containers ایکسٹینشن](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) کے ذریعے کھولے گا:

1. Docker Desktop شروع کریں (اگر پہلے سے انسٹال نہیں کیا تو انسٹال کریں)
2. پروجیکٹ کھولیں:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. جب VS Code کی ونڈو کھلے اور پروجیکٹ فائلز ظاہر ہوں (اس میں چند منٹ لگ سکتے ہیں)، تو ایک ٹرمینل ونڈو کھولیں۔
4. [ڈپلائمنٹ کے مراحل](../../../../../md/01.Introduction/01) کے ساتھ جاری رکھیں

### مقامی ماحول

1. یقینی بنائیں کہ درج ذیل ٹولز انسٹال ہیں:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## ماڈل کا ٹیسٹ کریں

1. Ollama سے کہیں کہ phi3:mini ماڈل ڈاؤن لوڈ اور چلائے:

    ```shell
    ollama run phi3:mini
    ```

    ماڈل ڈاؤن لوڈ کرنے میں چند منٹ لگیں گے۔

2. جب آپ آؤٹ پٹ میں "success" دیکھیں، تو آپ اس ماڈل کو پرامپٹ سے پیغام بھیج سکتے ہیں۔

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. چند سیکنڈز کے بعد، آپ کو ماڈل کی طرف سے جواب کا سلسلہ نظر آئے گا۔

4. زبان کے ماڈلز کے ساتھ استعمال ہونے والی مختلف تکنیکوں کے بارے میں جاننے کے لیے، Python نوٹ بک [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) کھولیں اور ہر سیل چلائیں۔ اگر آپ نے 'phi3:mini' کے علاوہ کوئی ماڈل استعمال کیا ہے، تو پہلے سیل میں `MODEL_NAME` کو تبدیل کریں۔

5. Python سے phi3:mini ماڈل کے ساتھ بات چیت کرنے کے لیے، Python فائل [chat.py](../../../../../code/01.Introduce/chat.py) کھولیں اور اسے چلائیں۔ آپ فائل کے اوپر `MODEL_NAME` کو حسب ضرورت تبدیل کر سکتے ہیں، اور سسٹم میسج میں ترمیم یا few-shot مثالیں بھی شامل کر سکتے ہیں اگر چاہیں۔

**دستخطی دستبرداری**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کے ذریعے ترجمہ کی گئی ہے۔ اگرچہ ہم درستگی کے لیے کوشاں ہیں، براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا عدم درستیاں ہو سکتی ہیں۔ اصل دستاویز اپنی مادری زبان میں ہی معتبر ماخذ سمجھی جانی چاہیے۔ اہم معلومات کے لیے پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کی ذمہ داری ہم پر عائد نہیں ہوتی۔