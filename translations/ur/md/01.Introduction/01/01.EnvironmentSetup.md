<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7e58d5075509bcb4a65bc8370bd21a8b",
  "translation_date": "2025-04-03T06:35:18+00:00",
  "source_file": "md\\01.Introduction\\01\\01.EnvironmentSetup.md",
  "language_code": "ur"
}
-->
# Phi-3 کو مقامی طور پر استعمال کرنے کا آغاز کریں

یہ گائیڈ آپ کی مدد کرے گا کہ آپ اپنے مقامی ماحول کو Phi-3 ماڈل چلانے کے لیے Ollama کے ساتھ سیٹ اپ کریں۔ آپ ماڈل کو مختلف طریقوں سے چلا سکتے ہیں، جیسے کہ GitHub Codespaces، VS Code Dev Containers، یا اپنے مقامی ماحول میں۔

## ماحول کی ترتیب

### GitHub Codespaces

آپ GitHub Codespaces کا استعمال کرتے ہوئے اس ٹیمپلیٹ کو ورچوئلی چلا سکتے ہیں۔ یہ بٹن آپ کے براؤزر میں ایک ویب بیسڈ VS Code انسٹینس کھولے گا:

1. ٹیمپلیٹ کھولیں (یہ عمل چند منٹ لے سکتا ہے):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. ایک ٹرمینل ونڈو کھولیں

### VS Code Dev Containers

⚠️ یہ آپشن صرف اس وقت کام کرے گا جب آپ کے Docker Desktop کو کم از کم 16 GB RAM مختص کی گئی ہو۔ اگر آپ کے پاس 16 GB RAM سے کم ہے، تو آپ [GitHub Codespaces آپشن](../../../../../md/01.Introduction/01) آزما سکتے ہیں یا [مقامی سیٹ اپ](../../../../../md/01.Introduction/01) کر سکتے ہیں۔

ایک متعلقہ آپشن VS Code Dev Containers ہے، جو پروجیکٹ کو آپ کے مقامی VS Code میں [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) کے ذریعے کھولے گا:

1. Docker Desktop شروع کریں (اگر پہلے سے انسٹال نہیں ہے تو انسٹال کریں)
2. پروجیکٹ کھولیں:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. جب VS Code ونڈو کھل جائے اور پروجیکٹ فائلز نظر آنا شروع ہو جائیں (یہ چند منٹ لے سکتا ہے)، تو ایک ٹرمینل ونڈو کھولیں۔
4. [ڈپلائمنٹ کے مراحل](../../../../../md/01.Introduction/01) کے ساتھ جاری رکھیں۔

### مقامی ماحول

1. یہ یقینی بنائیں کہ درج ذیل ٹولز انسٹال ہیں:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## ماڈل کو ٹیسٹ کریں

1. Ollama سے phi3:mini ماڈل ڈاؤن لوڈ اور چلانے کو کہیں:

    ```shell
    ollama run phi3:mini
    ```

    یہ ماڈل ڈاؤن لوڈ کرنے میں چند منٹ لگیں گے۔

2. جب آپ آؤٹ پٹ میں "success" دیکھیں، تو آپ پرامپٹ سے اس ماڈل کو پیغام بھیج سکتے ہیں۔

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. چند سیکنڈز کے بعد، آپ کو ماڈل سے ایک رسپانس اسٹریم نظر آنا شروع ہو جائے گا۔

4. مختلف تکنیکوں کے بارے میں جاننے کے لیے جو لینگویج ماڈلز کے ساتھ استعمال ہوتی ہیں، Python نوٹ بک [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) کھولیں اور ہر سیل کو چلائیں۔ اگر آپ نے 'phi3:mini' کے علاوہ کوئی اور ماڈل استعمال کیا ہے، تو فائل کے شروع میں `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` کو ضرورت کے مطابق تبدیل کریں، اور آپ سسٹم میسج یا چند نمونہ مثالیں بھی شامل کر سکتے ہیں۔

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے بھرپور کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا خامیاں ہوسکتی ہیں۔ اصل دستاویز کو اس کی مقامی زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے لیے ہم ذمہ دار نہیں ہیں۔