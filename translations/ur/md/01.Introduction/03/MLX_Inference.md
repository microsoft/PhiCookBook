<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "700b9a537ce4426de5a7ccfa8e96e581",
  "translation_date": "2025-04-03T06:55:37+00:00",
  "source_file": "md\\01.Introduction\\03\\MLX_Inference.md",
  "language_code": "ur"
}
-->
# **ایپل ایم ایل ایکس فریم ورک کے ساتھ Phi-3 کی انفیرنس**

## **ایم ایل ایکس فریم ورک کیا ہے؟**

ایم ایل ایکس ایک آرے فریم ورک ہے جو ایپل سیلیکون پر مشین لرننگ ریسرچ کے لیے بنایا گیا ہے، اور یہ ایپل مشین لرننگ ریسرچ کی جانب سے پیش کیا گیا ہے۔

ایم ایل ایکس کو مشین لرننگ ریسرچرز نے مشین لرننگ ریسرچرز کے لیے ڈیزائن کیا ہے۔ یہ فریم ورک یوزر فرینڈلی ہونے کے ساتھ ساتھ ماڈلز کو ٹرین اور ڈیپلائے کرنے کے لیے موثر بھی ہے۔ فریم ورک کا ڈیزائن خود بھی تصوراتی طور پر سادہ ہے۔ ہمارا مقصد یہ ہے کہ ریسرچرز کے لیے ایم ایل ایکس کو بڑھانا اور بہتر بنانا آسان ہو تاکہ وہ نئے خیالات پر جلدی کام کر سکیں۔

ایپل سیلیکون ڈیوائسز پر ایم ایل ایکس کے ذریعے ایل ایل ایمز کو تیز کیا جا سکتا ہے، اور ماڈلز کو لوکل طور پر بہت آسانی سے چلایا جا سکتا ہے۔

## **ایم ایل ایکس کے ذریعے Phi-3-mini کی انفیرنس**

### **1. اپنا ایم ایل ایکس ماحول ترتیب دیں**

1. Python 3.11.x
2. ایم ایل ایکس لائبریری انسٹال کریں


```bash

pip install mlx-lm

```

### **2. ایم ایل ایکس کے ساتھ ٹرمینل میں Phi-3-mini چلانا**


```bash

python -m mlx_lm.generate --model microsoft/Phi-3-mini-4k-instruct --max-token 2048 --prompt  "<|user|>\nCan you introduce yourself<|end|>\n<|assistant|>"

```

نتیجہ (میرا ماحول Apple M1 Max,64GB ہے) یہ ہے:

![Terminal](../../../../../translated_images/01.0d0f100b646a4e4c4f1cd36c1a05727cd27f1e696ed642c06cf6e2c9bbf425a4.ur.png)

### **3. ٹرمینل میں ایم ایل ایکس کے ذریعے Phi-3-mini کو کوانٹائز کرنا**


```bash

python -m mlx_lm.convert --hf-path microsoft/Phi-3-mini-4k-instruct

```

***نوٹ:*** ماڈل کو mlx_lm.convert کے ذریعے کوانٹائز کیا جا سکتا ہے، اور ڈیفالٹ کوانٹائزیشن INT4 ہے۔ یہ مثال Phi-3-mini کو INT4 میں کوانٹائز کرنے کے لیے ہے۔

ماڈل کو mlx_lm.convert کے ذریعے کوانٹائز کیا جا سکتا ہے، اور ڈیفالٹ کوانٹائزیشن INT4 ہے۔ یہ مثال Phi-3-mini کو INT4 میں کوانٹائز کرنے کے لیے ہے۔ کوانٹائزیشن کے بعد، یہ ڈیفالٹ ڈائریکٹری ./mlx_model میں محفوظ ہوگا۔

ہم ٹرمینل سے ایم ایل ایکس کے ذریعے کوانٹائزڈ ماڈل کو ٹیسٹ کر سکتے ہیں


```bash

python -m mlx_lm.generate --model ./mlx_model/ --max-token 2048 --prompt  "<|user|>\nCan you introduce yourself<|end|>\n<|assistant|>"

```

نتیجہ یہ ہے:

![INT4](../../../../../translated_images/02.04e0be1f18a90a58ad47e0c9d9084ac94d0f1a8c02fa707d04dd2dfc7e9117c6.ur.png)


### **4. ایم ایل ایکس کے ساتھ Jupyter Notebook میں Phi-3-mini چلانا**


![Notebook](../../../../../translated_images/03.0cf0092fe143357656bb5a7bc6427c41d8528d772d38a82d0b2693e2a3eeb16e.ur.png)

***نوٹ:*** براہ کرم یہ مثال پڑھیں [اس لنک پر کلک کریں](../../../../../code/03.Inference/MLX/MLX_DEMO.ipynb)


## **وسائل**

1. ایپل ایم ایل ایکس فریم ورک کے بارے میں جانیں [https://ml-explore.github.io](https://ml-explore.github.io/mlx/build/html/index.html)

2. ایپل ایم ایل ایکس GitHub ریپو [https://github.com/ml-explore](https://github.com/ml-explore)

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ اگرچہ ہم درستگی کی کوشش کرتے ہیں، براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی مقامی زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔