<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f1ff728038c4f554b660a36b76cbdd6e",
  "translation_date": "2025-05-07T14:41:52+00:00",
  "source_file": "md/01.Introduction/03/overview.md",
  "language_code": "ur"
}
-->
فی-3-منی کے سیاق و سباق میں، انفرنس سے مراد وہ عمل ہے جس میں ماڈل کو استعمال کرتے ہوئے ان پٹ ڈیٹا کی بنیاد پر پیش گوئیاں کرنا یا نتائج پیدا کرنا شامل ہے۔ میں آپ کو فی-3-منی اور اس کی انفرنس صلاحیتوں کے بارے میں مزید تفصیلات فراہم کرتا ہوں۔

فی-3-منی مائیکروسافٹ کی جانب سے جاری کردہ فی-3 سیریز کے ماڈلز کا حصہ ہے۔ یہ ماڈلز چھوٹے زبان کے ماڈلز (SLMs) کے امکانات کو دوبارہ متعین کرنے کے لیے ڈیزائن کیے گئے ہیں۔

یہاں فی-3-منی اور اس کی انفرنس صلاحیتوں کے بارے میں چند اہم نکات ہیں:

## **فی-3-منی کا جائزہ:**
- فی-3-منی کا پیرامیٹر سائز 3.8 ارب ہے۔
- یہ نہ صرف روایتی کمپیوٹنگ ڈیوائسز پر چل سکتا ہے بلکہ ایج ڈیوائسز جیسے موبائل اور IoT ڈیوائسز پر بھی چلانے کے قابل ہے۔
- فی-3-منی کی ریلیز افراد اور اداروں کو مختلف ہارڈویئر ڈیوائسز پر SLMs کو تعینات کرنے کے قابل بناتی ہے، خاص طور پر وسائل محدود ماحول میں۔
- یہ مختلف ماڈل فارمیٹس کو کور کرتا ہے، جن میں روایتی PyTorch فارمیٹ، gguf فارمیٹ کا کوانٹائزڈ ورژن، اور ONNX پر مبنی کوانٹائزڈ ورژن شامل ہیں۔

## **فی-3-منی تک رسائی:**
فی-3-منی تک رسائی کے لیے، آپ کوپائلٹ ایپلیکیشن میں [Semantic Kernel](https://github.com/microsoft/SemanticKernelCookBook?WT.mc_id=aiml-138114-kinfeylo) استعمال کر سکتے ہیں۔ Semantic Kernel عام طور پر Azure OpenAI Service، Hugging Face پر اوپن سورس ماڈلز، اور مقامی ماڈلز کے ساتھ مطابقت رکھتا ہے۔
آپ کوانٹائزڈ ماڈلز کو کال کرنے کے لیے [Ollama](https://ollama.com) یا [LlamaEdge](https://llamaedge.com) بھی استعمال کر سکتے ہیں۔ Ollama انفرادی صارفین کو مختلف کوانٹائزڈ ماڈلز کال کرنے کی اجازت دیتا ہے، جبکہ LlamaEdge GGUF ماڈلز کے لیے کراس-پلیٹ فارم دستیابی فراہم کرتا ہے۔

## **کوانٹائزڈ ماڈلز:**
بہت سے صارفین مقامی انفرنس کے لیے کوانٹائزڈ ماڈلز استعمال کرنا ترجیح دیتے ہیں۔ مثال کے طور پر، آپ Ollama کو براہ راست چلا کر فی-3 ماڈل استعمال کر سکتے ہیں یا اسے آف لائن Modelfile کے ذریعے کنفیگر کر سکتے ہیں۔ Modelfile GGUF فائل کا راستہ اور پرامپٹ فارمیٹ مخصوص کرتا ہے۔

## **جنریٹو AI کے امکانات:**
فی-3-منی جیسے SLMs کو ملا کر جنریٹو AI کے نئے امکانات کھلتے ہیں۔ انفرنس صرف پہلا قدم ہے؛ یہ ماڈلز وسائل محدود، تاخیر سے بند، اور لاگت محدود حالات میں مختلف کاموں کے لیے استعمال کیے جا سکتے ہیں۔

## **فی-3-منی کے ساتھ جنریٹو AI کو انلاک کرنا: انفرنس اور تعیناتی کے لیے رہنما**  
Semantic Kernel، Ollama/LlamaEdge، اور ONNX Runtime کو استعمال کرتے ہوئے فی-3-منی ماڈلز تک رسائی اور انفرنس کرنا سیکھیں، اور مختلف ایپلیکیشن سیناریوز میں جنریٹو AI کے امکانات دریافت کریں۔

**خصوصیات**  
phi3-mini ماڈل کی انفرنس درج ذیل میں:

- [Semantic Kernel](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/semantickernel?WT.mc_id=aiml-138114-kinfeylo)
- [Ollama](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ollama?WT.mc_id=aiml-138114-kinfeylo)
- [LlamaEdge WASM](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm?WT.mc_id=aiml-138114-kinfeylo)
- [ONNX Runtime](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/onnx?WT.mc_id=aiml-138114-kinfeylo)
- [iOS](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ios?WT.mc_id=aiml-138114-kinfeylo)

خلاصہ یہ کہ، فی-3-منی ڈویلپرز کو مختلف ماڈل فارمیٹس کو دریافت کرنے اور مختلف ایپلیکیشن سیناریوز میں جنریٹو AI سے فائدہ اٹھانے کی اجازت دیتا ہے۔

**دستخطی دستبرد**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کے ذریعے ترجمہ کی گئی ہے۔ اگرچہ ہم درستگی کے لیے کوشاں ہیں، براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا نادرستی ہو سکتی ہیں۔ اصل دستاویز اپنی مادری زبان میں معتبر ماخذ سمجھی جانی چاہیے۔ اہم معلومات کے لیے پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کی ذمہ داری ہم پر عائد نہیں ہوتی۔