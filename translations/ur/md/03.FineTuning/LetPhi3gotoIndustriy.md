<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "ef0e3b9f4e65cc05e80efb30723aed40",
  "translation_date": "2025-04-03T08:27:00+00:00",
  "source_file": "md\\03.FineTuning\\LetPhi3gotoIndustriy.md",
  "language_code": "ur"
}
-->
# **Phi-3 کو ایک صنعت کا ماہر بنائیں**

Phi-3 ماڈل کو کسی صنعت میں شامل کرنے کے لیے، آپ کو صنعت کے کاروباری ڈیٹا کو Phi-3 ماڈل میں شامل کرنا ہوگا۔ ہمارے پاس دو مختلف اختیارات ہیں: پہلا RAG (Retrieval Augmented Generation) اور دوسرا Fine Tuning۔

## **RAG بمقابلہ Fine-Tuning**

### **Retrieval Augmented Generation**

RAG ڈیٹا کی بازیافت + متن کی تخلیق کا امتزاج ہے۔ ادارے کے منظم اور غیر منظم ڈیٹا کو ویکٹر ڈیٹابیس میں محفوظ کیا جاتا ہے۔ جب متعلقہ مواد کی تلاش کی جاتی ہے، تو متعلقہ خلاصہ اور مواد کو تلاش کرکے ایک سیاق و سباق بنایا جاتا ہے، اور پھر LLM/SLM کی متن مکمل کرنے کی صلاحیت کے ساتھ مواد تخلیق کیا جاتا ہے۔

### **Fine-tuning**

Fine-tuning کسی ماڈل کی بہتری پر مبنی ہے۔ اس میں ماڈل الگورتھم سے شروع کرنے کی ضرورت نہیں ہوتی، لیکن ڈیٹا کو مسلسل جمع کرنے کی ضرورت ہوتی ہے۔ اگر آپ کو صنعت کے اطلاقات میں زیادہ درست اصطلاحات اور زبان کے اظہار کی ضرورت ہے، تو Fine-tuning آپ کے لیے بہتر انتخاب ہوگا۔ لیکن اگر آپ کا ڈیٹا بار بار تبدیل ہوتا ہے، تو Fine-tuning پیچیدہ ہوسکتا ہے۔

### **انتخاب کیسے کریں**

1. اگر ہمارے جواب کے لیے بیرونی ڈیٹا کی شمولیت ضروری ہو، تو RAG بہترین انتخاب ہوگا۔

2. اگر آپ کو مستحکم اور درست صنعت کی معلومات پیدا کرنے کی ضرورت ہو، تو Fine-tuning ایک اچھا انتخاب ہوگا۔ RAG متعلقہ مواد کو ترجیح دیتا ہے لیکن ہمیشہ خاص باریکیوں کو صحیح طریقے سے نہیں سمجھ پاتا۔

3. Fine-tuning کو اعلیٰ معیار کے ڈیٹا سیٹ کی ضرورت ہوتی ہے، اور اگر یہ صرف ایک چھوٹے دائرے کے ڈیٹا پر مبنی ہو، تو اس سے زیادہ فرق نہیں پڑے گا۔ RAG زیادہ لچکدار ہے۔

4. Fine-tuning ایک "بلیک باکس" ہے، ایک قسم کی پیچیدگی، اور اس کے اندرونی میکانزم کو سمجھنا مشکل ہوتا ہے۔ لیکن RAG ڈیٹا کے ماخذ کو تلاش کرنا آسان بنا سکتا ہے، جس سے غلطیوں یا مواد کی خرابیوں کو مؤثر طریقے سے ایڈجسٹ کیا جاسکتا ہے اور بہتر شفافیت فراہم کی جاسکتی ہے۔

### **مناظر**

1. عمودی صنعتوں میں مخصوص پیشہ ورانہ الفاظ اور اظہار کی ضرورت ہوتی ہے، ***Fine-tuning*** بہترین انتخاب ہوگا۔

2. سوال و جواب کے نظام میں، مختلف معلوماتی نکات کے امتزاج شامل ہوتا ہے، ***RAG*** بہترین انتخاب ہوگا۔

3. خودکار کاروباری عمل کے امتزاج کے لیے ***RAG + Fine-tuning*** بہترین انتخاب ہوگا۔

## **RAG کا استعمال کیسے کریں**

![rag](../../../../translated_images/rag.36e7cb856f120334d577fde60c6a5d7c5eecae255dac387669303d30b4b3efa4.ur.png)

ویکٹر ڈیٹابیس ڈیٹا کا ایک مجموعہ ہے جو ریاضیاتی شکل میں محفوظ کیا جاتا ہے۔ ویکٹر ڈیٹابیس مشین لرننگ ماڈلز کو پچھلے ان پٹ یاد رکھنے میں آسانی فراہم کرتا ہے، جس سے مشین لرننگ کو تلاش، سفارشات، اور متن تخلیق جیسے استعمال کے معاملات کی حمایت کرنے کے لیے استعمال کیا جا سکتا ہے۔ ڈیٹا کو مشابہت میٹرکس کی بنیاد پر شناخت کیا جا سکتا ہے، نہ کہ صرف عین مطابق مماثلتوں کی بنیاد پر، جس سے کمپیوٹر ماڈلز ڈیٹا کے سیاق و سباق کو سمجھ سکیں۔

ویکٹر ڈیٹابیس RAG کو عملی جامہ پہنانے کی کلید ہے۔ ہم ڈیٹا کو ویکٹر اسٹوریج میں تبدیل کرسکتے ہیں جیسے ٹیکسٹ-ایمبیڈنگ-3، جینا-اے آئی-ایمبیڈنگ وغیرہ کے ذریعے۔

RAG ایپلیکیشن بنانے کے بارے میں مزید جانیں [https://github.com/microsoft/Phi-3CookBook](https://github.com/microsoft/Phi-3CookBook?WT.mc_id=aiml-138114-kinfeylo)

## **Fine-tuning کا استعمال کیسے کریں**

Fine-tuning میں عام طور پر استعمال ہونے والے الگورتھمز Lora اور QLora ہیں۔ انتخاب کیسے کریں؟
- [اس نمونہ نوٹ بک کے ذریعے مزید جانیں](../../../../code/04.Finetuning/Phi_3_Inference_Finetuning.ipynb)
- [Python FineTuning نمونہ کا مثال](../../../../code/04.Finetuning/FineTrainingScript.py)

### **Lora اور QLora**

![lora](../../../../translated_images/qlora.6aeba71122bc0c8d56ccf0bc36b861304939fee087f43c1fc6cc5c9cb8764725.ur.png)

LoRA (Low-Rank Adaptation) اور QLoRA (Quantized Low-Rank Adaptation) دونوں تکنیکیں ہیں جو بڑے زبان کے ماڈلز (LLMs) کو Parameter Efficient Fine Tuning (PEFT) کے ذریعے Fine-tune کرنے کے لیے استعمال کی جاتی ہیں۔ PEFT تکنیکیں روایتی طریقوں کے مقابلے میں ماڈلز کو زیادہ مؤثر طریقے سے تربیت دینے کے لیے ڈیزائن کی گئی ہیں۔

LoRA ایک اسٹینڈ الون Fine-tuning تکنیک ہے جو وزن اپڈیٹ میٹرکس پر کم رینک اپروکسی میشن لاگو کرکے میموری کی جگہ کو کم کرتی ہے۔ یہ تیز تربیتی وقت فراہم کرتی ہے اور کارکردگی کو روایتی Fine-tuning طریقوں کے قریب رکھتی ہے۔

QLoRA LoRA کا ایک توسیعی ورژن ہے جو مزید میموری کے استعمال کو کم کرنے کے لیے کوانٹائزیشن تکنیکوں کو شامل کرتا ہے۔ QLoRA پری ٹرین کیے گئے LLM کے وزن پیرامیٹرز کو 4-bit precision میں کوانٹائز کرتا ہے، جو LoRA کے مقابلے میں زیادہ میموری مؤثر ہے۔ تاہم، QLoRA کی تربیت LoRA کی تربیت کے مقابلے میں تقریباً 30% سست ہوتی ہے کیونکہ اضافی کوانٹائزیشن اور ڈی کوانٹائزیشن کے مراحل ہوتے ہیں۔

QLoRA کوانٹائزیشن کے دوران پیدا ہونے والی غلطیوں کو درست کرنے کے لیے LoRA کو ایک معاون کے طور پر استعمال کرتا ہے۔ QLoRA بڑے ماڈلز کو، جن میں اربوں پیرامیٹرز ہوتے ہیں، نسبتا چھوٹے اور آسانی سے دستیاب GPUs پر Fine-tune کرنے کے قابل بناتا ہے۔ مثلاً، QLoRA 70B پیرامیٹر ماڈل کو Fine-tune کرسکتا ہے جس کے لیے 36 GPUs کی ضرورت ہوتی ہے صرف 2 GPUs پر۔

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے پوری کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غلط فہمیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ماخذ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ورانہ انسانی ترجمہ تجویز کیا جاتا ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔