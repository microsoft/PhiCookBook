{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# اولاما + اوپن اے آئی + پائتھون\n",
    "\n",
    "## 1. ماڈل کا نام بتائیں\n",
    "\n",
    "اگر آپ نے \"phi3:mini\" کے علاوہ کوئی دوسرا ماڈل استعمال کیا ہے، تو نیچے دیے گئے سیل میں اس کی قدر تبدیل کریں۔ یہ متغیر نوٹ بک کے کوڈ میں استعمال ہوگا۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"phi3:mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. اوپن اے آئی کلائنٹ سیٹ اپ کریں\n",
    "\n",
    "عام طور پر اوپن اے آئی کلائنٹ کو OpenAI.com یا Azure OpenAI کے ساتھ بڑے زبان کے ماڈلز کے ساتھ تعامل کرنے کے لیے استعمال کیا جاتا ہے۔  \n",
    "تاہم، اسے Ollama کے ساتھ بھی استعمال کیا جا سکتا ہے، کیونکہ Ollama ایک اوپن اے آئی کے موافق اینڈ پوائنٹ \"http://localhost:11434/v1\" پر فراہم کرتا ہے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"nokeyneeded\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. چیٹ مکمل کریں\n",
    "\n",
    "اب ہم OpenAI SDK استعمال کر سکتے ہیں تاکہ گفتگو کے لیے جواب تیار کیا جا سکے۔ یہ درخواست بلیوں کے بارے میں ایک ہائیکو تیار کرے گی:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about a hungry cat\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. پرامپٹ انجینئرنگ\n",
    "\n",
    "زبان کے ماڈل کو بھیجا جانے والا پہلا پیغام \"سسٹم میسج\" یا \"سسٹم پرامپٹ\" کہلاتا ہے، اور یہ ماڈل کے لیے عمومی ہدایات طے کرتا ہے۔  \n",
    "آپ اپنی مرضی کا سسٹم پرامپٹ فراہم کر سکتے ہیں تاکہ زبان کے ماڈل کو مختلف انداز میں آؤٹ پٹ تیار کرنے کی رہنمائی دی جا سکے۔  \n",
    "نیچے دیے گئے `SYSTEM_MESSAGE` کو تبدیل کریں تاکہ ماڈل آپ کے پسندیدہ مشہور فلم/ٹی وی کردار کی طرح جواب دے، یا دوسرے سسٹم پرامپٹس کے لیے [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts?tab=readme-ov-file#prompts) سے تحریک حاصل کریں۔\n",
    "\n",
    "جب آپ نے سسٹم میسج کو اپنی مرضی کے مطابق بنا لیا ہو، تو `USER_MESSAGE` میں پہلا صارف سوال فراہم کریں۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "I want you to act like Elmo from Sesame Street.\n",
    "I want you to respond and answer like Elmo using the tone, manner and vocabulary that Elmo would use.\n",
    "Do not write any explanations. Only answer like Elmo.\n",
    "You must know all of the knowledge of Elmo, and nothing more.\n",
    "\"\"\"\n",
    "\n",
    "USER_MESSAGE = \"\"\"\n",
    "Hi Elmo, how are you doing today?\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. چند مثالیں\n",
    "\n",
    "زبان کے ماڈل کو رہنمائی دینے کا ایک اور طریقہ یہ ہے کہ \"چند مثالیں\" فراہم کی جائیں، یعنی سوالات اور جوابات کی ایک ترتیب جو یہ ظاہر کرے کہ ماڈل کو کیسے جواب دینا چاہیے۔\n",
    "\n",
    "نیچے دی گئی مثال میں زبان کے ماڈل کو ایک تدریسی معاون (teaching assistant) کی طرح کام کرنے کی کوشش کی گئی ہے۔ اس میں چند سوالات اور جوابات دیے گئے ہیں جو ایک تدریسی معاون دے سکتا ہے، اور پھر ماڈل کو ایک ایسا سوال دیا گیا ہے جو ایک طالب علم پوچھ سکتا ہے۔\n",
    "\n",
    "پہلے اسے آزمائیں، اور پھر `SYSTEM_MESSAGE`, `EXAMPLES`, اور `USER_MESSAGE` کو ایک نئے منظرنامے کے لیے تبدیل کریں۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are a helpful assistant that helps students with their homework.\n",
    "Instead of providing the full answer, you respond with a hint or a clue.\n",
    "\"\"\"\n",
    "\n",
    "EXAMPLES = [\n",
    "    (\n",
    "        \"What is the capital of France?\",\n",
    "        \"Can you remember the name of the city that is known for the Eiffel Tower?\"\n",
    "    ),\n",
    "    (\n",
    "        \"What is the square root of 144?\",\n",
    "        \"What number multiplied by itself equals 144?\"\n",
    "    ),\n",
    "    (   \"What is the atomic number of oxygen?\",\n",
    "        \"How many protons does an oxygen atom have?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "USER_MESSAGE = \"What is the largest planet in our solar system?\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": EXAMPLES[0][0]},\n",
    "        {\"role\": \"assistant\", \"content\": EXAMPLES[0][1]},\n",
    "        {\"role\": \"user\", \"content\": EXAMPLES[1][0]},\n",
    "        {\"role\": \"assistant\", \"content\": EXAMPLES[1][1]},\n",
    "        {\"role\": \"user\", \"content\": EXAMPLES[2][0]},\n",
    "        {\"role\": \"assistant\", \"content\": EXAMPLES[2][1]},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE},\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. معلومات کی بازیافت کے ساتھ تخلیق\n",
    "\n",
    "RAG (معلومات کی بازیافت کے ساتھ تخلیق) ایک تکنیک ہے جس کے ذریعے کسی زبان کے ماڈل کو کسی خاص موضوع کے لیے سوالات کے درست جواب دینے کے قابل بنایا جاتا ہے۔ اس میں پہلے معلومات کے ذخیرے سے متعلقہ معلومات حاصل کی جاتی ہیں اور پھر ان معلومات کی بنیاد پر جواب تخلیق کیا جاتا ہے۔\n",
    "\n",
    "ہم نے ایک مقامی CSV فائل فراہم کی ہے جس میں ہائبرڈ گاڑیوں کے بارے میں ڈیٹا موجود ہے۔ نیچے دیا گیا کوڈ CSV فائل کو پڑھتا ہے، صارف کے سوال سے متعلقہ معلومات تلاش کرتا ہے، اور پھر حاصل شدہ معلومات کی بنیاد پر جواب تخلیق کرتا ہے۔ یاد رکھیں کہ یہ پچھلی مثالوں کے مقابلے میں زیادہ وقت لے سکتا ہے کیونکہ یہ ماڈل کو زیادہ ڈیٹا بھیجتا ہے۔ اگر آپ دیکھیں کہ جواب ابھی بھی ڈیٹا پر مبنی نہیں ہے، تو آپ سسٹم انجینئرنگ آزما سکتے ہیں یا دوسرے ماڈلز استعمال کر سکتے ہیں۔ عمومی طور پر، RAG بڑے ماڈلز یا SLMs کے خاص طور پر تربیت یافتہ ورژنز کے ساتھ زیادہ مؤثر ہوتا ہے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are a helpful assistant that answers questions about cars based off a hybrid car data set.\n",
    "You must use the data set to answer the questions, you should not provide any information that is not in the provided sources.\n",
    "\"\"\"\n",
    "\n",
    "USER_MESSAGE = \"how fast is a prius?\"\n",
    "\n",
    "# Open the CSV and store in a list\n",
    "with open(\"hybrid.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    rows = list(reader)\n",
    "\n",
    "# Normalize the user question to replace punctuation and make lowercase\n",
    "normalized_message = USER_MESSAGE.lower().replace(\"?\", \"\").replace(\"(\", \" \").replace(\")\", \" \")\n",
    "\n",
    "# Search the CSV for user question using very naive search\n",
    "words = normalized_message.split()\n",
    "matches = []\n",
    "for row in rows[1:]:\n",
    "    # if the word matches any word in row, add the row to the matches\n",
    "    if any(word in row[0].lower().split() for word in words) or any(word in row[5].lower().split() for word in words):\n",
    "        matches.append(row)\n",
    "\n",
    "# Format as a markdown table, since language models understand markdown\n",
    "matches_table = \" | \".join(rows[0]) + \"\\n\" + \" | \".join(\" --- \" for _ in range(len(rows[0]))) + \"\\n\"\n",
    "matches_table += \"\\n\".join(\" | \".join(row) for row in matches)\n",
    "print(f\"Found {len(matches)} matches:\")\n",
    "print(matches_table)\n",
    "\n",
    "# Now we can use the matches to generate a response\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE + \"\\nSources: \" + matches_table},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ڈسکلیمر**:  \nیہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "coopTranslator": {
   "original_hash": "6f9e40a7dbbd892aae50aff77da4b4be",
   "translation_date": "2025-09-12T20:10:04+00:00",
   "source_file": "code/01.Introduce/ollama.ipynb",
   "language_code": "ur"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}