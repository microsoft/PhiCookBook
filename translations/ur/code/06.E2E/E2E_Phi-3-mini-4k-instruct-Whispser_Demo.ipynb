{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### تعارف:\n",
    "انٹرایکٹو Phi 3 Mini 4K انسٹرکٹ چیٹ بوٹ ایک ایسا ٹول ہے جو صارفین کو Microsoft Phi 3 Mini 4K انسٹرکٹ ڈیمو کے ساتھ متن یا آڈیو ان پٹ کے ذریعے بات چیت کرنے کی اجازت دیتا ہے۔ یہ چیٹ بوٹ مختلف کاموں کے لیے استعمال کیا جا سکتا ہے، جیسے ترجمہ، موسم کی معلومات، اور عمومی معلومات حاصل کرنا۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "اپنا Huggingface ایکسیس ٹوکن بنائیں\n",
    "\n",
    "نیا ٹوکن بنائیں  \n",
    "نیا نام فراہم کریں  \n",
    "لکھنے کی اجازت منتخب کریں  \n",
    "ٹوکن کو کاپی کریں اور محفوظ جگہ پر محفوظ کریں\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "یہ Python کوڈ دو اہم کام انجام دیتا ہے: `os` ماڈیول کو درآمد کرنا اور ایک ماحول متغیر (environment variable) سیٹ کرنا۔\n",
    "\n",
    "1. `os` ماڈیول کو درآمد کرنا:\n",
    "   - Python میں `os` ماڈیول آپریٹنگ سسٹم کے ساتھ تعامل کرنے کا ایک طریقہ فراہم کرتا ہے۔ یہ آپ کو مختلف آپریٹنگ سسٹم سے متعلق کام انجام دینے کی اجازت دیتا ہے، جیسے ماحول متغیرات تک رسائی حاصل کرنا، فائلوں اور ڈائریکٹریز کے ساتھ کام کرنا وغیرہ۔\n",
    "   - اس کوڈ میں، `os` ماڈیول کو `import` بیان کے ذریعے درآمد کیا گیا ہے۔ یہ بیان `os` ماڈیول کی فعالیت کو موجودہ Python اسکرپٹ میں استعمال کے لیے دستیاب بناتا ہے۔\n",
    "\n",
    "2. ماحول متغیر سیٹ کرنا:\n",
    "   - ماحول متغیر ایک قدر ہے جسے آپریٹنگ سسٹم پر چلنے والے پروگرامز کے ذریعے حاصل کیا جا سکتا ہے۔ یہ ترتیب کی ترتیبات یا دیگر معلومات کو ذخیرہ کرنے کا ایک طریقہ ہے جو متعدد پروگرامز کے ذریعے استعمال کی جا سکتی ہیں۔\n",
    "   - اس کوڈ میں، ایک نیا ماحول متغیر `os.environ` ڈکشنری کا استعمال کرتے ہوئے سیٹ کیا جا رہا ہے۔ ڈکشنری کی کلید `'HF_TOKEN'` ہے، اور قدر `HUGGINGFACE_TOKEN` متغیر سے تفویض کی گئی ہے۔\n",
    "   - `HUGGINGFACE_TOKEN` متغیر اس کوڈ کے بالکل اوپر بیان کیا گیا ہے، اور اسے ایک سٹرنگ قدر `\"hf_**************\"` تفویض کی گئی ہے، جو `#@param` نحو کا استعمال کرتی ہے۔ یہ نحو اکثر Jupyter نوٹ بکس میں استعمال ہوتی ہے تاکہ صارف ان پٹ اور پیرامیٹر ترتیب کو براہ راست نوٹ بک انٹرفیس میں ترتیب دے سکیں۔\n",
    "   - `'HF_TOKEN'` ماحول متغیر کو سیٹ کرنے سے، یہ پروگرام کے دیگر حصوں یا اسی آپریٹنگ سسٹم پر چلنے والے دیگر پروگرامز کے ذریعے حاصل کیا جا سکتا ہے۔\n",
    "\n",
    "مجموعی طور پر، یہ کوڈ `os` ماڈیول کو درآمد کرتا ہے اور ایک ماحول متغیر `'HF_TOKEN'` کو `HUGGINGFACE_TOKEN` متغیر میں فراہم کردہ قدر کے ساتھ سیٹ کرتا ہے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "یہ کوڈ اسنیپٹ ایک فنکشن clear_output کو بیان کرتا ہے جو Jupyter Notebook یا IPython میں موجودہ سیل کے آؤٹ پٹ کو صاف کرنے کے لیے استعمال ہوتا ہے۔ آئیے کوڈ کو تفصیل سے سمجھتے ہیں:\n",
    "\n",
    "فنکشن clear_output ایک پیرامیٹر لیتا ہے جسے wait کہا جاتا ہے، جو ایک بولین ویلیو ہے۔ ڈیفالٹ طور پر، wait کو False پر سیٹ کیا جاتا ہے۔ یہ پیرامیٹر طے کرتا ہے کہ آیا فنکشن کو موجودہ آؤٹ پٹ کو صاف کرنے سے پہلے نئے آؤٹ پٹ کے دستیاب ہونے کا انتظار کرنا چاہیے یا نہیں۔\n",
    "\n",
    "یہ فنکشن خود موجودہ سیل کے آؤٹ پٹ کو صاف کرنے کے لیے استعمال ہوتا ہے۔ Jupyter Notebook یا IPython میں، جب کوئی سیل آؤٹ پٹ پیدا کرتا ہے، جیسے پرنٹ کیا گیا متن یا گرافیکل پلاٹس، تو وہ آؤٹ پٹ سیل کے نیچے دکھایا جاتا ہے۔ clear_output فنکشن آپ کو اس آؤٹ پٹ کو صاف کرنے کی اجازت دیتا ہے۔\n",
    "\n",
    "فنکشن کی اصل عملدرآمد کوڈ اسنیپٹ میں فراہم نہیں کی گئی ہے، جیسا کہ الیپسس (...) سے ظاہر ہوتا ہے۔ الیپسس اصل کوڈ کے لیے ایک پلیس ہولڈر کی نمائندگی کرتا ہے جو آؤٹ پٹ کو صاف کرنے کا کام انجام دیتا ہے۔ فنکشن کی عملدرآمد ممکنہ طور پر Jupyter Notebook یا IPython API کے ساتھ تعامل کے ذریعے موجودہ سیل سے آؤٹ پٹ کو ہٹانے میں شامل ہو سکتی ہے۔\n",
    "\n",
    "مجموعی طور پر، یہ فنکشن Jupyter Notebook یا IPython میں موجودہ سیل کے آؤٹ پٹ کو صاف کرنے کا ایک آسان طریقہ فراہم کرتا ہے، جو انٹرایکٹو کوڈنگ سیشنز کے دوران دکھائے گئے آؤٹ پٹ کو منظم اور اپ ڈیٹ کرنا آسان بناتا ہے۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ایج ٹی ٹی ایس سروس کا استعمال کرتے ہوئے ٹیکسٹ ٹو اسپیچ (TTS) انجام دیں۔ آئیے متعلقہ فنکشنز کے نفاذ کو ایک ایک کرکے دیکھتے ہیں:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: یہ فنکشن ایک ان پٹ ویلیو لیتا ہے اور ٹی ٹی ایس وائس کے لیے ریٹ اسٹرنگ کا حساب لگاتا ہے۔ ان پٹ ویلیو تقریر کی مطلوبہ رفتار کی نمائندگی کرتا ہے، جہاں ویلیو 1 نارمل رفتار کو ظاہر کرتا ہے۔ فنکشن ان پٹ ویلیو سے 1 کو منہا کرتا ہے، اسے 100 سے ضرب دیتا ہے، اور پھر یہ فیصلہ کرتا ہے کہ آیا ان پٹ ویلیو 1 یا اس سے زیادہ ہے۔ فنکشن ریٹ اسٹرنگ کو \"{sign}{rate}\" فارمیٹ میں واپس کرتا ہے۔\n",
    "\n",
    "2. `make_chunks(input_text, language)`: یہ فنکشن ان پٹ ٹیکسٹ اور زبان کو پیرامیٹرز کے طور پر لیتا ہے۔ یہ زبان کے مخصوص اصولوں کی بنیاد پر ان پٹ ٹیکسٹ کو چنکس میں تقسیم کرتا ہے۔ اس نفاذ میں، اگر زبان \"انگریزی\" ہو، تو فنکشن ٹیکسٹ کو ہر پیریڈ (\".\") پر تقسیم کرتا ہے اور کسی بھی اضافی یا ابتدائی اسپیس کو ہٹا دیتا ہے۔ پھر یہ ہر چنک کے آخر میں ایک پیریڈ شامل کرتا ہے اور فلٹر شدہ چنکس کی فہرست واپس کرتا ہے۔\n",
    "\n",
    "3. `tts_file_name(text)`: یہ فنکشن ان پٹ ٹیکسٹ کی بنیاد پر ٹی ٹی ایس آڈیو فائل کے لیے فائل نام تیار کرتا ہے۔ یہ ٹیکسٹ پر کئی تبدیلیاں انجام دیتا ہے: اگر موجود ہو تو پیریڈ کو ہٹانا، ٹیکسٹ کو لوئر کیس میں تبدیل کرنا، ابتدائی اور اضافی اسپیس کو ہٹانا، اور اسپیس کو انڈر سکور سے تبدیل کرنا۔ پھر یہ ٹیکسٹ کو زیادہ سے زیادہ 25 کریکٹرز تک محدود کرتا ہے (اگر زیادہ ہو) یا مکمل ٹیکسٹ استعمال کرتا ہے اگر یہ خالی ہو۔ آخر میں، یہ [`uuid`] ماڈیول کا استعمال کرتے ہوئے ایک رینڈم اسٹرنگ تیار کرتا ہے اور اسے محدود ٹیکسٹ کے ساتھ \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\" فارمیٹ میں فائل نام بنانے کے لیے جوڑتا ہے۔\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: یہ فنکشن متعدد آڈیو فائلز کو ایک آڈیو فائل میں ضم کرتا ہے۔ یہ آڈیو فائلز کے راستوں کی فہرست اور آؤٹ پٹ راستہ پیرامیٹرز کے طور پر لیتا ہے۔ فنکشن ایک خالی `AudioSegment` آبجیکٹ [`merged_audio`] کے نام سے شروع کرتا ہے۔ پھر یہ ہر آڈیو فائل کے راستے کے ذریعے تکرار کرتا ہے، `pydub` لائبریری کے `AudioSegment.from_file()` طریقہ کا استعمال کرتے ہوئے آڈیو فائل کو لوڈ کرتا ہے، اور موجودہ آڈیو فائل کو [`merged_audio`] آبجیکٹ میں شامل کرتا ہے۔ آخر میں، یہ ضم شدہ آڈیو کو مخصوص آؤٹ پٹ راستے پر MP3 فارمیٹ میں ایکسپورٹ کرتا ہے۔\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: یہ فنکشن ایج ٹی ٹی ایس سروس کا استعمال کرتے ہوئے ٹی ٹی ایس آپریشن انجام دیتا ہے۔ یہ ٹیکسٹ چنکس کی فہرست، تقریر کی رفتار، وائس کا نام، اور محفوظ کرنے کا راستہ پیرامیٹرز کے طور پر لیتا ہے۔ اگر چنکس کی تعداد 1 سے زیادہ ہو، تو فنکشن انفرادی چنک آڈیو فائلز کو اسٹور کرنے کے لیے ایک ڈائریکٹری بناتا ہے۔ پھر یہ ہر چنک کے ذریعے تکرار کرتا ہے، `calculate_rate_string()` فنکشن، وائس کا نام، اور چنک ٹیکسٹ کا استعمال کرتے ہوئے ایج ٹی ٹی ایس کمانڈ تیار کرتا ہے، اور `os.system()` فنکشن کا استعمال کرتے ہوئے کمانڈ کو انجام دیتا ہے۔ اگر کمانڈ کامیابی سے انجام پائے، تو یہ تیار کردہ آڈیو فائل کے راستے کو ایک فہرست میں شامل کرتا ہے۔ تمام چنکس کو پروسیس کرنے کے بعد، یہ انفرادی آڈیو فائلز کو `merge_audio_files()` فنکشن کا استعمال کرتے ہوئے ضم کرتا ہے اور ضم شدہ آڈیو کو مخصوص محفوظ راستے پر محفوظ کرتا ہے۔ اگر صرف ایک چنک ہو، تو یہ براہ راست ایج ٹی ٹی ایس کمانڈ تیار کرتا ہے اور آڈیو کو محفوظ راستے پر محفوظ کرتا ہے۔ آخر میں، یہ تیار کردہ آڈیو فائل کے محفوظ راستے کو واپس کرتا ہے۔\n",
    "\n",
    "6. `random_audio_name_generate()`: یہ فنکشن [`uuid`] ماڈیول کا استعمال کرتے ہوئے ایک رینڈم آڈیو فائل نام تیار کرتا ہے۔ یہ ایک رینڈم UUID تیار کرتا ہے، اسے ایک اسٹرنگ میں تبدیل کرتا ہے، پہلے 8 کریکٹرز لیتا ہے، \".mp3\" ایکسٹینشن شامل کرتا ہے، اور رینڈم آڈیو فائل نام واپس کرتا ہے۔\n",
    "\n",
    "7. `talk(input_text)`: یہ فنکشن ٹی ٹی ایس آپریشن انجام دینے کے لیے مرکزی انٹری پوائنٹ ہے۔ یہ ان پٹ ٹیکسٹ کو پیرامیٹر کے طور پر لیتا ہے۔ یہ پہلے ان پٹ ٹیکسٹ کی لمبائی چیک کرتا ہے تاکہ یہ فیصلہ کیا جا سکے کہ آیا یہ ایک طویل جملہ ہے (600 کریکٹرز یا اس سے زیادہ)۔ لمبائی اور `translate_text_flag` ویریبل کی ویلیو کی بنیاد پر، یہ زبان کا تعین کرتا ہے اور `make_chunks()` فنکشن کا استعمال کرتے ہوئے ٹیکسٹ چنکس کی فہرست تیار کرتا ہے۔ پھر یہ آڈیو فائل کے لیے محفوظ راستہ تیار کرنے کے لیے `random_audio_name_generate()` فنکشن کا استعمال کرتا ہے۔ آخر میں، یہ ٹی ٹی ایس آپریشن انجام دینے کے لیے `edge_free_tts()` فنکشن کو کال کرتا ہے اور تیار کردہ آڈیو فائل کے محفوظ راستے کو واپس کرتا ہے۔\n",
    "\n",
    "مجموعی طور پر، یہ فنکشنز ان پٹ ٹیکسٹ کو چنکس میں تقسیم کرنے، آڈیو فائل کے لیے فائل نام تیار کرنے، ایج ٹی ٹی ایس سروس کا استعمال کرتے ہوئے ٹی ٹی ایس آپریشن انجام دینے، اور انفرادی آڈیو فائلز کو ایک آڈیو فائل میں ضم کرنے کے لیے مل کر کام کرتے ہیں۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "دو فنکشنز: convert_to_text اور run_text_prompt کی عملدرآمد، اور دو کلاسز: str اور Audio کی وضاحت۔\n",
    "\n",
    "convert_to_text فنکشن ایک audio_path کو ان پٹ کے طور پر لیتا ہے اور ایک ماڈل جسے whisper_model کہا جاتا ہے، کے ذریعے آڈیو کو متن میں تبدیل کرتا ہے۔ یہ فنکشن پہلے چیک کرتا ہے کہ آیا gpu کا فلیگ True پر سیٹ ہے۔ اگر ایسا ہو، تو whisper_model کو کچھ پیرامیٹرز کے ساتھ استعمال کیا جاتا ہے جیسے word_timestamps=True، fp16=True، language='English'، اور task='translate'۔ اگر gpu کا فلیگ False ہو، تو whisper_model کو fp16=False کے ساتھ استعمال کیا جاتا ہے۔ نتیجے میں حاصل ہونے والا ٹرانسکرپشن 'scan.txt' نامی فائل میں محفوظ کیا جاتا ہے اور متن کے طور پر واپس کیا جاتا ہے۔\n",
    "\n",
    "run_text_prompt فنکشن ایک message اور chat_history کو ان پٹ کے طور پر لیتا ہے۔ یہ phi_demo فنکشن کو استعمال کرتا ہے تاکہ ان پٹ میسج کی بنیاد پر ایک چیٹ بوٹ سے جواب حاصل کیا جا سکے۔ حاصل شدہ جواب کو talk فنکشن میں بھیجا جاتا ہے، جو جواب کو ایک آڈیو فائل میں تبدیل کرتا ہے اور فائل کا راستہ واپس کرتا ہے۔ Audio کلاس آڈیو فائل کو دکھانے اور چلانے کے لیے استعمال کی جاتی ہے۔ آڈیو کو IPython.display ماڈیول کے display فنکشن کے ذریعے دکھایا جاتا ہے، اور Audio آبجیکٹ autoplay=True پیرامیٹر کے ساتھ بنایا جاتا ہے تاکہ آڈیو خود بخود چلنا شروع ہو جائے۔ chat_history کو ان پٹ میسج اور حاصل شدہ جواب کے ساتھ اپڈیٹ کیا جاتا ہے، اور ایک خالی سٹرنگ اور اپڈیٹ شدہ chat_history واپس کیا جاتا ہے۔\n",
    "\n",
    "str کلاس Python کی ایک بلٹ ان کلاس ہے جو حروف کے تسلسل کی نمائندگی کرتی ہے۔ یہ مختلف طریقے فراہم کرتی ہے جو سٹرنگز کو منظم کرنے اور ان کے ساتھ کام کرنے کے لیے استعمال ہوتے ہیں، جیسے capitalize، casefold، center، count، encode، endswith، expandtabs، find، format، index، isalnum، isalpha، isascii، isdecimal، isdigit، isidentifier، islower، isnumeric، isprintable، isspace، istitle، isupper، join، ljust، lower، lstrip، partition، replace، removeprefix، removesuffix، rfind، rindex، rjust، rpartition، rsplit، rstrip، split، splitlines، startswith، strip، swapcase، title، translate، upper، zfill، اور مزید۔ یہ طریقے آپ کو تلاش، تبدیل، فارمیٹ، اور سٹرنگز کو منظم کرنے جیسے آپریشنز انجام دینے کی اجازت دیتے ہیں۔\n",
    "\n",
    "Audio کلاس ایک کسٹم کلاس ہے جو ایک آڈیو آبجیکٹ کی نمائندگی کرتی ہے۔ یہ Jupyter Notebook کے ماحول میں ایک آڈیو پلیئر بنانے کے لیے استعمال ہوتی ہے۔ کلاس مختلف پیرامیٹرز قبول کرتی ہے جیسے data، filename، url، embed، rate، autoplay، اور normalize۔ data پیرامیٹر ایک numpy array، سیمپلز کی فہرست، ایک فائل یا URL کی نمائندگی کرنے والی سٹرنگ، یا raw PCM ڈیٹا ہو سکتا ہے۔ filename پیرامیٹر آڈیو ڈیٹا کو لوڈ کرنے کے لیے ایک مقامی فائل کی وضاحت کے لیے استعمال ہوتا ہے، اور url پیرامیٹر آڈیو ڈیٹا کو ڈاؤن لوڈ کرنے کے لیے ایک URL کی وضاحت کے لیے استعمال ہوتا ہے۔ embed پیرامیٹر یہ طے کرتا ہے کہ آیا آڈیو ڈیٹا کو ایک data URI کے ذریعے ایمبیڈ کیا جائے یا اصل سورس سے حوالہ دیا جائے۔ rate پیرامیٹر آڈیو ڈیٹا کی سیمپلنگ ریٹ کی وضاحت کرتا ہے۔ autoplay پیرامیٹر یہ طے کرتا ہے کہ آیا آڈیو خود بخود چلنا شروع ہو جائے۔ normalize پیرامیٹر یہ وضاحت کرتا ہے کہ آیا آڈیو ڈیٹا کو نارملائز (ریسکیل) کیا جائے تاکہ زیادہ سے زیادہ ممکنہ رینج حاصل ہو۔ Audio کلاس reload جیسے طریقے فراہم کرتی ہے تاکہ فائل یا URL سے آڈیو ڈیٹا کو دوبارہ لوڈ کیا جا سکے، اور src_attr، autoplay_attr، اور element_id_attr جیسے attributes فراہم کرتی ہے تاکہ HTML میں آڈیو عنصر کے متعلقہ attributes کو حاصل کیا جا سکے۔\n",
    "\n",
    "مجموعی طور پر، یہ فنکشنز اور کلاسز آڈیو کو متن میں تبدیل کرنے، چیٹ بوٹ سے آڈیو جوابات حاصل کرنے، اور Jupyter Notebook کے ماحول میں آڈیو کو دکھانے اور چلانے کے لیے استعمال ہوتے ہیں۔\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ڈسکلیمر**:  \nیہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T19:52:24+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "ur"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}