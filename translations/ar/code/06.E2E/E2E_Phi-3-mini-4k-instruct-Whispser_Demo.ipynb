{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## روبوت الدردشة التفاعلي Phi 3 Mini 4K مع Whisper\n",
    "\n",
    "### المقدمة:\n",
    "روبوت الدردشة التفاعلي Phi 3 Mini 4K هو أداة تتيح للمستخدمين التفاعل مع عرض Microsoft Phi 3 Mini 4K التوضيحي باستخدام إدخال نصي أو صوتي. يمكن استخدام روبوت الدردشة لمجموعة متنوعة من المهام، مثل الترجمة، تحديثات الطقس، وجمع المعلومات العامة.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[إنشاء رمز الوصول الخاص بك على Huggingface](https://huggingface.co/settings/tokens)\n",
    "\n",
    "إنشاء رمز جديد  \n",
    "قم بتوفير اسم جديد  \n",
    "اختر أذونات الكتابة  \n",
    "انسخ الرمز واحفظه في مكان آمن  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "يشرح هذا الملف البرمجي المكتوب بلغة Python وظيفتين رئيسيتين: استيراد الوحدة `os` وتعيين متغير بيئة.\n",
    "\n",
    "1. استيراد الوحدة `os`:\n",
    "   - توفر وحدة `os` في Python طريقة للتفاعل مع نظام التشغيل. تتيح لك تنفيذ مهام مختلفة متعلقة بنظام التشغيل، مثل الوصول إلى متغيرات البيئة، العمل مع الملفات والمجلدات، وغيرها.\n",
    "   - في هذا الكود، يتم استيراد وحدة `os` باستخدام عبارة `import`. هذه العبارة تجعل وظائف وحدة `os` متاحة للاستخدام في البرنامج النصي الحالي بلغة Python.\n",
    "\n",
    "2. تعيين متغير بيئة:\n",
    "   - متغير البيئة هو قيمة يمكن الوصول إليها بواسطة البرامج التي تعمل على نظام التشغيل. وهو وسيلة لتخزين إعدادات التكوين أو معلومات أخرى يمكن استخدامها بواسطة برامج متعددة.\n",
    "   - في هذا الكود، يتم تعيين متغير بيئة جديد باستخدام قاموس `os.environ`. مفتاح القاموس هو `'HF_TOKEN'`، ويتم تعيين القيمة من المتغير `HUGGINGFACE_TOKEN`.\n",
    "   - يتم تعريف المتغير `HUGGINGFACE_TOKEN` مباشرة فوق هذا المقطع البرمجي، ويتم تعيين قيمة نصية له `\"hf_**************\"` باستخدام صيغة `#@param`. تُستخدم هذه الصيغة غالبًا في دفاتر Jupyter للسماح بإدخال المستخدم وتكوين المعلمات مباشرة من واجهة الدفتر.\n",
    "   - من خلال تعيين متغير البيئة `'HF_TOKEN'`، يمكن الوصول إليه بواسطة أجزاء أخرى من البرنامج أو برامج أخرى تعمل على نفس نظام التشغيل.\n",
    "\n",
    "بشكل عام، يقوم هذا الكود باستيراد وحدة `os` وتعيين متغير بيئة باسم `'HF_TOKEN'` بالقيمة المقدمة في المتغير `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "يعرّف هذا المقتطف البرمجي وظيفة تسمى clear_output تُستخدم لمسح المخرجات الخاصة بالخلية الحالية في Jupyter Notebook أو IPython. دعونا نفصل الكود ونفهم وظيفته:\n",
    "\n",
    "تأخذ الوظيفة clear_output معلمة واحدة تُسمى wait، وهي قيمة منطقية (boolean). بشكل افتراضي، يتم تعيين wait إلى False. تحدد هذه المعلمة ما إذا كان يجب على الوظيفة الانتظار حتى تتوفر مخرجات جديدة لاستبدال المخرجات الحالية قبل مسحها.\n",
    "\n",
    "تُستخدم الوظيفة نفسها لمسح المخرجات الخاصة بالخلية الحالية. في Jupyter Notebook أو IPython، عندما تنتج الخلية مخرجات، مثل النصوص المطبوعة أو الرسومات البيانية، يتم عرض تلك المخرجات أسفل الخلية. تتيح وظيفة clear_output مسح تلك المخرجات.\n",
    "\n",
    "لم يتم توفير تنفيذ الوظيفة في مقتطف الكود، كما هو موضح بعلامة الحذف (...). تمثل علامة الحذف مكانًا مخصصًا للكود الفعلي الذي يقوم بمسح المخرجات. قد يتضمن تنفيذ الوظيفة التفاعل مع واجهة برمجة التطبيقات الخاصة بـ Jupyter Notebook أو IPython لإزالة المخرجات الحالية من الخلية.\n",
    "\n",
    "بشكل عام، توفر هذه الوظيفة طريقة مريحة لمسح المخرجات الخاصة بالخلية الحالية في Jupyter Notebook أو IPython، مما يسهل إدارة وتحديث المخرجات المعروضة أثناء جلسات البرمجة التفاعلية.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "استخدام تحويل النص إلى كلام (TTS) باستخدام خدمة Edge TTS. دعونا نستعرض تنفيذ الوظائف ذات الصلة واحدة تلو الأخرى:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: تأخذ هذه الوظيفة قيمة الإدخال وتحسب سلسلة معدل الصوت لـ TTS. تمثل قيمة الإدخال السرعة المطلوبة للصوت، حيث تمثل القيمة 1 السرعة العادية. تقوم الوظيفة بحساب سلسلة المعدل عن طريق طرح 1 من قيمة الإدخال، ضربها في 100، ثم تحديد الإشارة بناءً على ما إذا كانت قيمة الإدخال أكبر من أو تساوي 1. تُرجع الوظيفة سلسلة المعدل بالتنسيق \"{sign}{rate}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: تأخذ هذه الوظيفة نص الإدخال ولغة كمعلمات. تقوم بتقسيم النص إلى أجزاء بناءً على قواعد اللغة المحددة. في هذا التنفيذ، إذا كانت اللغة \"English\"، تقوم الوظيفة بتقسيم النص عند كل نقطة (\".\") وتزيل أي مسافات زائدة في البداية أو النهاية. ثم تضيف نقطة إلى كل جزء وتُرجع قائمة الأجزاء المفلترة.\n",
    "\n",
    "3. `tts_file_name(text)`: تقوم هذه الوظيفة بإنشاء اسم ملف للصوت الناتج عن TTS بناءً على النص المدخل. تقوم بإجراء عدة تحويلات على النص: إزالة النقطة النهائية (إذا كانت موجودة)، تحويل النص إلى أحرف صغيرة، إزالة المسافات الزائدة في البداية والنهاية، واستبدال المسافات بالشرطات السفلية. ثم تقطع النص إلى حد أقصى 25 حرفًا (إذا كان أطول) أو تستخدم النص بالكامل إذا كان فارغًا. أخيرًا، تُنشئ سلسلة عشوائية باستخدام وحدة [`uuid`] وتدمجها مع النص المقتطع لإنشاء اسم الملف بالتنسيق \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: تقوم هذه الوظيفة بدمج ملفات صوتية متعددة في ملف صوتي واحد. تأخذ قائمة بمسارات الملفات الصوتية ومسار الإخراج كمعلمات. تقوم الوظيفة بتهيئة كائن فارغ يسمى [`merged_audio`] من نوع `AudioSegment`. ثم تقوم بالمرور عبر كل مسار ملف صوتي، تحميل الملف الصوتي باستخدام الطريقة `AudioSegment.from_file()` من مكتبة `pydub`، وإضافة الملف الصوتي الحالي إلى الكائن [`merged_audio`]. أخيرًا، تصدر الصوت المدمج إلى مسار الإخراج المحدد بتنسيق MP3.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: تقوم هذه الوظيفة بتنفيذ عملية TTS باستخدام خدمة Edge TTS. تأخذ قائمة من أجزاء النص، سرعة الصوت، اسم الصوت، ومسار الحفظ كمعلمات. إذا كان عدد الأجزاء أكبر من 1، تقوم الوظيفة بإنشاء دليل لتخزين ملفات الصوت الفردية للأجزاء. ثم تمر عبر كل جزء، تبني أمر Edge TTS باستخدام وظيفة `calculate_rate_string()`، اسم الصوت، ونص الجزء، وتنفذ الأمر باستخدام وظيفة `os.system()`. إذا نجح تنفيذ الأمر، تضيف مسار الملف الصوتي الناتج إلى قائمة. بعد معالجة جميع الأجزاء، تقوم بدمج الملفات الصوتية الفردية باستخدام وظيفة `merge_audio_files()` وتحفظ الصوت المدمج في مسار الحفظ المحدد. إذا كان هناك جزء واحد فقط، تقوم مباشرةً بإنشاء أمر Edge TTS وتحفظ الصوت في مسار الحفظ. أخيرًا، تُرجع مسار الحفظ للملف الصوتي الناتج.\n",
    "\n",
    "6. `random_audio_name_generate()`: تقوم هذه الوظيفة بإنشاء اسم ملف صوتي عشوائي باستخدام وحدة [`uuid`]. تُنشئ UUID عشوائي، تحوله إلى سلسلة نصية، تأخذ أول 8 أحرف، تضيف امتداد \".mp3\"، وتُرجع اسم الملف الصوتي العشوائي.\n",
    "\n",
    "7. `talk(input_text)`: هذه الوظيفة هي نقطة الدخول الرئيسية لتنفيذ عملية TTS. تأخذ نص الإدخال كمعلمة. أولاً، تتحقق من طول النص لتحديد ما إذا كان جملة طويلة (600 حرفًا أو أكثر). بناءً على الطول وقيمة المتغير `translate_text_flag`، تحدد اللغة وتُنشئ قائمة أجزاء النص باستخدام وظيفة `make_chunks()`. ثم تُنشئ مسار حفظ للملف الصوتي باستخدام وظيفة `random_audio_name_generate()`. أخيرًا، تستدعي وظيفة `edge_free_tts()` لتنفيذ عملية TTS وتُرجع مسار الحفظ للملف الصوتي الناتج.\n",
    "\n",
    "بشكل عام، تعمل هذه الوظائف معًا لتقسيم النص المدخل إلى أجزاء، إنشاء اسم ملف للصوت الناتج، تنفيذ عملية TTS باستخدام خدمة Edge TTS، ودمج الملفات الصوتية الفردية في ملف صوتي واحد.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تنفيذ وظيفتين: convert_to_text و run_text_prompt، بالإضافة إلى تعريف فئتين: str و Audio.\n",
    "\n",
    "وظيفة convert_to_text تأخذ مسار ملف صوتي كمدخل وتقوم بتحويل الصوت إلى نص باستخدام نموذج يسمى whisper_model. أولاً، تتحقق الوظيفة مما إذا كانت علامة gpu مضبوطة على True. إذا كانت كذلك، يتم استخدام whisper_model مع بعض المعايير مثل word_timestamps=True، fp16=True، language='English'، و task='translate'. إذا كانت علامة gpu مضبوطة على False، يتم استخدام whisper_model مع fp16=False. يتم حفظ النص الناتج في ملف باسم 'scan.txt' ويتم إرجاعه كنص.\n",
    "\n",
    "وظيفة run_text_prompt تأخذ رسالة وسجل محادثة كمدخل. تستخدم الوظيفة phi_demo لتوليد استجابة من روبوت الدردشة بناءً على الرسالة المدخلة. يتم تمرير الاستجابة الناتجة إلى وظيفة talk، التي تحول الاستجابة إلى ملف صوتي وتعيد مسار الملف. يتم استخدام الفئة Audio لعرض وتشغيل الملف الصوتي. يتم عرض الصوت باستخدام وظيفة display من وحدة IPython.display، ويتم إنشاء كائن Audio مع المعامل autoplay=True، بحيث يبدأ تشغيل الصوت تلقائيًا. يتم تحديث سجل المحادثة بالرسالة المدخلة والاستجابة الناتجة، ويتم إرجاع سلسلة فارغة وسجل المحادثة المحدث.\n",
    "\n",
    "الفئة str هي فئة مدمجة في بايثون تمثل سلسلة من الأحرف. توفر العديد من الطرق لمعالجة والعمل مع النصوص، مثل capitalize، casefold، center، count، encode، endswith، expandtabs، find، format، index، isalnum، isalpha، isascii، isdecimal، isdigit، isidentifier، islower، isnumeric، isprintable، isspace، istitle، isupper، join، ljust، lower، lstrip، partition، replace، removeprefix، removesuffix، rfind، rindex، rjust، rpartition، rsplit، rstrip، split، splitlines، startswith، strip، swapcase، title، translate، upper، zfill، والمزيد. تتيح هذه الطرق إجراء عمليات مثل البحث، الاستبدال، التنسيق، ومعالجة النصوص.\n",
    "\n",
    "الفئة Audio هي فئة مخصصة تمثل كائن صوتي. تُستخدم لإنشاء مشغل صوتي في بيئة Jupyter Notebook. تقبل الفئة العديد من المعاملات مثل data، filename، url، embed، rate، autoplay، و normalize. يمكن أن يكون المعامل data مصفوفة numpy، قائمة عينات، سلسلة تمثل اسم ملف أو عنوان URL، أو بيانات PCM خام. يتم استخدام المعامل filename لتحديد ملف محلي لتحميل بيانات الصوت منه، والمعامل url لتحديد عنوان URL لتنزيل بيانات الصوت منه. يحدد المعامل embed ما إذا كانت بيانات الصوت يجب أن تكون مضمنة باستخدام URI للبيانات أو مرجعة من المصدر الأصلي. يحدد المعامل rate معدل أخذ العينات لبيانات الصوت. يحدد المعامل autoplay ما إذا كان يجب أن يبدأ تشغيل الصوت تلقائيًا. يحدد المعامل normalize ما إذا كانت بيانات الصوت يجب أن تكون مُطَبَّعة (معاد قياسها) إلى النطاق الأقصى الممكن. توفر الفئة Audio أيضًا طرقًا مثل reload لإعادة تحميل بيانات الصوت من ملف أو عنوان URL، وخصائص مثل src_attr، autoplay_attr، و element_id_attr لاسترداد الخصائص المقابلة لعنصر الصوت في HTML.\n",
    "\n",
    "بشكل عام، تُستخدم هذه الوظائف والفئات لتحويل الصوت إلى نص، توليد استجابات صوتية من روبوت الدردشة، وعرض وتشغيل الصوت في بيئة Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**إخلاء المسؤولية**:  \nتم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق. للحصول على معلومات حاسمة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة ناتجة عن استخدام هذه الترجمة.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T19:51:41+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "ar"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}