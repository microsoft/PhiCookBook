<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c8273672cc57df2be675407a1383aaf0",
  "translation_date": "2025-07-16T17:42:23+00:00",
  "source_file": "md/01.Introduction/01/01.AISafety.md",
  "language_code": "ar"
}
-->
# أمان الذكاء الاصطناعي لنماذج Phi  
تم تطوير عائلة نماذج Phi وفقًا لـ [معيار الذكاء الاصطناعي المسؤول من مايكروسوفت](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFl)، وهو مجموعة متطلبات على مستوى الشركة تستند إلى المبادئ الستة التالية: المساءلة، الشفافية، العدالة، الموثوقية والسلامة، الخصوصية والأمان، والشمولية التي تشكل [مبادئ الذكاء الاصطناعي المسؤول من مايكروسوفت](https://www.microsoft.com/ai/responsible-ai).

مثل نماذج Phi السابقة، تم اعتماد تقييم أمان متعدد الأوجه ونهج أمان بعد التدريب، مع اتخاذ تدابير إضافية لأخذ القدرات متعددة اللغات في هذا الإصدار بعين الاعتبار. يوضح نهجنا في تدريب الأمان والتقييمات، بما في ذلك الاختبار عبر لغات وفئات مخاطر متعددة، في [ورقة أمان Phi بعد التدريب](https://arxiv.org/abs/2407.13833). بينما تستفيد نماذج Phi من هذا النهج، يجب على المطورين تطبيق أفضل ممارسات الذكاء الاصطناعي المسؤول، بما في ذلك رسم الخرائط وقياس وتخفيف المخاطر المرتبطة بحالة الاستخدام والسياق الثقافي واللغوي الخاص بهم.

## أفضل الممارسات  

مثل النماذج الأخرى، يمكن لعائلة نماذج Phi أن تتصرف بطرق قد تكون غير عادلة أو غير موثوقة أو مسيئة.

بعض السلوكيات المحدودة لنماذج SLM وLLM التي يجب أن تكون على دراية بها تشمل:

- **جودة الخدمة:** تم تدريب نماذج Phi بشكل أساسي على نصوص باللغة الإنجليزية. ستشهد اللغات غير الإنجليزية أداءً أقل جودة. قد تعاني تنويعات اللغة الإنجليزية التي تمثل تمثيلاً أقل في بيانات التدريب من أداء أسوأ مقارنة بالإنجليزية الأمريكية القياسية.  
- **تمثيل الأضرار واستمرار الصور النمطية:** يمكن لهذه النماذج أن تمثل مجموعات من الناس بشكل مفرط أو ناقص، أو تمحو تمثيل بعض المجموعات، أو تعزز الصور النمطية المهينة أو السلبية. على الرغم من وجود أمان بعد التدريب، قد تظل هذه القيود موجودة بسبب اختلاف مستويات تمثيل المجموعات المختلفة أو انتشار أمثلة الصور النمطية السلبية في بيانات التدريب التي تعكس أنماطًا واقعية وانحيازات مجتمعية.  
- **المحتوى غير المناسب أو المسيء:** قد تنتج هذه النماذج أنواعًا أخرى من المحتوى غير المناسب أو المسيء، مما قد يجعل استخدامها غير مناسب في السياقات الحساسة دون تدابير إضافية خاصة بحالة الاستخدام.  
- **موثوقية المعلومات:** يمكن لنماذج اللغة توليد محتوى غير منطقي أو اختلاق محتوى قد يبدو معقولًا لكنه غير دقيق أو قديم.  
- **نطاق محدود للبرمجة:** تعتمد غالبية بيانات تدريب Phi-3 على Python وتستخدم حزمًا شائعة مثل "typing, math, random, collections, datetime, itertools". إذا قام النموذج بإنشاء سكريبتات Python تستخدم حزمًا أخرى أو سكريبتات بلغات أخرى، نوصي بشدة بأن يتحقق المستخدمون يدويًا من جميع استخدامات API.

يجب على المطورين تطبيق أفضل ممارسات الذكاء الاصطناعي المسؤول، وهم مسؤولون عن ضمان امتثال حالة الاستخدام المحددة للقوانين واللوائح ذات الصلة (مثل الخصوصية، التجارة، إلخ).

## اعتبارات الذكاء الاصطناعي المسؤول  

مثل نماذج اللغة الأخرى، يمكن لسلسلة نماذج Phi أن تتصرف بطرق قد تكون غير عادلة أو غير موثوقة أو مسيئة. بعض السلوكيات المحدودة التي يجب الانتباه لها تشمل:

**جودة الخدمة:** تم تدريب نماذج Phi بشكل أساسي على نصوص باللغة الإنجليزية. ستشهد اللغات غير الإنجليزية أداءً أقل جودة. قد تعاني تنويعات اللغة الإنجليزية التي تمثل تمثيلاً أقل في بيانات التدريب من أداء أسوأ مقارنة بالإنجليزية الأمريكية القياسية.

**تمثيل الأضرار واستمرار الصور النمطية:** يمكن لهذه النماذج أن تمثل مجموعات من الناس بشكل مفرط أو ناقص، أو تمحو تمثيل بعض المجموعات، أو تعزز الصور النمطية المهينة أو السلبية. على الرغم من وجود أمان بعد التدريب، قد تظل هذه القيود موجودة بسبب اختلاف مستويات تمثيل المجموعات المختلفة أو انتشار أمثلة الصور النمطية السلبية في بيانات التدريب التي تعكس أنماطًا واقعية وانحيازات مجتمعية.

**المحتوى غير المناسب أو المسيء:** قد تنتج هذه النماذج أنواعًا أخرى من المحتوى غير المناسب أو المسيء، مما قد يجعل استخدامها غير مناسب في السياقات الحساسة دون تدابير إضافية خاصة بحالة الاستخدام.  
موثوقية المعلومات: يمكن لنماذج اللغة توليد محتوى غير منطقي أو اختلاق محتوى قد يبدو معقولًا لكنه غير دقيق أو قديم.

**نطاق محدود للبرمجة:** تعتمد غالبية بيانات تدريب Phi-3 على Python وتستخدم حزمًا شائعة مثل "typing, math, random, collections, datetime, itertools". إذا قام النموذج بإنشاء سكريبتات Python تستخدم حزمًا أخرى أو سكريبتات بلغات أخرى، نوصي بشدة بأن يتحقق المستخدمون يدويًا من جميع استخدامات API.

يجب على المطورين تطبيق أفضل ممارسات الذكاء الاصطناعي المسؤول، وهم مسؤولون عن ضمان امتثال حالة الاستخدام المحددة للقوانين واللوائح ذات الصلة (مثل الخصوصية، التجارة، إلخ). تشمل المجالات المهمة للنظر فيها:

**التخصيص:** قد لا تكون النماذج مناسبة للسيناريوهات التي قد يكون لها تأثير كبير على الوضع القانوني أو تخصيص الموارد أو فرص الحياة (مثل السكن، التوظيف، الائتمان، إلخ) دون تقييمات إضافية وتقنيات إزالة التحيز.

**السيناريوهات عالية المخاطر:** يجب على المطورين تقييم ملاءمة استخدام النماذج في السيناريوهات عالية المخاطر حيث قد تكون المخرجات غير العادلة أو غير الموثوقة أو المسيئة مكلفة للغاية أو تؤدي إلى ضرر. يشمل ذلك تقديم النصائح في المجالات الحساسة أو المتخصصة حيث الدقة والموثوقية أمران حاسمان (مثل النصائح القانونية أو الصحية). يجب تنفيذ تدابير حماية إضافية على مستوى التطبيق وفقًا لسياق النشر.

**المعلومات المضللة:** قد تنتج النماذج معلومات غير دقيقة. يجب على المطورين اتباع أفضل ممارسات الشفافية وإبلاغ المستخدمين النهائيين بأنهم يتفاعلون مع نظام ذكاء اصطناعي. على مستوى التطبيق، يمكن للمطورين بناء آليات تغذية راجعة وخطوط أنابيب لتأسيس الردود على معلومات سياقية خاصة بحالة الاستخدام، وهي تقنية تعرف باسم التوليد المعزز بالاسترجاع (RAG).

**توليد المحتوى الضار:** يجب على المطورين تقييم المخرجات وفقًا لسياقها واستخدام المصنفات الأمنية المتاحة أو الحلول المخصصة المناسبة لحالة الاستخدام.

**سوء الاستخدام:** قد تكون هناك أشكال أخرى من سوء الاستخدام مثل الاحتيال، الرسائل المزعجة، أو إنتاج البرمجيات الخبيثة، ويجب على المطورين التأكد من أن تطبيقاتهم لا تنتهك القوانين واللوائح المعمول بها.

### التخصيص الدقيق وأمان محتوى الذكاء الاصطناعي  

بعد تخصيص النموذج، نوصي بشدة بالاستفادة من تدابير [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) لمراقبة المحتوى الذي تولده النماذج، والتعرف على المخاطر والتهديدات المحتملة وحجبها، بالإضافة إلى معالجة قضايا الجودة.

![Phi3AISafety](../../../../../translated_images/01.phi3aisafety.c0d7fc42f5a5c405.ar.png)

يدعم [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) كلًا من المحتوى النصي والصوري. يمكن نشره في السحابة، الحاويات المنفصلة، وعلى الأجهزة الطرفية/المدمجة.

## نظرة عامة على Azure AI Content Safety  

خدمة Azure AI Content Safety ليست حلاً موحدًا يناسب الجميع؛ يمكن تخصيصها لتتوافق مع سياسات الشركات المحددة. بالإضافة إلى ذلك، تتيح نماذجها متعددة اللغات فهم عدة لغات في آن واحد.

![AIContentSafety](../../../../../translated_images/01.AIcontentsafety.a288819b8ce8da1a.ar.png)

- **Azure AI Content Safety**  
- **Microsoft Developer**  
- **5 فيديوهات**

تكتشف خدمة Azure AI Content Safety المحتوى الضار الذي ينشئه المستخدمون أو الذكاء الاصطناعي في التطبيقات والخدمات. تشمل واجهات برمجة التطبيقات للنصوص والصور التي تتيح لك اكتشاف المواد الضارة أو غير المناسبة.

[قائمة تشغيل AI Content Safety](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

**إخلاء المسؤولية**:  
تمت ترجمة هذا المستند باستخدام خدمة الترجمة الآلية [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق به. للمعلومات الهامة، يُنصح بالاعتماد على الترجمة البشرية المهنية. نحن غير مسؤولين عن أي سوء فهم أو تفسير ناتج عن استخدام هذه الترجمة.