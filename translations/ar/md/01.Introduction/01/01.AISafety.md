<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "839ccc4b3886ef10cfd4e64977f5792d",
  "translation_date": "2026-01-04T07:16:31+00:00",
  "source_file": "md/01.Introduction/01/01.AISafety.md",
  "language_code": "ar"
}
-->
# سلامة الذكاء الاصطناعي لنماذج Phi
تم تطوير عائلة نماذج Phi وفقًا لـ [المعيار المسؤول للذكاء الاصطناعي من Microsoft](https://www.microsoft.com/ai/principles-and-approach#responsible-ai-standard)، وهو مجموعة متطلبات على مستوى الشركة تستند إلى المبادئ الستة التالية: المساءلة، الشفافية، العدالة، الاعتمادية والسلامة، الخصوصية والأمن، والشمولية والتي تشكل [مبادئ Microsoft للذكاء الاصطناعي المسؤول](https://www.microsoft.com/ai/responsible-ai). 

مثل نماذج Phi السابقة، تم اعتماد تقييم متعدد الجوانب للسلامة ونهج ما بعد التدريب على السلامة، مع اتخاذ تدابير إضافية لمراعاة القدرات متعددة اللغات في هذا الإصدار. يوضح نهجنا لتدريب السلامة والتقييمات بما في ذلك الاختبار عبر لغات وفئات مخاطر متعددة في [ورقة Phi عن السلامة بعد التدريب](https://arxiv.org/abs/2407.13833). بينما تستفيد نماذج Phi من هذا النهج، يجب على المطورين تطبيق أفضل ممارسات الذكاء الاصطناعي المسؤول، بما في ذلك رسم خرائط المخاطر وقياسها والتخفيف منها المرتبطة بحالة الاستخدام الخاصة بهم والسياق الثقافي واللغوي.

## أفضل الممارسات

مثل النماذج الأخرى، يمكن لعائلة نماذج Phi أن تتصرف بطرق قد تكون غير عادلة أو غير موثوقة أو مسيئة.

بعض السلوكيات المقيدة لـ SLM و LLM التي يجب أن تكون على دراية بها تشمل:

- **جودة الخدمة:** تم تدريب نماذج Phi بشكل أساسي على نصوص باللغة الإنجليزية. ستشهد اللغات الأخرى أداءً أسوأ. قد تشهد تنوعات اللغة الإنجليزية التي تمثل تمثيلاً أقل في بيانات التدريب أداءً أسوأ من الإنجليزية الأمريكية القياسية.
- **تمثيل الأذى واستمرار القوالب النمطية:** يمكن لهذه النماذج أن تُمثل مجموعات من الأشخاص بصورة مبالغ فيها أو ناقصة، أو تمحو تمثيل بعض المجموعات، أو تعزز القوالب النمطية المهينة أو السلبية. على الرغم من ما بعد تدريب السلامة، قد تظل هذه القيود موجودة بسبب اختلاف مستويات تمثيل المجموعات المختلفة أو انتشار أمثلة القوالب النمطية السلبية في بيانات التدريب التي تعكس أنماط العالم الواقعي والتحيزات المجتمعية.
- **محتوى غير مناسب أو مسيء:** قد تنتج هذه النماذج أنواعًا أخرى من المحتوى غير المناسب أو المسيء، مما قد يجعل من غير المناسب نشرها في سياقات حساسة دون تدابير تخفيف إضافية خاصة بحالة الاستخدام.
موثوقية المعلومات: يمكن لنماذج اللغة توليد محتوى غير منطقی أو اختلاق محتوى قد يبدو معقولًا لكنه غیر دقيق أو قدیم.
- **نطاق محدود للرمز:** تعتمد غالبية بيانات تدريب Phi-3 على Python وتستخدم حزمًا شائعة مثل "typing, math, random, collections, datetime, itertools". إذا قام النموذج بتوليد سكربتات Python تستخدم حزمًا أخرى أو سكربتات بلغات أخرى، نوصي بشدة أن يتحقق المستخدمون يدويًا من جميع استخدامات API.

يجب على المطورين تطبيق أفضل ممارسات الذكاء الاصطناعي المسؤول ويكونون مسؤولين عن ضمان امتثال حالة الاستخدام المحددة للقوانين واللوائح ذات الصلة (مثل: الخصوصية، التجارة، إلخ). 

## اعتبارات الذكاء الاصطناعي المسؤول

مثل نماذج اللغة الأخرى، يمكن لسلسلة نماذج Phi أن تتصرف بطرق قد تكون غير عادلة أو غير موثوقة أو مسيئة. بعض السلوكيات المقيدة التي يجب الانتباه إليها تشمل:

**جودة الخدمة:** تم تدريب نماذج Phi بشكل أساسي على نصوص باللغة الإنجليزية. ستشهد اللغات الأخرى أداءً أسوأ. قد تشهد تنوعات اللغة الإنجليزية التي تمثل تمثيلاً أقل في بيانات التدريب أداءً أسوأ من الإنجليزية الأمريكية القياسية.

**تمثيل الأذى واستمرار القوالب النمطية:** يمكن لهذه النماذج أن تُمثل مجموعات من الأشخاص بصورة مبالغ فيها أو ناقصة، أو تمحو تمثيل بعض المجموعات، أو تعزز القوالب النمطية المهينة أو السلبية. على الرغم من ما بعد تدريب السلامة، قد تظل هذه القيود موجودة بسبب اختلاف مستويات تمثيل المجموعات المختلفة أو انتشار أمثلة القوالب النمطية السلبية في بيانات التدريب التي تعكس أنماط العالم الواقعي والتحيزات المجتمعية.

**محتوى غير مناسب أو مسيء:** قد تنتج هذه النماذج أنواعًا أخرى من المحتوى غير المناسب أو المسيء، مما قد يجعل من غير المناسب نشرها في سياقات حساسة دون تدابير تخفيف إضافية خاصة بحالة الاستخدام.
موثوقية المعلومات: يمكن لنماذج اللغة توليد محتوى غير منطقي أو اختلاق محتوى قد يبدو معقولًا لكنه غیر دقيق أو قدیم.

**نطاق محدود للرمز:** تعتمد غالبية بيانات تدريب Phi-3 على Python وتستخدم حزمًا شائعة مثل "typing, math, random, collections, datetime, itertools". إذا قام النموذج بتوليد سكربتات Python تستخدم حزمًا أخرى أو سكربتات بلغات أخرى، نوصي بشدة أن يتحقق المستخدمون يدويًا من جميع استخدامات API.

يجب على المطورين تطبيق أفضل ممارسات الذكاء الاصطناعي المسؤول ويكونون مسؤولين عن ضمان امتثال حالة الاستخدام المحددة للقوانين واللوائح ذات الصلة (مثل: الخصوصية، التجارة، إلخ). المجالات المهمة للنظر فيها تشمل:

**التخصيص:** قد لا تكون النماذج مناسبة للسيناريوهات التي يمكن أن يكون لها تأثير كبير على الوضع القانوني أو تخصيص الموارد أو فرص الحياة (مثل: الإسكان، التوظيف، الائتمان، إلخ) دون تقييمات إضافية وتقنيات إزالة التحيز.

**السيناريوهات عالية المخاطر:** يجب على المطورين تقييم مدى ملاءمة استخدام النماذج في السيناريوهات عالية المخاطر حيث قد تكون المخرجات غير العادلة أو غير الموثوقة أو المسيئة مكلفة للغاية أو تؤدي إلى ضرر. يشمل ذلك تقديم المشورة في مجالات حساسة أو متخصصة حيث تكون الدقة والموثوقية أمرًا حاسمًا (مثل: المشورة القانونية أو الصحية). يجب تنفيذ ضوابط إضافية على مستوى التطبيق وفقًا لسياق النشر.

**المعلومات المضللة:** قد تنتج النماذج معلومات غير دقيقة. يجب على المطورين اتباع ممارسات الشفافية وإبلاغ المستخدمين النهائيين بأنهم يتفاعلون مع نظام ذكاء اصطناعي. على مستوى التطبيق، يمكن للمطورين بناء آليات ملاحظات وخطوط أنابيب لتثبيت الاستجابات في معلومات سياقية ومحددة لحالة الاستخدام، وهي تقنية تعرف باسم Retrieval Augmented Generation (RAG).

**توليد محتوى ضار:** يجب على المطورين تقييم المخرجات وفقًا لسياقها واستخدام مصنّفات السلامة المتاحة أو حلول مخصصة مناسبة لحالة الاستخدام الخاصة بهم.

**سوء الاستخدام:** قد تكون هناك أشكال أخرى من سوء الاستخدام مثل الاحتيال أو الرسائل المزعجة أو إنتاج البرمجيات الخبيثة ممكنة، ويجب على المطورين التأكد من أن تطبيقاتهم لا تنتهك القوانين واللوائح المعمول بها.

### الضبط الدقيق وسلامة محتوى الذكاء الاصطناعي

بعد ضبط نموذج بشكل دقيق، نوصي بشدة بالاستفادة من تدابير [خدمة Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) لمراقبة المحتوى الذي تنتجه النماذج، وتحديد ومنع المخاطر والتهديدات وقضايا الجودة المحتملة.

![سلامة Phi3](../../../../../translated_images/01.phi3aisafety.c0d7fc42f5a5c405.ar.png)

[خدمة Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) تدعم كلًا من محتوى النص والصور. يمكن نشرها في السحابة، والحاويات المنفصلة، وعلى أجهزة الحافة/المضمنة.

## نظرة عامة على Azure AI Content Safety

خدمة Azure AI Content Safety ليست حلًا موحدًا لكل الحالات؛ يمكن تخصيصها لتتماشى مع سياسات الأعمال المحددة. بالإضافة إلى ذلك، تمكّنها نماذجها متعددة اللغات من فهم عدة لغات في وقت واحد.

![سلامة محتوى الذكاء الاصطناعي](../../../../../translated_images/01.AIcontentsafety.a288819b8ce8da1a.ar.png)

- **خدمة Azure AI Content Safety**
- **مطور Microsoft**
- **5 مقاطع فيديو**

تقوم خدمة Azure AI Content Safety باكتشاف المحتوى الضار الذي ينشئه المستخدم أو الذي تم إنشاؤه بواسطة الذكاء الاصطناعي في التطبيقات والخدمات. تتضمن واجهات برمجة تطبيقات للنص والصورة تتيح لك اكتشاف المواد الضارة أو غير المناسبة.

[قائمة تشغيل سلامة محتوى الذكاء الاصطناعي](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
إخلاء المسؤولية:
تمت ترجمة هذا المستند باستخدام خدمة ترجمة آلية [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى إلى الدقة، يُرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر المعتبر. للمعلومات الحرجة، يُنصح بالاستعانة بترجمة بشرية محترفة. لا نتحمّل أي مسؤولية عن أي سوء فهم أو تفسير ناتج عن استخدام هذه الترجمة.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->