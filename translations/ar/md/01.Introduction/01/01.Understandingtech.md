<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-07T10:52:21+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "ar"
}
-->
# التقنيات الرئيسية المذكورة تشمل

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - واجهة برمجة تطبيقات منخفضة المستوى لتسريع التعلم الآلي عبر الأجهزة، مبنية فوق DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - منصة حوسبة متوازية ونموذج واجهة برمجة التطبيقات (API) طورتها Nvidia، تتيح المعالجة العامة على وحدات معالجة الرسوميات (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - صيغة مفتوحة مصممة لتمثيل نماذج التعلم الآلي وتوفر قابلية التشغيل المتبادل بين أُطُر التعلم المختلفة.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - صيغة تُستخدم لتمثيل وتحديث نماذج التعلم الآلي، مفيدة بشكل خاص للنماذج اللغوية الصغيرة التي يمكن تشغيلها بكفاءة على وحدات المعالجة المركزية مع تقنين 4-8 بت.

## DirectML

DirectML هي واجهة برمجة تطبيقات منخفضة المستوى تُمكّن من تسريع التعلم الآلي عبر الأجهزة. مبنية فوق DirectX 12 للاستفادة من تسريع GPU، وهي مستقلة عن البائع، بمعنى أنها لا تتطلب تغييرات في الكود لتعمل عبر بائعي GPU مختلفين. تُستخدم أساسًا في تدريب النماذج وأحمال العمل الخاصة بالاستدلال على وحدات معالجة الرسوميات.

أما بالنسبة لدعم الأجهزة، فقد صُممت DirectML للعمل مع مجموعة واسعة من وحدات معالجة الرسوميات، بما في ذلك وحدات AMD المتكاملة والمنفصلة، ووحدات Intel المتكاملة، ووحدات NVIDIA المنفصلة. هي جزء من منصة Windows AI ومدعومة على Windows 10 و11، مما يسمح بتدريب النماذج والاستدلال عليها على أي جهاز يعمل بنظام Windows.

كانت هناك تحديثات وفرص مرتبطة بـ DirectML، مثل دعم ما يصل إلى 150 عامل ONNX واستخدامها من قِبل كل من ONNX runtime و WinML. وهي مدعومة من قِبل كبار بائعي الأجهزة المتكاملة (IHVs)، الذين ينفذون أوامر ميتا مختلفة.

## CUDA

CUDA، التي تعني Compute Unified Device Architecture، هي منصة حوسبة متوازية ونموذج واجهة برمجة التطبيقات (API) أنشأتها Nvidia. تتيح لمطوري البرمجيات استخدام وحدة معالجة الرسوميات المدعومة بـ CUDA للمعالجة العامة – وهو نهج يُعرف بـ GPGPU (الحوسبة العامة على وحدات معالجة الرسوميات). CUDA هي مُمكّن رئيسي لتسريع Nvidia باستخدام GPU وتُستخدم على نطاق واسع في مجالات متعددة، منها التعلم الآلي، الحوسبة العلمية، ومعالجة الفيديو.

دعم الأجهزة لـ CUDA مخصص لوحدات Nvidia، فهي تقنية مملوكة طورتها Nvidia. كل معمارية تدعم إصدارات محددة من مجموعة أدوات CUDA، التي توفر المكتبات والأدوات اللازمة للمطورين لبناء وتشغيل تطبيقات CUDA.

## ONNX

ONNX (Open Neural Network Exchange) هي صيغة مفتوحة مصممة لتمثيل نماذج التعلم الآلي. توفر تعريفًا لنموذج رسم بياني للحوسبة قابل للتوسيع، بالإضافة إلى تعريفات لعوامل مدمجة وأنواع بيانات قياسية. تتيح ONNX للمطورين نقل النماذج بين أُطُر التعلم المختلفة، مما يمكّن التشغيل المتبادل ويسهل إنشاء ونشر تطبيقات الذكاء الاصطناعي.

يمكن لـ Phi3 mini العمل مع ONNX Runtime على المعالج المركزي وGPU عبر أجهزة متعددة، بما في ذلك منصات الخوادم، وأنظمة Windows، Linux، Mac المكتبية، والمعالجات المركزية المحمولة.
التكوينات المحسنة التي أضفناها هي

- نماذج ONNX لـ int4 DML: تقنين إلى int4 عبر AWQ
- نموذج ONNX لـ fp16 CUDA
- نموذج ONNX لـ int4 CUDA: تقنين إلى int4 عبر RTN
- نموذج ONNX لـ int4 CPU و Mobile: تقنين إلى int4 عبر RTN

## Llama.cpp

Llama.cpp هي مكتبة برمجيات مفتوحة المصدر مكتوبة بلغة C++. تقوم بتنفيذ الاستدلال على نماذج اللغة الكبيرة المختلفة (LLMs)، بما في ذلك Llama. تم تطويرها جنبًا إلى جنب مع مكتبة ggml (مكتبة تينسور عامة الغرض)، وتهدف llama.cpp إلى توفير استدلال أسرع واستخدام أقل للذاكرة مقارنة بالتنفيذ الأصلي بلغة Python. تدعم تحسين الأجهزة، والتقنين، وتوفر واجهة برمجة تطبيقات بسيطة وأمثلة3. إذا كنت مهتمًا باستدلال LLM بكفاءة، فإن llama.cpp تستحق الاستكشاف حيث يمكن لـ Phi3 تشغيل Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) هي صيغة تُستخدم لتمثيل وتحديث نماذج التعلم الآلي. هي مفيدة بشكل خاص للنماذج اللغوية الصغيرة (SLMs) التي يمكن تشغيلها بكفاءة على وحدات المعالجة المركزية مع تقنين 4-8 بت. GGUF مفيدة للنماذج الأولية السريعة وتشغيل النماذج على أجهزة الحافة أو في مهام الدُفعات مثل خطوط CI/CD.

**إخلاء مسؤولية**:  
تمت ترجمة هذا المستند باستخدام خدمة الترجمة الآلية [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الرسمي والمعتمد. للمعلومات الحساسة، يُنصح بالاعتماد على الترجمة البشرية المهنية. نحن غير مسؤولين عن أي سوء فهم أو تفسير خاطئ ناتج عن استخدام هذه الترجمة.