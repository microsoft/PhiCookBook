<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:40:35+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "ar"
}
-->
# التقنيات الرئيسية المذكورة تشمل

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - واجهة برمجة تطبيقات منخفضة المستوى لتسريع التعلم الآلي باستخدام العتاد، مبنية على DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - منصة حوسبة متوازية ونموذج واجهة برمجة تطبيقات (API) طورتها Nvidia، تتيح المعالجة العامة على وحدات معالجة الرسوميات (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - صيغة مفتوحة مصممة لتمثيل نماذج التعلم الآلي وتوفر التوافقية بين أُطُر التعلم الآلي المختلفة.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - صيغة تُستخدم لتمثيل وتحديث نماذج التعلم الآلي، مفيدة بشكل خاص للنماذج اللغوية الصغيرة التي يمكن تشغيلها بفعالية على وحدات المعالجة المركزية مع تقنين 4-8 بت.

## DirectML

DirectML هي واجهة برمجة تطبيقات منخفضة المستوى تتيح تسريع التعلم الآلي باستخدام العتاد. مبنية على DirectX 12 للاستفادة من تسريع GPU، وهي مستقلة عن الشركة المصنعة، مما يعني أنها لا تتطلب تغييرات في الكود لتعمل عبر مختلف موردي وحدات معالجة الرسوميات. تُستخدم بشكل أساسي في تدريب النماذج وأعباء العمل الخاصة بالاستدلال على وحدات معالجة الرسوميات.

بالنسبة لدعم العتاد، تم تصميم DirectML للعمل مع مجموعة واسعة من وحدات معالجة الرسوميات، بما في ذلك وحدات AMD المدمجة والمنفصلة، ووحدات Intel المدمجة، ووحدات NVIDIA المنفصلة. هي جزء من منصة Windows AI ومدعومة على Windows 10 و 11، مما يسمح بتدريب النماذج والاستدلال عليها على أي جهاز يعمل بنظام Windows.

كانت هناك تحديثات وفرص متعلقة بـ DirectML، مثل دعم ما يصل إلى 150 عامل تشغيل ONNX واستخدامها من قبل كل من ONNX runtime و WinML. تدعمها كبرى شركات العتاد المتكاملة (IHVs)، حيث تنفذ كل منها أوامر ميتا مختلفة.

## CUDA

CUDA، التي تعني Compute Unified Device Architecture، هي منصة حوسبة متوازية ونموذج واجهة برمجة تطبيقات (API) أنشأتها Nvidia. تتيح لمطوري البرمجيات استخدام وحدة معالجة الرسوميات المدعومة بـ CUDA للمعالجة العامة – وهو نهج يُعرف بـ GPGPU (الحوسبة العامة على وحدات معالجة الرسوميات). تُعد CUDA عنصرًا أساسيًا في تسريع وحدات معالجة الرسوميات من Nvidia وتستخدم على نطاق واسع في مجالات متعددة، بما في ذلك التعلم الآلي، الحوسبة العلمية، ومعالجة الفيديو.

دعم العتاد لـ CUDA يقتصر على وحدات معالجة الرسوميات من Nvidia، لأنها تقنية مملوكة طورتها Nvidia. كل بنية تدعم إصدارات محددة من مجموعة أدوات CUDA، التي توفر المكتبات والأدوات اللازمة للمطورين لبناء وتشغيل تطبيقات CUDA.

## ONNX

ONNX (Open Neural Network Exchange) هي صيغة مفتوحة مصممة لتمثيل نماذج التعلم الآلي. توفر تعريفًا لنموذج رسم بياني حسابي قابل للتوسيع، بالإضافة إلى تعريفات لعوامل تشغيل مدمجة وأنواع بيانات قياسية. تتيح ONNX للمطورين نقل النماذج بين أُطُر التعلم الآلي المختلفة، مما يعزز التوافقية ويسهل إنشاء ونشر تطبيقات الذكاء الاصطناعي.

يمكن لـ Phi3 mini العمل مع ONNX Runtime على وحدة المعالجة المركزية ووحدة معالجة الرسوميات عبر الأجهزة، بما في ذلك منصات الخوادم، وأجهزة Windows، وLinux، وMac المكتبية، ووحدات المعالجة المركزية المحمولة.
التكوينات المحسنة التي أضفناها هي

- نماذج ONNX لـ int4 DML: تقنين إلى int4 عبر AWQ
- نموذج ONNX لـ fp16 CUDA
- نموذج ONNX لـ int4 CUDA: تقنين إلى int4 عبر RTN
- نموذج ONNX لـ int4 CPU و Mobile: تقنين إلى int4 عبر RTN

## Llama.cpp

Llama.cpp هي مكتبة برمجية مفتوحة المصدر مكتوبة بلغة C++. تقوم بتنفيذ الاستدلال على نماذج اللغة الكبيرة المختلفة (LLMs)، بما في ذلك Llama. تم تطويرها جنبًا إلى جنب مع مكتبة ggml (مكتبة موترات عامة الاستخدام)، وتهدف llama.cpp إلى توفير استدلال أسرع واستهلاك أقل للذاكرة مقارنة بالتنفيذ الأصلي بلغة Python. تدعم تحسينات العتاد، والتقنين، وتوفر واجهة برمجة تطبيقات بسيطة وأمثلة. إذا كنت مهتمًا باستدلال نماذج اللغة الكبيرة بكفاءة، فإن llama.cpp تستحق التجربة حيث يمكن لـ Phi3 تشغيل Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) هي صيغة تُستخدم لتمثيل وتحديث نماذج التعلم الآلي. هي مفيدة بشكل خاص للنماذج اللغوية الصغيرة (SLMs) التي يمكن تشغيلها بفعالية على وحدات المعالجة المركزية مع تقنين 4-8 بت. GGUF مفيدة للنماذج الأولية السريعة وتشغيل النماذج على أجهزة الحافة أو في مهام الدُفعات مثل خطوط CI/CD.

**إخلاء المسؤولية**:  
تمت ترجمة هذا المستند باستخدام خدمة الترجمة الآلية [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق به. للمعلومات الهامة، يُنصح بالاعتماد على الترجمة البشرية المهنية. نحن غير مسؤولين عن أي سوء فهم أو تفسير ناتج عن استخدام هذه الترجمة.