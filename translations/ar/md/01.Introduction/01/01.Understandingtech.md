<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-03-27T06:06:45+00:00",
  "source_file": "md\\01.Introduction\\01\\01.Understandingtech.md",
  "language_code": "ar"
}
-->
# التقنيات الرئيسية المذكورة تشمل

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - واجهة برمجة تطبيقات منخفضة المستوى للتعلم الآلي المسرّع بالأجهزة، مبنية على DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - منصة حوسبة موازية ونموذج واجهة برمجة تطبيقات (API) طورتها Nvidia، تتيح المعالجة العامة على وحدات معالجة الرسومات (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - تنسيق مفتوح مصمم لتمثيل نماذج التعلم الآلي، ويوفر التوافق بين مختلف أطر التعلم الآلي.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - تنسيق يستخدم لتمثيل وتحديث نماذج التعلم الآلي، مفيد بشكل خاص للنماذج اللغوية الصغيرة التي تعمل بكفاءة على وحدات المعالجة المركزية باستخدام التكميم بـ 4-8 بت.

## DirectML

DirectML هي واجهة برمجة تطبيقات منخفضة المستوى تمكّن التعلم الآلي المسرّع بالأجهزة. تم بناؤها على DirectX 12 للاستفادة من تسريع GPU وهي غير مرتبطة بمورد معين، مما يعني أنها لا تتطلب تغييرات في الكود للعمل عبر مختلف موردي وحدات معالجة الرسومات. تُستخدم بشكل أساسي لتدريب النماذج وأعباء العمل المتعلقة بالاستدلال على وحدات معالجة الرسومات.

فيما يتعلق بدعم الأجهزة، تم تصميم DirectML للعمل مع مجموعة واسعة من وحدات معالجة الرسومات، بما في ذلك وحدات AMD المدمجة والمنفصلة، ووحدات Intel المدمجة، ووحدات NVIDIA المنفصلة. إنها جزء من منصة Windows AI وتُدعم على نظامي Windows 10 و11، مما يسمح بتدريب النماذج والاستدلال على أي جهاز يعمل بنظام Windows.

كانت هناك تحديثات وفرص متعلقة بـ DirectML، مثل دعم ما يصل إلى 150 من مشغلات ONNX واستخدامها من قبل كل من وقت تشغيل ONNX وWinML. يتم دعمها من قبل كبار موردي الأجهزة المتكاملة (IHVs)، حيث يقوم كل منهم بتنفيذ أوامر ميتا مختلفة.

## CUDA

CUDA، التي تعني Compute Unified Device Architecture، هي منصة حوسبة موازية ونموذج واجهة برمجة تطبيقات (API) تم إنشاؤها بواسطة Nvidia. تتيح للمطورين استخدام وحدة معالجة رسومات (GPU) تدعم CUDA للمعالجة العامة – وهي مقاربة تُعرف بـ GPGPU (الحوسبة العامة على وحدات معالجة الرسومات). تعد CUDA عاملاً رئيسياً في تمكين تسريع GPU الخاص بـ Nvidia وتُستخدم على نطاق واسع في مجالات متنوعة، بما في ذلك التعلم الآلي والحوسبة العلمية ومعالجة الفيديو.

الدعم المادي لـ CUDA خاص بوحدات معالجة الرسومات الخاصة بـ Nvidia، حيث إنها تقنية مملوكة طورتها Nvidia. كل بنية تدعم إصدارات محددة من مجموعة أدوات CUDA، التي توفر المكتبات والأدوات اللازمة للمطورين لبناء وتشغيل تطبيقات CUDA.

## ONNX

ONNX (Open Neural Network Exchange) هو تنسيق مفتوح مصمم لتمثيل نماذج التعلم الآلي. يوفر تعريفًا لنموذج رسم بياني للحسابات قابل للتوسعة، بالإضافة إلى تعريفات للمشغلات المدمجة وأنواع البيانات القياسية. يسمح ONNX للمطورين بنقل النماذج بين مختلف أطر التعلم الآلي، مما يتيح التوافق ويسهّل إنشاء ونشر تطبيقات الذكاء الاصطناعي.

يمكن تشغيل Phi3 mini باستخدام وقت تشغيل ONNX على وحدات المعالجة المركزية ووحدات معالجة الرسومات عبر الأجهزة، بما في ذلك منصات الخوادم، وأجهزة سطح المكتب التي تعمل بنظام Windows وLinux وMac، ووحدات المعالجة المركزية المحمولة. التكوينات المحسّنة التي أضفناها هي:

- نماذج ONNX لـ int4 DML: تم تكميمها إلى int4 عبر AWQ  
- نموذج ONNX لـ fp16 CUDA  
- نموذج ONNX لـ int4 CUDA: تم تكميمه إلى int4 عبر RTN  
- نموذج ONNX لـ int4 CPU وMobile: تم تكميمه إلى int4 عبر RTN  

## Llama.cpp

Llama.cpp هي مكتبة برمجية مفتوحة المصدر مكتوبة بلغة C++. تقوم بتنفيذ الاستدلال على مختلف النماذج اللغوية الكبيرة (LLMs)، بما في ذلك Llama. تم تطويرها جنبًا إلى جنب مع مكتبة ggml (مكتبة عامة للتعامل مع التنسورات)، وتهدف Llama.cpp إلى توفير استدلال أسرع واستخدام أقل للذاكرة مقارنة بالتنفيذ الأصلي بلغة Python. تدعم المكتبة تحسينات الأجهزة، والتكميم، وتوفر واجهة برمجة تطبيقات بسيطة وأمثلة. إذا كنت مهتمًا باستدلال فعال للنماذج اللغوية الكبيرة، فإن Llama.cpp تستحق الاستكشاف حيث يمكن تشغيل Phi3 باستخدامها.

## GGUF

GGUF (Generic Graph Update Format) هو تنسيق يستخدم لتمثيل وتحديث نماذج التعلم الآلي. وهو مفيد بشكل خاص للنماذج اللغوية الصغيرة (SLMs) التي يمكن تشغيلها بكفاءة على وحدات المعالجة المركزية باستخدام التكميم بـ 4-8 بت. يوفر GGUF فائدة كبيرة للنماذج السريعة التجريبية وتشغيل النماذج على الأجهزة الطرفية أو في وظائف الدُفعات مثل خطوط CI/CD.

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الرسمي. للحصول على معلومات حساسة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسير خاطئ ينشأ نتيجة استخدام هذه الترجمة.