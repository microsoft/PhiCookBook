<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-07-16T18:06:49+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "ar"
}
-->
# البدء مع Phi-3 محليًا

سيساعدك هذا الدليل في إعداد بيئتك المحلية لتشغيل نموذج Phi-3 باستخدام Ollama. يمكنك تشغيل النموذج بعدة طرق مختلفة، بما في ذلك استخدام GitHub Codespaces، أو VS Code Dev Containers، أو بيئتك المحلية.

## إعداد البيئة

### GitHub Codespaces

يمكنك تشغيل هذا القالب افتراضيًا باستخدام GitHub Codespaces. الزر سيفتح نسخة من VS Code تعمل عبر الويب في متصفحك:

1. افتح القالب (قد يستغرق ذلك عدة دقائق):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. افتح نافذة طرفية

### VS Code Dev Containers

⚠️ هذا الخيار سيعمل فقط إذا كان Docker Desktop مخصصًا له على الأقل 16 جيجابايت من الذاكرة العشوائية. إذا كانت الذاكرة أقل من 16 جيجابايت، يمكنك تجربة خيار [GitHub Codespaces](../../../../../md/01.Introduction/01) أو [إعداده محليًا](../../../../../md/01.Introduction/01).

خيار ذو صلة هو VS Code Dev Containers، الذي سيفتح المشروع في VS Code المحلي باستخدام [امتداد Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. شغّل Docker Desktop (قم بتثبيته إذا لم يكن مثبتًا)
2. افتح المشروع:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. في نافذة VS Code التي تفتح، بمجرد ظهور ملفات المشروع (قد يستغرق ذلك عدة دقائق)، افتح نافذة طرفية.
4. تابع مع [خطوات النشر](../../../../../md/01.Introduction/01)

### البيئة المحلية

1. تأكد من تثبيت الأدوات التالية:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## اختبار النموذج

1. اطلب من Ollama تنزيل وتشغيل نموذج phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    سيستغرق تنزيل النموذج بضع دقائق.

2. بمجرد رؤية "success" في المخرجات، يمكنك إرسال رسالة إلى ذلك النموذج من خلال الموجه.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. بعد عدة ثوانٍ، يجب أن ترى تدفق استجابة من النموذج.

4. لتتعرف على التقنيات المختلفة المستخدمة مع نماذج اللغة، افتح دفتر Python [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) وقم بتشغيل كل خلية. إذا استخدمت نموذجًا غير 'phi3:mini'، قم بتغيير `MODEL_NAME` في الخلية الأولى.

5. لإجراء محادثة مع نموذج phi3:mini من Python، افتح ملف Python [chat.py](../../../../../code/01.Introduce/chat.py) وقم بتشغيله. يمكنك تغيير `MODEL_NAME` في أعلى الملف حسب الحاجة، ويمكنك أيضًا تعديل رسالة النظام أو إضافة أمثلة قليلة إذا رغبت.

**إخلاء المسؤولية**:  
تمت ترجمة هذا المستند باستخدام خدمة الترجمة الآلية [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق به. للمعلومات الهامة، يُنصح بالاعتماد على الترجمة البشرية المهنية. نحن غير مسؤولين عن أي سوء فهم أو تفسير ناتج عن استخدام هذه الترجمة.