<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-03-27T05:42:05+00:00",
  "source_file": "md\\01.Introduction\\01\\01.EnvironmentSetup.md",
  "language_code": "ar"
}
-->
# البدء مع Phi-3 محليًا

سيساعدك هذا الدليل على إعداد بيئتك المحلية لتشغيل نموذج Phi-3 باستخدام Ollama. يمكنك تشغيل النموذج بطرق متعددة، بما في ذلك استخدام GitHub Codespaces، أو VS Code Dev Containers، أو بيئتك المحلية.

## إعداد البيئة

### GitHub Codespaces

يمكنك تشغيل هذا القالب افتراضيًا باستخدام GitHub Codespaces. الزر أدناه سيفتح نافذة VS Code مستندة إلى الويب في متصفحك:

1. افتح القالب (قد يستغرق ذلك عدة دقائق):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. افتح نافذة طرفية.

### VS Code Dev Containers

⚠️ هذا الخيار سيعمل فقط إذا كانت ذاكرة RAM المخصصة لـ Docker Desktop على الأقل 16 جيجابايت. إذا كانت لديك ذاكرة أقل من 16 جيجابايت، يمكنك تجربة [خيار GitHub Codespaces](../../../../../md/01.Introduction/01) أو [إعداده محليًا](../../../../../md/01.Introduction/01).

خيار مشابه هو VS Code Dev Containers، والذي سيفتح المشروع في VS Code المحلي لديك باستخدام [إضافة Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. قم بتشغيل Docker Desktop (قم بتثبيته إذا لم يكن مثبتًا بالفعل).
2. افتح المشروع:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. في نافذة VS Code التي تفتح، بمجرد ظهور ملفات المشروع (قد يستغرق ذلك عدة دقائق)، افتح نافذة طرفية.
4. تابع مع [خطوات النشر](../../../../../md/01.Introduction/01).

### البيئة المحلية

1. تأكد من تثبيت الأدوات التالية:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## اختبار النموذج

1. اطلب من Ollama تنزيل وتشغيل نموذج phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    سيستغرق ذلك عدة دقائق لتنزيل النموذج.

2. بمجرد رؤية "success" في الناتج، يمكنك إرسال رسالة إلى النموذج من خلال المحث.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. بعد عدة ثوانٍ، يجب أن ترى استجابة تصل من النموذج.

4. للتعرف على تقنيات مختلفة تُستخدم مع نماذج اللغة، افتح دفتر Python [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) وقم بتشغيل كل خلية. إذا كنت قد استخدمت نموذجًا غير 'phi3:mini'، قم بتغيير `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` في أعلى الملف حسب الحاجة، ويمكنك أيضًا تعديل رسالة النظام أو إضافة أمثلة few-shot إذا رغبت.

**إخلاء المسؤولية**:  
تمت ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق. للحصول على معلومات حاسمة، يُوصى باستخدام ترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة ناتجة عن استخدام هذه الترجمة.