<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-07T10:51:55+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "ar"
}
-->
# ابدأ مع Phi-3 محليًا

سيساعدك هذا الدليل في إعداد بيئتك المحلية لتشغيل نموذج Phi-3 باستخدام Ollama. يمكنك تشغيل النموذج بعدة طرق مختلفة، بما في ذلك استخدام GitHub Codespaces، أو VS Code Dev Containers، أو بيئتك المحلية.

## إعداد البيئة

### GitHub Codespaces

يمكنك تشغيل هذا القالب افتراضيًا باستخدام GitHub Codespaces. الزر سيفتح نسخة VS Code على الويب في متصفحك:

1. افتح القالب (قد يستغرق ذلك عدة دقائق):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. افتح نافذة طرفية

### VS Code Dev Containers

⚠️ هذا الخيار يعمل فقط إذا كان Docker Desktop مخصصًا له على الأقل 16 جيجابايت من الذاكرة العشوائية. إذا كانت الذاكرة أقل من 16 جيجابايت، يمكنك تجربة [خيار GitHub Codespaces](../../../../../md/01.Introduction/01) أو [إعداده محليًا](../../../../../md/01.Introduction/01).

خيار ذو صلة هو VS Code Dev Containers، الذي سيفتح المشروع في VS Code المحلي باستخدام [امتداد Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. ابدأ Docker Desktop (قم بتثبيته إذا لم يكن مثبتًا بالفعل)
2. افتح المشروع:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. في نافذة VS Code التي تفتح، بمجرد ظهور ملفات المشروع (قد يستغرق ذلك عدة دقائق)، افتح نافذة طرفية.
4. تابع مع [خطوات النشر](../../../../../md/01.Introduction/01)

### البيئة المحلية

1. تأكد من تثبيت الأدوات التالية:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## اختبار النموذج

1. اطلب من Ollama تنزيل وتشغيل نموذج phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    سيستغرق ذلك بضع دقائق لتنزيل النموذج.

2. بمجرد أن ترى "success" في الإخراج، يمكنك إرسال رسالة إلى ذلك النموذج من خلال الموجه.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. بعد عدة ثوانٍ، يجب أن ترى تدفق الرد من النموذج.

4. لتتعرف على التقنيات المختلفة المستخدمة مع نماذج اللغة، افتح دفتر Python [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) وقم بتشغيل كل خلية. إذا استخدمت نموذجًا غير 'phi3:mini'، قم بتغيير `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` في أعلى الملف حسب الحاجة، ويمكنك أيضًا تعديل رسالة النظام أو إضافة أمثلة قليلة اللقطات إذا رغبت في ذلك.

**تنويه**:  
تمت ترجمة هذا المستند باستخدام خدمة الترجمة الآلية [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق به. بالنسبة للمعلومات الحساسة، يُنصح بالاستعانة بالترجمة البشرية المهنية. نحن غير مسؤولين عن أي سوء فهم أو تفسير ناتج عن استخدام هذه الترجمة.