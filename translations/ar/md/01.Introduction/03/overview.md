<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f1ff728038c4f554b660a36b76cbdd6e",
  "translation_date": "2025-03-27T07:56:16+00:00",
  "source_file": "md\\01.Introduction\\03\\overview.md",
  "language_code": "ar"
}
-->
في سياق Phi-3-mini، يشير الاستنتاج إلى عملية استخدام النموذج لإجراء توقعات أو إنشاء مخرجات بناءً على بيانات الإدخال. دعني أقدم لك المزيد من التفاصيل حول Phi-3-mini وقدراته في الاستنتاج.

Phi-3-mini هو جزء من سلسلة نماذج Phi-3 التي أصدرتها شركة مايكروسوفت. تم تصميم هذه النماذج لإعادة تعريف ما يمكن تحقيقه باستخدام نماذج اللغة الصغيرة (SLMs).

إليك بعض النقاط الرئيسية حول Phi-3-mini وقدراته في الاستنتاج:

## **نظرة عامة على Phi-3-mini:**
- يحتوي Phi-3-mini على حجم معلمات يبلغ 3.8 مليار.
- يمكن تشغيله ليس فقط على أجهزة الحوسبة التقليدية ولكن أيضًا على الأجهزة الطرفية مثل الأجهزة المحمولة وأجهزة إنترنت الأشياء.
- يتيح إصدار Phi-3-mini للأفراد والشركات نشر نماذج SLM على أجهزة مختلفة، خاصة في البيئات ذات الموارد المحدودة.
- يدعم صيغًا مختلفة للنماذج، بما في ذلك صيغة PyTorch التقليدية، النسخة الكمومية من صيغة gguf، والنسخة الكمومية القائمة على ONNX.

## **الوصول إلى Phi-3-mini:**
للوصول إلى Phi-3-mini، يمكنك استخدام [Semantic Kernel](https://github.com/microsoft/SemanticKernelCookBook?WT.mc_id=aiml-138114-kinfeylo) في تطبيق Copilot. يتوافق Semantic Kernel بشكل عام مع خدمة Azure OpenAI، النماذج مفتوحة المصدر على Hugging Face، والنماذج المحلية.
يمكنك أيضًا استخدام [Ollama](https://ollama.com) أو [LlamaEdge](https://llamaedge.com) لاستدعاء النماذج الكمومية. يسمح Ollama للمستخدمين الأفراد باستدعاء نماذج كمومية مختلفة، بينما يوفر LlamaEdge توفرًا عبر المنصات لنماذج GGUF.

## **النماذج الكمومية:**
يفضل العديد من المستخدمين استخدام النماذج الكمومية للاستنتاج المحلي. على سبيل المثال، يمكنك تشغيل Ollama مباشرة باستخدام Phi-3 أو تكوينه دون اتصال باستخدام ملف النموذج. يحدد ملف النموذج مسار ملف GGUF وصيغة المطالبة.

## **إمكانيات الذكاء الاصطناعي التوليدي:**
دمج نماذج SLM مثل Phi-3-mini يفتح آفاقًا جديدة للذكاء الاصطناعي التوليدي. الاستنتاج هو مجرد الخطوة الأولى؛ يمكن استخدام هذه النماذج لمهام متنوعة في سيناريوهات ذات موارد محدودة، تأخر زمني منخفض، وتكاليف مقيدة.

## **فتح آفاق الذكاء الاصطناعي التوليدي باستخدام Phi-3-mini: دليل للاستنتاج والنشر** 
تعرف على كيفية استخدام Semantic Kernel، Ollama/LlamaEdge، و ONNX Runtime للوصول إلى نماذج Phi-3-mini والاستنتاج منها، واستكشف إمكانيات الذكاء الاصطناعي التوليدي في سيناريوهات تطبيق متنوعة.

**الميزات**
الاستنتاج باستخدام نموذج phi3-mini في:

- [Semantic Kernel](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/semantickernel?WT.mc_id=aiml-138114-kinfeylo)
- [Ollama](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ollama?WT.mc_id=aiml-138114-kinfeylo)
- [LlamaEdge WASM](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm?WT.mc_id=aiml-138114-kinfeylo)
- [ONNX Runtime](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/onnx?WT.mc_id=aiml-138114-kinfeylo)
- [iOS](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ios?WT.mc_id=aiml-138114-kinfeylo)

باختصار، يتيح Phi-3-mini للمطورين استكشاف صيغ نماذج مختلفة والاستفادة من الذكاء الاصطناعي التوليدي في سيناريوهات تطبيق متنوعة.

**إخلاء المسؤولية**:  
تم ترجمة هذه الوثيقة باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار الوثيقة الأصلية بلغتها الأصلية المصدر الموثوق. بالنسبة للمعلومات الحساسة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسير خاطئ ينشأ نتيجة استخدام هذه الترجمة.