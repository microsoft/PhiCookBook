<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-07-16T21:41:57+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ar"
}
-->
# **تكميم عائلة Phi**

يشير تكميم النموذج إلى عملية تحويل معلمات النموذج (مثل الأوزان وقيم التنشيط) في نموذج الشبكة العصبية من نطاق قيم كبير (عادةً نطاق قيم مستمر) إلى نطاق قيم أصغر ومحدود. تتيح هذه التقنية تقليل حجم النموذج وتعقيد العمليات الحسابية، وتحسين كفاءة تشغيل النموذج في بيئات ذات موارد محدودة مثل الأجهزة المحمولة أو الأنظمة المدمجة. يحقق تكميم النموذج ضغطًا عن طريق تقليل دقة المعلمات، لكنه قد يؤدي أيضًا إلى فقدان معين في الدقة. لذلك، من الضروري في عملية التكميم تحقيق توازن بين حجم النموذج، وتعقيد العمليات الحسابية، والدقة. تشمل طرق التكميم الشائعة التكميم بنقطة ثابتة، والتكميم بنقطة عائمة، وغيرها. يمكنك اختيار استراتيجية التكميم المناسبة حسب السيناريو والاحتياجات المحددة.

نأمل في نشر نموذج GenAI على أجهزة الحافة وتمكين المزيد من الأجهزة من الدخول في سيناريوهات GenAI، مثل الأجهزة المحمولة، وأجهزة AI PC/Copilot+PC، والأجهزة التقليدية لإنترنت الأشياء. من خلال نموذج التكميم، يمكننا نشره على أجهزة حافة مختلفة بناءً على نوع الجهاز. وبالاقتران مع إطار تسريع النموذج ونموذج التكميم الذي توفره الشركات المصنعة للأجهزة، يمكننا بناء سيناريوهات تطبيق SLM أفضل.

في سيناريو التكميم، لدينا دقات مختلفة (INT4، INT8، FP16، FP32). فيما يلي شرح للدقات الشائعة الاستخدام في التكميم:

### **INT4**

تكميم INT4 هو طريقة تكميم جذرية تقوم بتحويل أوزان وقيم التنشيط في النموذج إلى أعداد صحيحة 4-بت. عادةً ما يؤدي تكميم INT4 إلى فقدان أكبر في الدقة بسبب نطاق التمثيل الأصغر والدقة الأقل. ومع ذلك، مقارنةً بتكميم INT8، يمكن لتكميم INT4 تقليل متطلبات التخزين وتعقيد العمليات الحسابية للنموذج بشكل أكبر. من الجدير بالذكر أن تكميم INT4 نادر نسبيًا في التطبيقات العملية، لأن الدقة المنخفضة جدًا قد تسبب تدهورًا ملحوظًا في أداء النموذج. بالإضافة إلى ذلك، لا تدعم جميع الأجهزة عمليات INT4، لذا يجب مراعاة توافق الأجهزة عند اختيار طريقة التكميم.

### **INT8**

تكميم INT8 هو عملية تحويل أوزان وقيم التنشيط في النموذج من أعداد عائمة إلى أعداد صحيحة 8-بت. على الرغم من أن النطاق الرقمي الذي تمثله أعداد INT8 أصغر وأقل دقة، إلا أنه يمكن أن يقلل بشكل كبير من متطلبات التخزين والحساب. في تكميم INT8، تمر الأوزان وقيم التنشيط بعملية تكميم تشمل التحجيم والإزاحة، للحفاظ على أكبر قدر ممكن من المعلومات الأصلية ذات النقطة العائمة. أثناء الاستدلال، يتم إعادة تحويل هذه القيم المكممة إلى أعداد عائمة للحساب، ثم تُعاد تكميمها إلى INT8 للخطوة التالية. توفر هذه الطريقة دقة كافية في معظم التطبيقات مع الحفاظ على كفاءة حسابية عالية.

### **FP16**

صيغة FP16، أي الأعداد العائمة 16-بت (float16)، تقلل من استهلاك الذاكرة إلى النصف مقارنةً بالأعداد العائمة 32-بت (float32)، مما يمنحها مزايا كبيرة في تطبيقات التعلم العميق واسعة النطاق. تسمح صيغة FP16 بتحميل نماذج أكبر أو معالجة بيانات أكثر ضمن حدود ذاكرة GPU نفسها. مع استمرار دعم أجهزة GPU الحديثة لعمليات FP16، قد يؤدي استخدام صيغة FP16 أيضًا إلى تحسين سرعة الحساب. ومع ذلك، تحتوي صيغة FP16 على عيوبها الجوهرية، وهي الدقة الأقل، التي قد تؤدي إلى عدم استقرار عددي أو فقدان الدقة في بعض الحالات.

### **FP32**

توفر صيغة FP32 دقة أعلى ويمكنها تمثيل نطاق واسع من القيم بدقة. في السيناريوهات التي تتطلب عمليات رياضية معقدة أو نتائج عالية الدقة، يُفضل استخدام صيغة FP32. ومع ذلك، تعني الدقة العالية استخدام ذاكرة أكبر وزمن حساب أطول. بالنسبة لنماذج التعلم العميق واسعة النطاق، خاصةً عندما يكون هناك عدد كبير من معلمات النموذج وكمية هائلة من البيانات، قد تتسبب صيغة FP32 في نفاد ذاكرة GPU أو انخفاض سرعة الاستدلال.

على الأجهزة المحمولة أو أجهزة إنترنت الأشياء، يمكننا تحويل نماذج Phi-3.x إلى INT4، بينما يمكن لأجهزة AI PC / Copilot PC استخدام دقة أعلى مثل INT8، FP16، FP32.

حاليًا، لدى الشركات المصنعة للأجهزة أُطُر مختلفة لدعم النماذج التوليدية، مثل OpenVINO من Intel، وQNN من Qualcomm، وMLX من Apple، وCUDA من Nvidia، وغيرها، مع دمج تكميم النموذج لإتمام النشر المحلي.

من الناحية التقنية، لدينا دعم لصيغ مختلفة بعد التكميم، مثل صيغة PyTorch / Tensorflow، GGUF، وONNX. لقد أجريت مقارنة بين الصيغ وسيناريوهات التطبيق بين GGUF وONNX. أوصي هنا بصيغة تكميم ONNX، التي تحظى بدعم جيد من إطار النموذج وحتى الأجهزة. في هذا الفصل، سنركز على ONNX Runtime لـ GenAI، وOpenVINO، وApple MLX لأداء تكميم النموذج (إذا كان لديك طريقة أفضل، يمكنك مشاركتها معنا عبر تقديم PR).

**يتضمن هذا الفصل**

1. [تكميم Phi-3.5 / 4 باستخدام llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [تكميم Phi-3.5 / 4 باستخدام امتدادات الذكاء الاصطناعي التوليدي لـ onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [تكميم Phi-3.5 / 4 باستخدام Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [تكميم Phi-3.5 / 4 باستخدام إطار Apple MLX](./UsingAppleMLXQuantifyingPhi.md)

**إخلاء المسؤولية**:  
تمت ترجمة هذا المستند باستخدام خدمة الترجمة الآلية [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق به. للمعلومات الهامة، يُنصح بالاعتماد على الترجمة البشرية المهنية. نحن غير مسؤولين عن أي سوء فهم أو تفسير ناتج عن استخدام هذه الترجمة.