<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-05T08:34:09+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ar"
}
-->
# **قياس عائلة Phi**

تشير كمية النموذج إلى عملية تحويل المعلمات (مثل الأوزان وقيم التنشيط) في نموذج الشبكة العصبية من نطاق قيمة كبير (عادة نطاق قيمة مستمر) إلى نطاق قيمة نهائي أصغر. يمكن لتقنية الكمية هذه تقليل حجم وتعقيد الحسابات للنموذج وتحسين كفاءة تشغيل النموذج في بيئات ذات موارد محدودة مثل الأجهزة المحمولة أو الأنظمة المدمجة. تحقق كمية النموذج الضغط من خلال تقليل دقة المعلمات، لكنها تؤدي أيضًا إلى فقدان معين في الدقة. لذلك، في عملية الكمية، من الضروري تحقيق توازن بين حجم النموذج، تعقيد الحساب، والدقة. تشمل طرق الكمية الشائعة الكمية بنقطة ثابتة، والكمية بنقطة عائمة، وغيرها. يمكنك اختيار استراتيجية الكمية المناسبة وفقًا للسيناريو والاحتياجات الخاصة.

نأمل في نشر نموذج GenAI على أجهزة الحافة والسماح للمزيد من الأجهزة بالدخول في سيناريوهات GenAI، مثل الأجهزة المحمولة، AI PC / Copilot+PC، والأجهزة التقليدية لإنترنت الأشياء. من خلال نموذج الكمية، يمكننا نشره على أجهزة الحافة المختلفة استنادًا إلى الأجهزة المختلفة. مجتمعة مع إطار تسريع النموذج ونموذج الكمية المقدمين من مصنعي الأجهزة، يمكننا بناء سيناريوهات تطبيق SLM أفضل.

في سيناريو الكمية، لدينا دقة مختلفة (INT4، INT8، FP16، FP32). فيما يلي شرح للدقة الكمية المستخدمة عادةً

### **INT4**

كمية INT4 هي طريقة كمية جذرية تقوم بكمية أوزان وقيم التنشيط للنموذج إلى أعداد صحيحة 4-بت. عادةً ما تؤدي كمية INT4 إلى فقدان أكبر للدقة بسبب نطاق التمثيل الأصغر والدقة الأدنى. ومع ذلك، بالمقارنة مع كمية INT8، يمكن لكمية INT4 تقليل متطلبات التخزين وتعقيد الحساب للنموذج بشكل أكبر. يجب ملاحظة أن كمية INT4 نادرة نسبيًا في التطبيقات العملية، لأن الدقة المنخفضة جدًا قد تسبب تدهورًا كبيرًا في أداء النموذج. بالإضافة إلى ذلك، لا يدعم جميع الأجهزة عمليات INT4، لذلك يجب مراعاة توافق الأجهزة عند اختيار طريقة الكمية.

### **INT8**

كمية INT8 هي عملية تحويل أوزان النموذج وقيم التنشيط من أعداد عائمة إلى أعداد صحيحة 8-بت. على الرغم من أن النطاق الرقمي الذي تمثله أعداد INT8 أصغر وأقل دقة، إلا أنه يمكن أن يقلل بشكل كبير من متطلبات التخزين والحساب. في كمية INT8، تمر الأوزان وقيم التنشيط بعملية كمية تشمل المقياس والإزاحة، للحفاظ على معلومات الأعداد العائمة الأصلية قدر الإمكان. أثناء الاستدلال، يتم إعادة تحويل هذه القيم المكّونة إلى أعداد عائمة للحساب، ثم يتم تحويلها مرة أخرى إلى INT8 للخطوة التالية. يمكن أن توفر هذه الطريقة دقة كافية في معظم التطبيقات مع الحفاظ على كفاءة حسابية عالية.

### **FP16**

صيغة FP16، أي الأعداد العائمة 16-بت (float16)، تقلل حجم الذاكرة إلى النصف مقارنة بالأعداد العائمة 32-بت (float32)، وهو ميزة كبيرة في تطبيقات التعلم العميق واسعة النطاق. تسمح صيغة FP16 بتحميل نماذج أكبر أو معالجة بيانات أكثر ضمن حدود ذاكرة GPU نفسها. مع استمرار دعم الأجهزة الحديثة لوحدات GPU لعمليات FP16، قد يجلب استخدام صيغة FP16 تحسينات في سرعة الحوسبة. مع ذلك، لدى صيغة FP16 عيوبها الذاتية، وهي الدقة المنخفضة، التي قد تؤدي إلى عدم استقرار رقمي أو فقدان الدقة في بعض الحالات.

### **FP32**

توفر صيغة FP32 دقة أعلى ويمكنها تمثيل نطاق واسع من القيم بدقة. في السيناريوهات التي تُجرى فيها عمليات رياضية معقدة أو تتطلب نتائج عالية الدقة، تُفضل صيغة FP32. ومع ذلك، تعني الدقة العالية استخدامًا أكبر للذاكرة ووقت حساب أطول. بالنسبة لنماذج التعلم العميق واسعة النطاق، خاصةً عندما تكون هناك العديد من معلمات النموذج وكمية كبيرة من البيانات، قد تؤدي صيغة FP32 إلى نقص في ذاكرة GPU أو انخفاض في سرعة الاستدلال.

على الأجهزة المحمولة أو أجهزة إنترنت الأشياء، يمكننا تحويل نماذج Phi-3.x إلى INT4، بينما يمكن لأجهزة AI PC / Copilot PC استخدام دقة أعلى مثل INT8، FP16، FP32.

حاليًا، لدى مصنعي الأجهزة المختلفين أطر مختلفة لدعم النماذج التوليدية، مثل OpenVINO من Intel، وQNN من Qualcomm، وMLX من Apple، وCUDA من Nvidia، وغيرها، مجتمعة مع كمية النموذج لإتمام النشر المحلي.

من الناحية التقنية، لدينا دعم صيغ مختلفة بعد الكمية، مثل صيغة PyTorch / TensorFlow، GGUF، وONNX. لقد قمت بإجراء مقارنة بين الصيغ وسيناريوهات التطبيق بين GGUF وONNX. هنا أوصي بصيغة كمية ONNX التي تحظى بدعم جيد من إطار النموذج حتى الأجهزة. في هذا الفصل، سنركز على ONNX Runtime لـ GenAI، OpenVINO، وApple MLX لأداء كمية النموذج (إذا كان لديك طريقة أفضل، يمكنك أيضًا تقديمها من خلال تقديم PR).

**يتضمن هذا الفصل**

1. [كمية Phi-3.5 / 4 باستخدام llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [كمية Phi-3.5 / 4 باستخدام امتدادات الذكاء التوليدي لـ onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [كمية Phi-3.5 / 4 باستخدام Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [كمية Phi-3.5 / 4 باستخدام إطار Apple MLX](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**تنويه**:
تمت ترجمة هذا المستند باستخدام خدمة الترجمة الآلية [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى جاهدين للدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق به. للحصول على معلومات هامة، يُنصح بالاعتماد على الترجمة البشرية المهنية. نحن غير مسؤولين عن أي سوء فهم أو تفسير خاطئ ينشأ عن استخدام هذه الترجمة.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->