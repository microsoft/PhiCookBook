<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-05-07T10:47:35+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ar"
}
-->
# **قياس كمية عائلة Phi**

يشير تقليل دقة النموذج إلى عملية تحويل المعاملات (مثل الأوزان وقيم التفعيل) في نموذج الشبكة العصبية من نطاق قيم كبير (عادةً نطاق قيم مستمر) إلى نطاق قيم محدود وأصغر. تتيح هذه التقنية تقليل حجم النموذج وتعقيد العمليات الحسابية، بالإضافة إلى تحسين كفاءة تشغيل النموذج في بيئات ذات موارد محدودة مثل الأجهزة المحمولة أو الأنظمة المدمجة. يحقق تقليل الدقة ضغطًا عن طريق تقليل دقة المعاملات، لكنه يسبب فقدًا معينًا في الدقة. لذلك، من الضروري تحقيق توازن بين حجم النموذج، وتعقيد العمليات الحسابية، والدقة أثناء عملية التقليل. تشمل طرق التقليل الشائعة تقليل الدقة بنقطة ثابتة، وتقليل الدقة بنقطة عائمة، وغيرها. يمكنك اختيار الاستراتيجية المناسبة حسب السيناريو والاحتياجات المحددة.

نأمل في نشر نموذج GenAI على أجهزة الحافة وتمكين المزيد من الأجهزة من الدخول في سيناريوهات GenAI، مثل الأجهزة المحمولة، و AI PC/Copilot+PC، وأجهزة إنترنت الأشياء التقليدية. من خلال نموذج التقليل، يمكننا نشره على أجهزة حافة مختلفة بناءً على اختلاف الأجهزة. وبالاقتران مع إطار تسريع النموذج ونموذج التقليل الذي توفره الشركات المصنعة للأجهزة، يمكننا بناء سيناريوهات تطبيق SLM أفضل.

في سيناريو التقليل، لدينا دقات مختلفة (INT4، INT8، FP16، FP32). فيما يلي شرح للدقات الشائعة الاستخدام في التقليل:

### **INT4**

تقليل دقة INT4 هو طريقة تقليل جذرية تقوم بتحويل أوزان وقيم تفعيل النموذج إلى أعداد صحيحة 4-بت. عادةً ما ينتج عن تقليل دقة INT4 فقدان أكبر في الدقة بسبب نطاق التمثيل الأصغر والدقة الأقل. ومع ذلك، بالمقارنة مع تقليل دقة INT8، يمكن لتقليل دقة INT4 أن يقلل بشكل أكبر من متطلبات التخزين وتعقيد العمليات الحسابية للنموذج. يجب ملاحظة أن تقليل دقة INT4 نادر نسبيًا في التطبيقات العملية، لأن الدقة المنخفضة جدًا قد تسبب تدهورًا كبيرًا في أداء النموذج. بالإضافة إلى ذلك، لا تدعم جميع الأجهزة عمليات INT4، لذا يجب مراعاة توافق الأجهزة عند اختيار طريقة التقليل.

### **INT8**

تقليل دقة INT8 هو عملية تحويل أوزان وقيم تفعيل النموذج من أعداد نقطية عائمة إلى أعداد صحيحة 8-بت. على الرغم من أن النطاق العددي الذي تمثله أعداد INT8 أصغر وأقل دقة، إلا أنه يمكن أن يقلل بشكل كبير من متطلبات التخزين والحساب. في تقليل دقة INT8، تمر الأوزان وقيم التفعيل بعملية تقليل تشمل التدرج والإزاحة، للحفاظ على معلومات النقطة العائمة الأصلية قدر الإمكان. أثناء الاستدلال، يتم إعادة تحويل هذه القيم المقلة إلى أعداد نقطة عائمة للحساب، ثم تُقلل مرة أخرى إلى INT8 للخطوة التالية. تتيح هذه الطريقة دقة كافية في معظم التطبيقات مع الحفاظ على كفاءة حسابية عالية.

### **FP16**

صيغة FP16، أي الأعداد العائمة 16-بت (float16)، تقلل من حجم الذاكرة إلى النصف مقارنة بالأعداد العائمة 32-بت (float32)، وهو ما يوفر مزايا كبيرة في تطبيقات التعلم العميق واسعة النطاق. تسمح صيغة FP16 بتحميل نماذج أكبر أو معالجة بيانات أكثر ضمن حدود ذاكرة GPU نفسها. مع استمرار دعم أجهزة GPU الحديثة لعمليات FP16، قد يؤدي استخدام صيغة FP16 أيضًا إلى تحسينات في سرعة الحساب. مع ذلك، تحتوي صيغة FP16 على عيوب متأصلة، وهي الدقة الأقل التي قد تؤدي إلى عدم استقرار عددي أو فقدان دقة في بعض الحالات.

### **FP32**

توفر صيغة FP32 دقة أعلى ويمكنها تمثيل نطاق واسع من القيم بدقة. في السيناريوهات التي يتم فيها إجراء عمليات رياضية معقدة أو تتطلب نتائج عالية الدقة، تُفضل صيغة FP32. لكن الدقة العالية تعني أيضًا استخدام ذاكرة أكبر ووقت حساب أطول. بالنسبة لنماذج التعلم العميق الكبيرة، خصوصًا عندما يكون هناك عدد كبير من معاملات النموذج وكمية ضخمة من البيانات، قد تسبب صيغة FP32 نفاد ذاكرة GPU أو تباطؤ في سرعة الاستدلال.

على الأجهزة المحمولة أو أجهزة إنترنت الأشياء، يمكننا تحويل نماذج Phi-3.x إلى INT4، بينما يمكن لأجهزة AI PC / Copilot PC استخدام دقات أعلى مثل INT8، FP16، FP32.

حاليًا، لدى الشركات المصنعة المختلفة للأجهزة أُطُر عمل مختلفة لدعم النماذج التوليدية، مثل OpenVINO من Intel، و QNN من Qualcomm، و MLX من Apple، و CUDA من Nvidia، وغيرها، بالاقتران مع تقليل دقة النموذج لإتمام النشر المحلي.

من الناحية التقنية، لدينا دعم صيغ مختلفة بعد التقليل، مثل صيغة PyTorch / Tensorflow، و GGUF، و ONNX. لقد أجريت مقارنة بين الصيغ وسيناريوهات التطبيق بين GGUF و ONNX. هنا أوصي بصيغة تقليل ONNX، التي تحظى بدعم جيد من إطار النموذج إلى الأجهزة. في هذا الفصل، سنركز على ONNX Runtime لـ GenAI، و OpenVINO، و Apple MLX لأداء تقليل دقة النموذج (إذا كان لديك طريقة أفضل، يمكنك أيضًا مشاركتها معنا عبر تقديم PR).

**يتضمن هذا الفصل**

1. [Quantizing Phi-3.5 / 4 using llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Quantizing Phi-3.5 / 4 using Generative AI extensions for onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Quantizing Phi-3.5 / 4 using Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Quantizing Phi-3.5 / 4 using Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

**إخلاء المسؤولية**:  
تمت ترجمة هذا المستند باستخدام خدمة الترجمة الآلية [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الرسمي والمعتمد. للمعلومات الهامة، يُنصح بالاستعانة بترجمة بشرية محترفة. نحن غير مسؤولين عن أي سوء فهم أو تفسير ناتج عن استخدام هذه الترجمة.