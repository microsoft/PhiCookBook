<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-03-27T08:20:50+00:00",
  "source_file": "md\\01.Introduction\\04\\QuantifyingPhi.md",
  "language_code": "ar"
}
-->
# **قياس عائلة Phi**

يشير تكميم النماذج إلى عملية تحويل معلمات النموذج (مثل الأوزان وقيم التفعيل) في نموذج الشبكة العصبية من نطاق قيم كبير (عادةً نطاق قيم مستمر) إلى نطاق قيم أصغر ومحدود. تتيح هذه التقنية تقليل حجم النموذج وتعقيد العمليات الحسابية، وتحسين كفاءة تشغيل النموذج في البيئات التي تعاني من قيود الموارد مثل الأجهزة المحمولة أو الأنظمة المدمجة. يتم تحقيق ضغط النموذج عن طريق تقليل دقة المعلمات، ولكنه في الوقت نفسه يسبب خسارة معينة في الدقة. لذلك، أثناء عملية التكميم، يجب تحقيق توازن بين حجم النموذج وتعقيد العمليات الحسابية والدقة. تشمل طرق التكميم الشائعة التكميم بالنقاط الثابتة والتكميم بالنقاط العائمة. يمكنك اختيار استراتيجية التكميم المناسبة بناءً على السيناريو المحدد والاحتياجات.

نأمل في نشر نموذج GenAI على الأجهزة الطرفية والسماح لمزيد من الأجهزة بالدخول إلى سيناريوهات GenAI، مثل الأجهزة المحمولة، وأجهزة الكمبيوتر المزودة بالذكاء الاصطناعي / Copilot+PC، والأجهزة التقليدية لإنترنت الأشياء. من خلال نموذج التكميم، يمكننا نشره على الأجهزة الطرفية المختلفة بناءً على الأجهزة المتنوعة. وبالاستفادة من إطار تسريع النموذج ونموذج التكميم المقدم من قبل الشركات المصنعة للأجهزة، يمكننا بناء سيناريوهات تطبيق SLM أفضل.

في سيناريو التكميم، لدينا دقات مختلفة (INT4، INT8، FP16، FP32). فيما يلي شرح للدقات الشائعة الاستخدام:

### **INT4**

تكميم INT4 هو طريقة تكميم جذرية تقوم بتحويل الأوزان وقيم التفعيل للنموذج إلى أعداد صحيحة مكونة من 4 بت. عادةً ما يؤدي تكميم INT4 إلى خسارة أكبر في الدقة نظرًا لنطاق التمثيل الأصغر والدقة الأقل. ومع ذلك، مقارنةً بتكميم INT8، يمكن لتكميم INT4 تقليل متطلبات التخزين وتعقيد العمليات الحسابية للنموذج بشكل أكبر. يجب ملاحظة أن تكميم INT4 نادر الاستخدام في التطبيقات العملية، لأن الدقة المنخفضة جدًا قد تؤدي إلى تدهور كبير في أداء النموذج. بالإضافة إلى ذلك، ليست كل الأجهزة تدعم عمليات INT4، لذلك يجب مراعاة التوافق مع الأجهزة عند اختيار طريقة التكميم.

### **INT8**

تكميم INT8 هو عملية تحويل أوزان وتفعيلات النموذج من أرقام بالنقاط العائمة إلى أعداد صحيحة مكونة من 8 بت. على الرغم من أن النطاق الرقمي الذي تمثله أعداد INT8 أصغر وأقل دقة، إلا أنه يمكن أن يقلل بشكل كبير من متطلبات التخزين والحساب. في تكميم INT8، تمر أوزان وقيم التفعيل للنموذج بعملية تكميم تشمل القياس والإزاحة للحفاظ على معلومات النقاط العائمة الأصلية قدر الإمكان. أثناء الاستدلال، يتم إعادة تحويل هذه القيم المكممة إلى أرقام بالنقاط العائمة للحساب، ثم يتم تكميمها مرة أخرى إلى INT8 للخطوة التالية. يمكن لهذه الطريقة توفير دقة كافية في معظم التطبيقات مع الحفاظ على كفاءة حسابية عالية.

### **FP16**

صيغة FP16، وهي أرقام بالنقاط العائمة المكونة من 16 بت (float16)، تقلل من استهلاك الذاكرة إلى النصف مقارنةً بالأرقام بالنقاط العائمة المكونة من 32 بت (float32)، مما يوفر مزايا كبيرة في تطبيقات التعلم العميق واسعة النطاق. تتيح صيغة FP16 تحميل نماذج أكبر أو معالجة بيانات أكثر ضمن نفس قيود ذاكرة GPU. ومع استمرار دعم أجهزة GPU الحديثة لعمليات FP16، قد يؤدي استخدام صيغة FP16 أيضًا إلى تحسينات في سرعة الحساب. ومع ذلك، فإن صيغة FP16 لها عيوبها المتأصلة، وهي الدقة الأقل، مما قد يؤدي إلى عدم استقرار رقمي أو فقدان الدقة في بعض الحالات.

### **FP32**

صيغة FP32 توفر دقة أعلى ويمكنها تمثيل نطاق واسع من القيم بدقة. في السيناريوهات التي تتطلب إجراء عمليات رياضية معقدة أو نتائج عالية الدقة، تُفضل صيغة FP32. ومع ذلك، فإن الدقة العالية تعني أيضًا استهلاكًا أكبر للذاكرة وزيادة في وقت الحساب. بالنسبة للنماذج الكبيرة للتعلم العميق، خاصةً عندما تحتوي على العديد من المعلمات وكمية هائلة من البيانات، قد تؤدي صيغة FP32 إلى نفاد ذاكرة GPU أو انخفاض سرعة الاستدلال.

على الأجهزة المحمولة أو أجهزة إنترنت الأشياء، يمكننا تحويل نماذج Phi-3.x إلى INT4، بينما يمكن لأجهزة AI PC / Copilot PC استخدام دقات أعلى مثل INT8 وFP16 وFP32.

حاليًا، لدى الشركات المصنعة للأجهزة المختلفة أطر عمل مختلفة لدعم النماذج التوليدية، مثل OpenVINO من Intel، وQNN من Qualcomm، وMLX من Apple، وCUDA من Nvidia، وغيرها، والتي يمكن استخدامها مع تكميم النماذج لإكمال النشر المحلي.

من الناحية التقنية، لدينا دعم لتنسيقات مختلفة بعد التكميم، مثل PyTorch / Tensorflow، GGUF، وONNX. لقد أجريت مقارنة بين تنسيقات GGUF وONNX وسيناريوهات استخدامها. هنا أوصي بتنسيق التكميم ONNX، الذي يتمتع بدعم جيد من إطار النموذج إلى الأجهزة. في هذا الفصل، سنركز على ONNX Runtime لـ GenAI، OpenVINO، وApple MLX لتنفيذ تكميم النماذج (إذا كان لديك طريقة أفضل، يمكنك أيضًا تقديمها لنا من خلال تقديم PR).

**يتضمن هذا الفصل**

1. [تكميم Phi-3.5 / 4 باستخدام llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [تكميم Phi-3.5 / 4 باستخدام ملحقات الذكاء الاصطناعي التوليدي لـ onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [تكميم Phi-3.5 / 4 باستخدام Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [تكميم Phi-3.5 / 4 باستخدام إطار عمل Apple MLX](./UsingAppleMLXQuantifyingPhi.md)

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق. للحصول على معلومات حاسمة، يُوصى باللجوء إلى ترجمة احترافية بواسطة مترجم بشري. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة تنشأ عن استخدام هذه الترجمة.