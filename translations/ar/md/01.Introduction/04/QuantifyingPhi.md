<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-04T07:20:33+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ar"
}
-->
# **تكميم عائلة Phi**

يشير تكميم النماذج إلى عملية رسم المعاملات (مثل الأوزان وقيم التنشيط) في نموذج الشبكة العصبية من نطاق قيم كبير (عادة نطاق قيم مستمر) إلى نطاق محدود أصغر. يمكن أن تقلل هذه التقنية من حجم النموذج وتعقيد الحسابات وتحسن كفاءة تشغيل النموذج في بيئات محدودة الموارد مثل الأجهزة المحمولة أو الأنظمة المدمجة. يحقق تكميم النماذج ضغطًا عن طريق تقليل دقة المعاملات، ولكنه يؤدي أيضًا إلى فقدان معين في الدقة. لذلك، في عملية التكميم، من الضروري موازنة حجم النموذج وتعقيد الحسابات والدقة. تشمل طرق التكميم الشائعة التكميم بنقطة ثابتة والتكميم بنقطة عائمة، وما إلى ذلك. يمكنك اختيار استراتيجية التكميم المناسبة وفقًا للسيناريو والاحتياجات المحددة.

نأمل نشر نموذج GenAI على أجهزة الحافة والسماح لمزيد من الأجهزة بالدخول في سيناريوهات GenAI، مثل الأجهزة المحمولة، AI PC/Copilot+PC، والأجهزة التقليدية المتصلة بالإنترنت للأشياء (IoT). من خلال نموذج التكميم، يمكننا نشره على أجهزة حافة مختلفة بناءً على اختلاف الأجهزة. بالتعاون مع إطار تسريع النماذج ونموذج التكميم المقدمين من مصنعي العتاد، يمكننا بناء سيناريوهات تطبيق SLM أفضل.

في سيناريو التكميم، لدينا دقات مختلفة (INT4, INT8, FP16, FP32). فيما يلي توضيح للدقات الشائعة المستخدمة في التكميم

### **INT4**

تكميم INT4 هو طريقة تكميم جذرية تقوم بتكميم أوزان وقيم التنشيط في النموذج إلى أعداد صحيحة بطول 4 بت. عادة ما يؤدي تكميم INT4 إلى فقدان دقة أكبر بسبب نطاق التمثيل الأصغر والدقة المنخفضة. ومع ذلك، مقارنة بتكميم INT8، يمكن لتكميم INT4 تقليل متطلبات التخزين وتعقيد العمليات الحسابية للنموذج بشكل أكبر. تجدر الإشارة إلى أن تكميم INT4 نادر نسبيًا في التطبيقات العملية، لأن الدقة المنخفضة جدًا قد تسبب تدهورًا كبيرًا في أداء النموذج. بالإضافة إلى ذلك، لا تدعم جميع الأجهزة عمليات INT4، لذلك يجب مراعاة توافق العتاد عند اختيار طريقة التكميم.

### **INT8**

تكميم INT8 هو عملية تحويل أوزان وتنشيطات النموذج من أعداد نقطية عائمة إلى أعداد صحيحة بطول 8 بت. على الرغم من أن نطاق الأعداد التي يمثلها INT8 أصغر وأقل دقة، إلا أنه يمكن أن يقلل بشكل كبير من متطلبات التخزين والحساب. في تكميم INT8، تمر الأوزان وقيم التنشيط في النموذج بعملية تكميم تتضمن التحجيم والانحراف، للحفاظ على معلومات النقطة العائمة الأصلية قدر الإمكان. أثناء الاستدلال، تتم إعادة تحويل هذه القيم المكممة إلى أعداد نقطية عائمة للحساب، ثم تُكمم مرة أخرى إلى INT8 للخطوة التالية. يمكن أن توفر هذه الطريقة دقة كافية في معظم التطبيقات مع الحفاظ على كفاءة حسابية عالية.

### **FP16**

تنسيق FP16، أي الأعداد العائمة بطول 16 بت (float16)، يقلل من حجم الذاكرة إلى النصف مقارنة بالأعداد العائمة بطول 32 بت (float32)، وهو ما يمثل ميزة كبيرة في تطبيقات التعلم العميق على نطاق واسع. يتيح تنسيق FP16 تحميل نماذج أكبر أو معالجة بيانات أكثر ضمن حدود ذاكرة GPU نفسها. ومع استمرار دعم عتاد GPU الحديث لعمليات FP16، قد يجلب استخدام تنسيق FP16 أيضًا تحسينات في سرعة الحساب. ومع ذلك، فإن تنسيق FP16 له عيوبه الكامنة أيضًا، وهي الدقة الأقل، مما قد يؤدي إلى عدم استقرار عددي أو فقدان الدقة في بعض الحالات.

### **FP32**

يوفر تنسيق FP32 دقة أعلى ويمكنه تمثيل مجموعة واسعة من القيم بدقة. في السيناريوهات التي تُجرى فيها عمليات رياضية معقدة أو عندما تكون النتائج عالية الدقة مطلوبة، يُفضل استخدام تنسيق FP32. ومع ذلك، الدقة العالية تعني أيضًا استخدام ذاكرة أكبر وزمن حساب أطول. بالنسبة لنماذج التعلم العميق الكبيرة، خاصة عندما يكون هناك العديد من معلمات النموذج وكمية ضخمة من البيانات، قد يتسبب تنسيق FP32 في نفاد ذاكرة GPU أو انخفاض في سرعة الاستدلال.

على الأجهزة المحمولة أو أجهزة IoT، يمكننا تحويل نماذج Phi-3.x إلى INT4، بينما يمكن لأجهزة AI PC / Copilot PC استخدام دقة أعلى مثل INT8, FP16, FP 32.

في الوقت الحالي، لدى مصنعي العتاد المختلفين أطر عمل مختلفة لدعم النماذج التوليدية، مثل OpenVINO من Intel، QNN من Qualcomm، MLX من Apple، وCUDA من Nvidia، وغيرها، بالتكامل مع تكميم النماذج لإكمال النشر المحلي.

من حيث التكنولوجيا، لدينا دعم لصيغ مختلفة بعد التكميم، مثل تنسيقات PyTorch / TensorFlow، GGUF، وONNX. لقد قمت بعمل مقارنة بين الصيغ وسيناريوهات التطبيق بين GGUF وONNX. هنا أوصي بصيغة تكميم ONNX، التي تحظى بدعم جيد من إطار النموذج إلى العتاد. في هذا الفصل، سنركز على ONNX Runtime للـ GenAI، وOpenVINO، وApple MLX لأداء تكميم النماذج (إذا كان لديك طريقة أفضل، يمكنك أيضًا تقديمها لنا عن طريق تقديم PR)

**يتضمن هذا الفصل**

1. [تكميم Phi-3.5 / 4 باستخدام llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [تكميم Phi-3.5 / 4 باستخدام امتدادات الذكاء التوليدي لـ onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [تكميم Phi-3.5 / 4 باستخدام Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [تكميم Phi-3.5 / 4 باستخدام إطار عمل Apple MLX](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
إخلاء المسؤولية:
تمت ترجمة هذا المستند باستخدام خدمة الترجمة الآلية Co-op Translator (https://github.com/Azure/co-op-translator). بينما نسعى إلى الدقة، يرجى العلم بأن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية هو المصدر المعتمد. للمعلومات الحرجة، يُنصح بالاستعانة بترجمة بشرية محترفة. لا نتحمل أي مسؤولية عن أي سوء فهم أو تفسير خاطئ ينشأ عن استخدام هذه الترجمة.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->