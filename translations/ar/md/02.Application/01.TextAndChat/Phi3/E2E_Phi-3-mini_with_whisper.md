<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "006e8cf75211d3297f24e1b22e38955f",
  "translation_date": "2025-03-27T10:48:37+00:00",
  "source_file": "md\\02.Application\\01.TextAndChat\\Phi3\\E2E_Phi-3-mini_with_whisper.md",
  "language_code": "ar"
}
-->
# روبوت الدردشة التفاعلي Phi 3 Mini 4K Instruct مع Whisper

## نظرة عامة

روبوت الدردشة التفاعلي Phi 3 Mini 4K Instruct هو أداة تتيح للمستخدمين التفاعل مع عرض Microsoft Phi 3 Mini 4K Instruct التجريبي باستخدام إدخال نصي أو صوتي. يمكن استخدام روبوت الدردشة هذا لمجموعة متنوعة من المهام، مثل الترجمة، تحديثات الطقس، وجمع المعلومات العامة.

### كيفية البدء

لاستخدام روبوت الدردشة هذا، اتبع الخطوات التالية:

1. افتح [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb)
2. في النافذة الرئيسية للمفكرة (notebook)، سترى واجهة صندوق دردشة تحتوي على مربع إدخال نصي وزر "إرسال".
3. لاستخدام روبوت الدردشة النصي، قم ببساطة بكتابة رسالتك في مربع الإدخال النصي وانقر على زر "إرسال". سيرد روبوت الدردشة بملف صوتي يمكن تشغيله مباشرةً من داخل المفكرة.

**ملاحظة**: تتطلب هذه الأداة وحدة معالجة رسومات (GPU) والوصول إلى نماذج Microsoft Phi-3 و OpenAI Whisper، والتي تُستخدم للتعرف على الكلام والترجمة.

### متطلبات GPU

لتشغيل هذا العرض التجريبي، تحتاج إلى ذاكرة GPU بسعة 12 جيجابايت.

تعتمد متطلبات الذاكرة لتشغيل عرض **Microsoft-Phi-3-Mini-4K instruct** التجريبي على GPU على عدة عوامل، مثل حجم البيانات المدخلة (الصوت أو النص)، اللغة المستخدمة في الترجمة، سرعة النموذج، والذاكرة المتوفرة على GPU.

بشكل عام، تم تصميم نموذج Whisper ليعمل على وحدات معالجة الرسومات. الحد الأدنى الموصى به من ذاكرة GPU لتشغيل نموذج Whisper هو 8 جيجابايت، ولكنه يمكن أن يتعامل مع كميات أكبر من الذاكرة إذا لزم الأمر.

من المهم ملاحظة أن تشغيل كمية كبيرة من البيانات أو حجم كبير من الطلبات على النموذج قد يتطلب ذاكرة GPU أكبر و/أو قد يتسبب في مشاكل في الأداء. يُوصى باختبار حالتك باستخدام تكوينات مختلفة ومراقبة استخدام الذاكرة لتحديد الإعدادات المثلى لاحتياجاتك الخاصة.

## مثال شامل (E2E) لروبوت الدردشة التفاعلي Phi 3 Mini 4K Instruct مع Whisper

توضح المفكرة بعنوان [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) كيفية استخدام عرض Microsoft Phi 3 Mini 4K Instruct التجريبي لتوليد النصوص من إدخال صوتي أو نص مكتوب. تحدد المفكرة عدة وظائف:

1. `tts_file_name(text)`: تُولد هذه الوظيفة اسم ملف بناءً على النص المدخل لحفظ ملف الصوت الناتج.
2. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: تستخدم هذه الوظيفة واجهة Edge TTS API لتوليد ملف صوتي من قائمة من أجزاء النص المدخل. تتضمن معلمات الإدخال قائمة الأجزاء، معدل الكلام، اسم الصوت، ومسار الخرج لحفظ ملف الصوت الناتج.
3. `talk(input_text)`: تُولد هذه الوظيفة ملفًا صوتيًا باستخدام واجهة Edge TTS API وحفظه باسم عشوائي في دليل /content/audio. معلمة الإدخال هي النص المدخل المراد تحويله إلى كلام.
4. `run_text_prompt(message, chat_history)`: تستخدم هذه الوظيفة عرض Microsoft Phi 3 Mini 4K Instruct التجريبي لتوليد ملف صوتي من رسالة مدخلة وتضيفه إلى سجل الدردشة.
5. `run_audio_prompt(audio, chat_history)`: تُحول هذه الوظيفة ملفًا صوتيًا إلى نص باستخدام واجهة Whisper API ثم تمرره إلى الوظيفة `run_text_prompt()`.
6. يُطلق الكود تطبيق Gradio يتيح للمستخدمين التفاعل مع عرض Phi 3 Mini 4K Instruct التجريبي عن طريق كتابة الرسائل أو تحميل الملفات الصوتية. يتم عرض الناتج كرسالة نصية داخل التطبيق.

## استكشاف الأخطاء وإصلاحها

### تثبيت تعريفات Cuda GPU

1. تأكد من تحديث تطبيقات Linux الخاصة بك

    ```bash
    sudo apt update
    ```

1. قم بتثبيت تعريفات Cuda

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. قم بتسجيل موقع تعريف Cuda

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. التحقق من حجم ذاكرة Nvidia GPU (مطلوب 12 جيجابايت من ذاكرة GPU)

    ```bash
    nvidia-smi
    ```

1. تفريغ الذاكرة المؤقتة: إذا كنت تستخدم PyTorch، يمكنك استدعاء torch.cuda.empty_cache() لتحرير جميع الذاكرة المؤقتة غير المستخدمة بحيث يمكن استخدامها بواسطة تطبيقات GPU الأخرى.

    ```python
    torch.cuda.empty_cache() 
    ```

1. التحقق من Cuda Nvidia

    ```bash
    nvcc --version
    ```

1. قم بتنفيذ المهام التالية لإنشاء رمز Hugging Face.

    - انتقل إلى [صفحة إعدادات رمز Hugging Face](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo).
    - اختر **رمز جديد**.
    - أدخل **اسم المشروع** الذي تريد استخدامه.
    - اختر **النوع** ليكون **كتابة**.

> **ملاحظة**
>
> إذا واجهت الخطأ التالي:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> لحل ذلك، اكتب الأمر التالي داخل الطرفية.
>
> ```bash
> sudo ldconfig
> ```

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة الآلية [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية هو المصدر الموثوق. للحصول على معلومات حاسمة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة ناتجة عن استخدام هذه الترجمة.