{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etkileşimli Phi 3 Mini 4K Talimatlı Sohbet Botu ile Whisper\n",
    "\n",
    "### Giriş:\n",
    "Etkileşimli Phi 3 Mini 4K Talimatlı Sohbet Botu, kullanıcıların Microsoft Phi 3 Mini 4K talimatlı demo ile metin veya sesli giriş kullanarak etkileşimde bulunmasını sağlayan bir araçtır. Sohbet botu, çeviri, hava durumu güncellemeleri ve genel bilgi toplama gibi çeşitli görevler için kullanılabilir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[Huggingface Erişim Jetonunuzu Oluşturun](https://huggingface.co/settings/tokens)\n",
    "\n",
    "Yeni bir jeton oluşturun  \n",
    "Yeni bir ad verin  \n",
    "Yazma izinlerini seçin  \n",
    "Jetonu kopyalayın ve güvenli bir yerde saklayın\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aşağıdaki Python kodu iki ana görevi yerine getirir: `os` modülünü içe aktarmak ve bir ortam değişkeni ayarlamak.\n",
    "\n",
    "1. `os` modülünü içe aktarmak:\n",
    "   - Python'daki `os` modülü, işletim sistemiyle etkileşim kurmanın bir yolunu sağlar. Ortam değişkenlerine erişmek, dosya ve dizinlerle çalışmak gibi çeşitli işletim sistemiyle ilgili görevleri gerçekleştirmenize olanak tanır.\n",
    "   - Bu kodda, `os` modülü `import` ifadesi kullanılarak içe aktarılmıştır. Bu ifade, `os` modülünün işlevselliğini mevcut Python betiğinde kullanılabilir hale getirir.\n",
    "\n",
    "2. Bir ortam değişkeni ayarlamak:\n",
    "   - Ortam değişkeni, işletim sisteminde çalışan programlar tarafından erişilebilen bir değerdir. Birden fazla program tarafından kullanılabilecek yapılandırma ayarlarını veya diğer bilgileri depolamanın bir yoludur.\n",
    "   - Bu kodda, yeni bir ortam değişkeni `os.environ` sözlüğü kullanılarak ayarlanıyor. Sözlüğün anahtarı `'HF_TOKEN'` ve değeri `HUGGINGFACE_TOKEN` değişkeninden atanıyor.\n",
    "   - `HUGGINGFACE_TOKEN` değişkeni, bu kod parçasının hemen üstünde tanımlanmıştır ve `\"hf_**************\"` gibi bir dize değeri `#@param` sözdizimi kullanılarak atanmıştır. Bu sözdizimi genellikle Jupyter defterlerinde, kullanıcı girdisi ve parametre yapılandırmasını doğrudan defter arayüzünde sağlamak için kullanılır.\n",
    "   - `'HF_TOKEN'` ortam değişkeni ayarlandığında, programın diğer bölümleri veya aynı işletim sisteminde çalışan diğer programlar tarafından erişilebilir hale gelir.\n",
    "\n",
    "Genel olarak, bu kod `os` modülünü içe aktarır ve `HUGGINGFACE_TOKEN` değişkeninde sağlanan değerle `'HF_TOKEN'` adlı bir ortam değişkeni ayarlar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu kod parçası, Jupyter Notebook veya IPython'daki mevcut hücrenin çıktısını temizlemek için kullanılan clear_output adlı bir fonksiyonu tanımlar. Kodun işlevselliğini anlamak için parçalayalım:\n",
    "\n",
    "clear_output fonksiyonu, wait adlı bir parametre alır. Bu parametre bir boolean (doğru/yanlış) değerdir. Varsayılan olarak, wait False olarak ayarlanmıştır. Bu parametre, fonksiyonun mevcut çıktıyı temizlemeden önce yeni bir çıktı mevcut olana kadar bekleyip beklemeyeceğini belirler.\n",
    "\n",
    "Fonksiyonun kendisi, mevcut hücrenin çıktısını temizlemek için kullanılır. Jupyter Notebook veya IPython'da bir hücre çıktı ürettiğinde, örneğin yazdırılmış metin veya grafiksel çizimler gibi, bu çıktı hücrenin altında görüntülenir. clear_output fonksiyonu, bu çıktıyı temizlemenize olanak tanır.\n",
    "\n",
    "Kod parçasında fonksiyonun uygulanışı verilmemiştir, bu durum üç nokta (...) ile belirtilmiştir. Üç nokta, çıktıyı temizleme işlemini gerçekleştiren gerçek kodun yerine bir yer tutucu görevi görür. Fonksiyonun uygulanışı, mevcut hücreden çıkışı kaldırmak için Jupyter Notebook veya IPython API'si ile etkileşim kurmayı içerebilir.\n",
    "\n",
    "Genel olarak, bu fonksiyon, Jupyter Notebook veya IPython'daki mevcut hücrenin çıktısını temizlemek için kullanışlı bir yöntem sunar. Bu, etkileşimli kodlama oturumları sırasında görüntülenen çıktıyı yönetmeyi ve güncellemeyi kolaylaştırır.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge TTS hizmetini kullanarak metinden konuşmaya (TTS) işlemi gerçekleştirin. İlgili fonksiyonların uygulamalarını tek tek inceleyelim:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Bu fonksiyon, bir giriş değeri alır ve TTS sesi için hız dizesini hesaplar. Giriş değeri, konuşmanın istenen hızını temsil eder; 1 değeri normal hızı ifade eder. Fonksiyon, giriş değerinden 1 çıkarır, 100 ile çarpar ve giriş değerinin 1'e eşit veya büyük olup olmadığına göre işareti belirler. Fonksiyon, \"{sign}{rate}\" formatında hız dizesini döndürür.\n",
    "\n",
    "2. `make_chunks(input_text, language)`: Bu fonksiyon, bir giriş metni ve bir dil parametresi alır. Giriş metnini dilin özel kurallarına göre parçalara böler. Bu uygulamada, dil \"English\" ise, fonksiyon metni her nokta (\".\") ile böler ve baştaki veya sondaki boşlukları kaldırır. Daha sonra her parçaya bir nokta ekler ve filtrelenmiş parça listesini döndürür.\n",
    "\n",
    "3. `tts_file_name(text)`: Bu fonksiyon, giriş metnine dayanarak TTS ses dosyası için bir dosya adı oluşturur. Metin üzerinde birkaç dönüşüm gerçekleştirir: sondaki noktayı (varsa) kaldırır, metni küçük harfe çevirir, baştaki ve sondaki boşlukları temizler ve boşlukları alt çizgi ile değiştirir. Daha sonra metni maksimum 25 karaktere kadar kısaltır (eğer daha uzunsa) veya metin boşsa tamamını kullanır. Son olarak, [`uuid`] modülünü kullanarak rastgele bir dize oluşturur ve kısaltılmış metinle birleştirerek \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\" formatında dosya adını oluşturur.\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Bu fonksiyon, birden fazla ses dosyasını tek bir ses dosyasında birleştirir. Bir ses dosyası yolu listesi ve bir çıktı yolu parametresi alır. Fonksiyon, [`merged_audio`] adlı boş bir `AudioSegment` nesnesi başlatır. Daha sonra her ses dosyası yolunu iterasyonla işler, `pydub` kütüphanesinden `AudioSegment.from_file()` yöntemiyle ses dosyasını yükler ve mevcut ses dosyasını [`merged_audio`] nesnesine ekler. Son olarak, birleştirilmiş sesi belirtilen çıktı yoluna MP3 formatında dışa aktarır.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: Bu fonksiyon, Edge TTS hizmetini kullanarak TTS işlemini gerçekleştirir. Bir metin parçası listesi, konuşma hızı, ses adı ve kaydetme yolu parametrelerini alır. Parça sayısı 1'den büyükse, fonksiyon bireysel parça ses dosyalarını depolamak için bir dizin oluşturur. Daha sonra her parçayı iterasyonla işler, `calculate_rate_string()` fonksiyonunu, ses adını ve parça metnini kullanarak bir Edge TTS komutu oluşturur ve `os.system()` fonksiyonuyla komutu çalıştırır. Komut başarıyla çalıştırılırsa, oluşturulan ses dosyasının yolunu bir listeye ekler. Tüm parçalar işlendiğinde, bireysel ses dosyalarını `merge_audio_files()` fonksiyonuyla birleştirir ve birleştirilmiş sesi belirtilen kaydetme yoluna kaydeder. Eğer sadece bir parça varsa, doğrudan Edge TTS komutunu oluşturur ve sesi kaydetme yoluna kaydeder. Son olarak, oluşturulan ses dosyasının kaydetme yolunu döndürür.\n",
    "\n",
    "6. `random_audio_name_generate()`: Bu fonksiyon, [`uuid`] modülünü kullanarak rastgele bir ses dosyası adı oluşturur. Rastgele bir UUID oluşturur, bunu bir dizeye çevirir, ilk 8 karakterini alır, \".mp3\" uzantısını ekler ve rastgele ses dosyası adını döndürür.\n",
    "\n",
    "7. `talk(input_text)`: Bu fonksiyon, TTS işlemini gerçekleştirmek için ana giriş noktasıdır. Bir giriş metni parametresi alır. Öncelikle giriş metninin uzunluğunu kontrol eder ve uzun bir cümle olup olmadığını (600 karakterden büyük veya eşit) belirler. Uzunluğa ve `translate_text_flag` değişkeninin değerine bağlı olarak dili belirler ve `make_chunks()` fonksiyonunu kullanarak metin parçaları listesini oluşturur. Daha sonra ses dosyası için bir kaydetme yolu oluşturmak üzere `random_audio_name_generate()` fonksiyonunu çağırır. Son olarak, TTS işlemini gerçekleştirmek için `edge_free_tts()` fonksiyonunu çağırır ve oluşturulan ses dosyasının kaydetme yolunu döndürür.\n",
    "\n",
    "Genel olarak, bu fonksiyonlar giriş metnini parçalara bölmek, ses dosyası için bir dosya adı oluşturmak, Edge TTS hizmetini kullanarak TTS işlemini gerçekleştirmek ve bireysel ses dosyalarını tek bir ses dosyasında birleştirmek için birlikte çalışır.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "İki fonksiyonun uygulanması: convert_to_text ve run_text_prompt, ayrıca iki sınıfın tanımı: str ve Audio.\n",
    "\n",
    "convert_to_text fonksiyonu, bir audio_path girdisi alır ve whisper_model adlı bir model kullanarak sesi metne dönüştürür. Fonksiyon, öncelikle gpu bayrağının True olarak ayarlanıp ayarlanmadığını kontrol eder. Eğer True ise, whisper_model belirli parametrelerle kullanılır: word_timestamps=True, fp16=True, language='English', ve task='translate'. Eğer gpu bayrağı False ise, whisper_model fp16=False ile kullanılır. Ortaya çıkan transkripsiyon 'scan.txt' adlı bir dosyaya kaydedilir ve metin olarak geri döndürülür.\n",
    "\n",
    "run_text_prompt fonksiyonu, bir message ve chat_history girdisi alır. Bu fonksiyon, phi_demo fonksiyonunu kullanarak girdiye dayalı olarak bir chatbot'tan yanıt oluşturur. Oluşturulan yanıt daha sonra talk fonksiyonuna iletilir, bu yanıtı bir ses dosyasına dönüştürür ve dosya yolunu geri döndürür. Audio sınıfı, ses dosyasını görüntülemek ve oynatmak için kullanılır. Ses, IPython.display modülünden display fonksiyonu kullanılarak görüntülenir ve Audio nesnesi autoplay=True parametresi ile oluşturulur, böylece ses otomatik olarak çalmaya başlar. chat_history, girdiyi ve oluşturulan yanıtı içerecek şekilde güncellenir ve boş bir string ile güncellenmiş chat_history geri döndürülür.\n",
    "\n",
    "str sınıfı, Python'da karakter dizilerini temsil eden yerleşik bir sınıftır. Bu sınıf, capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill ve daha fazlası gibi çeşitli yöntemler sağlar. Bu yöntemler, arama, değiştirme, biçimlendirme ve dizilerle çalışma gibi işlemleri gerçekleştirmenize olanak tanır.\n",
    "\n",
    "Audio sınıfı, bir ses nesnesini temsil eden özel bir sınıftır. Jupyter Notebook ortamında bir ses oynatıcı oluşturmak için kullanılır. Sınıf, data, filename, url, embed, rate, autoplay ve normalize gibi çeşitli parametreleri kabul eder. data parametresi bir numpy array, örneklerin bir listesi, bir dosya adı veya URL'yi temsil eden bir string ya da ham PCM verisi olabilir. filename parametresi, ses verilerini yüklemek için yerel bir dosya belirtmek için kullanılır ve url parametresi, ses verilerini indirmek için bir URL belirtmek için kullanılır. embed parametresi, ses verilerinin bir veri URI kullanılarak mı yoksa orijinal kaynaktan mı referans alınacağını belirler. rate parametresi, ses verilerinin örnekleme oranını belirtir. autoplay parametresi, sesin otomatik olarak çalınıp çalınmayacağını belirler. normalize parametresi, ses verilerinin maksimum olası aralığa yeniden ölçeklenip ölçeklenmeyeceğini belirtir. Audio sınıfı ayrıca reload gibi ses verilerini dosya veya URL'den yeniden yüklemek için yöntemler ve src_attr, autoplay_attr ve element_id_attr gibi HTML'deki ses öğesi için ilgili özellikleri almak için öznitelikler sağlar.\n",
    "\n",
    "Genel olarak, bu fonksiyonlar ve sınıflar, sesi metne dönüştürmek, bir chatbot'tan sesli yanıtlar oluşturmak ve Jupyter Notebook ortamında sesleri görüntülemek ve oynatmak için kullanılır.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Feragatname**:  \nBu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hata veya yanlışlıklar içerebileceğini lütfen unutmayın. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalardan sorumlu değiliz.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T19:58:15+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "tr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}