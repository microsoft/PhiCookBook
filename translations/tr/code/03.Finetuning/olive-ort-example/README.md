<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4164123a700fecd535d850f09506d72a",
  "translation_date": "2025-07-16T16:03:33+00:00",
  "source_file": "code/03.Finetuning/olive-ort-example/README.md",
  "language_code": "tr"
}
-->
# Olive kullanarak Phi3'Ã¼ ince ayar yapma

Bu Ã¶rnekte Olive kullanarak:

1. LoRA adaptÃ¶rÃ¼nÃ¼ cÃ¼mleleri ÃœzgÃ¼n, Mutlu, Korku, ÅaÅŸkÄ±n olarak sÄ±nÄ±flandÄ±rmak iÃ§in ince ayar yapacaksÄ±nÄ±z.
1. AdaptÃ¶r aÄŸÄ±rlÄ±klarÄ±nÄ± temel modele birleÅŸtireceksiniz.
1. Modeli `int4` formatÄ±nda optimize edip kuantize edeceksiniz.

AyrÄ±ca, ince ayar yapÄ±lmÄ±ÅŸ modeli ONNX Runtime (ORT) Generate API kullanarak nasÄ±l Ã§Ä±karÄ±m yapacaÄŸÄ±nÄ±zÄ± gÃ¶stereceÄŸiz.

> **âš ï¸ Ä°nce ayar iÃ§in uygun bir GPU'ya ihtiyacÄ±nÄ±z olacak - Ã¶rneÄŸin, A10, V100, A100.**

## ğŸ’¾ Kurulum

Yeni bir Python sanal ortamÄ± oluÅŸturun (Ã¶rneÄŸin, `conda` kullanarak):

```bash
conda create -n olive-ai python=3.11
conda activate olive-ai
```

Sonra, Olive ve ince ayar iÅŸ akÄ±ÅŸÄ± iÃ§in gerekli baÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin:

```bash
cd Phi-3CookBook/code/04.Finetuning/olive-ort-example
pip install olive-ai[gpu]
pip install -r requirements.txt
```

## ğŸ§ª Olive kullanarak Phi3'Ã¼ ince ayar yapma
[Olive yapÄ±landÄ±rma dosyasÄ±](../../../../../code/03.Finetuning/olive-ort-example/phrase-classification.json) aÅŸaÄŸÄ±daki *iÅŸ akÄ±ÅŸÄ±* ve *adÄ±mlarÄ±* iÃ§erir:

Phi3 -> LoRA -> MergeAdapterWeights -> ModelBuilder

YÃ¼ksek seviyede, bu iÅŸ akÄ±ÅŸÄ± ÅŸunlarÄ± yapacak:

1. Phi3'Ã¼ [dataset/data-classification.json](../../../../../code/03.Finetuning/olive-ort-example/dataset/dataset-classification.json) verisi kullanarak (150 adÄ±m boyunca, deÄŸiÅŸtirebilirsiniz) ince ayar yapacak.
1. LoRA adaptÃ¶r aÄŸÄ±rlÄ±klarÄ±nÄ± temel modele birleÅŸtirecek. Bu, ONNX formatÄ±nda tek bir model dosyasÄ± elde etmenizi saÄŸlar.
1. Model Builder, modeli ONNX runtime iÃ§in optimize edecek *ve* modeli `int4` formatÄ±nda kuantize edecek.

Ä°ÅŸ akÄ±ÅŸÄ±nÄ± Ã§alÄ±ÅŸtÄ±rmak iÃ§in:

```bash
olive run --config phrase-classification.json
```

Olive tamamlandÄ±ÄŸÄ±nda, optimize edilmiÅŸ `int4` ince ayar yapÄ±lmÄ±ÅŸ Phi3 modeliniz ÅŸu konumda olacaktÄ±r: `code/04.Finetuning/olive-ort-example/models/lora-merge-mb/gpu-cuda_model`.

## ğŸ§‘â€ğŸ’» Ä°nce ayar yapÄ±lmÄ±ÅŸ Phi3'Ã¼ uygulamanÄ±za entegre edin

UygulamayÄ± Ã§alÄ±ÅŸtÄ±rmak iÃ§in:

```bash
python app/app.py --phrase "cricket is a wonderful sport!" --model-path models/lora-merge-mb/gpu-cuda_model
```

Bu yanÄ±t, cÃ¼mlenin tek kelimelik sÄ±nÄ±flandÄ±rmasÄ± olmalÄ±dÄ±r (ÃœzgÃ¼n/Mutlu/Korku/ÅaÅŸkÄ±n).

**Feragatname**:  
Bu belge, AI Ã§eviri servisi [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶sterilse de, otomatik Ã§evirilerin hatalar veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±nÄ±z. Orijinal belge, kendi dilinde yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ± sonucu oluÅŸabilecek yanlÄ±ÅŸ anlamalar veya yorum hatalarÄ±ndan sorumlu deÄŸiliz.