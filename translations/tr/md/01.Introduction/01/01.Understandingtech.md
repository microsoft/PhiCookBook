<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:44:21+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "tr"
}
-->
# Bahsedilen temel teknolojiler şunlardır

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12 üzerine inşa edilmiş, donanım hızlandırmalı makine öğrenimi için düşük seviyeli bir API.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia tarafından geliştirilen, grafik işlem birimleri (GPU) üzerinde genel amaçlı işlem yapmayı sağlayan paralel hesaplama platformu ve uygulama programlama arayüzü (API) modeli.
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - Farklı makine öğrenimi çerçeveleri arasında birlikte çalışabilirlik sağlayan, makine öğrenimi modellerini temsil etmek için tasarlanmış açık format.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - Makine öğrenimi modellerini temsil etmek ve güncellemek için kullanılan bir format; özellikle 4-8 bit kuantizasyon ile CPU’larda etkili çalışan küçük dil modelleri için faydalıdır.

## DirectML

DirectML, donanım hızlandırmalı makine öğrenimini mümkün kılan düşük seviyeli bir API’dir. GPU hızlandırmasını kullanmak için DirectX 12 üzerine inşa edilmiştir ve satıcı bağımsızdır; yani farklı GPU üreticileri arasında çalışması için kod değişikliği gerektirmez. Genellikle GPU’larda model eğitimi ve çıkarım iş yükleri için kullanılır.

Donanım desteği açısından, DirectML AMD entegre ve ayrık GPU’lar, Intel entegre GPU’lar ve NVIDIA ayrık GPU’lar dahil olmak üzere geniş bir GPU yelpazesiyle çalışacak şekilde tasarlanmıştır. Windows AI Platform’un bir parçasıdır ve Windows 10 & 11 üzerinde desteklenir; böylece herhangi bir Windows cihazında model eğitimi ve çıkarımı yapılabilir.

DirectML ile ilgili güncellemeler ve fırsatlar olmuştur; örneğin 150’ye kadar ONNX operatörünü desteklemesi ve hem ONNX runtime hem de WinML tarafından kullanılması. Büyük Entegre Donanım Üreticileri (IHV’ler) tarafından desteklenmekte ve çeşitli metakomutları uygulamaktadır.

## CUDA

CUDA, Compute Unified Device Architecture’ın kısaltmasıdır ve Nvidia tarafından oluşturulan paralel hesaplama platformu ve uygulama programlama arayüzü (API) modelidir. Yazılım geliştiricilerin CUDA destekli grafik işlem birimini (GPU) genel amaçlı işlem için kullanmalarını sağlar; bu yaklaşıma GPGPU (Grafik İşlem Birimlerinde Genel Amaçlı Hesaplama) denir. CUDA, Nvidia’nın GPU hızlandırmasının temelini oluşturur ve makine öğrenimi, bilimsel hesaplama ve video işleme gibi birçok alanda yaygın olarak kullanılır.

CUDA’nın donanım desteği Nvidia’nın GPU’ları ile sınırlıdır, çünkü bu teknoloji Nvidia’ya aittir. Her mimari, geliştiricilerin CUDA uygulamaları oluşturup çalıştırması için gerekli kütüphaneleri ve araçları sağlayan CUDA araç setinin belirli sürümlerini destekler.

## ONNX

ONNX (Open Neural Network Exchange), makine öğrenimi modellerini temsil etmek için tasarlanmış açık bir formattır. Genişletilebilir bir hesaplama grafiği modeli tanımı ile birlikte yerleşik operatörler ve standart veri tiplerinin tanımlarını sağlar. ONNX, geliştiricilerin modelleri farklı ML çerçeveleri arasında taşımalarına olanak tanır, birlikte çalışabilirliği artırır ve yapay zeka uygulamalarının oluşturulmasını ve dağıtılmasını kolaylaştırır.

Phi3 mini, ONNX Runtime ile CPU ve GPU üzerinde, sunucu platformları, Windows, Linux ve Mac masaüstleri ile mobil CPU’lar dahil olmak üzere çeşitli cihazlarda çalışabilir.  
Eklediğimiz optimize edilmiş konfigürasyonlar şunlardır:

- int4 DML için ONNX modelleri: AWQ ile int4’e kuantize edilmiş  
- fp16 CUDA için ONNX modeli  
- int4 CUDA için ONNX modeli: RTN ile int4’e kuantize edilmiş  
- int4 CPU ve Mobil için ONNX modeli: RTN ile int4’e kuantize edilmiş  

## Llama.cpp

Llama.cpp, C++ ile yazılmış açık kaynaklı bir yazılım kütüphanesidir. Llama dahil olmak üzere çeşitli Büyük Dil Modelleri (LLM) üzerinde çıkarım yapar. Genel amaçlı tensör kütüphanesi ggml ile birlikte geliştirilmiştir ve orijinal Python uygulamasına kıyasla daha hızlı çıkarım ve daha düşük bellek kullanımı sağlamayı hedefler. Donanım optimizasyonu, kuantizasyon desteği sunar ve basit bir API ile örnekler içerir. Verimli LLM çıkarımıyla ilgileniyorsanız, Phi3’ün Llama.cpp’yi çalıştırabilmesi nedeniyle llama.cpp keşfetmeye değerdir.

## GGUF

GGUF (Generic Graph Update Format), makine öğrenimi modellerini temsil etmek ve güncellemek için kullanılan bir formattır. Özellikle 4-8 bit kuantizasyon ile CPU’larda etkili çalışan küçük dil modelleri (SLM) için faydalıdır. GGUF, hızlı prototipleme ve modelleri uç cihazlarda veya CI/CD gibi toplu işlerde çalıştırmak için avantaj sağlar.

**Feragatname**:  
Bu belge, AI çeviri servisi [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hatalar veya yanlışlıklar içerebileceğini lütfen unutmayın. Orijinal belge, kendi dilinde yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımı sonucu ortaya çıkabilecek yanlış anlamalar veya yorum hatalarından sorumlu değiliz.