<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:21:57+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "tr"
}
-->
# Bahsedilen temel teknolojiler şunlardır

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12 üzerine inşa edilmiş, donanım hızlandırmalı makine öğrenimi için düşük seviyeli bir API.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia tarafından geliştirilen, grafik işlem birimleri (GPU'lar) üzerinde genel amaçlı işlem yapılmasını sağlayan paralel hesaplama platformu ve uygulama programlama arayüzü (API) modeli.
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - Farklı ML çerçeveleri arasında birlikte çalışabilirlik sağlayan, makine öğrenimi modellerini temsil etmek için tasarlanmış açık format.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - Özellikle 4-8 bit kuantizasyonla CPU'larda etkili şekilde çalışabilen küçük dil modellerini temsil etmek ve güncellemek için kullanılan bir format.

## DirectML

DirectML, donanım hızlandırmalı makine öğrenimini mümkün kılan düşük seviyeli bir API'dir. GPU hızlandırmasından yararlanmak için DirectX 12 üzerine kuruludur ve farklı GPU üreticileri arasında kod değişikliği gerektirmeden çalışabilen satıcıdan bağımsız bir yapıya sahiptir. Genellikle GPU'larda model eğitimi ve çıkarım işlemleri için kullanılır.

Donanım desteği açısından, DirectML AMD entegre ve ayrık GPU'lar, Intel entegre GPU'lar ve NVIDIA ayrık GPU'lar dahil olmak üzere geniş bir GPU yelpazesiyle uyumlu olacak şekilde tasarlanmıştır. Windows AI Platform'un bir parçasıdır ve Windows 10 & 11 üzerinde desteklenir; böylece herhangi bir Windows cihazında model eğitimi ve çıkarımı yapılabilir.

DirectML ile ilgili güncellemeler ve fırsatlar olmuştur; örneğin 150'ye kadar ONNX operatörünü desteklemesi ve hem ONNX runtime hem de WinML tarafından kullanılması gibi. Büyük Entegre Donanım Üreticileri (IHV'ler) tarafından desteklenir ve çeşitli metakomutları uygularlar.

## CUDA

CUDA, Compute Unified Device Architecture’ın kısaltmasıdır ve Nvidia tarafından geliştirilen paralel hesaplama platformu ile uygulama programlama arayüzü (API) modelidir. CUDA, yazılım geliştiricilerin CUDA destekli grafik işlem birimini (GPU) genel amaçlı işlem için kullanmalarına olanak tanır; bu yaklaşıma GPGPU (Genel Amaçlı Grafik İşlem Birimlerinde Hesaplama) denir. CUDA, Nvidia'nın GPU hızlandırmasının temelini oluşturur ve makine öğrenimi, bilimsel hesaplama ve video işleme gibi birçok alanda yaygın olarak kullanılır.

CUDA donanım desteği Nvidia'nın GPU'ları ile sınırlıdır çünkü bu teknoloji Nvidia'ya aittir. Her mimari, geliştiricilerin CUDA uygulamaları oluşturup çalıştırması için gerekli kütüphaneleri ve araçları sağlayan CUDA araç setinin belirli sürümlerini destekler.

## ONNX

ONNX (Open Neural Network Exchange), makine öğrenimi modellerini temsil etmek için tasarlanmış açık bir formattır. Genişletilebilir bir hesaplama grafik modeli tanımı, yerleşik operatörler ve standart veri tipleri tanımları sağlar. ONNX, geliştiricilerin modelleri farklı ML çerçeveleri arasında taşımasına olanak tanır; bu da birlikte çalışabilirliği artırır ve yapay zeka uygulamalarının oluşturulmasını ve dağıtımını kolaylaştırır.

Phi3 mini, sunucu platformları, Windows, Linux ve Mac masaüstleri ile mobil CPU'lar dahil olmak üzere cihazlarda CPU ve GPU üzerinde ONNX Runtime ile çalışabilir. Eklediğimiz optimize edilmiş konfigürasyonlar şunlardır:

- int4 DML için ONNX modelleri: AWQ ile int4'e kuantize edilmiş
- fp16 CUDA için ONNX modeli
- int4 CUDA için ONNX modeli: RTN ile int4'e kuantize edilmiş
- int4 CPU ve Mobil için ONNX modeli: RTN ile int4'e kuantize edilmiş

## Llama.cpp

Llama.cpp, C++ ile yazılmış açık kaynaklı bir yazılım kütüphanesidir. Llama dahil olmak üzere çeşitli Büyük Dil Modelleri (LLM) üzerinde çıkarım yapar. ggml kütüphanesi (genel amaçlı tensör kütüphanesi) ile birlikte geliştirilmiştir ve orijinal Python uygulamasına kıyasla daha hızlı çıkarım ve daha düşük bellek kullanımı sağlamayı hedefler. Donanım optimizasyonu, kuantizasyon destekler ve basit bir API ile örnekler sunar. Verimli LLM çıkarımıyla ilgileniyorsanız, llama.cpp'yi incelemeye değer; çünkü Phi3, Llama.cpp çalıştırabilir.

## GGUF

GGUF (Generic Graph Update Format), makine öğrenimi modellerini temsil etmek ve güncellemek için kullanılan bir formattır. Özellikle 4-8 bit kuantizasyonla CPU'larda etkili şekilde çalışabilen küçük dil modelleri (SLM) için faydalıdır. GGUF, hızlı prototipleme ve modelleri uç cihazlarda veya CI/CD boru hatları gibi toplu işlerde çalıştırmak için avantaj sağlar.

**Feragatname**:  
Bu belge, AI çeviri servisi [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba sarf etsek de, otomatik çevirilerin hata veya yanlışlıklar içerebileceğini lütfen unutmayınız. Orijinal belge, kendi dilindeki haliyle yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımı sonucunda ortaya çıkabilecek yanlış anlamalar veya yorum hatalarından sorumlu değiliz.