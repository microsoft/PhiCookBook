<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3139a6a82f357a9f90f1fe51c4caf65a",
  "translation_date": "2025-05-09T13:55:43+00:00",
  "source_file": "md/01.Introduction/04/UsingIntelOpenVINOQuantifyingPhi.md",
  "language_code": "tr"
}
-->
# **Intel OpenVINO kullanarak Phi-3.5'in KantitleÅŸtirilmesi**

Intel, Ã§ok sayÄ±da kullanÄ±cÄ±sÄ± olan en kÃ¶klÃ¼ CPU Ã¼reticisidir. Makine Ã¶ÄŸrenimi ve derin Ã¶ÄŸrenmenin yÃ¼kseliÅŸiyle birlikte Intel, AI hÄ±zlandÄ±rma yarÄ±ÅŸÄ±na katÄ±lmÄ±ÅŸtÄ±r. Model Ã§Ä±karÄ±mÄ± iÃ§in Intel sadece GPU ve CPU deÄŸil, aynÄ± zamanda NPU da kullanmaktadÄ±r.

Phi-3.x Ailesini uÃ§ tarafta daÄŸÄ±tmayÄ± hedefliyoruz ve AI PC ile Copilot PC'nin en Ã¶nemli parÃ§asÄ± olmasÄ±nÄ± umuyoruz. UÃ§ tarafta modelin yÃ¼klenmesi farklÄ± donanÄ±m Ã¼reticilerinin iÅŸ birliÄŸine baÄŸlÄ±dÄ±r. Bu bÃ¶lÃ¼mde aÄŸÄ±rlÄ±klÄ± olarak Intel OpenVINO'nun kantitatif model uygulama senaryosuna odaklanacaÄŸÄ±z.

## **OpenVINO Nedir**

OpenVINO, buluttan uca derin Ã¶ÄŸrenme modellerini optimize etmek ve daÄŸÄ±tmak iÃ§in aÃ§Ä±k kaynaklÄ± bir araÃ§ setidir. PyTorch, TensorFlow, ONNX gibi popÃ¼ler Ã§erÃ§evelerden gelen modellerle, Ã¼retken AI, video, ses ve dil gibi Ã§eÅŸitli kullanÄ±m durumlarÄ±nda derin Ã¶ÄŸrenme Ã§Ä±karÄ±mÄ±nÄ± hÄ±zlandÄ±rÄ±r. Modelleri dÃ¶nÃ¼ÅŸtÃ¼rÃ¼n ve optimize edin, ardÄ±ndan IntelÂ® donanÄ±mlarÄ± ve ortamlarÄ±nÄ±n karÄ±ÅŸÄ±mÄ±nda, yerel veya cihazda, tarayÄ±cÄ±da veya bulutta daÄŸÄ±tÄ±m yapÄ±n.

ArtÄ±k OpenVINO ile Intel donanÄ±mÄ±nda GenAI modelini hÄ±zlÄ±ca kantitleÅŸtirebilir ve model referansÄ±nÄ± hÄ±zlandÄ±rabilirsiniz.

OpenVINO ÅŸu anda Phi-3.5-Vision ve Phi-3.5 Instruct modellerinin kantitleÅŸtirme dÃ¶nÃ¼ÅŸÃ¼mÃ¼nÃ¼ desteklemektedir.

### **Ortam Kurulumu**

LÃ¼tfen aÅŸaÄŸÄ±daki ortam baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ±n yÃ¼klÃ¼ olduÄŸundan emin olun, bu requirement.txt dosyasÄ±dÄ±r

```txt

--extra-index-url https://download.pytorch.org/whl/cpu
optimum-intel>=1.18.2
nncf>=2.11.0
openvino>=2024.3.0
transformers>=4.40
openvino-genai>=2024.3.0.0

```

### **OpenVINO kullanarak Phi-3.5-Instructâ€™in KantitleÅŸtirilmesi**

Terminalde, lÃ¼tfen bu betiÄŸi Ã§alÄ±ÅŸtÄ±rÄ±n

```bash


export llm_model_id = "microsoft/Phi-3.5-mini-instruct"

export llm_model_path = "your save quantizing Phi-3.5-instruct location"

optimum-cli export openvino --model {llm_model_id} --task text-generation-with-past --weight-format int4 --group-size 128 --ratio 0.6  --sym  --trust-remote-code {llm_model_path}


```

### **OpenVINO kullanarak Phi-3.5-Visionâ€™un KantitleÅŸtirilmesi**

LÃ¼tfen bu betiÄŸi Python veya Jupyter lab ortamÄ±nda Ã§alÄ±ÅŸtÄ±rÄ±n

```python

import requests
from pathlib import Path
from ov_phi3_vision import convert_phi3_model
import nncf

if not Path("ov_phi3_vision.py").exists():
    r = requests.get(url="https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/phi-3-vision/ov_phi3_vision.py")
    open("ov_phi3_vision.py", "w").write(r.text)


if not Path("gradio_helper.py").exists():
    r = requests.get(url="https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/phi-3-vision/gradio_helper.py")
    open("gradio_helper.py", "w").write(r.text)

if not Path("notebook_utils.py").exists():
    r = requests.get(url="https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py")
    open("notebook_utils.py", "w").write(r.text)



model_id = "microsoft/Phi-3.5-vision-instruct"
out_dir = Path("../model/phi-3.5-vision-128k-instruct-ov")
compression_configuration = {
    "mode": nncf.CompressWeightsMode.INT4_SYM,
    "group_size": 64,
    "ratio": 0.6,
}
if not out_dir.exists():
    convert_phi3_model(model_id, out_dir, compression_configuration)

```

### **ğŸ¤– Intel OpenVINO ile Phi-3.5 iÃ§in Ã–rnekler**

| Laboratuvarlar    | TanÄ±tÄ±m | Git |
| -------- | ------- |  ------- |
| ğŸš€ Lab-Phi-3.5 Instruct TanÄ±tÄ±mÄ±  | AI PC'nizde Phi-3.5 Instruct nasÄ±l kullanÄ±lÄ±r Ã¶ÄŸrenin    |  [Git](../../../../../code/09.UpdateSamples/Aug/intel-phi35-instruct-zh.ipynb)    |
| ğŸš€ Lab-Phi-3.5 Vision (gÃ¶rÃ¼ntÃ¼) TanÄ±tÄ±mÄ± | AI PC'nizde Phi-3.5 Vision ile gÃ¶rÃ¼ntÃ¼ analizini Ã¶ÄŸrenin      |  [Git](../../../../../code/09.UpdateSamples/Aug/intel-phi35-vision-img.ipynb)    |
| ğŸš€ Lab-Phi-3.5 Vision (video) TanÄ±tÄ±mÄ±   | AI PC'nizde Phi-3.5 Vision ile video analizini Ã¶ÄŸrenin    |  [Git](../../../../../code/09.UpdateSamples/Aug/intel-phi35-vision-video.ipynb)    |


## **Kaynaklar**

1. Intel OpenVINO hakkÄ±nda daha fazla bilgi edinin [https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html)

2. Intel OpenVINO GitHub Deposu [https://github.com/openvinotoolkit/openvino.genai](https://github.com/openvinotoolkit/openvino.genai)

**Feragatname**:  
Bu belge, AI Ã§eviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba gÃ¶stersek de, otomatik Ã§evirilerin hatalar veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±nÄ±z. Orijinal belge, kendi dilindeki haliyle yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ± sonucu ortaya Ã§Ä±kabilecek yanlÄ±ÅŸ anlamalar veya yanlÄ±ÅŸ yorumlamalardan sorumlu deÄŸiliz.