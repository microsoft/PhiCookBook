<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "bc29f7fe7fc16bed6932733eac8c81b8",
  "translation_date": "2025-05-09T19:23:23+00:00",
  "source_file": "md/02.Application/02.Code/Phi3/VSCodeExt/HOL/AIPC/02.PromptflowWithNPU.md",
  "language_code": "tr"
}
-->
# **Lab 2 - Phi-3-mini ile AIPC'de Prompt flow'u Çalıştırma**

## **Prompt flow nedir**

Prompt flow, LLM tabanlı yapay zeka uygulamalarının fikir aşamasından prototip oluşturma, test, değerlendirme, üretime alma ve izlemeye kadar olan tüm geliştirme sürecini kolaylaştırmak için tasarlanmış bir geliştirme araçları paketidir. Prompt mühendisliğini çok daha basit hale getirir ve üretim kalitesinde LLM uygulamaları oluşturmanızı sağlar.

Prompt flow ile şunları yapabilirsiniz:

- LLM’leri, promptları, Python kodunu ve diğer araçları birbirine bağlayan çalıştırılabilir iş akışları oluşturmak.

- Özellikle LLM’lerle olan etkileşimi kolayca hata ayıklamak ve yinelemek.

- İş akışlarınızı değerlendirmek, daha büyük veri setleriyle kalite ve performans metriklerini hesaplamak.

- Test ve değerlendirmeyi CI/CD sisteminize entegre ederek iş akışınızın kalitesini sağlamak.

- İş akışlarınızı seçtiğiniz servis platformuna dağıtmak veya uygulamanızın kod tabanına kolayca entegre etmek.

- (Opsiyonel ama şiddetle tavsiye edilir) Azure AI’daki Prompt flow’un bulut sürümünü kullanarak ekibinizle iş birliği yapmak.

## **AIPC nedir**

Bir AI PC, CPU, GPU ve NPU’ya sahiptir; her biri yapay zeka hızlandırma yeteneklerine sahiptir. NPU, yani sinir işleme birimi, yapay zeka (AI) ve makine öğrenimi (ML) görevlerini bulutta işlenmek üzere veri göndermek yerine doğrudan PC’nizde gerçekleştiren özel bir hızlandırıcıdır. GPU ve CPU da bu iş yüklerini işleyebilir, ancak NPU özellikle düşük güç tüketimiyle AI hesaplamalarında çok iyidir. AI PC, bilgisayarlarımızın çalışma şeklinde temel bir değişimi temsil eder. Daha önce var olmayan bir sorun için çözüm değildir. Bunun yerine, günlük PC kullanımlarında büyük bir iyileştirme vaat eder.

Peki nasıl çalışır? Devasa büyük dil modelleri (LLM’ler) ve çok sayıda halka açık veriyle eğitilmiş üretken AI’ya kıyasla, PC’nizde gerçekleşecek AI her seviyede çok daha erişilebilir. Konsept daha kolay anlaşılır ve verileriniz üzerinde eğitildiği için, buluta erişmeye gerek kalmadan, faydaları daha geniş bir kitle için daha cazip hale gelir.

Kısa vadede, AI PC dünyası kişisel asistanlar ve PC’nizde doğrudan çalışan daha küçük AI modellerini içerir; verilerinizi kullanarak günlük yaptığınız şeyler için kişisel, özel ve daha güvenli AI iyileştirmeleri sunar – toplantı notları almak, bir fantazi futbol ligi düzenlemek, fotoğraf ve video düzenlemede otomatik iyileştirmeler yapmak veya herkesin geliş-gidiş saatlerine göre mükemmel bir aile buluşması programı hazırlamak gibi.

## **AIPC üzerinde jenerasyon kod akışları oluşturma**

***Note*** ：Ortam kurulumunu tamamlamadıysanız, lütfen [Lab 0 -Installations](./01.Installations.md) sayfasını ziyaret edin.

1. Visual Studio Code’da Prompt flow Uzantısını açın ve boş bir flow projesi oluşturun

![create](../../../../../../../../../translated_images/pf_create.d6172d8277a78a7fa82cd6ff727ed44e037fa78b662f1f62d5963f36d712d229.tr.png)

2. Girdi ve Çıktı parametreleri ekleyin ve yeni flow olarak Python Kodu ekleyin

![flow](../../../../../../../../../translated_images/pf_flow.d5646a323fb7f444c0b98b4521057a592325c583e7ba18bc31500bc0415e9ef3.tr.png)

Bu yapıyı (flow.dag.yaml) iş akışınızı oluşturmak için referans alabilirsiniz

```yaml

inputs:
  question:
    type: string
    default: how to write Bubble Algorithm
outputs:
  answer:
    type: string
    reference: ${Chat_With_Phi3.output}
nodes:
- name: Chat_With_Phi3
  type: python
  source:
    type: code
    path: Chat_With_Phi3.py
  inputs:
    question: ${inputs.question}


```

3. ***Chat_With_Phi3.py*** dosyasına kod ekleyin

```python


from promptflow.core import tool

# import torch
from transformers import AutoTokenizer, pipeline,TextStreamer
import intel_npu_acceleration_library as npu_lib

import warnings

import asyncio
import platform

class Phi3CodeAgent:
    
    model = None
    tokenizer = None
    text_streamer = None
    
    model_id = "microsoft/Phi-3-mini-4k-instruct"

    @staticmethod
    def init_phi3():
        
        if Phi3CodeAgent.model is None or Phi3CodeAgent.tokenizer is None or Phi3CodeAgent.text_streamer is None:
            Phi3CodeAgent.model = npu_lib.NPUModelForCausalLM.from_pretrained(
                                    Phi3CodeAgent.model_id,
                                    torch_dtype="auto",
                                    dtype=npu_lib.int4,
                                    trust_remote_code=True
                                )
            Phi3CodeAgent.tokenizer = AutoTokenizer.from_pretrained(Phi3CodeAgent.model_id)
            Phi3CodeAgent.text_streamer = TextStreamer(Phi3CodeAgent.tokenizer, skip_prompt=True)

    

    @staticmethod
    def chat_with_phi3(prompt):
        
        Phi3CodeAgent.init_phi3()

        messages = "<|system|>You are a AI Python coding assistant. Please help me to generate code in Python.The answer only genertated Python code, but any comments and instructions do not need to be generated<|end|><|user|>" + prompt +"<|end|><|assistant|>"



        generation_args = {
            "max_new_tokens": 1024,
            "return_full_text": False,
            "temperature": 0.3,
            "do_sample": False,
            "streamer": Phi3CodeAgent.text_streamer,
        }

        pipe = pipeline(
            "text-generation",
            model=Phi3CodeAgent.model,
            tokenizer=Phi3CodeAgent.tokenizer,
            # **generation_args
        )

        result = ''

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            response = pipe(messages, **generation_args)
            result =response[0]['generated_text']
            return result


@tool
def my_python_tool(question: str) -> str:
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    return Phi3CodeAgent.chat_with_phi3(question)


```

4. Jenerasyon kodunun doğru çalışıp çalışmadığını kontrol etmek için Debug veya Run’dan akışı test edebilirsiniz

![RUN](../../../../../../../../../translated_images/pf_run.d918637dc00f61e9bdeec37d4cc9646f77d270ac9203bcce13569f3157202b6e.tr.png)

5. Terminalde geliştirme API’si olarak akışı çalıştırın

```

pf flow serve --source ./ --port 8080 --host localhost   

```

Postman / Thunder Client ile test edebilirsiniz

### **Note**

1. İlk çalıştırma uzun sürer. Phi-3 modelini Hugging face CLI’dan indirmeniz önerilir.

2. Intel NPU’nun sınırlı hesaplama gücü göz önünde bulundurularak Phi-3-mini-4k-instruct kullanılması tavsiye edilir.

3. INT4 dönüşümü için Intel NPU Hızlandırması kullanıyoruz, ancak servisi yeniden çalıştırırsanız cache ve nc_workshop klasörlerini silmeniz gerekir.

## **Kaynaklar**

1. Promptflow öğrenmek için [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Intel NPU Hızlandırmasını öğrenmek için [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Örnek Kod, indirmek için [Local NPU Agent Sample Code](../../../../../../../../../code/07.Lab/01/AIPC)

**Feragatname**:  
Bu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hatalar veya yanlışlıklar içerebileceğini lütfen unutmayınız. Orijinal belge, kendi ana dilindeki haliyle yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımı sonucu ortaya çıkabilecek yanlış anlamalar veya yorumlamalardan sorumlu değiliz.