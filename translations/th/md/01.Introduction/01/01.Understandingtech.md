<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:44:47+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "th"
}
-->
# เทคโนโลยีสำคัญที่กล่าวถึงได้แก่

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - API ระดับต่ำสำหรับการเร่งความเร็วการเรียนรู้ของเครื่องด้วยฮาร์ดแวร์ สร้างขึ้นบน DirectX 12  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - แพลตฟอร์มการประมวลผลแบบขนานและโมเดล API ที่พัฒนาโดย Nvidia ช่วยให้สามารถประมวลผลทั่วไปบน GPU ได้  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - รูปแบบเปิดที่ออกแบบมาเพื่อแทนโมเดลการเรียนรู้ของเครื่อง ช่วยให้สามารถใช้งานร่วมกันระหว่างเฟรมเวิร์ก ML ต่างๆ ได้  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - รูปแบบที่ใช้แทนและอัปเดตโมเดลการเรียนรู้ของเครื่อง โดยเฉพาะอย่างยิ่งสำหรับโมเดลภาษาเล็กที่สามารถทำงานได้ดีบน CPU ด้วยการควอนไทซ์ 4-8 บิต  

## DirectML

DirectML เป็น API ระดับต่ำที่ช่วยให้การเรียนรู้ของเครื่องเร่งความเร็วด้วยฮาร์ดแวร์ โดยสร้างขึ้นบน DirectX 12 เพื่อใช้ประโยชน์จากการเร่งความเร็วของ GPU และไม่ขึ้นกับผู้ผลิตฮาร์ดแวร์ หมายความว่าไม่ต้องแก้ไขโค้ดเพื่อใช้งานกับ GPU จากผู้ผลิตต่างๆ โดยหลักจะใช้สำหรับงานฝึกสอนโมเดลและการทำนายบน GPU

ในแง่ของการรองรับฮาร์ดแวร์ DirectML ถูกออกแบบให้ทำงานร่วมกับ GPU หลากหลายประเภท รวมถึง GPU แบบบูรณาการและแยกของ AMD, GPU แบบบูรณาการของ Intel และ GPU แยกของ NVIDIA เป็นส่วนหนึ่งของ Windows AI Platform และรองรับบน Windows 10 และ 11 ทำให้สามารถฝึกสอนและทำนายโมเดลบนอุปกรณ์ Windows ได้ทุกเครื่อง

มีการอัปเดตและโอกาสที่เกี่ยวข้องกับ DirectML เช่น รองรับตัวดำเนินการ ONNX ได้ถึง 150 ตัว และถูกใช้งานโดยทั้ง ONNX runtime และ WinML โดยได้รับการสนับสนุนจากผู้ผลิตฮาร์ดแวร์รายใหญ่ (IHVs) ซึ่งแต่ละรายได้พัฒนา metacommands ต่างๆ  

## CUDA

CUDA ย่อมาจาก Compute Unified Device Architecture เป็นแพลตฟอร์มการประมวลผลแบบขนานและโมเดล API ที่สร้างโดย Nvidia ช่วยให้นักพัฒนาซอฟต์แวร์ใช้ GPU ที่รองรับ CUDA สำหรับการประมวลผลทั่วไป ซึ่งเรียกว่า GPGPU (General-Purpose computing on Graphics Processing Units) CUDA เป็นเทคโนโลยีสำคัญที่ช่วยเร่งความเร็ว GPU ของ Nvidia และถูกใช้อย่างแพร่หลายในหลายสาขา เช่น การเรียนรู้ของเครื่อง, การคำนวณทางวิทยาศาสตร์ และการประมวลผลวิดีโอ

การรองรับฮาร์ดแวร์ของ CUDA เฉพาะกับ GPU ของ Nvidia เนื่องจากเป็นเทคโนโลยีที่พัฒนาโดย Nvidia โดยแต่ละสถาปัตยกรรมจะรองรับเวอร์ชันของ CUDA toolkit ที่แตกต่างกัน ซึ่งมีไลบรารีและเครื่องมือที่จำเป็นสำหรับนักพัฒนาในการสร้างและรันแอปพลิเคชัน CUDA  

## ONNX

ONNX (Open Neural Network Exchange) เป็นรูปแบบเปิดที่ออกแบบมาเพื่อแทนโมเดลการเรียนรู้ของเครื่อง โดยให้คำจำกัดความของโมเดลกราฟการคำนวณที่ขยายได้ รวมถึงคำจำกัดความของตัวดำเนินการในตัวและชนิดข้อมูลมาตรฐาน ONNX ช่วยให้นักพัฒนาย้ายโมเดลระหว่างเฟรมเวิร์ก ML ต่างๆ ได้อย่างราบรื่น เพิ่มความสามารถในการทำงานร่วมกันและทำให้ง่ายต่อการสร้างและปรับใช้แอปพลิเคชัน AI

Phi3 mini สามารถรันด้วย ONNX Runtime บน CPU และ GPU ในอุปกรณ์หลากหลาย รวมถึงแพลตฟอร์มเซิร์ฟเวอร์, Windows, Linux และ Mac เดสก์ท็อป รวมถึง CPU บนมือถือ  
การตั้งค่าที่เราได้เพิ่มเข้ามา ได้แก่

- โมเดล ONNX สำหรับ int4 DML: ควอนไทซ์เป็น int4 ผ่าน AWQ  
- โมเดล ONNX สำหรับ fp16 CUDA  
- โมเดล ONNX สำหรับ int4 CUDA: ควอนไทซ์เป็น int4 ผ่าน RTN  
- โมเดล ONNX สำหรับ int4 CPU และ Mobile: ควอนไทซ์เป็น int4 ผ่าน RTN  

## Llama.cpp

Llama.cpp เป็นไลบรารีซอฟต์แวร์โอเพนซอร์สที่เขียนด้วย C++ ใช้สำหรับการทำนายผลบนโมเดลภาษาใหญ่ (LLMs) ต่างๆ รวมถึง Llama พัฒนาไปพร้อมกับไลบรารี ggml (ไลบรารีเทนเซอร์ทั่วไป) โดย llama.cpp มุ่งเน้นให้การทำนายผลเร็วขึ้นและใช้หน่วยความจำน้อยกว่าการใช้งาน Python แบบดั้งเดิม รองรับการปรับแต่งฮาร์ดแวร์, การควอนไทซ์ และมี API ที่เรียบง่ายพร้อมตัวอย่าง หากคุณสนใจการทำนายผล LLM ที่มีประสิทธิภาพ llama.cpp เป็นตัวเลือกที่น่าสนใจเนื่องจาก Phi3 สามารถรัน Llama.cpp ได้  

## GGUF

GGUF (Generic Graph Update Format) เป็นรูปแบบที่ใช้แทนและอัปเดตโมเดลการเรียนรู้ของเครื่อง โดยเฉพาะอย่างยิ่งสำหรับโมเดลภาษาเล็ก (SLMs) ที่สามารถทำงานได้ดีบน CPU ด้วยการควอนไทซ์ 4-8 บิต GGUF มีประโยชน์สำหรับการสร้างต้นแบบอย่างรวดเร็วและการรันโมเดลบนอุปกรณ์ขอบเครือข่ายหรือในงานแบตช์ เช่น ใน CI/CD pipeline

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษาอัตโนมัติ [Co-op Translator](https://github.com/Azure/co-op-translator) แม้เราจะพยายามให้ความถูกต้องสูงสุด แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาต้นทางถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลโดยผู้เชี่ยวชาญมนุษย์ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดใด ๆ ที่เกิดจากการใช้การแปลนี้