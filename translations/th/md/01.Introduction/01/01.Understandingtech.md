<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:23:11+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "th"
}
-->
# เทคโนโลยีสำคัญที่กล่าวถึงได้แก่

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - API ระดับต่ำสำหรับการเรียนรู้ของเครื่องที่เร่งความเร็วด้วยฮาร์ดแวร์ ซึ่งสร้างขึ้นบน DirectX 12  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - แพลตฟอร์มการประมวลผลแบบขนานและโมเดล API ที่พัฒนาโดย Nvidia ช่วยให้สามารถประมวลผลทั่วไปบนหน่วยประมวลผลกราฟิก (GPU)  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - รูปแบบเปิดที่ออกแบบมาเพื่อแทนโมเดลการเรียนรู้ของเครื่องและรองรับการทำงานร่วมกันระหว่างเฟรมเวิร์ก ML ต่างๆ  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - รูปแบบที่ใช้แทนและอัปเดตโมเดลการเรียนรู้ของเครื่อง โดยเหมาะสำหรับโมเดลภาษาขนาดเล็กที่สามารถทำงานได้ดีบน CPU ด้วยการควอนไทซ์ 4-8 บิต

## DirectML

DirectML เป็น API ระดับต่ำที่ช่วยให้การเรียนรู้ของเครื่องเร่งความเร็วด้วยฮาร์ดแวร์ สามารถใช้งานร่วมกับ DirectX 12 เพื่อใช้ประโยชน์จากการเร่งความเร็ว GPU และไม่ขึ้นกับผู้ผลิตฮาร์ดแวร์ หมายความว่าไม่ต้องแก้ไขโค้ดเพื่อใช้งานกับ GPU ของผู้ผลิตต่างๆ โดยส่วนใหญ่ใช้สำหรับงานฝึกสอนโมเดลและการทำนายบน GPU

สำหรับการรองรับฮาร์ดแวร์ DirectML ถูกออกแบบมาให้ทำงานกับ GPU หลากหลายประเภท รวมถึง GPU แบบบูรณาการและแยกของ AMD, GPU แบบบูรณาการของ Intel และ GPU แบบแยกของ NVIDIA เป็นส่วนหนึ่งของ Windows AI Platform และรองรับบน Windows 10 และ 11 ช่วยให้สามารถฝึกสอนและทำนายโมเดลบนอุปกรณ์ Windows ได้ทุกเครื่อง

มีการอัปเดตและโอกาสต่างๆ เกี่ยวกับ DirectML เช่น รองรับ operator ONNX สูงสุดถึง 150 ตัว และถูกใช้งานทั้งใน ONNX runtime และ WinML ได้รับการสนับสนุนจาก Integrated Hardware Vendors (IHVs) รายใหญ่ ซึ่งแต่ละรายจะมีการนำ metacommands ต่างๆ ไปใช้งาน

## CUDA

CUDA ย่อมาจาก Compute Unified Device Architecture เป็นแพลตฟอร์มการประมวลผลแบบขนานและโมเดล API ที่พัฒนาโดย Nvidia ช่วยให้นักพัฒนาซอฟต์แวร์สามารถใช้ GPU ที่รองรับ CUDA สำหรับการประมวลผลทั่วไป ซึ่งเรียกวิธีนี้ว่า GPGPU (General-Purpose computing on Graphics Processing Units) CUDA เป็นหัวใจสำคัญในการเร่งความเร็ว GPU ของ Nvidia และถูกใช้อย่างกว้างขวางในหลายสาขา เช่น การเรียนรู้ของเครื่อง การประมวลผลทางวิทยาศาสตร์ และการประมวลผลวิดีโอ

การรองรับฮาร์ดแวร์ของ CUDA จะจำเพาะเจาะจงกับ GPU ของ Nvidia เนื่องจากเป็นเทคโนโลยีเฉพาะของ Nvidia โดยแต่ละสถาปัตยกรรมจะรองรับเวอร์ชันของ CUDA toolkit ที่แตกต่างกัน ซึ่งมีไลบรารีและเครื่องมือที่จำเป็นสำหรับนักพัฒนาในการสร้างและรันแอปพลิเคชัน CUDA

## ONNX

ONNX (Open Neural Network Exchange) เป็นรูปแบบเปิดที่ออกแบบมาเพื่อแทนโมเดลการเรียนรู้ของเครื่อง โดยให้คำจำกัดความของโมเดลกราฟการคำนวณที่ขยายได้ รวมถึงคำจำกัดความของ operator ที่มีอยู่ในตัวและชนิดข้อมูลมาตรฐาน ONNX ช่วยให้นักพัฒนาย้ายโมเดลระหว่างเฟรมเวิร์ก ML ต่างๆ ได้อย่างราบรื่น สนับสนุนการทำงานร่วมกันและทำให้ง่ายต่อการสร้างและปรับใช้แอป AI

Phi3 mini สามารถรันด้วย ONNX Runtime บน CPU และ GPU ในอุปกรณ์หลากหลาย รวมถึงแพลตฟอร์มเซิร์ฟเวอร์ Windows, Linux, Mac เดสก์ท็อป และ CPU มือถือ  
การตั้งค่าที่เราเพิ่มเข้ามาได้แก่

- โมเดล ONNX สำหรับ int4 DML: ควอนไทซ์เป็น int4 ผ่าน AWQ  
- โมเดล ONNX สำหรับ fp16 CUDA  
- โมเดล ONNX สำหรับ int4 CUDA: ควอนไทซ์เป็น int4 ผ่าน RTN  
- โมเดล ONNX สำหรับ int4 CPU และ Mobile: ควอนไทซ์เป็น int4 ผ่าน RTN

## Llama.cpp

Llama.cpp เป็นไลบรารีซอฟต์แวร์โอเพนซอร์สที่เขียนด้วยภาษา C++ ใช้สำหรับทำการทำนายบน Large Language Models (LLMs) ต่างๆ รวมถึง Llama พัฒนาไปพร้อมกับไลบรารี ggml (ไลบรารี tensor ทั่วไป) โดย llama.cpp มุ่งเน้นให้การทำนายเร็วขึ้นและใช้หน่วยความจำน้อยกว่าการใช้งาน Python แบบดั้งเดิม รองรับการปรับแต่งฮาร์ดแวร์ การควอนไทซ์ และมี API และตัวอย่างใช้งานง่ายๆ หากสนใจการทำนาย LLM ที่มีประสิทธิภาพ llama.cpp เป็นตัวเลือกที่น่าสนใจเนื่องจาก Phi3 สามารถรัน Llama.cpp ได้

## GGUF

GGUF (Generic Graph Update Format) เป็นรูปแบบที่ใช้แทนและอัปเดตโมเดลการเรียนรู้ของเครื่อง เหมาะสำหรับโมเดลภาษาขนาดเล็ก (SLMs) ที่สามารถทำงานได้ดีบน CPU ด้วยการควอนไทซ์ 4-8 บิต GGUF มีประโยชน์สำหรับการสร้างต้นแบบอย่างรวดเร็วและรันโมเดลบนอุปกรณ์ edge หรือในงานแบตช์ เช่น ใน CI/CD pipeline

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารฉบับนี้ได้รับการแปลโดยใช้บริการแปลภาษาอัตโนมัติ [Co-op Translator](https://github.com/Azure/co-op-translator) แม้เราจะพยายามให้ความถูกต้องสูงสุด แต่โปรดทราบว่าการแปลโดยอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่แม่นยำ เอกสารต้นฉบับในภาษาต้นฉบับถือเป็นแหล่งข้อมูลที่น่าเชื่อถือที่สุด สำหรับข้อมูลที่สำคัญ ควรใช้บริการแปลโดยผู้เชี่ยวชาญมนุษย์ เราจะไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดใด ๆ ที่เกิดจากการใช้การแปลฉบับนี้