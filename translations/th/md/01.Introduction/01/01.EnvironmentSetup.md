# เริ่มต้นใช้งาน Phi-3 ในเครื่องของคุณ

คำแนะนำนี้จะช่วยให้คุณตั้งค่าสภาพแวดล้อมในเครื่องเพื่อรันโมเดล Phi-3 โดยใช้ Ollama คุณสามารถรันโมเดลได้หลายวิธี รวมถึงการใช้ GitHub Codespaces, VS Code Dev Containers หรือสภาพแวดล้อมในเครื่องของคุณเอง

## การตั้งค่าสภาพแวดล้อม

### GitHub Codespaces

คุณสามารถรันเทมเพลตนี้แบบเสมือนจริงโดยใช้ GitHub Codespaces ปุ่มนี้จะเปิด VS Code เวอร์ชันเว็บในเบราว์เซอร์ของคุณ:

1. เปิดเทมเพลต (อาจใช้เวลาหลายวินาที):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. เปิดหน้าต่างเทอร์มินัล

### VS Code Dev Containers

⚠️ ตัวเลือกนี้จะใช้งานได้ก็ต่อเมื่อ Docker Desktop ของคุณมี RAM อย่างน้อย 16 GB หากคุณมี RAM น้อยกว่า 16 GB คุณสามารถลองใช้ [GitHub Codespaces](../../../../../md/01.Introduction/01) หรือ [ตั้งค่าในเครื่อง](../../../../../md/01.Introduction/01) แทน

อีกตัวเลือกหนึ่งคือ VS Code Dev Containers ซึ่งจะเปิดโปรเจกต์ใน VS Code บนเครื่องของคุณโดยใช้ [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. เริ่มต้น Docker Desktop (ติดตั้งถ้ายังไม่มี)
2. เปิดโปรเจกต์:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. ในหน้าต่าง VS Code ที่เปิดขึ้น เมื่อไฟล์โปรเจกต์แสดงขึ้น (อาจใช้เวลาหลายวินาที) ให้เปิดหน้าต่างเทอร์มินัล
4. ดำเนินการต่อด้วย [ขั้นตอนการดีพลอย](../../../../../md/01.Introduction/01)

### สภาพแวดล้อมในเครื่อง

1. ตรวจสอบให้แน่ใจว่าได้ติดตั้งเครื่องมือต่อไปนี้แล้ว:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## ทดสอบโมเดล

1. ขอให้ Ollama ดาวน์โหลดและรันโมเดล phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    การดาวน์โหลดโมเดลนี้จะใช้เวลาสักครู่

2. เมื่อคุณเห็นข้อความ "success" ในผลลัพธ์ คุณสามารถส่งข้อความไปยังโมเดลนั้นจากพรอมต์ได้

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. หลังจากผ่านไปไม่กี่วินาที คุณจะเห็นการตอบกลับจากโมเดลแบบสตรีม

4. หากต้องการเรียนรู้เทคนิคต่าง ๆ ที่ใช้กับโมเดลภาษา ให้เปิดโน้ตบุ๊ก Python [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) และรันแต่ละเซลล์ หากคุณใช้โมเดลอื่นที่ไม่ใช่ 'phi3:mini' ให้เปลี่ยนค่า `MODEL_NAME` ในเซลล์แรก

5. หากต้องการสนทนากับโมเดล phi3:mini จาก Python ให้เปิดไฟล์ Python [chat.py](../../../../../code/01.Introduce/chat.py) และรันไฟล์นี้ คุณสามารถเปลี่ยนค่า `MODEL_NAME` ที่ด้านบนของไฟล์ตามต้องการ และยังสามารถแก้ไขข้อความระบบหรือเพิ่มตัวอย่าง few-shot ได้ถ้าต้องการ

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษาอัตโนมัติ [Co-op Translator](https://github.com/Azure/co-op-translator) แม้เราจะพยายามให้ความถูกต้องสูงสุด แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาต้นทางถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลโดยผู้เชี่ยวชาญมนุษย์ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดใด ๆ ที่เกิดจากการใช้การแปลนี้