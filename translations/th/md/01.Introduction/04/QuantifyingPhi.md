<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-05-09T13:27:41+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "th"
}
-->
# **การทำ Quantify ให้กับ Phi Family**

การทำ quantization ของโมเดลหมายถึงกระบวนการแปลงพารามิเตอร์ (เช่น น้ำหนักและค่าการกระตุ้น) ในโมเดลเครือข่ายประสาทเทียม จากช่วงค่าขนาดใหญ่ (โดยปกติเป็นช่วงค่าต่อเนื่อง) ไปยังช่วงค่าที่จำกัดและมีขนาดเล็กกว่า เทคโนโลยีนี้ช่วยลดขนาดและความซับซ้อนในการคำนวณของโมเดล รวมถึงเพิ่มประสิทธิภาพการทำงานของโมเดลในสภาพแวดล้อมที่มีข้อจำกัดด้านทรัพยากร เช่น อุปกรณ์มือถือหรือระบบฝังตัว การทำ quantization ของโมเดลจะบีบอัดโดยการลดความแม่นยำของพารามิเตอร์ แต่ก็มีการสูญเสียความแม่นยำบางส่วน ดังนั้นในการทำ quantization จึงต้องมีการปรับสมดุลระหว่างขนาดโมเดล ความซับซ้อนในการคำนวณ และความแม่นยำ วิธีการ quantization ที่พบบ่อยได้แก่ fixed-point quantization, floating-point quantization เป็นต้น คุณสามารถเลือกกลยุทธ์ quantization ที่เหมาะสมตามสถานการณ์และความต้องการเฉพาะได้

เรามุ่งหวังที่จะนำโมเดล GenAI ไปใช้งานบนอุปกรณ์ edge และเปิดโอกาสให้อุปกรณ์หลากหลายประเภทเข้ามาอยู่ในสถานการณ์ GenAI เช่น อุปกรณ์มือถือ, AI PC/Copilot+PC และอุปกรณ์ IoT แบบดั้งเดิม ผ่านโมเดลที่ผ่านการ quantization เราสามารถนำไปใช้งานบนอุปกรณ์ edge ต่างๆ ตามแต่ละอุปกรณ์ เมื่อรวมกับเฟรมเวิร์กเร่งความเร็วโมเดลและโมเดลที่ผ่านการ quantization ที่ผู้ผลิตฮาร์ดแวร์จัดหาให้ เราสามารถสร้างสถานการณ์การใช้งาน SLM ที่ดียิ่งขึ้นได้

ในสถานการณ์การทำ quantization เรามีความแม่นยำที่แตกต่างกัน (INT4, INT8, FP16, FP32) ด้านล่างนี้เป็นคำอธิบายของความแม่นยำที่ใช้กันบ่อย

### **INT4**

การทำ INT4 quantization เป็นวิธีการ quantization ที่รุนแรงโดยแปลงน้ำหนักและค่าการกระตุ้นของโมเดลเป็นจำนวนเต็ม 4 บิต การทำ INT4 quantization มักทำให้เกิดการสูญเสียความแม่นยำมากขึ้นเนื่องจากช่วงค่าที่แสดงผลได้เล็กลงและความแม่นยำน้อยกว่า อย่างไรก็ตาม เมื่อเทียบกับ INT8 quantization การทำ INT4 quantization สามารถลดความต้องการพื้นที่จัดเก็บและความซับซ้อนในการคำนวณของโมเดลได้มากขึ้น ควรทราบว่า INT4 quantization ค่อนข้างหายากในแอปพลิเคชันจริง เพราะความแม่นยำต่ำเกินไปอาจทำให้ประสิทธิภาพของโมเดลลดลงอย่างมาก นอกจากนี้ไม่ใช่ฮาร์ดแวร์ทุกชนิดที่รองรับการทำงานแบบ INT4 ดังนั้นจึงต้องพิจารณาความเข้ากันได้ของฮาร์ดแวร์เมื่อเลือกวิธีการ quantization

### **INT8**

INT8 quantization คือกระบวนการแปลงน้ำหนักและค่าการกระตุ้นของโมเดลจากตัวเลขแบบ floating point เป็นจำนวนเต็ม 8 บิต แม้ว่าช่วงค่าที่ INT8 สามารถแสดงได้นั้นจะเล็กกว่าและมีความแม่นยำน้อยกว่า แต่ก็ช่วยลดความต้องการพื้นที่จัดเก็บและการคำนวณได้อย่างมาก ใน INT8 quantization น้ำหนักและค่าการกระตุ้นของโมเดลจะผ่านกระบวนการ quantization รวมถึงการปรับสเกลและการชดเชย เพื่อรักษาข้อมูลแบบ floating point เดิมไว้ให้มากที่สุด ในระหว่างการ inference ค่าที่ผ่านการ quantization เหล่านี้จะถูก dequantize กลับเป็น floating point เพื่อคำนวณ จากนั้นจะถูก quantize กลับเป็น INT8 สำหรับขั้นตอนถัดไป วิธีนี้ให้ความแม่นยำเพียงพอในแอปพลิเคชันส่วนใหญ่พร้อมกับรักษาประสิทธิภาพการคำนวณที่สูง

### **FP16**

ฟอร์แมต FP16 หรือที่เรียกว่าตัวเลข floating point 16 บิต (float16) ช่วยลดการใช้หน่วยความจำลงครึ่งหนึ่งเมื่อเทียบกับตัวเลข floating point 32 บิต (float32) ซึ่งมีข้อได้เปรียบอย่างมากในงานเรียนรู้เชิงลึกขนาดใหญ่ ฟอร์แมต FP16 ช่วยให้โหลดโมเดลขนาดใหญ่ขึ้นหรือประมวลผลข้อมูลได้มากขึ้นภายในข้อจำกัดหน่วยความจำ GPU เดียวกัน เนื่องจากฮาร์ดแวร์ GPU สมัยใหม่ยังคงสนับสนุนการทำงานแบบ FP16 การใช้ฟอร์แมต FP16 อาจช่วยเพิ่มความเร็วในการคำนวณ อย่างไรก็ตาม ฟอร์แมต FP16 ก็มีข้อเสียในตัวเอง คือความแม่นยำที่ต่ำกว่า ซึ่งอาจทำให้เกิดความไม่เสถียรของตัวเลขหรือตัวเลขสูญเสียความแม่นยำในบางกรณี

### **FP32**

ฟอร์แมต FP32 ให้ความแม่นยำสูงและสามารถแสดงค่าช่วงกว้างได้อย่างแม่นยำ ในสถานการณ์ที่ต้องทำการคำนวณทางคณิตศาสตร์ที่ซับซ้อนหรือจำเป็นต้องการผลลัพธ์ที่มีความแม่นยำสูง ฟอร์แมต FP32 จะเป็นตัวเลือกที่เหมาะสม อย่างไรก็ตาม ความแม่นยำสูงก็หมายถึงการใช้หน่วยความจำมากขึ้นและเวลาคำนวณที่นานขึ้น สำหรับโมเดลเรียนรู้เชิงลึกขนาดใหญ่ โดยเฉพาะเมื่อมีพารามิเตอร์จำนวนมากและข้อมูลมหาศาล ฟอร์แมต FP32 อาจทำให้หน่วยความจำ GPU ไม่เพียงพอหรือความเร็วในการ inference ลดลง

บนอุปกรณ์มือถือหรืออุปกรณ์ IoT เราสามารถแปลงโมเดล Phi-3.x เป็น INT4 ได้ ขณะที่ AI PC / Copilot PC สามารถใช้ความแม่นยำสูงกว่า เช่น INT8, FP16, FP32

ปัจจุบัน ผู้ผลิตฮาร์ดแวร์แต่ละรายมีเฟรมเวิร์กที่สนับสนุนโมเดล generative ต่างกัน เช่น Intel’s OpenVINO, Qualcomm’s QNN, Apple’s MLX และ Nvidia’s CUDA เป็นต้น โดยรวมกับการทำ quantization ของโมเดลเพื่อทำการติดตั้งแบบ local

ในด้านเทคโนโลยี เรามีการสนับสนุนฟอร์แมตต่างๆ หลังการทำ quantization เช่น ฟอร์แมต PyTorch / Tensorflow, GGUF และ ONNX ผมได้ทำการเปรียบเทียบฟอร์แมตและสถานการณ์การใช้งานระหว่าง GGUF กับ ONNX โดยแนะนำฟอร์แมตการทำ quantization แบบ ONNX ซึ่งได้รับการสนับสนุนอย่างดีตั้งแต่เฟรมเวิร์กของโมเดลจนถึงฮาร์ดแวร์ ในบทนี้เราจะเน้นไปที่ ONNX Runtime สำหรับ GenAI, OpenVINO และ Apple MLX เพื่อทำการ quantization ของโมเดล (ถ้าคุณมีวิธีที่ดีกว่าสามารถส่ง PR มาให้เราได้)

**บทนี้ประกอบด้วย**

1. [Quantizing Phi-3.5 / 4 using llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Quantizing Phi-3.5 / 4 using Generative AI extensions for onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Quantizing Phi-3.5 / 4 using Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Quantizing Phi-3.5 / 4 using Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้เราจะพยายามให้มีความถูกต้อง แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความคลาดเคลื่อน เอกสารต้นฉบับในภาษาดั้งเดิมถือเป็นแหล่งข้อมูลที่ถูกต้องที่สุด สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลโดยผู้เชี่ยวชาญมนุษย์ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความที่ผิดพลาดใด ๆ ที่เกิดจากการใช้การแปลนี้