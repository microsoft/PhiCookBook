<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-05T12:20:02+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "th"
}
-->
# **การปรับปริมาณค่าของตระกูล Phi**

การปรับปริมาณค่าของโมเดลหมายถึงกระบวนการแมปพารามิเตอร์ (เช่น น้ำหนักและค่าการกระตุ้น) ในโมเดลเครือข่ายประสาทเทียมจากช่วงค่าขนาดใหญ่ (โดยปกติเป็นช่วงค่าต่อเนื่อง) ไปยังช่วงค่าที่มีขนาดเล็กกว่าและจำกัด เทคโนโลยีนี้สามารถลดขนาดและความซับซ้อนด้านการคำนวณของโมเดล และช่วยเพิ่มประสิทธิภาพในการทำงานของโมเดลในสภาพแวดล้อมที่มีทรัพยากรจำกัด เช่น อุปกรณ์มือถือหรือระบบฝังตัว การปรับปริมาณค่าของโมเดลทำให้เกิดการบีบอัดโดยลดความแม่นยำของพารามิเตอร์ แต่ก็ทำให้สูญเสียความแม่นยำบางส่วน ดังนั้น ในกระบวนการปรับปริมาณจึงจำเป็นต้องสร้างสมดุลระหว่างขนาดของโมเดล ความซับซ้อนของการคำนวณ และความแม่นยำ วิธีการปรับปริมาณค่าที่พบบ่อย ได้แก่ การปรับปริมาณค่าคงที่ (fixed-point), การปรับปริมาณค่าลอยตัว (floating-point) เป็นต้น คุณสามารถเลือกกลยุทธ์การปรับปริมาณค่าที่เหมาะสมตามสถานการณ์และความต้องการเฉพาะ

เราหวังที่จะนำโมเดล GenAI ไปใช้งานบนอุปกรณ์ขอบเครือข่ายและให้อุปกรณ์จำนวนมากขึ้นสามารถเข้าสู่สถานการณ์ GenAI ได้ เช่น อุปกรณ์มือถือ, AI PC/Copilot+PC และอุปกรณ์ IoT แบบดั้งเดิม ผ่านโมเดลที่ปรับปริมาณค่าที่สามารถนำไปใช้กับอุปกรณ์ขอบแบบต่าง ๆ ตามอุปกรณ์ที่แตกต่างกัน ร่วมกับกรอบเร่งความเร็วโมเดลและโมเดลที่ปรับปริมาณค่าที่ผู้ผลิตฮาร์ดแวร์จัดหา เราจะสามารถสร้างสถานการณ์แอปพลิเคชัน SLM ที่ดีขึ้นได้

ในสถานการณ์การปรับปริมาณค่ามีความแม่นยำต่าง ๆ (INT4, INT8, FP16, FP32) ดังนี้เป็นคำอธิบายเกี่ยวกับความแม่นยำที่ใช้ปรับปริมาณค่าที่พบบ่อย

### **INT4**

การปรับปริมาณค่า INT4 เป็นวิธีการปรับปริมาณค่าที่รุนแรง โดยแปลงน้ำหนักและค่าการกระตุ้นของโมเดลเป็นจำนวนเต็ม 4 บิต การปรับปริมาณค่า INT4 มักจะทำให้เกิดการสูญเสียความแม่นยำมากขึ้นเนื่องจากช่วงค่าที่แสดงและความแม่นยำลดลง อย่างไรก็ตามเมื่อเทียบกับการปรับปริมาณค่า INT8 การปรับปริมาณค่า INT4 สามารถลดความต้องการพื้นที่เก็บข้อมูลและความซับซ้อนการคำนวณของโมเดลได้มากขึ้น ควรสังเกตว่าการปรับปริมาณค่า INT4 ค่อนข้างหายากในแอปพลิเคชันจริง เพราะความแม่นยำที่ต่ำเกินไปอาจทำให้ประสิทธิภาพของโมเดลลดลงอย่างมีนัยสำคัญ นอกจากนี้ ฮาร์ดแวร์บางชนิดไม่ได้รองรับการทำงานแบบ INT4 ดังนั้นต้องพิจารณาความเข้ากันได้ของฮาร์ดแวร์เมื่อเลือกวิธีการปรับปริมาณค่า

### **INT8**

การปรับปริมาณค่า INT8 คือกระบวนการแปลงน้ำหนักและค่าการกระตุ้นของโมเดลจากตัวเลขแบบลอยตัวเป็นจำนวนเต็ม 8 บิต แม้ว่าช่วงตัวเลขที่แสดงโดยจำนวนเต็ม INT8 จะเล็กกว่าและความแม่นยำต่ำกว่า แต่สามารถลดความต้องการพื้นที่เก็บข้อมูลและการคำนวณได้อย่างมีนัยสำคัญ ในการปรับปริมาณค่า INT8 น้ำหนักและค่าการกระตุ้นของโมเดลจะผ่านกระบวนการปรับปริมาณรวมถึงการปรับมาตราส่วนและค่าออฟเซ็ต เพื่อให้เก็บรักษาข้อมูลแบบลอยตัวต้นฉบับไว้มากที่สุดเท่าที่จะเป็นไปได้ ในระหว่างการทำนายค่าเหล่านี้จะถูกแปลงกลับเป็นตัวเลขแบบลอยตัวเพื่อคำนวณ และจากนั้นจึงถูกปรับปริมาณกลับเป็น INT8 สำหรับขั้นตอนต่อไป วิธีนี้สามารถให้ความแม่นยำเพียงพอในแอปพลิเคชันส่วนใหญ่ในขณะที่ยังคงรักษาประสิทธิภาพการคำนวณสูง

### **FP16**

รูปแบบ FP16 หรือที่เรียกว่าตัวเลขแบบลอยตัว 16 บิต (float16) ช่วยลดการใช้หน่วยความจำลงครึ่งหนึ่งเมื่อเทียบกับตัวเลขแบบลอยตัว 32 บิต (float32) ซึ่งมีข้อได้เปรียบสำคัญในแอปพลิเคชันการเรียนรู้เชิงลึกขนาดใหญ่ รูปแบบ FP16 ช่วยให้โหลดโมเดลขนาดใหญ่ขึ้นหรือประมวลผลข้อมูลได้มากขึ้นภายใต้ข้อจำกัดของหน่วยความจำ GPU เดียวกัน เนื่องจากฮาร์ดแวร์ GPU สมัยใหม่ยังคงรองรับการทำงาน FP16 การใช้รูปแบบ FP16 อาจช่วยเพิ่มความเร็วในการคำนวณได้ อย่างไรก็ตาม FP16 ก็มีข้อจำกัดในเรื่องความแม่นยำที่ต่ำกว่า อาจทำให้เกิดความไม่เสถียรทางตัวเลขหรือการสูญเสียความแม่นยำในบางกรณี

### **FP32**

รูปแบบ FP32 มีความแม่นยำสูงกว่าและสามารถแสดงช่วงค่าที่กว้างได้อย่างแม่นยำ ในสถานการณ์ที่มีการดำเนินการทางคณิตศาสตร์ที่ซับซ้อนหรือจำเป็นต้องได้ผลลัพธ์ที่มีความแม่นยำสูง รูปแบบ FP32 จะเป็นที่ต้องการ อย่างไรก็ตาม ความแม่นยำสูงหมายถึงการใช้หน่วยความจำมากขึ้นและเวลาการคำนวณที่นานขึ้น สำหรับโมเดลการเรียนรู้เชิงลึกขนาดใหญ่ โดยเฉพาะอย่างยิ่งเมื่อมีพารามิเตอร์โมเดลจำนวนมากและข้อมูลจำนวนมหาศาล รูปแบบ FP32 อาจทำให้หน่วยความจำ GPU ไม่เพียงพอหรือความเร็วในการทำนายล่าช้าได้

ในอุปกรณ์มือถือหรืออุปกรณ์ IoT เราสามารถแปลงโมเดล Phi-3.x เป็น INT4 ได้ ขณะที่ AI PC / Copilot PC สามารถใช้ความแม่นยำที่สูงขึ้น เช่น INT8, FP16, FP32

ปัจจุบันผู้ผลิตฮาร์ดแวร์ที่แตกต่างกันมีกรอบงานต่าง ๆ เพื่อรองรับโมเดลเชิงสร้างสรรค์ เช่น OpenVINO ของ Intel, QNN ของ Qualcomm, MLX ของ Apple และ CUDA ของ Nvidia เป็นต้น ร่วมกับการปรับปริมาณค่าโมเดลเพื่อเสร็จสิ้นการปรับใช้งานในพื้นที่

ในแง่เทคโนโลยี เรามีการรองรับรูปแบบต่าง ๆ หลังการปรับปริมาณ เช่น รูปแบบ PyTorch / TensorFlow, GGUF และ ONNX ผมได้ทำการเปรียบเทียบรูปแบบและสถานการณ์การใช้งานระหว่าง GGUF และ ONNX ที่นี่ผมแนะนำรูปแบบการปรับปริมาณค่า ONNX ซึ่งได้รับการสนับสนุนที่ดีจากกรอบโมเดลถึงฮาร์ดแวร์ ในบทนี้ เราจะมุ่งเน้นที่ ONNX Runtime สำหรับ GenAI, OpenVINO และ Apple MLX ในการปรับปริมาณค่าโมเดล (หากมีวิธีที่ดีกว่าสามารถส่ง PR มาให้เราได้)

**บทนี้ประกอบด้วย**

1. [การปรับปริมาณค่า Phi-3.5 / 4 โดยใช้ llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [การปรับปริมาณค่า Phi-3.5 / 4 โดยใช้ส่วนขยาย Generative AI สำหรับ onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [การปรับปริมาณค่า Phi-3.5 / 4 โดยใช้ Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [การปรับปริมาณค่า Phi-3.5 / 4 โดยใช้ Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**ข้อจำกัดความรับผิดชอบ**: เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้เราจะพยายามให้ความถูกต้องสูงสุด แต่โปรดทราบว่าการแปลโดยอัตโนมัติอาจมีข้อผิดพลาดหรือความคลาดเคลื่อนได้ เอกสารฉบับต้นฉบับในภาษาต้นกำเนิดถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญแนะนำให้ใช้การแปลโดยผู้เชี่ยวชาญมนุษย์ เรายินดีไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดพลาดที่เกิดจากการใช้การแปลฉบับนี้
<!-- CO-OP TRANSLATOR DISCLAIMER END -->