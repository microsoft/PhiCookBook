<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-07-16T21:46:57+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "th"
}
-->
# **การทำ Quantize ให้กับตระกูล Phi**

การทำ quantization ของโมเดลหมายถึงกระบวนการแปลงพารามิเตอร์ (เช่น น้ำหนักและค่าการกระตุ้น) ในโมเดลเครือข่ายประสาทเทียมจากช่วงค่าที่กว้าง (โดยปกติเป็นช่วงค่าต่อเนื่อง) ให้เป็นช่วงค่าที่จำกัดและเล็กลง เทคโนโลยีนี้ช่วยลดขนาดและความซับซ้อนในการคำนวณของโมเดล รวมถึงเพิ่มประสิทธิภาพการทำงานของโมเดลในสภาพแวดล้อมที่มีทรัพยากรจำกัด เช่น อุปกรณ์มือถือหรือระบบฝังตัว การทำ quantization จะบีบอัดโดยลดความแม่นยำของพารามิเตอร์ แต่ก็ทำให้เกิดการสูญเสียความแม่นยำบางส่วน ดังนั้นในกระบวนการ quantization จำเป็นต้องหาจุดสมดุลระหว่างขนาดโมเดล ความซับซ้อนในการคำนวณ และความแม่นยำ วิธีการ quantization ที่ใช้กันทั่วไปได้แก่ fixed-point quantization, floating-point quantization เป็นต้น คุณสามารถเลือกกลยุทธ์การ quantization ที่เหมาะสมตามสถานการณ์และความต้องการเฉพาะ

เราหวังว่าจะสามารถนำโมเดล GenAI ไปใช้งานบนอุปกรณ์ edge และเปิดโอกาสให้อุปกรณ์หลากหลายชนิดเข้าสู่สถานการณ์ GenAI เช่น อุปกรณ์มือถือ, AI PC/Copilot+PC และอุปกรณ์ IoT แบบดั้งเดิม ผ่านโมเดลที่ถูก quantize เราสามารถนำไปใช้งานบนอุปกรณ์ edge ต่าง ๆ ตามชนิดของอุปกรณ์นั้น ๆ ร่วมกับเฟรมเวิร์กเร่งความเร็วโมเดลและโมเดล quantization ที่ผู้ผลิตฮาร์ดแวร์จัดเตรียมไว้ เราจะสามารถสร้างสถานการณ์การใช้งาน SLM ที่ดียิ่งขึ้นได้

ในสถานการณ์การ quantization เรามีความแม่นยำที่แตกต่างกัน (INT4, INT8, FP16, FP32) ด้านล่างนี้เป็นคำอธิบายของความแม่นยำที่ใช้กันบ่อย

### **INT4**

การ quantize แบบ INT4 เป็นวิธีการ quantize ที่รุนแรง โดยแปลงน้ำหนักและค่าการกระตุ้นของโมเดลเป็นจำนวนเต็ม 4 บิต การ quantize แบบ INT4 มักทำให้เกิดการสูญเสียความแม่นยำมากขึ้นเนื่องจากช่วงค่าที่แทนได้น้อยและความแม่นยำต่ำกว่า อย่างไรก็ตาม เมื่อเทียบกับ INT8 การ quantize แบบ INT4 สามารถลดความต้องการพื้นที่เก็บข้อมูลและความซับซ้อนในการคำนวณของโมเดลได้มากขึ้น ควรทราบว่าการ quantize แบบ INT4 ค่อนข้างหายากในแอปพลิเคชันจริง เพราะความแม่นยำที่ต่ำเกินไปอาจทำให้ประสิทธิภาพของโมเดลลดลงอย่างมาก นอกจากนี้ไม่ใช่ฮาร์ดแวร์ทุกตัวที่รองรับการทำงานแบบ INT4 ดังนั้นจึงต้องพิจารณาความเข้ากันได้ของฮาร์ดแวร์เมื่อเลือกวิธีการ quantize

### **INT8**

การ quantize แบบ INT8 คือกระบวนการแปลงน้ำหนักและค่าการกระตุ้นของโมเดลจากตัวเลขทศนิยมแบบ floating point เป็นจำนวนเต็ม 8 บิต แม้ว่าช่วงค่าที่ INT8 แทนได้นั้นจะเล็กกว่าและความแม่นยำต่ำกว่า แต่ก็ช่วยลดความต้องการพื้นที่เก็บข้อมูลและการคำนวณได้อย่างมาก ในการ quantize แบบ INT8 น้ำหนักและค่าการกระตุ้นของโมเดลจะผ่านกระบวนการ quantize รวมถึงการปรับสเกลและออฟเซ็ต เพื่อรักษาข้อมูล floating point เดิมให้มากที่สุด ในระหว่างการ inference ค่าที่ถูก quantize เหล่านี้จะถูกแปลงกลับเป็น floating point เพื่อคำนวณ จากนั้นจึงถูก quantize กลับเป็น INT8 สำหรับขั้นตอนถัดไป วิธีนี้ให้ความแม่นยำเพียงพอในแอปพลิเคชันส่วนใหญ่ในขณะที่ยังคงประสิทธิภาพการคำนวณสูง

### **FP16**

รูปแบบ FP16 หรือจำนวนทศนิยมแบบ floating point 16 บิต (float16) ช่วยลดการใช้หน่วยความจำลงครึ่งหนึ่งเมื่อเทียบกับ floating point 32 บิต (float32) ซึ่งมีข้อได้เปรียบอย่างมากในงาน deep learning ขนาดใหญ่ รูปแบบ FP16 ช่วยให้สามารถโหลดโมเดลที่ใหญ่ขึ้นหรือประมวลผลข้อมูลมากขึ้นภายในข้อจำกัดหน่วยความจำ GPU เดียวกัน เนื่องจากฮาร์ดแวร์ GPU สมัยใหม่ยังคงสนับสนุนการทำงานแบบ FP16 การใช้รูปแบบ FP16 อาจช่วยเพิ่มความเร็วในการคำนวณได้ด้วย อย่างไรก็ตาม FP16 ก็มีข้อจำกัดในเรื่องความแม่นยำที่ต่ำกว่า ซึ่งอาจทำให้เกิดความไม่เสถียรทางตัวเลขหรือการสูญเสียความแม่นยำในบางกรณี

### **FP32**

รูปแบบ FP32 ให้ความแม่นยำสูงและสามารถแทนค่าช่วงกว้างได้อย่างถูกต้อง ในสถานการณ์ที่ต้องทำการคำนวณทางคณิตศาสตร์ที่ซับซ้อนหรือจำเป็นต้องได้ผลลัพธ์ที่มีความแม่นยำสูง รูปแบบ FP32 จะเป็นตัวเลือกที่เหมาะสม อย่างไรก็ตาม ความแม่นยำสูงก็หมายถึงการใช้หน่วยความจำมากขึ้นและเวลาคำนวณที่นานขึ้น สำหรับโมเดล deep learning ขนาดใหญ่ โดยเฉพาะเมื่อมีพารามิเตอร์จำนวนมากและข้อมูลมหาศาล รูปแบบ FP32 อาจทำให้หน่วยความจำ GPU ไม่เพียงพอหรือความเร็วในการ inference ลดลง

บนอุปกรณ์มือถือหรืออุปกรณ์ IoT เราสามารถแปลงโมเดล Phi-3.x เป็น INT4 ได้ ขณะที่ AI PC / Copilot PC สามารถใช้ความแม่นยำสูงกว่า เช่น INT8, FP16, FP32

ปัจจุบัน ผู้ผลิตฮาร์ดแวร์แต่ละรายมีเฟรมเวิร์กที่รองรับโมเดล generative แตกต่างกัน เช่น OpenVINO ของ Intel, QNN ของ Qualcomm, MLX ของ Apple และ CUDA ของ Nvidia เป็นต้น ร่วมกับการ quantize โมเดลเพื่อให้สามารถ deploy แบบ local ได้

ในด้านเทคโนโลยี เรามีการรองรับรูปแบบต่าง ๆ หลังการ quantize เช่น รูปแบบ PyTorch / Tensorflow, GGUF และ ONNX ผมได้ทำการเปรียบเทียบรูปแบบและสถานการณ์การใช้งานระหว่าง GGUF กับ ONNX โดยแนะนำให้ใช้รูปแบบ quantize ของ ONNX ซึ่งได้รับการสนับสนุนที่ดีตั้งแต่เฟรมเวิร์กโมเดลจนถึงฮาร์ดแวร์ ในบทนี้เราจะเน้นที่ ONNX Runtime สำหรับ GenAI, OpenVINO และ Apple MLX ในการทำ quantize โมเดล (ถ้าคุณมีวิธีที่ดีกว่า สามารถส่ง PR มาให้เราได้)

**บทนี้ประกอบด้วย**

1. [การทำ Quantize Phi-3.5 / 4 ด้วย llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [การทำ Quantize Phi-3.5 / 4 ด้วย Generative AI extensions สำหรับ onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [การทำ Quantize Phi-3.5 / 4 ด้วย Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [การทำ Quantize Phi-3.5 / 4 ด้วย Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษาอัตโนมัติ [Co-op Translator](https://github.com/Azure/co-op-translator) แม้เราจะพยายามให้ความถูกต้องสูงสุด แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาต้นทางถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลโดยผู้เชี่ยวชาญมนุษย์ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดใด ๆ ที่เกิดจากการใช้การแปลนี้