<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7f72d7981ed3640865700f51ae407da4",
  "translation_date": "2026-01-14T15:41:14+00:00",
  "source_file": "md/02.Application/01.TextAndChat/Phi3/E2E_Phi-3-mini_with_whisper.md",
  "language_code": "th"
}
-->
# หุ่นยนต์แชท Interactive Phi 3 Mini 4K Instruct พร้อม Whisper

## ภาพรวม

หุ่นยนต์แชท Interactive Phi 3 Mini 4K Instruct เป็นเครื่องมือที่ช่วยให้ผู้ใช้สามารถโต้ตอบกับเดโม Microsoft Phi 3 Mini 4K instruct ได้โดยใช้ข้อความหรือเสียงเป็นข้อมูลนำเข้า หุ่นยนต์แชทนี้สามารถใช้ทำงานหลากหลาย เช่น การแปลภาษา การอัปเดตสภาพอากาศ และการรวบรวมข้อมูลทั่วไป

### เริ่มต้นใช้งาน

เพื่อใช้หุ่นยนต์แชทนี้ ให้ทำตามคำแนะนำดังนี้:

1. เปิด [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) ฉบับใหม่
2. ในหน้าต่างหลักของโน้ตบุ๊ก คุณจะเห็นอินเทอร์เฟซกล่องแชทพร้อมช่องกรอกข้อความและปุ่ม "Send"
3. เพื่อใช้หุ่นยนต์แชทผ่านข้อความ เพียงพิมพ์ข้อความของคุณในช่องกรอกข้อความแล้วคลิกปุ่ม "Send" หุ่นยนต์แชทจะตอบกลับด้วยไฟล์เสียงที่สามารถเล่นจากในโน้ตบุ๊กได้โดยตรง

**หมายเหตุ**: เครื่องมือนี้ต้องใช้ GPU และการเข้าถึงโมเดล Microsoft Phi-3 และ OpenAI Whisper ซึ่งใช้สำหรับการรู้จำเสียงและการแปลภาษา

### ข้อกำหนดของ GPU

ในการรันเดโมนี้ คุณต้องมีหน่วยความจำ GPU ขนาด 12GB

ข้อกำหนดหน่วยความจำสำหรับการรันเดโม **Microsoft-Phi-3-Mini-4K instruct** บน GPU จะขึ้นอยู่กับหลายปัจจัย เช่น ขนาดของข้อมูลนำเข้า (เสียงหรือข้อความ) ภาษาที่ใช้สำหรับการแปล ความเร็วของโมเดล และหน่วยความจำ GPU ที่มีอยู่

โดยทั่วไป โมเดล Whisper ถูกออกแบบมาให้รันบน GPU ปริมาณขั้นต่ำที่แนะนำสำหรับการรันโมเดล Whisper คือ 8 GB แต่สามารถจัดการกับหน่วยความจำที่มากกว่านี้ได้หากจำเป็น

สิ่งสำคัญคือต้องทราบว่าการรันข้อมูลจำนวนมากหรือการร้องขอปริมาณสูงในโมเดลอาจต้องการหน่วยความจำ GPU มากขึ้นและ/หรืออาจทำให้เกิดปัญหาด้านประสิทธิภาพได้ จึงแนะนำให้ทดสอบกรณีการใช้งานของคุณด้วยการตั้งค่าที่หลากหลายและตรวจสอบการใช้หน่วยความจำเพื่อกำหนดค่าที่เหมาะสมกับความต้องการเฉพาะของคุณ

## ตัวอย่าง E2E สำหรับหุ่นยนต์แชท Interactive Phi 3 Mini 4K Instruct พร้อม Whisper

โน้ตบุ๊กจูปิเตอร์ที่ชื่อว่า [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) แสดงวิธีการใช้ Microsoft Phi 3 Mini 4K instruct Demo ในการสร้างข้อความจากข้อมูลเสียงหรือข้อความที่เขียน โน้ตบุ๊กกำหนดฟังก์ชันหลายตัวดังนี้:

1. `tts_file_name(text)`: ฟังก์ชันนี้สร้างชื่อไฟล์โดยอิงจากข้อความนำเข้าเพื่อใช้บันทึกไฟล์เสียงที่สร้างขึ้น
1. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: ฟังก์ชันนี้ใช้ Edge TTS API เพื่อสร้างไฟล์เสียงจากรายการชิ้นส่วนของข้อความนำเข้า พารามิเตอร์นำเข้าคือ รายการชิ้นส่วน, ความเร็วในการพูด, ชื่อเสียง, และเส้นทางสำหรับบันทึกไฟล์เสียงที่สร้างขึ้น
1. `talk(input_text)`: ฟังก์ชันนี้สร้างไฟล์เสียงโดยใช้ Edge TTS API และบันทึกไฟล์ด้วยชื่อไฟล์สุ่มในไดเรกทอรี /content/audio พารามิเตอร์นำเข้าคือข้อความนำเข้าที่จะถูกแปลงเป็นเสียงพูด
1. `run_text_prompt(message, chat_history)`: ฟังก์ชันนี้ใช้เดโม Microsoft Phi 3 Mini 4K instruct เพื่อสร้างไฟล์เสียงจากข้อความนำเข้าและแนบไปยังประวัติการสนทนา
1. `run_audio_prompt(audio, chat_history)`: ฟังก์ชันนี้แปลงไฟล์เสียงเป็นข้อความโดยใช้ Whisper model API และส่งต่อไปยังฟังก์ชัน `run_text_prompt()`
1. โค้ดนี้เปิดแอป Gradio ที่ช่วยให้ผู้ใช้โต้ตอบกับเดโม Phi 3 Mini 4K instruct โดยการพิมพ์ข้อความหรืออัปโหลดไฟล์เสียง ผลลัพธ์จะแสดงเป็นข้อความภายในแอป

## การแก้ไขปัญหา

การติดตั้งไดรเวอร์ Cuda GPU

1. ตรวจสอบให้แน่ใจว่าแอปพลิเคชัน Linux ของคุณเป็นเวอร์ชันล่าสุด

    ```bash
    sudo apt update
    ```

1. ติดตั้งไดรเวอร์ Cuda

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. ลงทะเบียนตำแหน่งไดรเวอร์ cuda

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. ตรวจสอบขนาดหน่วยความจำ Nvidia GPU (ต้องการหน่วยความจำ GPU ขนาด 12GB)

    ```bash
    nvidia-smi
    ```

1. เคลียร์แคช: หากคุณใช้ PyTorch คุณสามารถเรียกใช้ torch.cuda.empty_cache() เพื่อปล่อยหน่วยความจำแคชที่ไม่ได้ใช้งานทั้งหมดเพื่อให้แอป GPU อื่น ๆ สามารถใช้งานได้

    ```python
    torch.cuda.empty_cache() 
    ```

1. ตรวจสอบ Nvidia Cuda

    ```bash
    nvcc --version
    ```

1. ดำเนินการดังต่อไปนี้เพื่อสร้างโทเค็น Hugging Face

    - ไปที่หน้า [การตั้งค่าโทเค็น Hugging Face](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo)
    - เลือก **โทเค็นใหม่ (New token)**
    - กรอก **ชื่อโปรเจค** ที่คุณต้องการใช้
    - เลือก **ประเภท(Type)** เป็น **เขียน (Write)**

> [!NOTE]
>
> หากคุณพบข้อผิดพลาดต่อไปนี้:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> เพื่อแก้ไข ให้พิมพ์คำสั่งต่อไปนี้ในเทอร์มินัลของคุณ
>
> ```bash
> sudo ldconfig
> ```

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**ข้อจำกัดความรับผิดชอบ**:  
เอกสารฉบับนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้เราจะพยายามให้ความถูกต้องสูงสุด แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารฉบับดั้งเดิมในภาษาต้นทางถือเป็นแหล่งข้อมูลที่มีอำนาจเหนือกว่า สำหรับข้อมูลสำคัญ แนะนำให้ใช้การแปลโดยมืออาชีพที่เป็นมนุษย์ เราจะไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความที่คลาดเคลื่อนใด ๆ ที่เกิดจากการใช้การแปลนี้
<!-- CO-OP TRANSLATOR DISCLAIMER END -->