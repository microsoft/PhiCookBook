<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "743d7e9cb9c4e8ea642d77bee657a7fa",
  "translation_date": "2025-07-17T09:57:09+00:00",
  "source_file": "md/03.FineTuning/LetPhi3gotoIndustriy.md",
  "language_code": "th"
}
-->
# **ให้ Phi-3 กลายเป็นผู้เชี่ยวชาญในอุตสาหกรรม**

เพื่อให้โมเดล Phi-3 สามารถนำไปใช้ในอุตสาหกรรมได้ คุณจำเป็นต้องเพิ่มข้อมูลธุรกิจในอุตสาหกรรมนั้นลงในโมเดล Phi-3 เรามีตัวเลือกสองแบบ คือ RAG (Retrieval Augmented Generation) และ Fine Tuning

## **RAG กับ Fine-Tuning**

### **Retrieval Augmented Generation**

RAG คือการดึงข้อมูล + การสร้างข้อความ ข้อมูลที่มีโครงสร้างและไม่มีโครงสร้างขององค์กรจะถูกเก็บไว้ในฐานข้อมูลเวกเตอร์ เมื่อค้นหาข้อมูลที่เกี่ยวข้อง จะพบสรุปและเนื้อหาที่เกี่ยวข้องเพื่อนำมาสร้างบริบท และรวมกับความสามารถในการเติมข้อความของ LLM/SLM เพื่อสร้างเนื้อหา

### **Fine-tuning**

Fine-tuning คือการปรับปรุงโมเดลที่มีอยู่ ไม่จำเป็นต้องเริ่มจากอัลกอริทึมของโมเดล แต่ต้องมีการสะสมข้อมูลอย่างต่อเนื่อง หากต้องการคำศัพท์และการแสดงออกทางภาษาที่แม่นยำในแอปพลิเคชันอุตสาหกรรม Fine-tuning จะเป็นตัวเลือกที่ดีกว่า แต่ถ้าข้อมูลเปลี่ยนแปลงบ่อย Fine-tuning อาจซับซ้อนขึ้น

### **วิธีการเลือก**

1. หากคำตอบของเราต้องการข้อมูลภายนอก RAG คือทางเลือกที่ดีที่สุด

2. หากต้องการผลลัพธ์ที่มั่นคงและแม่นยำในความรู้เฉพาะทาง Fine-tuning จะเป็นตัวเลือกที่ดี RAG เน้นการดึงเนื้อหาที่เกี่ยวข้องแต่บางครั้งอาจไม่แม่นยำในรายละเอียดเฉพาะทาง

3. Fine-tuning ต้องการชุดข้อมูลคุณภาพสูง และถ้าเป็นข้อมูลเพียงเล็กน้อยจะไม่เห็นความแตกต่างมากนัก RAG มีความยืดหยุ่นมากกว่า

4. Fine-tuning เป็นกล่องดำ เป็นเรื่องลึกลับ และยากที่จะเข้าใจกลไกภายใน แต่ RAG ช่วยให้ค้นหาที่มาของข้อมูลได้ง่ายขึ้น จึงช่วยปรับปรุงปัญหาการสร้างข้อมูลผิดพลาดหรือ hallucination และเพิ่มความโปร่งใสได้ดีกว่า

### **สถานการณ์**

1. อุตสาหกรรมเฉพาะทางที่ต้องการคำศัพท์และการแสดงออกเฉพาะ ***Fine-tuning*** คือทางเลือกที่ดีที่สุด

2. ระบบถามตอบที่ต้องสังเคราะห์ความรู้จากหลายจุด ***RAG*** คือทางเลือกที่ดีที่สุด

3. การผสมผสานกระบวนการธุรกิจอัตโนมัติ ***RAG + Fine-tuning*** คือทางเลือกที่ดีที่สุด

## **วิธีใช้ RAG**

![rag](../../../../translated_images/rag.2014adc59e6f6007bafac13e800a6cbc3e297fbb9903efe20a93129bd13987e9.th.png)

ฐานข้อมูลเวกเตอร์คือการเก็บข้อมูลในรูปแบบทางคณิตศาสตร์ ฐานข้อมูลเวกเตอร์ช่วยให้โมเดลการเรียนรู้ของเครื่องจำข้อมูลที่ป้อนเข้าก่อนหน้าได้ง่ายขึ้น ทำให้สามารถนำไปใช้ในกรณีต่างๆ เช่น การค้นหา การแนะนำ และการสร้างข้อความ ข้อมูลสามารถระบุได้จากมาตรวัดความคล้ายคลึงแทนที่จะต้องตรงกันเป๊ะ ทำให้โมเดลคอมพิวเตอร์เข้าใจบริบทของข้อมูลได้

ฐานข้อมูลเวกเตอร์คือกุญแจสำคัญในการทำให้ RAG เป็นจริง เราสามารถแปลงข้อมูลเป็นรูปแบบเวกเตอร์ผ่านโมเดลเวกเตอร์ เช่น text-embedding-3, jina-ai-embedding เป็นต้น

เรียนรู้เพิ่มเติมเกี่ยวกับการสร้างแอป RAG ได้ที่ [https://github.com/microsoft/Phi-3CookBook](https://github.com/microsoft/Phi-3CookBook?WT.mc_id=aiml-138114-kinfeylo)

## **วิธีใช้ Fine-tuning**

อัลกอริทึมที่ใช้บ่อยใน Fine-tuning คือ Lora และ QLora จะเลือกอย่างไร?
- [เรียนรู้เพิ่มเติมจากตัวอย่างโน้ตบุ๊กนี้](../../../../code/04.Finetuning/Phi_3_Inference_Finetuning.ipynb)
- [ตัวอย่าง Python FineTuning Sample](../../../../code/04.Finetuning/FineTrainingScript.py)

### **Lora และ QLora**

![lora](../../../../translated_images/qlora.e6446c988ee04ca08807488bb7d9e2c0ea7ef4af9d000fc6d13032b4ac2de18d.th.png)

LoRA (Low-Rank Adaptation) และ QLoRA (Quantized Low-Rank Adaptation) เป็นเทคนิคที่ใช้ในการปรับแต่งโมเดลภาษาใหญ่ (LLMs) ด้วยวิธี Parameter Efficient Fine Tuning (PEFT) ซึ่งออกแบบมาเพื่อฝึกโมเดลได้อย่างมีประสิทธิภาพมากกว่าวิธีดั้งเดิม

LoRA เป็นเทคนิคการปรับแต่งแบบสแตนด์อโลนที่ลดการใช้หน่วยความจำโดยการประมาณค่าต่ำของเมทริกซ์อัปเดตน้ำหนัก ช่วยให้การฝึกเร็วขึ้นและรักษาประสิทธิภาพใกล้เคียงกับวิธี fine-tuning แบบดั้งเดิม

QLoRA เป็นเวอร์ชันขยายของ LoRA ที่เพิ่มเทคนิคการควอนไทซ์เพื่อลดการใช้หน่วยความจำมากขึ้น QLoRA ควอนไทซ์ความแม่นยำของพารามิเตอร์น้ำหนักใน LLM ที่ผ่านการฝึกมาแล้วเป็นความแม่นยำ 4 บิต ซึ่งประหยัดหน่วยความจำมากกว่า LoRA แต่การฝึก QLoRA ช้ากว่า LoRA ประมาณ 30% เนื่องจากมีขั้นตอนควอนไทซ์และดีควอนไทซ์เพิ่มขึ้น

QLoRA ใช้ LoRA เป็นตัวช่วยแก้ไขข้อผิดพลาดที่เกิดจากการควอนไทซ์ QLoRA ช่วยให้สามารถปรับแต่งโมเดลขนาดใหญ่ที่มีพารามิเตอร์หลายพันล้านตัวบน GPU ขนาดเล็กที่มีอยู่ทั่วไปได้ เช่น QLoRA สามารถปรับแต่งโมเดลขนาด 70B พารามิเตอร์ที่ต้องการ 36 GPUs ด้วยเพียง 2 GPUs

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษาอัตโนมัติ [Co-op Translator](https://github.com/Azure/co-op-translator) แม้เราจะพยายามให้ความถูกต้องสูงสุด แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาต้นทางถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลโดยผู้เชี่ยวชาญมนุษย์ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดใด ๆ ที่เกิดจากการใช้การแปลนี้