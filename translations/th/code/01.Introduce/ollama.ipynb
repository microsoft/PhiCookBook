{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama + OpenAI + Python\n",
    "\n",
    "## 1. ระบุชื่อโมเดล\n",
    "\n",
    "หากคุณดึงโมเดลที่แตกต่างจาก \"phi3:mini\" มาใช้ ให้เปลี่ยนค่าที่เซลล์ด้านล่าง  \n",
    "ตัวแปรนี้จะถูกใช้ในโค้ดตลอดทั้งโน้ตบุ๊ก\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"phi3:mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ตั้งค่า Open AI client\n",
    "\n",
    "โดยทั่วไป OpenAI client จะถูกใช้งานร่วมกับ OpenAI.com หรือ Azure OpenAI เพื่อโต้ตอบกับโมเดลภาษาขนาดใหญ่  \n",
    "อย่างไรก็ตาม มันสามารถใช้งานร่วมกับ Ollama ได้เช่นกัน เนื่องจาก Ollama มี endpoint ที่เข้ากันได้กับ OpenAI ที่ \"http://localhost:11434/v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"nokeyneeded\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. สร้างการตอบกลับในแชท\n",
    "\n",
    "ตอนนี้เราสามารถใช้ OpenAI SDK เพื่อสร้างการตอบกลับสำหรับการสนทนาได้ คำขอนี้ควรสร้างไฮกุเกี่ยวกับแมว:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about a hungry cat\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. การออกแบบคำสั่ง (Prompt Engineering)\n",
    "\n",
    "ข้อความแรกที่ส่งไปยังโมเดลภาษาถูกเรียกว่า \"ข้อความระบบ\" หรือ \"คำสั่งระบบ\" ซึ่งมีหน้าที่กำหนดคำแนะนำโดยรวมสำหรับโมเดล  \n",
    "คุณสามารถกำหนดคำสั่งระบบของคุณเองเพื่อแนะนำให้โมเดลภาษาแสดงผลลัพธ์ในรูปแบบที่แตกต่างออกไป  \n",
    "ปรับแต่ง `SYSTEM_MESSAGE` ด้านล่างเพื่อให้โมเดลตอบเหมือนตัวละครในภาพยนตร์หรือรายการทีวีที่คุณชื่นชอบ หรือหาแรงบันดาลใจสำหรับคำสั่งระบบอื่นๆ ได้จาก [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts?tab=readme-ov-file#prompts)\n",
    "\n",
    "เมื่อคุณปรับแต่งข้อความระบบเรียบร้อยแล้ว ให้ส่งคำถามแรกของผู้ใช้ใน `USER_MESSAGE`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "I want you to act like Elmo from Sesame Street.\n",
    "I want you to respond and answer like Elmo using the tone, manner and vocabulary that Elmo would use.\n",
    "Do not write any explanations. Only answer like Elmo.\n",
    "You must know all of the knowledge of Elmo, and nothing more.\n",
    "\"\"\"\n",
    "\n",
    "USER_MESSAGE = \"\"\"\n",
    "Hi Elmo, how are you doing today?\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ตัวอย่างการใช้งานแบบ Few Shot\n",
    "\n",
    "อีกวิธีหนึ่งในการแนะนำโมเดลภาษา คือการให้ \"few shots\" ซึ่งเป็นชุดตัวอย่างของคำถาม/คำตอบที่แสดงให้เห็นว่าโมเดลควรตอบสนองอย่างไร\n",
    "\n",
    "ตัวอย่างด้านล่างนี้พยายามให้โมเดลภาษาแสดงบทบาทเป็นผู้ช่วยสอน โดยการให้ตัวอย่างคำถามและคำตอบที่ผู้ช่วยสอนอาจให้ และจากนั้นจึงให้โมเดลตอบคำถามที่นักเรียนอาจถาม\n",
    "\n",
    "ลองใช้งานก่อน แล้วปรับ `SYSTEM_MESSAGE`, `EXAMPLES`, และ `USER_MESSAGE` ให้เหมาะสมกับสถานการณ์ใหม่\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are a helpful assistant that helps students with their homework.\n",
    "Instead of providing the full answer, you respond with a hint or a clue.\n",
    "\"\"\"\n",
    "\n",
    "EXAMPLES = [\n",
    "    (\n",
    "        \"What is the capital of France?\",\n",
    "        \"Can you remember the name of the city that is known for the Eiffel Tower?\"\n",
    "    ),\n",
    "    (\n",
    "        \"What is the square root of 144?\",\n",
    "        \"What number multiplied by itself equals 144?\"\n",
    "    ),\n",
    "    (   \"What is the atomic number of oxygen?\",\n",
    "        \"How many protons does an oxygen atom have?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "USER_MESSAGE = \"What is the largest planet in our solar system?\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": EXAMPLES[0][0]},\n",
    "        {\"role\": \"assistant\", \"content\": EXAMPLES[0][1]},\n",
    "        {\"role\": \"user\", \"content\": EXAMPLES[1][0]},\n",
    "        {\"role\": \"assistant\", \"content\": EXAMPLES[1][1]},\n",
    "        {\"role\": \"user\", \"content\": EXAMPLES[2][0]},\n",
    "        {\"role\": \"assistant\", \"content\": EXAMPLES[2][1]},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE},\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. การสร้างเนื้อหาโดยใช้การดึงข้อมูลเสริม\n",
    "\n",
    "RAG (Retrieval Augmented Generation) เป็นเทคนิคที่ช่วยให้โมเดลภาษาสามารถตอบคำถามได้อย่างแม่นยำในโดเมนเฉพาะ โดยเริ่มจากการดึงข้อมูลที่เกี่ยวข้องจากแหล่งความรู้ก่อน แล้วจึงสร้างคำตอบโดยอ้างอิงจากข้อมูลนั้น\n",
    "\n",
    "เราได้เตรียมไฟล์ CSV ไว้ในเครื่องซึ่งมีข้อมูลเกี่ยวกับรถยนต์ไฮบริด โค้ดด้านล่างจะอ่านไฟล์ CSV ค้นหาข้อมูลที่ตรงกับคำถามของผู้ใช้ และสร้างคำตอบโดยอ้างอิงจากข้อมูลที่พบ โปรดทราบว่าสิ่งนี้จะใช้เวลานานกว่าตัวอย่างก่อนหน้า เนื่องจากต้องส่งข้อมูลจำนวนมากขึ้นไปยังโมเดล หากคุณสังเกตว่าคำตอบยังไม่อ้างอิงกับข้อมูล คุณสามารถลองปรับแต่งระบบหรือใช้โมเดลอื่น โดยทั่วไป RAG จะมีประสิทธิภาพมากขึ้นเมื่อใช้กับโมเดลขนาดใหญ่หรือเวอร์ชันที่ปรับแต่งของ SLMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are a helpful assistant that answers questions about cars based off a hybrid car data set.\n",
    "You must use the data set to answer the questions, you should not provide any information that is not in the provided sources.\n",
    "\"\"\"\n",
    "\n",
    "USER_MESSAGE = \"how fast is a prius?\"\n",
    "\n",
    "# Open the CSV and store in a list\n",
    "with open(\"hybrid.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    rows = list(reader)\n",
    "\n",
    "# Normalize the user question to replace punctuation and make lowercase\n",
    "normalized_message = USER_MESSAGE.lower().replace(\"?\", \"\").replace(\"(\", \" \").replace(\")\", \" \")\n",
    "\n",
    "# Search the CSV for user question using very naive search\n",
    "words = normalized_message.split()\n",
    "matches = []\n",
    "for row in rows[1:]:\n",
    "    # if the word matches any word in row, add the row to the matches\n",
    "    if any(word in row[0].lower().split() for word in words) or any(word in row[5].lower().split() for word in words):\n",
    "        matches.append(row)\n",
    "\n",
    "# Format as a markdown table, since language models understand markdown\n",
    "matches_table = \" | \".join(rows[0]) + \"\\n\" + \" | \".join(\" --- \" for _ in range(len(rows[0]))) + \"\\n\"\n",
    "matches_table += \"\\n\".join(\" | \".join(row) for row in matches)\n",
    "print(f\"Found {len(matches)} matches:\")\n",
    "print(matches_table)\n",
    "\n",
    "# Now we can use the matches to generate a response\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE + \"\\nSources: \" + matches_table},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ข้อจำกัดความรับผิดชอบ**:  \nเอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลโดยอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษาจากผู้เชี่ยวชาญ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "coopTranslator": {
   "original_hash": "6f9e40a7dbbd892aae50aff77da4b4be",
   "translation_date": "2025-09-12T23:32:08+00:00",
   "source_file": "code/01.Introduce/ollama.ipynb",
   "language_code": "th"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}