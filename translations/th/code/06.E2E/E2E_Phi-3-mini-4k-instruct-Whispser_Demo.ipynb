{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## แชทบอท Phi 3 Mini 4K แบบโต้ตอบพร้อม Whisper\n",
    "\n",
    "### บทนำ:\n",
    "แชทบอท Phi 3 Mini 4K แบบโต้ตอบเป็นเครื่องมือที่ช่วยให้ผู้ใช้สามารถโต้ตอบกับ Microsoft Phi 3 Mini 4K instruct demo ผ่านการป้อนข้อมูลด้วยข้อความหรือเสียง แชทบอทนี้สามารถใช้งานได้หลากหลาย เช่น การแปลภาษา การอัปเดตสภาพอากาศ และการรวบรวมข้อมูลทั่วไป\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[สร้าง Huggingface Access Token ของคุณ](https://huggingface.co/settings/tokens)\n",
    "\n",
    "สร้างโทเค็นใหม่  \n",
    "ตั้งชื่อใหม่  \n",
    "เลือกสิทธิ์การเขียน  \n",
    "คัดลอกโทเค็นและเก็บไว้ในที่ปลอดภัย\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python และโค้ดนี้มีหน้าที่หลักสองอย่าง: การนำเข้าโมดูล `os` และการตั้งค่าตัวแปรสภาพแวดล้อม\n",
    "\n",
    "1. การนำเข้าโมดูล `os`:\n",
    "   - โมดูล `os` ใน Python ช่วยให้สามารถโต้ตอบกับระบบปฏิบัติการได้ โดยให้คุณทำงานที่เกี่ยวข้องกับระบบปฏิบัติการ เช่น การเข้าถึงตัวแปรสภาพแวดล้อม การทำงานกับไฟล์และไดเรกทอรี เป็นต้น\n",
    "   - ในโค้ดนี้ โมดูล `os` ถูกนำเข้าด้วยคำสั่ง `import` ซึ่งทำให้ฟังก์ชันการทำงานของโมดูล `os` พร้อมใช้งานในสคริปต์ Python ปัจจุบัน\n",
    "\n",
    "2. การตั้งค่าตัวแปรสภาพแวดล้อม:\n",
    "   - ตัวแปรสภาพแวดล้อมคือตัวแปรที่สามารถเข้าถึงได้โดยโปรแกรมที่ทำงานบนระบบปฏิบัติการ เป็นวิธีการเก็บค่าการตั้งค่าหรือข้อมูลอื่น ๆ ที่สามารถใช้ร่วมกันโดยหลายโปรแกรม\n",
    "   - ในโค้ดนี้ มีการตั้งค่าตัวแปรสภาพแวดล้อมใหม่โดยใช้พจนานุกรม `os.environ` โดยคีย์ของพจนานุกรมคือ `'HF_TOKEN'` และค่าถูกกำหนดจากตัวแปร `HUGGINGFACE_TOKEN`\n",
    "   - ตัวแปร `HUGGINGFACE_TOKEN` ถูกกำหนดไว้ด้านบนของโค้ดนี้ และได้รับค่าข้อความ `\"hf_**************\"` โดยใช้ไวยากรณ์ `#@param` ซึ่งไวยากรณ์นี้มักใช้ใน Jupyter notebooks เพื่อให้ผู้ใช้สามารถป้อนข้อมูลและตั้งค่าพารามิเตอร์ได้โดยตรงในอินเทอร์เฟซของโน้ตบุ๊ก\n",
    "   - เมื่อมีการตั้งค่าตัวแปรสภาพแวดล้อม `'HF_TOKEN'` ตัวแปรนี้สามารถถูกเข้าถึงได้โดยส่วนอื่น ๆ ของโปรแกรมหรือโปรแกรมอื่น ๆ ที่ทำงานบนระบบปฏิบัติการเดียวกัน\n",
    "\n",
    "โดยรวมแล้ว โค้ดนี้นำเข้าโมดูล `os` และตั้งค่าตัวแปรสภาพแวดล้อมชื่อ `'HF_TOKEN'` ด้วยค่าที่กำหนดในตัวแปร `HUGGINGFACE_TOKEN`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "โค้ดนี้เป็นตัวอย่างของฟังก์ชันที่ชื่อว่า `clear_output` ซึ่งใช้สำหรับล้างผลลัพธ์ของเซลล์ปัจจุบันใน Jupyter Notebook หรือ IPython มาดูรายละเอียดของโค้ดและทำความเข้าใจการทำงานของมัน:\n",
    "\n",
    "ฟังก์ชัน `clear_output` รับพารามิเตอร์หนึ่งตัวที่ชื่อว่า `wait` ซึ่งเป็นค่าบูลีน โดยค่าเริ่มต้นของ `wait` ถูกตั้งไว้เป็น `False` พารามิเตอร์นี้ใช้กำหนดว่าฟังก์ชันควรรอจนกว่าผลลัพธ์ใหม่จะพร้อมสำหรับแทนที่ผลลัพธ์เดิมก่อนที่จะล้างผลลัพธ์หรือไม่\n",
    "\n",
    "ตัวฟังก์ชันเองถูกใช้สำหรับล้างผลลัพธ์ของเซลล์ปัจจุบัน ใน Jupyter Notebook หรือ IPython เมื่อเซลล์สร้างผลลัพธ์ เช่น ข้อความที่พิมพ์ออกมาหรือกราฟ ผลลัพธ์นั้นจะถูกแสดงอยู่ด้านล่างเซลล์ ฟังก์ชัน `clear_output` ช่วยให้คุณสามารถล้างผลลัพธ์เหล่านั้นได้\n",
    "\n",
    "การทำงานของฟังก์ชันไม่ได้ถูกระบุในโค้ดตัวอย่างนี้ โดยมีเครื่องหมายจุดไข่ปลา (...) แสดงถึงตำแหน่งที่ควรจะมีโค้ดจริงสำหรับการล้างผลลัพธ์ การทำงานของฟังก์ชันอาจเกี่ยวข้องกับการใช้งาน API ของ Jupyter Notebook หรือ IPython เพื่อทำการลบผลลัพธ์ที่มีอยู่ในเซลล์\n",
    "\n",
    "โดยรวมแล้ว ฟังก์ชันนี้เป็นเครื่องมือที่สะดวกสำหรับการล้างผลลัพธ์ของเซลล์ปัจจุบันใน Jupyter Notebook หรือ IPython ซึ่งช่วยให้การจัดการและอัปเดตผลลัพธ์ที่แสดงในระหว่างการเขียนโค้ดแบบโต้ตอบเป็นไปได้ง่ายขึ้น\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ดำเนินการแปลงข้อความเป็นเสียง (TTS) โดยใช้บริการ Edge TTS มาดูการทำงานของฟังก์ชันที่เกี่ยวข้องทีละขั้นตอน:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: ฟังก์ชันนี้รับค่าที่ป้อนเข้ามาและคำนวณอัตราความเร็วของเสียง TTS ค่า input_value แสดงถึงความเร็วที่ต้องการของเสียง โดยค่าที่เป็น 1 หมายถึงความเร็วปกติ ฟังก์ชันจะคำนวณอัตราความเร็วโดยการลบ 1 จากค่า input_value คูณด้วย 100 และกำหนดเครื่องหมาย (+ หรือ -) ตามว่าค่า input_value มากกว่าหรือเท่ากับ 1 หรือไม่ ฟังก์ชันจะคืนค่าอัตราความเร็วในรูปแบบ \"{sign}{rate}\"\n",
    "\n",
    "2. `make_chunks(input_text, language)`: ฟังก์ชันนี้รับข้อความและภาษาที่ป้อนเข้ามาเป็นพารามิเตอร์ มันจะแบ่งข้อความออกเป็นส่วนย่อยตามกฎเฉพาะของภาษา ในการใช้งานนี้ หากภาษาคือ \"English\" ฟังก์ชันจะแบ่งข้อความที่จุด (\".\") และลบช่องว่างที่ขึ้นต้นหรือท้ายออก จากนั้นจะเพิ่มจุดต่อท้ายแต่ละส่วนและคืนค่ารายการของส่วนที่กรองแล้ว\n",
    "\n",
    "3. `tts_file_name(text)`: ฟังก์ชันนี้สร้างชื่อไฟล์สำหรับไฟล์เสียง TTS ตามข้อความที่ป้อนเข้ามา มันจะทำการเปลี่ยนแปลงข้อความหลายอย่าง เช่น ลบจุดที่ท้ายข้อความ (ถ้ามี) เปลี่ยนข้อความเป็นตัวพิมพ์เล็ก ลบช่องว่างที่ขึ้นต้นและท้าย และแทนที่ช่องว่างด้วยเครื่องหมายขีดล่าง จากนั้นจะตัดข้อความให้มีความยาวสูงสุด 25 ตัวอักษร (ถ้ายาวกว่า) หรือใช้ข้อความเต็มถ้าไม่มีข้อความ สุดท้ายจะสร้างสตริงสุ่มโดยใช้โมดูล [`uuid`] และรวมเข้ากับข้อความที่ตัดแล้วเพื่อสร้างชื่อไฟล์ในรูปแบบ \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: ฟังก์ชันนี้รวมไฟล์เสียงหลายไฟล์เป็นไฟล์เสียงเดียว มันรับรายการของเส้นทางไฟล์เสียงและเส้นทางไฟล์ผลลัพธ์เป็นพารามิเตอร์ ฟังก์ชันจะเริ่มต้นวัตถุ `AudioSegment` ว่างเปล่าที่เรียกว่า [`merged_audio`] จากนั้นจะวนลูปผ่านแต่ละเส้นทางไฟล์เสียง โหลดไฟล์เสียงโดยใช้เมธอด `AudioSegment.from_file()` จากไลบรารี `pydub` และเพิ่มไฟล์เสียงปัจจุบันเข้าไปในวัตถุ [`merged_audio`] สุดท้ายจะส่งออกไฟล์เสียงที่รวมแล้วไปยังเส้นทางไฟล์ที่ระบุในรูปแบบ MP3\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: ฟังก์ชันนี้ดำเนินการ TTS โดยใช้บริการ Edge TTS มันรับรายการของข้อความที่แบ่งส่วน ความเร็วของเสียง ชื่อเสียง และเส้นทางการบันทึกเป็นพารามิเตอร์ หากจำนวนส่วนมากกว่า 1 ฟังก์ชันจะสร้างไดเรกทอรีสำหรับเก็บไฟล์เสียงของแต่ละส่วน จากนั้นจะวนลูปผ่านแต่ละส่วน สร้างคำสั่ง Edge TTS โดยใช้ฟังก์ชัน `calculate_rate_string()` ชื่อเสียง และข้อความของแต่ละส่วน และดำเนินการคำสั่งโดยใช้ฟังก์ชัน `os.system()` หากการดำเนินการคำสั่งสำเร็จ จะเพิ่มเส้นทางของไฟล์เสียงที่สร้างขึ้นในรายการ หลังจากประมวลผลทุกส่วนแล้ว จะรวมไฟล์เสียงแต่ละไฟล์โดยใช้ฟังก์ชัน `merge_audio_files()` และบันทึกไฟล์เสียงที่รวมแล้วไปยังเส้นทางการบันทึกที่ระบุ หากมีเพียงส่วนเดียว จะสร้างคำสั่ง Edge TTS โดยตรงและบันทึกเสียงไปยังเส้นทางการบันทึก สุดท้ายจะคืนค่าเส้นทางการบันทึกของไฟล์เสียงที่สร้างขึ้น\n",
    "\n",
    "6. `random_audio_name_generate()`: ฟังก์ชันนี้สร้างชื่อไฟล์เสียงแบบสุ่มโดยใช้โมดูล [`uuid`] มันสร้าง UUID แบบสุ่ม แปลงเป็นสตริง เลือกตัวอักษร 8 ตัวแรก เพิ่มนามสกุล \".mp3\" และคืนค่าชื่อไฟล์เสียงแบบสุ่ม\n",
    "\n",
    "7. `talk(input_text)`: ฟังก์ชันนี้เป็นจุดเริ่มต้นหลักสำหรับการดำเนินการ TTS มันรับข้อความที่ป้อนเข้ามาเป็นพารามิเตอร์ ก่อนอื่นจะตรวจสอบความยาวของข้อความที่ป้อนเข้ามาเพื่อกำหนดว่าข้อความนั้นเป็นประโยคยาวหรือไม่ (มากกว่าหรือเท่ากับ 600 ตัวอักษร) ตามความยาวและค่าของตัวแปร `translate_text_flag` มันจะกำหนดภาษาและสร้างรายการของข้อความที่แบ่งส่วนโดยใช้ฟังก์ชัน `make_chunks()` จากนั้นจะสร้างเส้นทางการบันทึกสำหรับไฟล์เสียงโดยใช้ฟังก์ชัน `random_audio_name_generate()` สุดท้ายจะเรียกใช้ฟังก์ชัน `edge_free_tts()` เพื่อดำเนินการ TTS และคืนค่าเส้นทางการบันทึกของไฟล์เสียงที่สร้างขึ้น\n",
    "\n",
    "โดยรวมแล้ว ฟังก์ชันเหล่านี้ทำงานร่วมกันเพื่อแบ่งข้อความที่ป้อนเข้ามาเป็นส่วนย่อย สร้างชื่อไฟล์สำหรับไฟล์เสียง ดำเนินการ TTS โดยใช้บริการ Edge TTS และรวมไฟล์เสียงแต่ละไฟล์เป็นไฟล์เสียงเดียว\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "การใช้งานของสองฟังก์ชัน: convert_to_text และ run_text_prompt รวมถึงการประกาศของสองคลาส: str และ Audio\n",
    "\n",
    "ฟังก์ชัน convert_to_text รับค่า audio_path เป็นอินพุตและแปลงเสียงเป็นข้อความโดยใช้โมเดลที่เรียกว่า whisper_model ฟังก์ชันนี้จะตรวจสอบก่อนว่าตัวแปร gpu ถูกตั้งค่าเป็น True หรือไม่ หากเป็น True จะใช้ whisper_model พร้อมกับพารามิเตอร์บางอย่าง เช่น word_timestamps=True, fp16=True, language='English', และ task='translate' หาก gpu เป็น False จะใช้ whisper_model พร้อมกับ fp16=False ข้อความที่ได้จากการถอดเสียงจะถูกบันทึกลงในไฟล์ชื่อ 'scan.txt' และส่งคืนเป็นข้อความ\n",
    "\n",
    "ฟังก์ชัน run_text_prompt รับค่า message และ chat_history เป็นอินพุต โดยใช้ฟังก์ชัน phi_demo เพื่อสร้างการตอบกลับจากแชทบอทตามข้อความที่ป้อนเข้าไป การตอบกลับที่สร้างขึ้นจะถูกส่งต่อไปยังฟังก์ชัน talk ซึ่งแปลงการตอบกลับเป็นไฟล์เสียงและส่งคืนเส้นทางของไฟล์ คลาส Audio ถูกใช้เพื่อแสดงและเล่นไฟล์เสียง เสียงจะถูกแสดงโดยใช้ฟังก์ชัน display จากโมดูล IPython.display และออบเจ็กต์ Audio ถูกสร้างขึ้นพร้อมกับพารามิเตอร์ autoplay=True เพื่อให้เสียงเริ่มเล่นโดยอัตโนมัติ chat_history จะถูกอัปเดตด้วยข้อความที่ป้อนและการตอบกลับที่สร้างขึ้น และส่งคืนสตริงว่างพร้อมกับ chat_history ที่อัปเดตแล้ว\n",
    "\n",
    "คลาส str เป็นคลาสที่มีอยู่ใน Python ซึ่งแสดงถึงลำดับของตัวอักษร โดยมีเมธอดต่าง ๆ สำหรับการจัดการและการทำงานกับสตริง เช่น capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill และอื่น ๆ เมธอดเหล่านี้ช่วยให้สามารถดำเนินการต่าง ๆ เช่น การค้นหา การแทนที่ การจัดรูปแบบ และการจัดการสตริงได้\n",
    "\n",
    "คลาส Audio เป็นคลาสที่กำหนดเองซึ่งแสดงถึงออบเจ็กต์เสียง โดยใช้เพื่อสร้างเครื่องเล่นเสียงในสภาพแวดล้อม Jupyter Notebook คลาสนี้รับพารามิเตอร์ต่าง ๆ เช่น data, filename, url, embed, rate, autoplay และ normalize พารามิเตอร์ data สามารถเป็น numpy array, ลิสต์ของตัวอย่าง, สตริงที่แสดงถึงชื่อไฟล์หรือ URL หรือข้อมูล PCM ดิบ พารามิเตอร์ filename ใช้เพื่อระบุไฟล์ในเครื่องเพื่อโหลดข้อมูลเสียง และพารามิเตอร์ url ใช้เพื่อระบุ URL เพื่อดาวน์โหลดข้อมูลเสียง พารามิเตอร์ embed กำหนดว่าข้อมูลเสียงควรถูกฝังโดยใช้ data URI หรืออ้างอิงจากแหล่งที่มาเดิม พารามิเตอร์ rate ระบุอัตราการสุ่มตัวอย่างของข้อมูลเสียง พารามิเตอร์ autoplay กำหนดว่าเสียงควรเริ่มเล่นโดยอัตโนมัติหรือไม่ พารามิเตอร์ normalize ระบุว่าข้อมูลเสียงควรถูกปรับให้เป็นช่วงสูงสุดที่เป็นไปได้หรือไม่ คลาส Audio ยังมีเมธอด เช่น reload เพื่อโหลดข้อมูลเสียงใหม่จากไฟล์หรือ URL และแอตทริบิวต์ เช่น src_attr, autoplay_attr และ element_id_attr เพื่อดึงแอตทริบิวต์ที่เกี่ยวข้องสำหรับออบเจ็กต์เสียงใน HTML\n",
    "\n",
    "โดยรวมแล้ว ฟังก์ชันและคลาสเหล่านี้ถูกใช้เพื่อแปลงเสียงเป็นข้อความ สร้างการตอบกลับเสียงจากแชทบอท และแสดงและเล่นเสียงในสภาพแวดล้อม Jupyter Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**ข้อจำกัดความรับผิดชอบ**:  \nเอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้องมากที่สุด แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T23:14:16+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "th"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}