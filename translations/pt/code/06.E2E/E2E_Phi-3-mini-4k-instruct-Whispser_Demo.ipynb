{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot Interativo Phi 3 Mini 4K Instruct com Whisper\n",
    "\n",
    "### Introdução:\n",
    "O Chatbot Interativo Phi 3 Mini 4K Instruct é uma ferramenta que permite aos utilizadores interagir com a demonstração Microsoft Phi 3 Mini 4K Instruct através de texto ou entrada de áudio. O chatbot pode ser utilizado para uma variedade de tarefas, como tradução, atualizações meteorológicas e recolha de informações gerais.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[Crie o seu Token de Acesso do Huggingface](https://huggingface.co/settings/tokens)\n",
    "\n",
    "Crie um novo token  \n",
    "Forneça um novo nome  \n",
    "Selecione permissões de escrita  \n",
    "Copie o token e guarde-o num local seguro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O seguinte código Python realiza duas tarefas principais: importar o módulo `os` e definir uma variável de ambiente.\n",
    "\n",
    "1. Importar o módulo `os`:\n",
    "   - O módulo `os` em Python fornece uma forma de interagir com o sistema operativo. Permite realizar várias tarefas relacionadas com o sistema operativo, como aceder a variáveis de ambiente, trabalhar com ficheiros e diretórios, entre outros.\n",
    "   - Neste código, o módulo `os` é importado utilizando a instrução `import`. Esta instrução torna a funcionalidade do módulo `os` disponível para uso no script Python atual.\n",
    "\n",
    "2. Definir uma variável de ambiente:\n",
    "   - Uma variável de ambiente é um valor que pode ser acedido por programas que estão a ser executados no sistema operativo. É uma forma de armazenar configurações ou outras informações que podem ser utilizadas por vários programas.\n",
    "   - Neste código, uma nova variável de ambiente está a ser definida utilizando o dicionário `os.environ`. A chave do dicionário é `'HF_TOKEN'`, e o valor é atribuído a partir da variável `HUGGINGFACE_TOKEN`.\n",
    "   - A variável `HUGGINGFACE_TOKEN` é definida logo acima deste trecho de código e é atribuída um valor de string `\"hf_**************\"` utilizando a sintaxe `#@param`. Esta sintaxe é frequentemente usada em Jupyter notebooks para permitir a entrada do utilizador e a configuração de parâmetros diretamente na interface do notebook.\n",
    "   - Ao definir a variável de ambiente `'HF_TOKEN'`, esta pode ser acedida por outras partes do programa ou por outros programas que estejam a ser executados no mesmo sistema operativo.\n",
    "\n",
    "Em resumo, este código importa o módulo `os` e define uma variável de ambiente chamada `'HF_TOKEN'` com o valor fornecido na variável `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este trecho de código define uma função chamada clear_output que é utilizada para limpar o output da célula atual no Jupyter Notebook ou IPython. Vamos analisar o código e compreender a sua funcionalidade:\n",
    "\n",
    "A função clear_output recebe um parâmetro chamado wait, que é um valor booleano. Por padrão, wait está definido como False. Este parâmetro determina se a função deve aguardar até que haja um novo output disponível para substituir o output existente antes de limpá-lo.\n",
    "\n",
    "A própria função é usada para limpar o output da célula atual. No Jupyter Notebook ou IPython, quando uma célula gera um output, como texto impresso ou gráficos, esse output é exibido abaixo da célula. A função clear_output permite que você limpe esse output.\n",
    "\n",
    "A implementação da função não é fornecida no trecho de código, conforme indicado pelos três pontos (...). Os três pontos representam um espaço reservado para o código real que realiza a limpeza do output. A implementação da função pode envolver a interação com a API do Jupyter Notebook ou IPython para remover o output existente da célula.\n",
    "\n",
    "De forma geral, esta função oferece uma maneira prática de limpar o output da célula atual no Jupyter Notebook ou IPython, facilitando a gestão e atualização do output exibido durante sessões interativas de programação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizar texto-para-fala (TTS) utilizando o serviço Edge TTS. Vamos analisar as implementações relevantes das funções uma a uma:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Esta função recebe um valor de entrada e calcula a string de velocidade para a voz TTS. O valor de entrada representa a velocidade desejada da fala, onde um valor de 1 representa a velocidade normal. A função calcula a string de velocidade subtraindo 1 do valor de entrada, multiplicando-o por 100 e determinando o sinal com base em se o valor de entrada é maior ou igual a 1. A função retorna a string de velocidade no formato \"{sign}{rate}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: Esta função recebe um texto de entrada e um idioma como parâmetros. Ela divide o texto de entrada em partes com base nas regras específicas do idioma. Nesta implementação, se o idioma for \"English\", a função divide o texto em cada ponto final (\".\") e remove quaisquer espaços em branco no início ou no fim. Em seguida, adiciona um ponto final a cada parte e retorna a lista filtrada de partes.\n",
    "\n",
    "3. `tts_file_name(text)`: Esta função gera um nome de ficheiro para o ficheiro de áudio TTS com base no texto de entrada. Ela realiza várias transformações no texto: remove um ponto final no final (se presente), converte o texto para minúsculas, elimina espaços em branco no início e no fim, e substitui espaços por sublinhados. Em seguida, trunca o texto para um máximo de 25 caracteres (se for mais longo) ou utiliza o texto completo se estiver vazio. Por fim, gera uma string aleatória utilizando o módulo [`uuid`] e combina-a com o texto truncado para criar o nome do ficheiro no formato \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Esta função combina vários ficheiros de áudio num único ficheiro de áudio. Ela recebe uma lista de caminhos de ficheiros de áudio e um caminho de saída como parâmetros. A função inicializa um objeto vazio `AudioSegment` chamado [`merged_audio`]. Em seguida, percorre cada caminho de ficheiro de áudio, carrega o ficheiro de áudio utilizando o método `AudioSegment.from_file()` da biblioteca `pydub` e adiciona o ficheiro de áudio atual ao objeto [`merged_audio`]. Por fim, exporta o áudio combinado para o caminho de saída especificado no formato MP3.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: Esta função realiza a operação TTS utilizando o serviço Edge TTS. Ela recebe uma lista de partes de texto, a velocidade da fala, o nome da voz e o caminho de salvamento como parâmetros. Se o número de partes for maior que 1, a função cria um diretório para armazenar os ficheiros de áudio individuais das partes. Em seguida, percorre cada parte, constrói um comando Edge TTS utilizando a função `calculate_rate_string()`, o nome da voz e o texto da parte, e executa o comando utilizando a função `os.system()`. Se a execução do comando for bem-sucedida, adiciona o caminho do ficheiro de áudio gerado a uma lista. Após processar todas as partes, combina os ficheiros de áudio individuais utilizando a função `merge_audio_files()` e salva o áudio combinado no caminho de salvamento especificado. Se houver apenas uma parte, gera diretamente o comando Edge TTS e salva o áudio no caminho de salvamento. Por fim, retorna o caminho de salvamento do ficheiro de áudio gerado.\n",
    "\n",
    "6. `random_audio_name_generate()`: Esta função gera um nome de ficheiro de áudio aleatório utilizando o módulo [`uuid`]. Ela gera um UUID aleatório, converte-o para uma string, utiliza os primeiros 8 caracteres, adiciona a extensão \".mp3\" e retorna o nome de ficheiro de áudio aleatório.\n",
    "\n",
    "7. `talk(input_text)`: Esta função é o ponto de entrada principal para realizar a operação TTS. Ela recebe um texto de entrada como parâmetro. Primeiro, verifica o comprimento do texto de entrada para determinar se é uma frase longa (com 600 ou mais caracteres). Com base no comprimento e no valor da variável `translate_text_flag`, determina o idioma e gera a lista de partes de texto utilizando a função `make_chunks()`. Em seguida, gera um caminho de salvamento para o ficheiro de áudio utilizando a função `random_audio_name_generate()`. Por fim, chama a função `edge_free_tts()` para realizar a operação TTS e retorna o caminho de salvamento do ficheiro de áudio gerado.\n",
    "\n",
    "No geral, estas funções trabalham em conjunto para dividir o texto de entrada em partes, gerar um nome de ficheiro para o ficheiro de áudio, realizar a operação TTS utilizando o serviço Edge TTS e combinar os ficheiros de áudio individuais num único ficheiro de áudio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A implementação de duas funções: convert_to_text e run_text_prompt, bem como a declaração de duas classes: str e Audio.\n",
    "\n",
    "A função convert_to_text recebe um audio_path como entrada e transcreve o áudio para texto utilizando um modelo chamado whisper_model. A função verifica primeiro se a flag gpu está definida como True. Se estiver, o whisper_model é utilizado com determinados parâmetros, como word_timestamps=True, fp16=True, language='English' e task='translate'. Se a flag gpu estiver definida como False, o whisper_model é utilizado com fp16=False. A transcrição resultante é então guardada num ficheiro chamado 'scan.txt' e retornada como texto.\n",
    "\n",
    "A função run_text_prompt recebe uma mensagem e um chat_history como entrada. Utiliza a função phi_demo para gerar uma resposta de um chatbot com base na mensagem de entrada. A resposta gerada é então passada para a função talk, que converte a resposta num ficheiro de áudio e retorna o caminho do ficheiro. A classe Audio é utilizada para exibir e reproduzir o ficheiro de áudio. O áudio é exibido utilizando a função display do módulo IPython.display, e o objeto Audio é criado com o parâmetro autoplay=True, para que o áudio comece a tocar automaticamente. O chat_history é atualizado com a mensagem de entrada e a resposta gerada, e uma string vazia e o chat_history atualizado são retornados.\n",
    "\n",
    "A classe str é uma classe incorporada no Python que representa uma sequência de caracteres. Oferece vários métodos para manipular e trabalhar com strings, como capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill, e mais. Estes métodos permitem realizar operações como pesquisar, substituir, formatar e manipular strings.\n",
    "\n",
    "A classe Audio é uma classe personalizada que representa um objeto de áudio. É utilizada para criar um leitor de áudio no ambiente Jupyter Notebook. A classe aceita vários parâmetros, como data, filename, url, embed, rate, autoplay e normalize. O parâmetro data pode ser um array numpy, uma lista de amostras, uma string que representa um nome de ficheiro ou URL, ou dados PCM brutos. O parâmetro filename é utilizado para especificar um ficheiro local de onde carregar os dados de áudio, e o parâmetro url é utilizado para especificar um URL para descarregar os dados de áudio. O parâmetro embed determina se os dados de áudio devem ser incorporados utilizando um URI de dados ou referenciados a partir da fonte original. O parâmetro rate especifica a taxa de amostragem dos dados de áudio. O parâmetro autoplay determina se o áudio deve começar a tocar automaticamente. O parâmetro normalize especifica se os dados de áudio devem ser normalizados (reajustados) para o intervalo máximo possível. A classe Audio também fornece métodos como reload para recarregar os dados de áudio a partir de um ficheiro ou URL, e atributos como src_attr, autoplay_attr e element_id_attr para recuperar os atributos correspondentes para o elemento de áudio em HTML.\n",
    "\n",
    "No geral, estas funções e classes são utilizadas para transcrever áudio para texto, gerar respostas em áudio de um chatbot e exibir e reproduzir áudio no ambiente Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Aviso Legal**:  \nEste documento foi traduzido utilizando o serviço de tradução por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precisão, é importante ter em conta que traduções automáticas podem conter erros ou imprecisões. O documento original na sua língua nativa deve ser considerado a fonte autoritária. Para informações críticas, recomenda-se a tradução profissional realizada por humanos. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações incorretas decorrentes da utilização desta tradução.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T16:36:57+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "pt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}