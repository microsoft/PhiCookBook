<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-07-16T18:09:05+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "pt"
}
-->
# Começar com o Phi-3 localmente

Este guia vai ajudar-te a configurar o teu ambiente local para correr o modelo Phi-3 usando o Ollama. Podes executar o modelo de várias formas, incluindo através do GitHub Codespaces, VS Code Dev Containers, ou no teu ambiente local.

## Configuração do ambiente

### GitHub Codespaces

Podes executar este template virtualmente usando o GitHub Codespaces. O botão vai abrir uma instância do VS Code baseada na web no teu navegador:

1. Abre o template (isto pode demorar alguns minutos):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Abre uma janela de terminal

### VS Code Dev Containers

⚠️ Esta opção só funciona se o teu Docker Desktop tiver pelo menos 16 GB de RAM alocados. Se tiveres menos de 16 GB de RAM, podes tentar a [opção GitHub Codespaces](../../../../../md/01.Introduction/01) ou [configurar localmente](../../../../../md/01.Introduction/01).

Uma opção relacionada é o VS Code Dev Containers, que vai abrir o projeto no teu VS Code local usando a [extensão Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Inicia o Docker Desktop (instala-o se ainda não estiver instalado)
2. Abre o projeto:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Na janela do VS Code que abrir, assim que os ficheiros do projeto aparecerem (isto pode demorar alguns minutos), abre uma janela de terminal.
4. Continua com os [passos de deployment](../../../../../md/01.Introduction/01)

### Ambiente Local

1. Assegura-te que as seguintes ferramentas estão instaladas:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Testar o modelo

1. Pede ao Ollama para descarregar e correr o modelo phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Isto vai demorar alguns minutos a descarregar o modelo.

2. Assim que vires "success" na saída, podes enviar uma mensagem para esse modelo a partir do prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Depois de alguns segundos, deverás ver um fluxo de resposta a chegar do modelo.

4. Para aprender sobre diferentes técnicas usadas com modelos de linguagem, abre o notebook Python [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) e executa cada célula. Se usaste um modelo diferente de 'phi3:mini', altera o `MODEL_NAME` na primeira célula.

5. Para ter uma conversa com o modelo phi3:mini a partir do Python, abre o ficheiro Python [chat.py](../../../../../code/01.Introduce/chat.py) e executa-o. Podes alterar o `MODEL_NAME` no topo do ficheiro conforme necessário, e também podes modificar a mensagem do sistema ou adicionar exemplos few-shot se quiseres.

**Aviso Legal**:  
Este documento foi traduzido utilizando o serviço de tradução automática [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precisão, por favor tenha em conta que traduções automáticas podem conter erros ou imprecisões. O documento original na sua língua nativa deve ser considerado a fonte autorizada. Para informações críticas, recomenda-se tradução profissional humana. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações incorretas decorrentes da utilização desta tradução.