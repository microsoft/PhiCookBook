<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:43:29+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "pt"
}
-->
# Tecnologias-chave mencionadas incluem

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - uma API de baixo nível para machine learning acelerado por hardware, construída sobre o DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - uma plataforma de computação paralela e modelo de interface de programação de aplicações (API) desenvolvido pela Nvidia, que permite processamento de uso geral em unidades de processamento gráfico (GPUs).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - um formato aberto concebido para representar modelos de machine learning que proporciona interoperabilidade entre diferentes frameworks de ML.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - um formato usado para representar e atualizar modelos de machine learning, particularmente útil para modelos de linguagem mais pequenos que podem funcionar eficazmente em CPUs com quantização de 4-8 bits.

## DirectML

DirectML é uma API de baixo nível que permite machine learning acelerado por hardware. Está construída sobre o DirectX 12 para tirar partido da aceleração por GPU e é independente do fornecedor, o que significa que não requer alterações no código para funcionar em diferentes fabricantes de GPU. É usada principalmente para tarefas de treino e inferência de modelos em GPUs.

Quanto ao suporte de hardware, o DirectML foi desenhado para funcionar com uma vasta gama de GPUs, incluindo GPUs integradas e dedicadas da AMD, GPUs integradas da Intel e GPUs dedicadas da NVIDIA. Faz parte da Windows AI Platform e é suportado no Windows 10 e 11, permitindo treino e inferência de modelos em qualquer dispositivo Windows.

Houve atualizações e oportunidades relacionadas com o DirectML, como o suporte a até 150 operadores ONNX e o seu uso tanto pelo ONNX runtime como pelo WinML. É apoiado por grandes fornecedores de hardware integrados (IHVs), cada um implementando vários metacomandos.

## CUDA

CUDA, que significa Compute Unified Device Architecture, é uma plataforma de computação paralela e modelo de interface de programação de aplicações (API) criado pela Nvidia. Permite aos programadores usar uma unidade de processamento gráfico (GPU) compatível com CUDA para processamento de uso geral – uma abordagem conhecida como GPGPU (General-Purpose computing on Graphics Processing Units). CUDA é um elemento fundamental para a aceleração por GPU da Nvidia e é amplamente utilizado em várias áreas, incluindo machine learning, computação científica e processamento de vídeo.

O suporte de hardware para CUDA é específico das GPUs da Nvidia, pois é uma tecnologia proprietária desenvolvida pela Nvidia. Cada arquitetura suporta versões específicas do toolkit CUDA, que fornece as bibliotecas e ferramentas necessárias para os programadores construírem e executarem aplicações CUDA.

## ONNX

ONNX (Open Neural Network Exchange) é um formato aberto concebido para representar modelos de machine learning. Fornece uma definição de um modelo extensível de grafo computacional, bem como definições de operadores incorporados e tipos de dados padrão. O ONNX permite aos programadores transferir modelos entre diferentes frameworks de ML, facilitando a interoperabilidade e tornando mais simples criar e implementar aplicações de IA.

O Phi3 mini pode correr com ONNX Runtime em CPU e GPU em vários dispositivos, incluindo plataformas de servidor, Windows, Linux e desktops Mac, bem como CPUs móveis.  
As configurações otimizadas que adicionámos são

- Modelos ONNX para int4 DML: Quantizados para int4 via AWQ  
- Modelo ONNX para fp16 CUDA  
- Modelo ONNX para int4 CUDA: Quantizado para int4 via RTN  
- Modelo ONNX para int4 CPU e Mobile: Quantizado para int4 via RTN  

## Llama.cpp

Llama.cpp é uma biblioteca de software open-source escrita em C++. Realiza inferência em vários Large Language Models (LLMs), incluindo o Llama. Desenvolvida em conjunto com a biblioteca ggml (uma biblioteca tensorial de uso geral), llama.cpp pretende oferecer inferência mais rápida e menor uso de memória comparado com a implementação original em Python. Suporta otimização de hardware, quantização e oferece uma API simples e exemplos. Se estiver interessado em inferência eficiente de LLMs, vale a pena explorar o llama.cpp, já que o Phi3 pode correr Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) é um formato usado para representar e atualizar modelos de machine learning. É particularmente útil para modelos de linguagem mais pequenos (SLMs) que podem funcionar eficazmente em CPUs com quantização de 4-8 bits. O GGUF é vantajoso para prototipagem rápida e para correr modelos em dispositivos edge ou em trabalhos batch, como pipelines de CI/CD.

**Aviso Legal**:  
Este documento foi traduzido utilizando o serviço de tradução automática [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos pela precisão, por favor tenha em conta que traduções automáticas podem conter erros ou imprecisões. O documento original na sua língua nativa deve ser considerado a fonte autorizada. Para informações críticas, recomenda-se tradução profissional humana. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações erradas decorrentes da utilização desta tradução.