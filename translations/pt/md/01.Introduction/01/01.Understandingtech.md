<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:19:56+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "pt"
}
-->
# Tecnologias-chave mencionadas incluem

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - uma API de baixo nível para aprendizado de máquina acelerado por hardware, construída sobre o DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - uma plataforma de computação paralela e modelo de interface de programação de aplicações (API) desenvolvida pela Nvidia, que permite processamento de uso geral em unidades de processamento gráfico (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - um formato aberto projetado para representar modelos de aprendizado de máquina que oferece interoperabilidade entre diferentes frameworks de ML.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - um formato usado para representar e atualizar modelos de aprendizado de máquina, particularmente útil para modelos de linguagem menores que podem rodar eficientemente em CPUs com quantização de 4-8 bits.

## DirectML

DirectML é uma API de baixo nível que permite aprendizado de máquina acelerado por hardware. Ela é construída sobre o DirectX 12 para utilizar a aceleração por GPU e é agnóstica em relação ao fornecedor, ou seja, não requer mudanças no código para funcionar em diferentes fabricantes de GPU. É usada principalmente para cargas de trabalho de treinamento e inferência de modelos em GPUs.

Quanto ao suporte de hardware, o DirectML foi projetado para funcionar com uma ampla variedade de GPUs, incluindo GPUs integradas e discretas da AMD, GPUs integradas da Intel e GPUs discretas da NVIDIA. Faz parte da Windows AI Platform e é suportado no Windows 10 e 11, permitindo treinamento e inferência de modelos em qualquer dispositivo Windows.

Houve atualizações e oportunidades relacionadas ao DirectML, como o suporte a até 150 operadores ONNX e seu uso tanto pelo ONNX runtime quanto pelo WinML. É respaldado por grandes Integrated Hardware Vendors (IHVs), cada um implementando diversos metacomandos.

## CUDA

CUDA, que significa Compute Unified Device Architecture, é uma plataforma de computação paralela e modelo de interface de programação de aplicações (API) criada pela Nvidia. Ela permite que desenvolvedores usem uma unidade de processamento gráfico (GPU) compatível com CUDA para processamento de uso geral — uma abordagem chamada GPGPU (General-Purpose computing on Graphics Processing Units). CUDA é um elemento-chave para a aceleração por GPU da Nvidia e é amplamente usada em diversos campos, incluindo aprendizado de máquina, computação científica e processamento de vídeo.

O suporte de hardware para CUDA é específico para GPUs da Nvidia, pois é uma tecnologia proprietária desenvolvida pela Nvidia. Cada arquitetura suporta versões específicas do toolkit CUDA, que fornece as bibliotecas e ferramentas necessárias para desenvolvedores criarem e executarem aplicações CUDA.

## ONNX

ONNX (Open Neural Network Exchange) é um formato aberto projetado para representar modelos de aprendizado de máquina. Ele fornece uma definição de um modelo extensível de grafo computacional, bem como definições de operadores embutidos e tipos de dados padrão. O ONNX permite que desenvolvedores movam modelos entre diferentes frameworks de ML, possibilitando interoperabilidade e facilitando a criação e implantação de aplicações de IA.

O Phi3 mini pode rodar com ONNX Runtime em CPU e GPU em vários dispositivos, incluindo plataformas de servidor, desktops Windows, Linux e Mac, além de CPUs móveis.  
As configurações otimizadas que adicionamos são:

- Modelos ONNX para int4 DML: Quantizados para int4 via AWQ  
- Modelo ONNX para fp16 CUDA  
- Modelo ONNX para int4 CUDA: Quantizado para int4 via RTN  
- Modelo ONNX para int4 CPU e Mobile: Quantizado para int4 via RTN  

## Llama.cpp

Llama.cpp é uma biblioteca de software open-source escrita em C++. Ela realiza inferência em vários Large Language Models (LLMs), incluindo o Llama. Desenvolvida junto com a biblioteca ggml (uma biblioteca tensorial de uso geral), llama.cpp tem como objetivo oferecer inferência mais rápida e menor uso de memória comparado à implementação original em Python. Suporta otimização de hardware, quantização e oferece uma API simples e exemplos. Se você tem interesse em inferência eficiente de LLMs, vale a pena explorar o llama.cpp, já que o Phi3 pode rodar Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) é um formato usado para representar e atualizar modelos de aprendizado de máquina. É particularmente útil para modelos de linguagem menores (SLMs) que podem rodar eficientemente em CPUs com quantização de 4-8 bits. O GGUF é vantajoso para prototipagem rápida e execução de modelos em dispositivos de borda ou em trabalhos em lote, como pipelines de CI/CD.

**Aviso Legal**:  
Este documento foi traduzido utilizando o serviço de tradução por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precisão, esteja ciente de que traduções automatizadas podem conter erros ou imprecisões. O documento original em seu idioma nativo deve ser considerado a fonte autorizada. Para informações críticas, recomenda-se a tradução profissional humana. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações incorretas decorrentes do uso desta tradução.