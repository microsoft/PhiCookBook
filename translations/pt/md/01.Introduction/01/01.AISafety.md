<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c8273672cc57df2be675407a1383aaf0",
  "translation_date": "2025-05-09T05:55:04+00:00",
  "source_file": "md/01.Introduction/01/01.AISafety.md",
  "language_code": "pt"
}
-->
# Segurança de IA para modelos Phi  
A família de modelos Phi foi desenvolvida de acordo com o [Microsoft Responsible AI Standard](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFl), que é um conjunto de requisitos corporativos baseados nos seguintes seis princípios: responsabilidade, transparência, justiça, confiabilidade e segurança, privacidade e segurança, e inclusão, que formam os [princípios de IA responsável da Microsoft](https://www.microsoft.com/ai/responsible-ai).

Assim como os modelos Phi anteriores, foi adotada uma avaliação multifacetada de segurança e uma abordagem de pós-treinamento para segurança, com medidas adicionais para levar em conta as capacidades multilíngues desta versão. Nossa abordagem para o treinamento e avaliações de segurança, incluindo testes em múltiplos idiomas e categorias de risco, está detalhada no [Phi Safety Post-Training Paper](https://arxiv.org/abs/2407.13833). Embora os modelos Phi se beneficiem dessa abordagem, os desenvolvedores devem aplicar as melhores práticas de IA responsável, incluindo mapear, medir e mitigar riscos associados ao seu caso de uso específico e ao contexto cultural e linguístico.

## Melhores Práticas  

Assim como outros modelos, a família Phi pode potencialmente apresentar comportamentos injustos, não confiáveis ou ofensivos.

Alguns dos comportamentos limitantes dos SLM e LLM que você deve estar ciente incluem:

- **Qualidade do Serviço:** Os modelos Phi são treinados principalmente com texto em inglês. Idiomas diferentes do inglês terão desempenho inferior. Variedades do inglês com menor representação nos dados de treinamento podem apresentar desempenho inferior ao inglês americano padrão.  
- **Representação de Danos e Perpetuação de Estereótipos:** Esses modelos podem super ou sub-representar grupos de pessoas, apagar a representação de alguns grupos ou reforçar estereótipos depreciativos ou negativos. Apesar do pós-treinamento de segurança, essas limitações podem persistir devido a diferentes níveis de representação dos grupos ou à prevalência de exemplos de estereótipos negativos nos dados de treinamento que refletem padrões do mundo real e vieses sociais.  
- **Conteúdo Inapropriado ou Ofensivo:** Esses modelos podem gerar outros tipos de conteúdo inapropriado ou ofensivo, o que pode tornar inadequado seu uso em contextos sensíveis sem mitigações adicionais específicas para o caso de uso.  
- **Confiabilidade da Informação:** Modelos de linguagem podem gerar conteúdo sem sentido ou fabricar informações que soem razoáveis, mas que são imprecisas ou desatualizadas.  
- **Escopo Limitado para Código:** A maior parte dos dados de treinamento do Phi-3 é baseada em Python e utiliza pacotes comuns como "typing, math, random, collections, datetime, itertools". Se o modelo gerar scripts Python que utilizem outros pacotes ou scripts em outras linguagens, recomendamos fortemente que os usuários verifiquem manualmente todas as chamadas de API.

Os desenvolvedores devem aplicar as melhores práticas de IA responsável e são responsáveis por garantir que um caso de uso específico cumpra as leis e regulamentações relevantes (ex: privacidade, comércio, etc.).

## Considerações sobre IA Responsável  

Assim como outros modelos de linguagem, os modelos da série Phi podem potencialmente apresentar comportamentos injustos, não confiáveis ou ofensivos. Alguns dos comportamentos limitantes a serem observados incluem:

**Qualidade do Serviço:** Os modelos Phi são treinados principalmente com texto em inglês. Idiomas diferentes do inglês terão desempenho inferior. Variedades do inglês com menor representação nos dados de treinamento podem apresentar desempenho inferior ao inglês americano padrão.

**Representação de Danos e Perpetuação de Estereótipos:** Esses modelos podem super ou sub-representar grupos de pessoas, apagar a representação de alguns grupos ou reforçar estereótipos depreciativos ou negativos. Apesar do pós-treinamento de segurança, essas limitações podem persistir devido a diferentes níveis de representação dos grupos ou à prevalência de exemplos de estereótipos negativos nos dados de treinamento que refletem padrões do mundo real e vieses sociais.

**Conteúdo Inapropriado ou Ofensivo:** Esses modelos podem gerar outros tipos de conteúdo inapropriado ou ofensivo, o que pode tornar inadequado seu uso em contextos sensíveis sem mitigações adicionais específicas para o caso de uso.  
**Confiabilidade da Informação:** Modelos de linguagem podem gerar conteúdo sem sentido ou fabricar informações que soem razoáveis, mas que são imprecisas ou desatualizadas.

**Escopo Limitado para Código:** A maior parte dos dados de treinamento do Phi-3 é baseada em Python e utiliza pacotes comuns como "typing, math, random, collections, datetime, itertools". Se o modelo gerar scripts Python que utilizem outros pacotes ou scripts em outras linguagens, recomendamos fortemente que os usuários verifiquem manualmente todas as chamadas de API.

Os desenvolvedores devem aplicar as melhores práticas de IA responsável e são responsáveis por garantir que um caso de uso específico cumpra as leis e regulamentações relevantes (ex: privacidade, comércio, etc.). Áreas importantes para consideração incluem:

**Alocação:** Os modelos podem não ser adequados para cenários que possam ter impacto significativo sobre status legal ou alocação de recursos ou oportunidades de vida (ex: moradia, emprego, crédito, etc.) sem avaliações adicionais e técnicas de desenviesamento complementares.

**Cenários de Alto Risco:** Os desenvolvedores devem avaliar a adequação do uso dos modelos em cenários de alto risco onde saídas injustas, não confiáveis ou ofensivas possam ser extremamente custosas ou causar danos. Isso inclui fornecer conselhos em domínios sensíveis ou especializados onde precisão e confiabilidade são críticas (ex: aconselhamento jurídico ou de saúde). Salvaguardas adicionais devem ser implementadas no nível da aplicação conforme o contexto de implantação.

**Desinformação:** Os modelos podem produzir informações imprecisas. Os desenvolvedores devem seguir as melhores práticas de transparência e informar os usuários finais de que estão interagindo com um sistema de IA. No nível da aplicação, os desenvolvedores podem construir mecanismos de feedback e pipelines para fundamentar as respostas em informações contextuais específicas do caso de uso, uma técnica conhecida como Retrieval Augmented Generation (RAG).

**Geração de Conteúdo Prejudicial:** Os desenvolvedores devem avaliar as saídas no contexto e usar classificadores de segurança disponíveis ou soluções personalizadas apropriadas para seu caso de uso.

**Uso Indevido:** Outras formas de uso indevido, como fraude, spam ou produção de malware, podem ser possíveis, e os desenvolvedores devem garantir que suas aplicações não violem leis e regulamentações aplicáveis.

### Fine-tuning e Segurança de Conteúdo de IA  

Após o fine-tuning de um modelo, recomendamos fortemente o uso das medidas do [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) para monitorar o conteúdo gerado pelos modelos, identificar e bloquear riscos potenciais, ameaças e problemas de qualidade.

![Phi3AISafety](../../../../../translated_images/01.phi3aisafety.b950fac78d0cda701abf8181b3cfdabf328f70d0d5c096d5ebf842a2db62615f.pt.png)

O [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) suporta conteúdo de texto e imagem. Pode ser implantado na nuvem, em containers desconectados e em dispositivos edge/embarcados.

## Visão Geral do Azure AI Content Safety  

O Azure AI Content Safety não é uma solução única para todos; pode ser personalizado para alinhar-se às políticas específicas das empresas. Além disso, seus modelos multilíngues permitem compreender múltiplos idiomas simultaneamente.

![AIContentSafety](../../../../../translated_images/01.AIcontentsafety.da9a83e9538e688418877be04138e05621b0ab1222565ac2761e28677a59fdb4.pt.png)

- **Azure AI Content Safety**  
- **Microsoft Developer**  
- **5 vídeos**

O serviço Azure AI Content Safety detecta conteúdo prejudicial gerado por usuários e por IA em aplicações e serviços. Inclui APIs de texto e imagem que permitem detectar material prejudicial ou inapropriado.

[AI Content Safety Playlist](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

**Aviso Legal**:  
Este documento foi traduzido usando o serviço de tradução por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precisão, por favor, esteja ciente de que traduções automáticas podem conter erros ou imprecisões. O documento original em seu idioma nativo deve ser considerado a fonte autorizada. Para informações críticas, recomenda-se tradução profissional humana. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações incorretas decorrentes do uso desta tradução.