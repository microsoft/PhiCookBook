<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7b08e277df2a9307f861ae54bc30c772",
  "translation_date": "2025-07-16T19:35:51+00:00",
  "source_file": "md/01.Introduction/02/06.NVIDIA.md",
  "language_code": "pt"
}
-->
## Família Phi na NVIDIA NIM

A NVIDIA NIM é um conjunto de microserviços fáceis de usar, concebidos para acelerar a implementação de modelos de IA generativa na cloud, centros de dados e estações de trabalho. Os NIMs são categorizados por família de modelos e por modelo individual. Por exemplo, o NVIDIA NIM para grandes modelos de linguagem (LLMs) traz o poder dos LLMs mais avançados para aplicações empresariais, oferecendo capacidades incomparáveis de processamento e compreensão de linguagem natural.

O NIM facilita às equipas de IT e DevOps a auto-hospedagem de grandes modelos de linguagem (LLMs) nos seus próprios ambientes geridos, ao mesmo tempo que fornece aos programadores APIs padrão da indústria que lhes permitem construir copilotos poderosos, chatbots e assistentes de IA capazes de transformar o seu negócio. Aproveitando a aceleração GPU de ponta da NVIDIA e a implementação escalável, o NIM oferece o caminho mais rápido para inferência com desempenho inigualável.

Pode usar o NVIDIA NIM para inferir modelos da Família Phi

![nim](../../../../../translated_images/pt/Phi-NIM.09bebb743387ee4a.png)

### **Exemplos - Phi-3-Vision na NVIDIA NIM**

Imagine que tem uma imagem (`demo.png`) e quer gerar código Python que processe essa imagem e guarde uma nova versão dela (`phi-3-vision.jpg`).

O código acima automatiza este processo ao:

1. Configurar o ambiente e as configurações necessárias.
2. Criar um prompt que instrui o modelo a gerar o código Python necessário.
3. Enviar o prompt para o modelo e recolher o código gerado.
4. Extrair e executar o código gerado.
5. Exibir as imagens original e processada.

Esta abordagem aproveita o poder da IA para automatizar tarefas de processamento de imagem, tornando mais fácil e rápido alcançar os seus objetivos.

[Solução de Código de Exemplo](../../../../../code/06.E2E/E2E_Nvidia_NIM_Phi3_Vision.ipynb)

Vamos analisar passo a passo o que o código completo faz:

1. **Instalar o Pacote Necessário**:
    ```python
    !pip install langchain_nvidia_ai_endpoints -U
    ```
    Este comando instala o pacote `langchain_nvidia_ai_endpoints`, garantindo que está na versão mais recente.

2. **Importar os Módulos Necessários**:
    ```python
    from langchain_nvidia_ai_endpoints import ChatNVIDIA
    import getpass
    import os
    import base64
    ```
    Estas importações trazem os módulos necessários para interagir com os endpoints de IA da NVIDIA, gerir passwords de forma segura, interagir com o sistema operativo e codificar/decodificar dados em formato base64.

3. **Configurar a Chave API**:
    ```python
    if not os.getenv("NVIDIA_API_KEY"):
        os.environ["NVIDIA_API_KEY"] = getpass.getpass("Enter your NVIDIA API key: ")
    ```
    Este código verifica se a variável de ambiente `NVIDIA_API_KEY` está definida. Caso contrário, solicita ao utilizador que introduza a sua chave API de forma segura.

4. **Definir o Modelo e o Caminho da Imagem**:
    ```python
    model = 'microsoft/phi-3-vision-128k-instruct'
    chat = ChatNVIDIA(model=model)
    img_path = './imgs/demo.png'
    ```
    Isto define o modelo a utilizar, cria uma instância de `ChatNVIDIA` com o modelo especificado e define o caminho para o ficheiro de imagem.

5. **Criar o Prompt de Texto**:
    ```python
    text = "Please create Python code for image, and use plt to save the new picture under imgs/ and name it phi-3-vision.jpg."
    ```
    Define um prompt de texto que instrui o modelo a gerar código Python para processar uma imagem.

6. **Codificar a Imagem em Base64**:
    ```python
    with open(img_path, "rb") as f:
        image_b64 = base64.b64encode(f.read()).decode()
    image = f'<img src="data:image/png;base64,{image_b64}" />'
    ```
    Este código lê o ficheiro de imagem, codifica-o em base64 e cria uma tag HTML de imagem com os dados codificados.

7. **Combinar Texto e Imagem no Prompt**:
    ```python
    prompt = f"{text} {image}"
    ```
    Combina o prompt de texto e a tag HTML da imagem numa única string.

8. **Gerar Código Usando o ChatNVIDIA**:
    ```python
    code = ""
    for chunk in chat.stream(prompt):
        print(chunk.content, end="")
        code += chunk.content
    ```
    Este código envia o prompt para o modelo `ChatNVIDIA` e recolhe o código gerado em partes, imprimindo e adicionando cada parte à string `code`.

9. **Extrair o Código Python do Conteúdo Gerado**:
    ```python
    begin = code.index('```python') + 9
    code = code[begin:]
    end = code.index('```')
    code = code[:end]
    ```
    Isto extrai o código Python real do conteúdo gerado, removendo a formatação markdown.

10. **Executar o Código Gerado**:
    ```python
    import subprocess
    result = subprocess.run(["python", "-c", code], capture_output=True)
    ```
    Executa o código Python extraído como um subprocesso e captura a sua saída.

11. **Exibir as Imagens**:
    ```python
    from IPython.display import Image, display
    display(Image(filename='./imgs/phi-3-vision.jpg'))
    display(Image(filename='./imgs/demo.png'))
    ```
    Estas linhas exibem as imagens usando o módulo `IPython.display`.

**Aviso Legal**:  
Este documento foi traduzido utilizando o serviço de tradução automática [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos pela precisão, por favor tenha em conta que traduções automáticas podem conter erros ou imprecisões. O documento original na sua língua nativa deve ser considerado a fonte autorizada. Para informações críticas, recomenda-se tradução profissional humana. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações incorretas decorrentes da utilização desta tradução.