<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:43:39+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "br"
}
-->
# Tecnologias-chave mencionadas incluem

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - uma API de baixo nível para aprendizado de máquina acelerado por hardware, construída sobre o DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - uma plataforma de computação paralela e modelo de interface de programação de aplicações (API) desenvolvida pela Nvidia, que permite processamento de uso geral em unidades de processamento gráfico (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - um formato aberto projetado para representar modelos de aprendizado de máquina que oferece interoperabilidade entre diferentes frameworks de ML.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - um formato usado para representar e atualizar modelos de aprendizado de máquina, especialmente útil para modelos de linguagem menores que podem rodar eficientemente em CPUs com quantização de 4-8 bits.

## DirectML

DirectML é uma API de baixo nível que permite aprendizado de máquina acelerado por hardware. Ela é construída sobre o DirectX 12 para utilizar a aceleração da GPU e é independente de fornecedor, ou seja, não requer alterações no código para funcionar em diferentes fabricantes de GPU. É usada principalmente para treinamento de modelos e tarefas de inferência em GPUs.

Quanto ao suporte de hardware, o DirectML foi projetado para funcionar com uma ampla variedade de GPUs, incluindo GPUs integradas e discretas da AMD, GPUs integradas da Intel e GPUs discretas da NVIDIA. Faz parte da Plataforma de IA do Windows e é suportado no Windows 10 e 11, permitindo treinamento e inferência de modelos em qualquer dispositivo Windows.

Houve atualizações e oportunidades relacionadas ao DirectML, como suporte a até 150 operadores ONNX e seu uso tanto pelo runtime ONNX quanto pelo WinML. É apoiado por grandes fornecedores integrados de hardware (IHVs), cada um implementando vários metacomandos.

## CUDA

CUDA, que significa Compute Unified Device Architecture, é uma plataforma de computação paralela e modelo de interface de programação de aplicações (API) criado pela Nvidia. Permite que desenvolvedores de software usem uma unidade de processamento gráfico (GPU) habilitada para CUDA para processamento de uso geral – uma abordagem chamada GPGPU (General-Purpose computing on Graphics Processing Units). CUDA é um elemento-chave para a aceleração por GPU da Nvidia e é amplamente utilizado em diversas áreas, incluindo aprendizado de máquina, computação científica e processamento de vídeo.

O suporte de hardware para CUDA é específico para GPUs da Nvidia, pois é uma tecnologia proprietária desenvolvida pela Nvidia. Cada arquitetura suporta versões específicas do toolkit CUDA, que fornece as bibliotecas e ferramentas necessárias para os desenvolvedores criarem e executarem aplicações CUDA.

## ONNX

ONNX (Open Neural Network Exchange) é um formato aberto projetado para representar modelos de aprendizado de máquina. Ele fornece uma definição de um modelo de grafo computacional extensível, além de definições de operadores embutidos e tipos de dados padrão. O ONNX permite que desenvolvedores movam modelos entre diferentes frameworks de ML, facilitando a interoperabilidade e tornando mais simples criar e implantar aplicações de IA.

O Phi3 mini pode rodar com ONNX Runtime em CPU e GPU em diversos dispositivos, incluindo plataformas de servidor, desktops Windows, Linux e Mac, e CPUs móveis.  
As configurações otimizadas que adicionamos são

- Modelos ONNX para int4 DML: Quantizados para int4 via AWQ  
- Modelo ONNX para fp16 CUDA  
- Modelo ONNX para int4 CUDA: Quantizado para int4 via RTN  
- Modelo ONNX para int4 CPU e Mobile: Quantizado para int4 via RTN  

## Llama.cpp

Llama.cpp é uma biblioteca de software open-source escrita em C++. Ela realiza inferência em vários Modelos de Linguagem Grande (LLMs), incluindo o Llama. Desenvolvida junto com a biblioteca ggml (uma biblioteca tensorial de uso geral), llama.cpp tem como objetivo oferecer inferência mais rápida e menor uso de memória em comparação com a implementação original em Python. Suporta otimização de hardware, quantização e oferece uma API simples e exemplos. Se você tem interesse em inferência eficiente de LLMs, vale a pena explorar o llama.cpp, já que o Phi3 pode rodar Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) é um formato usado para representar e atualizar modelos de aprendizado de máquina. É especialmente útil para modelos de linguagem menores (SLMs) que podem rodar eficientemente em CPUs com quantização de 4-8 bits. GGUF é vantajoso para prototipagem rápida e execução de modelos em dispositivos de borda ou em jobs em lote, como pipelines de CI/CD.

**Aviso Legal**:  
Este documento foi traduzido utilizando o serviço de tradução por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precisão, esteja ciente de que traduções automáticas podem conter erros ou imprecisões. O documento original em seu idioma nativo deve ser considerado a fonte autorizada. Para informações críticas, recomenda-se tradução profissional humana. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações incorretas decorrentes do uso desta tradução.