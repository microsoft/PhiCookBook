<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:20:20+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "br"
}
-->
# Teknologi kunci yang disebutkan meliputi

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - API tingkat rendah untuk pembelajaran mesin yang dipercepat perangkat keras, dibangun di atas DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - platform komputasi paralel dan model API yang dikembangkan oleh Nvidia, memungkinkan pemrosesan tujuan umum pada unit pemrosesan grafis (GPU).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - format terbuka yang dirancang untuk merepresentasikan model pembelajaran mesin dan menyediakan interoperabilitas antar berbagai framework ML.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - format yang digunakan untuk merepresentasikan dan memperbarui model pembelajaran mesin, sangat berguna untuk model bahasa kecil yang dapat berjalan efektif di CPU dengan kuantisasi 4-8 bit.

## DirectML

DirectML adalah API tingkat rendah yang memungkinkan pembelajaran mesin dipercepat perangkat keras. Dibangun di atas DirectX 12 untuk memanfaatkan akselerasi GPU dan bersifat vendor-agnostik, artinya tidak memerlukan perubahan kode agar dapat bekerja di berbagai vendor GPU. API ini terutama digunakan untuk pelatihan model dan beban kerja inferensi pada GPU.

Mengenai dukungan perangkat keras, DirectML dirancang untuk bekerja dengan berbagai GPU, termasuk GPU terintegrasi dan diskrit AMD, GPU terintegrasi Intel, dan GPU diskrit NVIDIA. Ini merupakan bagian dari Windows AI Platform dan didukung di Windows 10 & 11, memungkinkan pelatihan model dan inferensi di perangkat Windows mana pun.

Telah ada pembaruan dan peluang terkait DirectML, seperti dukungan hingga 150 operator ONNX dan digunakan oleh runtime ONNX serta WinML. DirectML didukung oleh vendor perangkat keras terintegrasi utama (IHV), yang masing-masing mengimplementasikan berbagai metaperintah.

## CUDA

CUDA, singkatan dari Compute Unified Device Architecture, adalah platform komputasi paralel dan model API yang dibuat oleh Nvidia. CUDA memungkinkan pengembang perangkat lunak menggunakan GPU yang mendukung CUDA untuk pemrosesan tujuan umum—pendekatan yang disebut GPGPU (General-Purpose computing on Graphics Processing Units). CUDA merupakan pendorong utama akselerasi GPU Nvidia dan banyak digunakan di berbagai bidang, termasuk pembelajaran mesin, komputasi ilmiah, dan pemrosesan video.

Dukungan perangkat keras untuk CUDA khusus pada GPU Nvidia, karena ini adalah teknologi proprietary yang dikembangkan oleh Nvidia. Setiap arsitektur mendukung versi tertentu dari toolkit CUDA, yang menyediakan perpustakaan dan alat yang diperlukan bagi pengembang untuk membangun dan menjalankan aplikasi CUDA.

## ONNX

ONNX (Open Neural Network Exchange) adalah format terbuka yang dirancang untuk merepresentasikan model pembelajaran mesin. ONNX menyediakan definisi model grafik komputasi yang dapat diperluas, serta definisi operator bawaan dan tipe data standar. ONNX memungkinkan pengembang memindahkan model antar berbagai framework ML, mendukung interoperabilitas dan mempermudah pembuatan serta penerapan aplikasi AI.

Phi3 mini dapat dijalankan dengan ONNX Runtime pada CPU dan GPU di berbagai perangkat, termasuk platform server, desktop Windows, Linux, dan Mac, serta CPU mobile.  
Konfigurasi yang telah kami optimalkan meliputi:

- Model ONNX untuk int4 DML: Dikualtasi ke int4 melalui AWQ  
- Model ONNX untuk fp16 CUDA  
- Model ONNX untuk int4 CUDA: Dikualtasi ke int4 melalui RTN  
- Model ONNX untuk int4 CPU dan Mobile: Dikualtasi ke int4 melalui RTN  

## Llama.cpp

Llama.cpp adalah perpustakaan perangkat lunak open-source yang ditulis dalam C++. Perpustakaan ini melakukan inferensi pada berbagai Large Language Models (LLM), termasuk Llama. Dikembangkan bersama dengan perpustakaan ggml (perpustakaan tensor tujuan umum), llama.cpp bertujuan menyediakan inferensi yang lebih cepat dan penggunaan memori yang lebih rendah dibandingkan implementasi Python aslinya. Llama.cpp mendukung optimasi perangkat keras, kuantisasi, serta menawarkan API sederhana dan contoh-contoh. Jika Anda tertarik pada inferensi LLM yang efisien, llama.cpp layak untuk dijelajahi karena Phi3 dapat menjalankan Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) adalah format yang digunakan untuk merepresentasikan dan memperbarui model pembelajaran mesin. Format ini sangat berguna untuk model bahasa kecil (SLM) yang dapat berjalan efektif di CPU dengan kuantisasi 4-8 bit. GGUF bermanfaat untuk prototipe cepat dan menjalankan model pada perangkat edge atau dalam pekerjaan batch seperti pipeline CI/CD.

**Aviso Legal**:  
Este documento foi traduzido usando o serviço de tradução por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precisão, por favor, esteja ciente de que traduções automáticas podem conter erros ou imprecisões. O documento original em seu idioma nativo deve ser considerado a fonte autorizada. Para informações críticas, recomenda-se tradução profissional humana. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações incorretas decorrentes do uso desta tradução.