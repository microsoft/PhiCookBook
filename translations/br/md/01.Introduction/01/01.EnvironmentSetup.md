<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-09T07:06:16+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "br"
}
-->
# Começar com Phi-3 localmente

Este guia vai ajudar você a configurar seu ambiente local para rodar o modelo Phi-3 usando Ollama. Você pode executar o modelo de algumas formas diferentes, incluindo o uso do GitHub Codespaces, VS Code Dev Containers ou seu ambiente local.

## Configuração do ambiente

### GitHub Codespaces

Você pode rodar este template virtualmente usando o GitHub Codespaces. O botão vai abrir uma instância do VS Code baseada na web no seu navegador:

1. Abra o template (isso pode levar alguns minutos):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Abra uma janela do terminal

### VS Code Dev Containers

⚠️ Esta opção só funciona se o seu Docker Desktop estiver alocado com pelo menos 16 GB de RAM. Se você tiver menos de 16 GB de RAM, pode tentar a [opção GitHub Codespaces](../../../../../md/01.Introduction/01) ou [configurar localmente](../../../../../md/01.Introduction/01).

Uma opção relacionada é o VS Code Dev Containers, que abrirá o projeto no seu VS Code local usando a [extensão Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Inicie o Docker Desktop (instale se ainda não estiver instalado)
2. Abra o projeto:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Na janela do VS Code que abrir, quando os arquivos do projeto aparecerem (isso pode levar alguns minutos), abra uma janela do terminal.
4. Continue com os [passos de deployment](../../../../../md/01.Introduction/01)

### Ambiente Local

1. Certifique-se de que as seguintes ferramentas estão instaladas:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Teste o modelo

1. Peça para o Ollama baixar e rodar o modelo phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Isso vai levar alguns minutos para baixar o modelo.

2. Quando você vir "success" na saída, poderá enviar uma mensagem para esse modelo pelo prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Após alguns segundos, você deverá ver uma resposta sendo transmitida do modelo.

4. Para aprender sobre diferentes técnicas usadas com modelos de linguagem, abra o notebook Python [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) e execute cada célula. Se você usou um modelo diferente de 'phi3:mini', altere o `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` no topo do arquivo conforme necessário, e também pode modificar a mensagem do sistema ou adicionar exemplos few-shot, se desejar.

**Aviso Legal**:  
Este documento foi traduzido utilizando o serviço de tradução automática [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precisão, esteja ciente de que traduções automáticas podem conter erros ou imprecisões. O documento original em seu idioma nativo deve ser considerado a fonte autorizada. Para informações críticas, recomenda-se tradução profissional realizada por humanos. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações equivocadas decorrentes do uso desta tradução.