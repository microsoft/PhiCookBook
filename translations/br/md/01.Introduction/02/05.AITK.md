<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4951d458c0b60c02cd1e751b40903877",
  "translation_date": "2025-07-16T19:25:16+00:00",
  "source_file": "md/01.Introduction/02/05.AITK.md",
  "language_code": "br"
}
-->
# Família Phi no AITK

[AI Toolkit para VS Code](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio) simplifica o desenvolvimento de aplicativos de IA generativa ao reunir ferramentas e modelos de ponta do Azure AI Foundry Catalog e outros catálogos como Hugging Face. Você poderá navegar pelo catálogo de modelos de IA alimentado pelos GitHub Models e Azure AI Foundry Model Catalogs, baixá-los localmente ou remotamente, ajustar, testar e usar em sua aplicação.

A prévia do AI Toolkit será executada localmente. A inferência local ou ajuste fino depende do modelo selecionado, podendo ser necessário ter uma GPU como NVIDIA CUDA GPU. Você também pode executar os GitHub Models diretamente com o AITK.

## Começando

[Saiba mais sobre como instalar o Windows Subsystem for Linux](https://learn.microsoft.com/windows/wsl/install?WT.mc_id=aiml-137032-kinfeylo)

e [como alterar a distribuição padrão](https://learn.microsoft.com/windows/wsl/install#change-the-default-linux-distribution-installed).

[Repositório AI Toolkit no GitHub](https://github.com/microsoft/vscode-ai-toolkit/)

- Windows, Linux, macOS

- Para ajuste fino tanto no Windows quanto no Linux, você precisará de uma GPU Nvidia. Além disso, **Windows** requer o subsistema para Linux com a distribuição Ubuntu 18.4 ou superior. [Saiba mais sobre como instalar o Windows Subsystem for Linux](https://learn.microsoft.com/windows/wsl/install) e [como alterar a distribuição padrão](https://learn.microsoft.com/windows/wsl/install#change-the-default-linux-distribution-installed).

### Instalar AI Toolkit

O AI Toolkit é distribuído como uma [Extensão do Visual Studio Code](https://code.visualstudio.com/docs/setup/additional-components#_vs-code-extensions), então você precisa instalar o [VS Code](https://code.visualstudio.com/docs/setup/windows?WT.mc_id=aiml-137032-kinfeylo) primeiro e baixar o AI Toolkit no [VS Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio).  
O [AI Toolkit está disponível no Visual Studio Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio) e pode ser instalado como qualquer outra extensão do VS Code.

Se você não está familiarizado com a instalação de extensões no VS Code, siga estes passos:

### Fazer login

1. Na Barra de Atividades do VS Code, selecione **Extensões**  
1. Na barra de busca das Extensões, digite "AI Toolkit"  
1. Selecione "AI Toolkit for Visual Studio Code"  
1. Clique em **Instalar**

Agora, você está pronto para usar a extensão!

Você será solicitado a fazer login no GitHub, então clique em "Permitir" para continuar. Você será redirecionado para a página de login do GitHub.

Faça login e siga os passos do processo. Após a conclusão com sucesso, você será redirecionado ao VS Code.

Depois que a extensão for instalada, o ícone do AI Toolkit aparecerá na sua Barra de Atividades.

Vamos explorar as ações disponíveis!

### Ações Disponíveis

A barra lateral principal do AI Toolkit está organizada em  

- **Models**  
- **Resources**  
- **Playground**  
- **Fine-tuning**  
- **Evaluation**

Estão disponíveis na seção Resources. Para começar, selecione **Model Catalog**.

### Baixar um modelo do catálogo

Ao abrir o AI Toolkit na barra lateral do VS Code, você pode escolher entre as seguintes opções:

![Catálogo de modelos do AI Toolkit](../../../../../translated_images/br/AItoolkitmodel_catalog.7a7be6a7d8468d31.png)

- Encontrar um modelo suportado no **Model Catalog** e baixar localmente  
- Testar a inferência do modelo no **Model Playground**  
- Ajustar o modelo localmente ou remotamente em **Model Fine-tuning**  
- Implantar modelos ajustados na nuvem via paleta de comandos do AI Toolkit  
- Avaliar modelos

> [!NOTE]
>
> **GPU Vs CPU**
>
> Você notará que os cards dos modelos mostram o tamanho do modelo, a plataforma e o tipo de acelerador (CPU, GPU). Para desempenho otimizado em **dispositivos Windows que possuem pelo menos uma GPU**, selecione versões de modelos que sejam específicas para Windows.
>
> Isso garante que você tenha um modelo otimizado para o acelerador DirectML.
>
> Os nomes dos modelos seguem o formato
>
> - `{model_name}-{accelerator}-{quantization}-{format}`.
>
>Para verificar se você tem uma GPU no seu dispositivo Windows, abra o **Gerenciador de Tarefas** e selecione a aba **Desempenho**. Se houver GPU(s), elas aparecerão com nomes como "GPU 0" ou "GPU 1".

### Executar o modelo no playground

Depois de configurar todos os parâmetros, clique em **Generate Project**.

Quando o modelo for baixado, selecione **Load in Playground** no card do modelo no catálogo:

- Iniciar o download do modelo  
- Instalar todos os pré-requisitos e dependências  
- Criar o workspace no VS Code

![Carregar modelo no playground](../../../../../translated_images/br/AItoolkitload_model_into_playground.dcef5355b1653b52.png)

### Usar a REST API na sua aplicação

O AI Toolkit vem com um servidor web REST API local **na porta 5272** que usa o [formato de chat completions do OpenAI](https://platform.openai.com/docs/api-reference/chat/create).

Isso permite que você teste sua aplicação localmente sem depender de um serviço de modelo de IA na nuvem. Por exemplo, o arquivo JSON a seguir mostra como configurar o corpo da requisição:

```json
{
    "model": "Phi-4",
    "messages": [
        {
            "role": "user",
            "content": "what is the golden ratio?"
        }
    ],
    "temperature": 0.7,
    "top_p": 1,
    "top_k": 10,
    "max_tokens": 100,
    "stream": true
}
```

Você pode testar a REST API usando (por exemplo) [Postman](https://www.postman.com/) ou a ferramenta CURL (Client URL):

```bash
curl -vX POST http://127.0.0.1:5272/v1/chat/completions -H 'Content-Type: application/json' -d @body.json
```

### Usando a biblioteca cliente OpenAI para Python

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://127.0.0.1:5272/v1/", 
    api_key="x" # required for the API but not used
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "what is the golden ratio?",
        }
    ],
    model="Phi-4",
)

print(chat_completion.choices[0].message.content)
```

### Usando a biblioteca cliente Azure OpenAI para .NET

Adicione a [biblioteca cliente Azure OpenAI para .NET](https://www.nuget.org/packages/Azure.AI.OpenAI/) ao seu projeto usando o NuGet:

```bash
dotnet add {project_name} package Azure.AI.OpenAI --version 1.0.0-beta.17
```

Adicione um arquivo C# chamado **OverridePolicy.cs** ao seu projeto e cole o seguinte código:

```csharp
// OverridePolicy.cs
using Azure.Core.Pipeline;
using Azure.Core;

internal partial class OverrideRequestUriPolicy(Uri overrideUri)
    : HttpPipelineSynchronousPolicy
{
    private readonly Uri _overrideUri = overrideUri;

    public override void OnSendingRequest(HttpMessage message)
    {
        message.Request.Uri.Reset(_overrideUri);
    }
}
```

Em seguida, cole o código abaixo no seu arquivo **Program.cs**:

```csharp
// Program.cs
using Azure.AI.OpenAI;

Uri localhostUri = new("http://localhost:5272/v1/chat/completions");

OpenAIClientOptions clientOptions = new();
clientOptions.AddPolicy(
    new OverrideRequestUriPolicy(localhostUri),
    Azure.Core.HttpPipelinePosition.BeforeTransport);
OpenAIClient client = new(openAIApiKey: "unused", clientOptions);

ChatCompletionsOptions options = new()
{
    DeploymentName = "Phi-4",
    Messages =
    {
        new ChatRequestSystemMessage("You are a helpful assistant. Be brief and succinct."),
        new ChatRequestUserMessage("What is the golden ratio?"),
    }
};

StreamingResponse<StreamingChatCompletionsUpdate> streamingChatResponse
    = await client.GetChatCompletionsStreamingAsync(options);

await foreach (StreamingChatCompletionsUpdate chatChunk in streamingChatResponse)
{
    Console.Write(chatChunk.ContentUpdate);
}
```


## Ajuste Fino com AI Toolkit

- Comece com descoberta de modelos e playground.  
- Ajuste fino e inferência de modelos usando recursos locais.  
- Ajuste fino e inferência remotos usando recursos do Azure.

[Ajuste Fino com AI Toolkit](../../03.FineTuning/Finetuning_VSCodeaitoolkit.md)

## Recursos de Perguntas e Respostas do AI Toolkit

Consulte nossa [página de Q&A](https://github.com/microsoft/vscode-ai-toolkit/blob/main/archive/QA.md) para as questões mais comuns e suas soluções.

**Aviso Legal**:  
Este documento foi traduzido utilizando o serviço de tradução por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precisão, esteja ciente de que traduções automáticas podem conter erros ou imprecisões. O documento original em seu idioma nativo deve ser considerado a fonte autorizada. Para informações críticas, recomenda-se tradução profissional humana. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações incorretas decorrentes do uso desta tradução.