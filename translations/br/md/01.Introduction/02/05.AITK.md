<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4951d458c0b60c02cd1e751b40903877",
  "translation_date": "2025-05-09T09:31:32+00:00",
  "source_file": "md/01.Introduction/02/05.AITK.md",
  "language_code": "br"
}
-->
# Phi Family in AITK

[AI Toolkit for VS Code](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio) facilita o desenvolvimento de aplicativos de IA generativa ao reunir ferramentas e modelos avançados de desenvolvimento de IA do Azure AI Foundry Catalog e outros catálogos como o Hugging Face. Você poderá navegar pelo catálogo de modelos de IA alimentado pelos GitHub Models e Azure AI Foundry Model Catalogs, baixá-los localmente ou remotamente, ajustar, testar e usar em sua aplicação.

O AI Toolkit Preview será executado localmente. A inferência local ou o ajuste fino, dependendo do modelo selecionado, pode exigir uma GPU como NVIDIA CUDA GPU. Também é possível executar os GitHub Models diretamente com o AITK.

## Começando

[Saiba mais sobre como instalar o Windows Subsystem for Linux](https://learn.microsoft.com/windows/wsl/install?WT.mc_id=aiml-137032-kinfeylo)

e [como alterar a distribuição padrão](https://learn.microsoft.com/windows/wsl/install#change-the-default-linux-distribution-installed).

[Repositório GitHub do AI Toolkit](https://github.com/microsoft/vscode-ai-toolkit/)

- Windows, Linux, macOS
  
- Para ajuste fino tanto no Windows quanto no Linux, é necessário ter uma GPU Nvidia. Além disso, **no Windows** é preciso o subsistema Linux com a distribuição Ubuntu 18.4 ou superior. [Saiba mais sobre como instalar o Windows Subsystem for Linux](https://learn.microsoft.com/windows/wsl/install) e [como alterar a distribuição padrão](https://learn.microsoft.com/windows/wsl/install#change-the-default-linux-distribution-installed).

### Instalar o AI Toolkit

O AI Toolkit é distribuído como uma [extensão do Visual Studio Code](https://code.visualstudio.com/docs/setup/additional-components#_vs-code-extensions), então você precisa instalar o [VS Code](https://code.visualstudio.com/docs/setup/windows?WT.mc_id=aiml-137032-kinfeylo) primeiro, e baixar o AI Toolkit no [VS Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio).  
O [AI Toolkit está disponível no Visual Studio Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio) e pode ser instalado como qualquer outra extensão do VS Code.

Se você não está familiarizado com a instalação de extensões no VS Code, siga estes passos:

### Fazer Login

1. Na Barra de Atividades do VS Code, selecione **Extensions**  
1. Na barra de busca das extensões, digite "AI Toolkit"  
1. Selecione "AI Toolkit for Visual Studio code"  
1. Clique em **Install**

Agora você está pronto para usar a extensão!

Será solicitado que você faça login no GitHub, então clique em "Allow" para continuar. Você será redirecionado para a página de login do GitHub.

Faça login e siga os passos do processo. Após a conclusão com sucesso, você será redirecionado de volta ao VS Code.

Após a instalação da extensão, o ícone do AI Toolkit aparecerá na sua Barra de Atividades.

Vamos explorar as ações disponíveis!

### Ações Disponíveis

A barra lateral principal do AI Toolkit está organizada em  

- **Models**  
- **Resources**  
- **Playground**  
- **Fine-tuning**  
- **Evaluation**

Estão disponíveis na seção Resources. Para começar, selecione **Model Catalog**.

### Baixar um modelo do catálogo

Ao iniciar o AI Toolkit pela barra lateral do VS Code, você pode escolher entre as seguintes opções:

![AI toolkit model catalog](../../../../../translated_images/AItoolkitmodel_catalog.eee6b38a71f628501d730ffe9c2ae69b8f18706e7492ac2371423b045485996e.br.png)

- Encontrar um modelo suportado no **Model Catalog** e baixar localmente  
- Testar a inferência do modelo no **Model Playground**  
- Ajustar o modelo localmente ou remotamente em **Model Fine-tuning**  
- Implantar modelos ajustados na nuvem via palette de comandos do AI Toolkit  
- Avaliar modelos

> [!NOTE]
>
> **GPU vs CPU**  
>
> Você vai notar que os cartões dos modelos mostram o tamanho do modelo, a plataforma e o tipo de acelerador (CPU, GPU). Para desempenho otimizado em **dispositivos Windows que tenham ao menos uma GPU**, selecione versões do modelo que são específicas para Windows.  
>
> Isso garante que você tenha um modelo otimizado para o acelerador DirectML.  
>
> Os nomes dos modelos seguem o formato  
>
> - `{model_name}-{accelerator}-{quantization}-{format}`.  
>
>Para verificar se seu dispositivo Windows possui GPU, abra o **Gerenciador de Tarefas** e selecione a aba **Performance**. Se você tiver GPUs, elas aparecerão listadas com nomes como "GPU 0" ou "GPU 1".

### Executar o modelo no playground

Após definir todos os parâmetros, clique em **Generate Project**.

Depois que seu modelo for baixado, selecione **Load in Playground** no cartão do modelo no catálogo:

- Iniciar o download do modelo  
- Instalar todos os pré-requisitos e dependências  
- Criar o workspace do VS Code

![Load model in playground](../../../../../translated_images/AItoolkitload_model_into_playground.e442d8013c65406e69471fb4f8e4e3800505255fe1bd7aa9422f02ee715bad57.br.png)

### Usar a REST API na sua aplicação

O AI Toolkit vem com um servidor web REST API local **na porta 5272** que usa o [formato de chat completions do OpenAI](https://platform.openai.com/docs/api-reference/chat/create).

Isso permite testar sua aplicação localmente sem depender de um serviço de modelo de IA na nuvem. Por exemplo, o arquivo JSON abaixo mostra como configurar o corpo da requisição:

```json
{
    "model": "Phi-4",
    "messages": [
        {
            "role": "user",
            "content": "what is the golden ratio?"
        }
    ],
    "temperature": 0.7,
    "top_p": 1,
    "top_k": 10,
    "max_tokens": 100,
    "stream": true
}
```

Você pode testar a REST API usando (por exemplo) o [Postman](https://www.postman.com/) ou a ferramenta CURL (Client URL):

```bash
curl -vX POST http://127.0.0.1:5272/v1/chat/completions -H 'Content-Type: application/json' -d @body.json
```

### Usando a biblioteca cliente OpenAI para Python

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://127.0.0.1:5272/v1/", 
    api_key="x" # required for the API but not used
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "what is the golden ratio?",
        }
    ],
    model="Phi-4",
)

print(chat_completion.choices[0].message.content)
```

### Usando a biblioteca cliente Azure OpenAI para .NET

Adicione a [biblioteca cliente Azure OpenAI para .NET](https://www.nuget.org/packages/Azure.AI.OpenAI/) ao seu projeto usando o NuGet:

```bash
dotnet add {project_name} package Azure.AI.OpenAI --version 1.0.0-beta.17
```

Adicione um arquivo C# chamado **OverridePolicy.cs** ao seu projeto e cole o código a seguir:

```csharp
// OverridePolicy.cs
using Azure.Core.Pipeline;
using Azure.Core;

internal partial class OverrideRequestUriPolicy(Uri overrideUri)
    : HttpPipelineSynchronousPolicy
{
    private readonly Uri _overrideUri = overrideUri;

    public override void OnSendingRequest(HttpMessage message)
    {
        message.Request.Uri.Reset(_overrideUri);
    }
}
```

Em seguida, cole o código abaixo no seu arquivo **Program.cs**:

```csharp
// Program.cs
using Azure.AI.OpenAI;

Uri localhostUri = new("http://localhost:5272/v1/chat/completions");

OpenAIClientOptions clientOptions = new();
clientOptions.AddPolicy(
    new OverrideRequestUriPolicy(localhostUri),
    Azure.Core.HttpPipelinePosition.BeforeTransport);
OpenAIClient client = new(openAIApiKey: "unused", clientOptions);

ChatCompletionsOptions options = new()
{
    DeploymentName = "Phi-4",
    Messages =
    {
        new ChatRequestSystemMessage("You are a helpful assistant. Be brief and succinct."),
        new ChatRequestUserMessage("What is the golden ratio?"),
    }
};

StreamingResponse<StreamingChatCompletionsUpdate> streamingChatResponse
    = await client.GetChatCompletionsStreamingAsync(options);

await foreach (StreamingChatCompletionsUpdate chatChunk in streamingChatResponse)
{
    Console.Write(chatChunk.ContentUpdate);
}
```


## Ajuste Fino com AI Toolkit

- Comece explorando modelos e playground.  
- Ajuste fino e inferência usando recursos locais.  
- Ajuste fino e inferência remotos usando recursos do Azure.

[Ajuste Fino com AI Toolkit](../../03.FineTuning/Finetuning_VSCodeaitoolkit.md)

## Recursos de Perguntas e Respostas do AI Toolkit

Consulte nossa [página de Q&A](https://github.com/microsoft/vscode-ai-toolkit/blob/main/archive/QA.md) para dúvidas comuns e soluções.

**Aviso Legal**:  
Este documento foi traduzido utilizando o serviço de tradução automática [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precisão, esteja ciente de que traduções automáticas podem conter erros ou imprecisões. O documento original em seu idioma nativo deve ser considerado a fonte oficial. Para informações críticas, recomenda-se tradução profissional feita por humanos. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações incorretas decorrentes do uso desta tradução.