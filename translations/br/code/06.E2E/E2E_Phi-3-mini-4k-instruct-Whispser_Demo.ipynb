{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot Interativo Phi 3 Mini 4K Instruct com Whisper\n",
    "\n",
    "### Introdução:\n",
    "O Chatbot Interativo Phi 3 Mini 4K Instruct é uma ferramenta que permite aos usuários interagir com a demonstração do Microsoft Phi 3 Mini 4K Instruct usando entrada de texto ou áudio. O chatbot pode ser utilizado para diversas tarefas, como tradução, atualizações meteorológicas e coleta de informações gerais.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[Crie seu Token de Acesso do Huggingface](https://huggingface.co/settings/tokens)\n",
    "\n",
    "Crie um novo token  \n",
    "Forneça um novo nome  \n",
    "Selecione permissões de escrita  \n",
    "Copie o token e salve-o em um local seguro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O seguinte código em Python realiza duas tarefas principais: importar o módulo `os` e configurar uma variável de ambiente.\n",
    "\n",
    "1. Importando o módulo `os`:\n",
    "   - O módulo `os` em Python fornece uma maneira de interagir com o sistema operacional. Ele permite realizar várias tarefas relacionadas ao sistema operacional, como acessar variáveis de ambiente, trabalhar com arquivos e diretórios, entre outras.\n",
    "   - Neste código, o módulo `os` é importado usando a instrução `import`. Essa instrução torna a funcionalidade do módulo `os` disponível para uso no script Python atual.\n",
    "\n",
    "2. Configurando uma variável de ambiente:\n",
    "   - Uma variável de ambiente é um valor que pode ser acessado por programas que estão sendo executados no sistema operacional. É uma forma de armazenar configurações ou outras informações que podem ser usadas por vários programas.\n",
    "   - Neste código, uma nova variável de ambiente está sendo configurada usando o dicionário `os.environ`. A chave do dicionário é `'HF_TOKEN'`, e o valor é atribuído a partir da variável `HUGGINGFACE_TOKEN`.\n",
    "   - A variável `HUGGINGFACE_TOKEN` é definida logo acima deste trecho de código e recebe um valor de string `\"hf_**************\"` usando a sintaxe `#@param`. Essa sintaxe é frequentemente usada em notebooks Jupyter para permitir entrada de usuário e configuração de parâmetros diretamente na interface do notebook.\n",
    "   - Ao configurar a variável de ambiente `'HF_TOKEN'`, ela pode ser acessada por outras partes do programa ou por outros programas que estão sendo executados no mesmo sistema operacional.\n",
    "\n",
    "Resumindo, este código importa o módulo `os` e configura uma variável de ambiente chamada `'HF_TOKEN'` com o valor fornecido na variável `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este trecho de código define uma função chamada clear_output, que é usada para limpar o output da célula atual no Jupyter Notebook ou IPython. Vamos analisar o código e entender sua funcionalidade:\n",
    "\n",
    "A função clear_output recebe um parâmetro chamado wait, que é um valor booleano. Por padrão, wait é definido como False. Esse parâmetro determina se a função deve aguardar até que um novo output esteja disponível para substituir o output existente antes de limpá-lo.\n",
    "\n",
    "A própria função é usada para limpar o output da célula atual. No Jupyter Notebook ou IPython, quando uma célula produz um output, como texto impresso ou gráficos, esse output é exibido abaixo da célula. A função clear_output permite que você limpe esse output.\n",
    "\n",
    "A implementação da função não é fornecida no trecho de código, conforme indicado pela elipse (...). A elipse representa um espaço reservado para o código real que realiza a limpeza do output. A implementação da função pode envolver a interação com a API do Jupyter Notebook ou IPython para remover o output existente da célula.\n",
    "\n",
    "De forma geral, essa função oferece uma maneira prática de limpar o output da célula atual no Jupyter Notebook ou IPython, facilitando o gerenciamento e a atualização do output exibido durante sessões interativas de codificação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizar conversão de texto em fala (TTS) usando o serviço Edge TTS. Vamos analisar as implementações das funções relevantes uma por uma:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Esta função recebe um valor de entrada e calcula a string de velocidade para a voz TTS. O valor de entrada representa a velocidade desejada da fala, onde um valor de 1 representa a velocidade normal. A função calcula a string de velocidade subtraindo 1 do valor de entrada, multiplicando por 100 e determinando o sinal com base em se o valor de entrada é maior ou igual a 1. A função retorna a string de velocidade no formato \"{sign}{rate}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: Esta função recebe um texto de entrada e um idioma como parâmetros. Ela divide o texto de entrada em partes com base nas regras específicas do idioma. Nesta implementação, se o idioma for \"English\", a função divide o texto em cada ponto final (\".\") e remove quaisquer espaços em branco no início ou no final. Em seguida, adiciona um ponto final a cada parte e retorna a lista filtrada de partes.\n",
    "\n",
    "3. `tts_file_name(text)`: Esta função gera um nome de arquivo para o arquivo de áudio TTS com base no texto de entrada. Ela realiza várias transformações no texto: remove um ponto final no final (se presente), converte o texto para minúsculas, remove espaços em branco no início e no final e substitui espaços por sublinhados. Em seguida, trunca o texto para um máximo de 25 caracteres (se for mais longo) ou usa o texto completo se estiver vazio. Por fim, gera uma string aleatória usando o módulo [`uuid`] e combina com o texto truncado para criar o nome do arquivo no formato \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Esta função combina vários arquivos de áudio em um único arquivo de áudio. Ela recebe uma lista de caminhos de arquivos de áudio e um caminho de saída como parâmetros. A função inicializa um objeto vazio `AudioSegment` chamado [`merged_audio`]. Em seguida, percorre cada caminho de arquivo de áudio, carrega o arquivo de áudio usando o método `AudioSegment.from_file()` da biblioteca `pydub` e adiciona o arquivo de áudio atual ao objeto [`merged_audio`]. Por fim, exporta o áudio combinado para o caminho de saída especificado no formato MP3.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: Esta função realiza a operação TTS usando o serviço Edge TTS. Ela recebe uma lista de partes de texto, a velocidade da fala, o nome da voz e o caminho de salvamento como parâmetros. Se o número de partes for maior que 1, a função cria um diretório para armazenar os arquivos de áudio individuais das partes. Em seguida, percorre cada parte, constrói um comando Edge TTS usando a função `calculate_rate_string()`, o nome da voz e o texto da parte, e executa o comando usando a função `os.system()`. Se a execução do comando for bem-sucedida, ela adiciona o caminho do arquivo de áudio gerado a uma lista. Após processar todas as partes, combina os arquivos de áudio individuais usando a função `merge_audio_files()` e salva o áudio combinado no caminho de salvamento especificado. Se houver apenas uma parte, ela gera diretamente o comando Edge TTS e salva o áudio no caminho de salvamento. Por fim, retorna o caminho de salvamento do arquivo de áudio gerado.\n",
    "\n",
    "6. `random_audio_name_generate()`: Esta função gera um nome de arquivo de áudio aleatório usando o módulo [`uuid`]. Ela gera um UUID aleatório, converte para uma string, pega os primeiros 8 caracteres, adiciona a extensão \".mp3\" e retorna o nome de arquivo de áudio aleatório.\n",
    "\n",
    "7. `talk(input_text)`: Esta função é o ponto de entrada principal para realizar a operação TTS. Ela recebe um texto de entrada como parâmetro. Primeiro, verifica o comprimento do texto de entrada para determinar se é uma frase longa (maior ou igual a 600 caracteres). Com base no comprimento e no valor da variável `translate_text_flag`, determina o idioma e gera a lista de partes de texto usando a função `make_chunks()`. Em seguida, gera um caminho de salvamento para o arquivo de áudio usando a função `random_audio_name_generate()`. Por fim, chama a função `edge_free_tts()` para realizar a operação TTS e retorna o caminho de salvamento do arquivo de áudio gerado.\n",
    "\n",
    "No geral, essas funções trabalham juntas para dividir o texto de entrada em partes, gerar um nome de arquivo para o arquivo de áudio, realizar a operação TTS usando o serviço Edge TTS e combinar os arquivos de áudio individuais em um único arquivo de áudio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A implementação de duas funções: convert_to_text e run_text_prompt, assim como a declaração de duas classes: str e Audio.\n",
    "\n",
    "A função convert_to_text recebe um audio_path como entrada e transcreve o áudio para texto usando um modelo chamado whisper_model. A função primeiro verifica se a flag gpu está configurada como True. Se estiver, o whisper_model é utilizado com certos parâmetros, como word_timestamps=True, fp16=True, language='English' e task='translate'. Se a flag gpu estiver configurada como False, o whisper_model é usado com fp16=False. A transcrição resultante é então salva em um arquivo chamado 'scan.txt' e retornada como texto.\n",
    "\n",
    "A função run_text_prompt recebe uma mensagem e um chat_history como entrada. Ela utiliza a função phi_demo para gerar uma resposta de um chatbot com base na mensagem de entrada. A resposta gerada é então passada para a função talk, que converte a resposta em um arquivo de áudio e retorna o caminho do arquivo. A classe Audio é usada para exibir e reproduzir o arquivo de áudio. O áudio é exibido usando a função display do módulo IPython.display, e o objeto Audio é criado com o parâmetro autoplay=True, para que o áudio comece a tocar automaticamente. O chat_history é atualizado com a mensagem de entrada e a resposta gerada, e uma string vazia e o chat_history atualizado são retornados.\n",
    "\n",
    "A classe str é uma classe embutida no Python que representa uma sequência de caracteres. Ela fornece vários métodos para manipular e trabalhar com strings, como capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill e mais. Esses métodos permitem realizar operações como busca, substituição, formatação e manipulação de strings.\n",
    "\n",
    "A classe Audio é uma classe personalizada que representa um objeto de áudio. Ela é usada para criar um player de áudio no ambiente do Jupyter Notebook. A classe aceita vários parâmetros, como data, filename, url, embed, rate, autoplay e normalize. O parâmetro data pode ser um array numpy, uma lista de amostras, uma string representando um nome de arquivo ou URL, ou dados PCM brutos. O parâmetro filename é usado para especificar um arquivo local de onde carregar os dados de áudio, e o parâmetro url é usado para especificar um URL para baixar os dados de áudio. O parâmetro embed determina se os dados de áudio devem ser incorporados usando um URI de dados ou referenciados da fonte original. O parâmetro rate especifica a taxa de amostragem dos dados de áudio. O parâmetro autoplay determina se o áudio deve começar a tocar automaticamente. O parâmetro normalize especifica se os dados de áudio devem ser normalizados (reajustados) para o alcance máximo possível. A classe Audio também fornece métodos como reload para recarregar os dados de áudio a partir de um arquivo ou URL, e atributos como src_attr, autoplay_attr e element_id_attr para recuperar os atributos correspondentes para o elemento de áudio em HTML.\n",
    "\n",
    "No geral, essas funções e classes são usadas para transcrever áudio para texto, gerar respostas em áudio de um chatbot e exibir e reproduzir áudio no ambiente do Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Aviso Legal**:  \nEste documento foi traduzido utilizando o serviço de tradução por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precisão, esteja ciente de que traduções automatizadas podem conter erros ou imprecisões. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informações críticas, recomenda-se a tradução profissional realizada por humanos. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações equivocadas decorrentes do uso desta tradução.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T19:56:58+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "br"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}