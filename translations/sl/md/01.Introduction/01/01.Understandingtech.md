<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:33:59+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "sl"
}
-->
# Key technologies mentioned include

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - low-level API za hardware-accelerated machine learning ki je zgrajen na DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - paralelna računalniška platforma in API model, ki ga je razvil Nvidia, omogoča splošno procesiranje na grafičnih procesorjih (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - odprt format za predstavitev modelov strojnega učenja, ki omogoča interoperabilnost med različnimi ML okviri.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - format za predstavitev in posodabljanje modelov strojnega učenja, posebej uporaben za manjše jezikovne modele, ki lahko učinkovito delujejo na CPU-jih s 4-8bitno kvantizacijo.

## DirectML

DirectML je nizkonivojski API, ki omogoča hardware pospešeno strojno učenje. Zgrajen je na DirectX 12 za uporabo GPU pospeška in ni vezan na določenega proizvajalca, kar pomeni, da ni potrebno spreminjati kode za delovanje na različnih GPU-jih. Primarno se uporablja za treniranje modelov in inferenco na GPU-jih.

Glede podpore strojne opreme je DirectML zasnovan za širok nabor GPU-jev, vključno z AMD integriranimi in diskretnimi GPU-ji, Intel integriranimi GPU-ji in NVIDIA diskretnimi GPU-ji. Je del Windows AI platforme in je podprt na Windows 10 in 11, kar omogoča treniranje in inferenco modelov na katerikoli Windows napravi.

Pri DirectML so bile uvedene posodobitve in nove možnosti, kot je podpora do 150 ONNX operatorjev in uporaba v ONNX runtime ter WinML. Podprt je s strani glavnih Integrated Hardware Vendors (IHVs), ki implementirajo različne metakomande.

## CUDA

CUDA, kar pomeni Compute Unified Device Architecture, je paralelna računalniška platforma in API model, ki ga je ustvaril Nvidia. Omogoča razvijalcem programske opreme, da uporabijo CUDA-zmožen grafični procesor (GPU) za splošno procesiranje – pristop, imenovan GPGPU (General-Purpose computing on Graphics Processing Units). CUDA je ključni dejavnik za Nvidia GPU pospešek in se široko uporablja na različnih področjih, vključno s strojno učenjem, znanstvenim računalništvom in video obdelavo.

Podpora strojne opreme za CUDA je specifična za Nvidia GPU-je, saj gre za lastniško tehnologijo, ki jo je razvila Nvidia. Vsaka arhitektura podpira določene različice CUDA orodij, ki nudijo potrebne knjižnice in orodja za razvoj in zagon CUDA aplikacij.

## ONNX

ONNX (Open Neural Network Exchange) je odprt format, zasnovan za predstavitev modelov strojnega učenja. Ponuja definicijo razširljivega modela računalniškega grafa, kot tudi definicije vgrajenih operatorjev in standardnih podatkovnih tipov. ONNX razvijalcem omogoča prenos modelov med različnimi ML okvirji, kar olajša interoperabilnost in poenostavi ustvarjanje ter uvajanje AI aplikacij.

Phi3 mini lahko teče z ONNX Runtime na CPU in GPU na različnih napravah, vključno s strežniškimi platformami, Windows, Linux in Mac namizji ter mobilnimi CPU-ji.
Optimizirane konfiguracije, ki smo jih dodali, so

- ONNX modeli za int4 DML: kvantizirani na int4 preko AWQ
- ONNX model za fp16 CUDA
- ONNX model za int4 CUDA: kvantiziran na int4 preko RTN
- ONNX model za int4 CPU in Mobile: kvantiziran na int4 preko RTN

## Llama.cpp

Llama.cpp je odprtokodna programska knjižnica, napisana v C++. Izvaja inferenco na različnih velikih jezikovnih modelih (LLM), vključno z Llama. Razvita skupaj z ggml knjižnico (splošno uporabna tenzorska knjižnica), llama.cpp cilja na hitrejšo inferenco in manjšo porabo pomnilnika v primerjavi z originalno Python implementacijo. Podpira optimizacijo strojne opreme, kvantizacijo in nudi preprost API ter primere. Če vas zanima učinkovita inferenca LLM, je llama.cpp vreden ogleda, saj lahko Phi3 poganja Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) je format za predstavitev in posodabljanje modelov strojnega učenja. Posebej je uporaben za manjše jezikovne modele (SLM), ki lahko učinkovito delujejo na CPU-jih s 4-8bitno kvantizacijo. GGUF je koristen za hitro prototipiranje in zagon modelov na robnih napravah ali v serijskih opravilih, kot so CI/CD pipeline-i.

**Opozorilo**:  
Ta dokument je bil preveden z uporabo storitve za avtomatski prevod [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas opozarjamo, da lahko avtomatizirani prevodi vsebujejo napake ali netočnosti. Izvirni dokument v izvirnem jeziku velja za avtoritativni vir. Za pomembne informacije priporočamo strokovni človeški prevod. Nismo odgovorni za morebitna nesporazume ali napačne interpretacije, ki izhajajo iz uporabe tega prevoda.