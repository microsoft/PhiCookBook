<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:48:46+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "sl"
}
-->
# Ključne omenjene tehnologije vključujejo

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - nizkonivojski API za strojno pospešeno strojno učenje, zgrajen na osnovi DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - platforma za vzporedno računalništvo in model programskega vmesnika (API), ki ga je razvil Nvidia, omogoča splošno procesiranje na grafičnih procesnih enotah (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - odprt format, zasnovan za predstavitev modelov strojnega učenja, ki omogoča interoperabilnost med različnimi ML okviri.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - format, uporabljen za predstavitev in posodabljanje modelov strojnega učenja, še posebej uporaben za manjše jezikovne modele, ki lahko učinkovito delujejo na CPU-jih s 4-8-bitno kvantizacijo.

## DirectML

DirectML je nizkonivojski API, ki omogoča strojno pospešeno strojno učenje. Zgrajen je na osnovi DirectX 12 za izkoriščanje pospeševanja z GPU-jem in je neodvisen od proizvajalca, kar pomeni, da ni potrebnih sprememb kode za delovanje na različnih GPU-jih. Uporablja se predvsem za usposabljanje modelov in izvajanje inferenc na GPU-jih.

Kar zadeva podporo strojne opreme, je DirectML zasnovan za delo z različnimi GPU-ji, vključno z AMD integriranimi in diskretnimi GPU-ji, Intel integriranimi GPU-ji ter NVIDIA diskretnimi GPU-ji. Je del Windows AI Platform in je podprt na Windows 10 in 11, kar omogoča usposabljanje modelov in inferenco na katerikoli napravi z Windows.

Pri DirectML so bile izvedene posodobitve in priložnosti, kot je podpora do 150 ONNX operatorjev, uporablja pa ga tako ONNX runtime kot WinML. Podprt je s strani glavnih proizvajalcev strojne opreme (IHVs), ki implementirajo različne metakomande.

## CUDA

CUDA, kar pomeni Compute Unified Device Architecture, je platforma za vzporedno računalništvo in model programskega vmesnika (API), ki ga je ustvaril Nvidia. Omogoča razvijalcem programske opreme uporabo CUDA-zmožnega grafičnega procesorja (GPU) za splošno procesiranje – pristop, imenovan GPGPU (General-Purpose computing on Graphics Processing Units). CUDA je ključni dejavnik pospeševanja Nvidia GPU-jev in se široko uporablja na različnih področjih, vključno s strojnim učenjem, znanstvenim računalništvom in obdelavo videa.

Podpora strojne opreme za CUDA je specifična za Nvidia GPU-je, saj gre za lastniško tehnologijo, ki jo je razvila Nvidia. Vsaka arhitektura podpira določene različice CUDA orodij, ki zagotavljajo potrebne knjižnice in orodja za razvijalce za izdelavo in zagon CUDA aplikacij.

## ONNX

ONNX (Open Neural Network Exchange) je odprt format, zasnovan za predstavitev modelov strojnega učenja. Ponuja definicijo razširljivega modela računske grafike, kot tudi definicije vgrajenih operatorjev in standardnih podatkovnih tipov. ONNX omogoča razvijalcem prenos modelov med različnimi ML okviri, kar omogoča interoperabilnost in olajša ustvarjanje ter uvajanje AI aplikacij.

Phi3 mini lahko deluje z ONNX Runtime na CPU in GPU na različnih napravah, vključno s strežniškimi platformami, Windows, Linux in Mac namizji ter mobilnimi CPU-ji.  
Optimizirane konfiguracije, ki smo jih dodali, so:

- ONNX modeli za int4 DML: kvantizirani v int4 preko AWQ  
- ONNX model za fp16 CUDA  
- ONNX model za int4 CUDA: kvantiziran v int4 preko RTN  
- ONNX model za int4 CPU in Mobile: kvantiziran v int4 preko RTN  

## Llama.cpp

Llama.cpp je odprtokodna programska knjižnica, napisana v C++. Izvaja inferenco na različnih velikih jezikovnih modelih (LLM), vključno z Llama. Razvita je skupaj z ggml knjižnico (splošna knjižnica za tenzorje) in si prizadeva zagotoviti hitrejšo inferenco ter manjšo porabo pomnilnika v primerjavi z izvirno Python implementacijo. Podpira strojno optimizacijo, kvantizacijo ter ponuja preprost API in primere. Če vas zanima učinkovita inferenca LLM, je llama.cpp vredna raziskave, saj Phi3 lahko poganja Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) je format, uporabljen za predstavitev in posodabljanje modelov strojnega učenja. Še posebej je uporaben za manjše jezikovne modele (SLM), ki lahko učinkovito delujejo na CPU-jih s 4-8-bitno kvantizacijo. GGUF je koristen za hitro prototipiranje in zagon modelov na robnih napravah ali v serijskih opravilih, kot so CI/CD pipeline-i.

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za avtomatski prevod AI [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas opozarjamo, da lahko avtomatizirani prevodi vsebujejo napake ali netočnosti. Izvirni dokument v njegovem izvirnem jeziku velja za avtoritativni vir. Za pomembne informacije priporočamo strokovni človeški prevod. Za morebitna nesporazume ali napačne interpretacije, ki izhajajo iz uporabe tega prevoda, ne odgovarjamo.