<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c8273672cc57df2be675407a1383aaf0",
  "translation_date": "2025-07-16T17:54:24+00:00",
  "source_file": "md/01.Introduction/01/01.AISafety.md",
  "language_code": "sl"
}
-->
# Varnost AI za modele Phi  
Družina modelov Phi je bila razvita v skladu z [Microsoft Responsible AI Standard](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFl), kar je podjetniški nabor zahtev, ki temeljijo na naslednjih šestih načelih: odgovornost, preglednost, pravičnost, zanesljivost in varnost, zasebnost in varnost ter vključevanje, ki sestavljajo [Microsoftova načela odgovorne umetne inteligence](https://www.microsoft.com/ai/responsible-ai).

Tako kot pri prejšnjih modelih Phi smo uporabili večplastno ocenjevanje varnosti in pristop varnostnega usposabljanja po treningu, pri čemer smo sprejeli dodatne ukrepe za upoštevanje večjezičnih zmogljivosti te izdaje. Naš pristop k varnostnemu usposabljanju in ocenjevanju, vključno s testiranjem v več jezikih in kategorijah tveganj, je opisan v [Phi Safety Post-Training Paper](https://arxiv.org/abs/2407.13833). Čeprav modeli Phi koristijo temu pristopu, naj razvijalci uporabljajo najboljše prakse odgovorne umetne inteligence, vključno z mapiranjem, merjenjem in ublažitvijo tveganj, povezanih z njihovim specifičnim primerom uporabe ter kulturnim in jezikovnim kontekstom.

## Najboljše prakse

Tako kot drugi modeli lahko tudi družina modelov Phi v določenih primerih deluje na načine, ki so nepravični, nezanesljivi ali žaljivi.

Nekatere omejitve vedenja SLM in LLM, na katere morate biti pozorni, vključujejo:

- **Kakovost storitve:** Modeli Phi so usposobljeni predvsem na angleškem besedilu. Jeziki, ki niso angleščina, bodo imeli slabšo zmogljivost. Različice angleščine z manjšo zastopanostjo v učnih podatkih lahko doživijo slabšo zmogljivost kot standardna ameriška angleščina.  
- **Predstavitev škode in ohranjanje stereotipov:** Ti modeli lahko prekomerno ali premalo predstavljajo določene skupine ljudi, izbrišejo predstavitev nekaterih skupin ali okrepijo ponižujoče ali negativne stereotipe. Kljub varnostnemu usposabljanju po treningu so te omejitve lahko še vedno prisotne zaradi različnih stopenj zastopanosti različnih skupin ali pogostosti primerov negativnih stereotipov v učnih podatkih, ki odražajo vzorce iz resničnega sveta in družbene pristranskosti.  
- **Neprimerno ali žaljivo vsebino:** Ti modeli lahko ustvarijo tudi druge vrste neprimerne ali žaljive vsebine, zaradi česar je njihova uporaba v občutljivih kontekstih brez dodatnih ukrepov, prilagojenih specifični uporabi, neprimerna.  
- **Zanesljivost informacij:** Jezikovni modeli lahko ustvarijo nesmiselno vsebino ali izmišljajo informacije, ki se morda zdijo verjetne, a so netočne ali zastarele.  
- **Omejen obseg za kodo:** Večina učnih podatkov Phi-3 temelji na Pythonu in uporablja pogoste pakete, kot so "typing, math, random, collections, datetime, itertools". Če model generira Python skripte, ki uporabljajo druge pakete ali skripte v drugih jezikih, močno priporočamo, da uporabniki ročno preverijo vse uporabe API-jev.

Razvijalci naj uporabljajo najboljše prakse odgovorne umetne inteligence in so odgovorni za zagotavljanje, da specifičen primer uporabe spoštuje veljavne zakone in predpise (npr. zasebnost, trgovina itd.).

## Premisleki o odgovorni umetni inteligenci

Tako kot drugi jezikovni modeli lahko tudi modeli iz serije Phi v določenih primerih delujejo na načine, ki so nepravični, nezanesljivi ali žaljivi. Nekatere omejitve, na katere je treba biti pozoren, vključujejo:

**Kakovost storitve:** Modeli Phi so usposobljeni predvsem na angleškem besedilu. Jeziki, ki niso angleščina, bodo imeli slabšo zmogljivost. Različice angleščine z manjšo zastopanostjo v učnih podatkih lahko doživijo slabšo zmogljivost kot standardna ameriška angleščina.

**Predstavitev škode in ohranjanje stereotipov:** Ti modeli lahko prekomerno ali premalo predstavljajo določene skupine ljudi, izbrišejo predstavitev nekaterih skupin ali okrepijo ponižujoče ali negativne stereotipe. Kljub varnostnemu usposabljanju po treningu so te omejitve lahko še vedno prisotne zaradi različnih stopenj zastopanosti različnih skupin ali pogostosti primerov negativnih stereotipov v učnih podatkih, ki odražajo vzorce iz resničnega sveta in družbene pristranskosti.

**Neprimerno ali žaljivo vsebino:** Ti modeli lahko ustvarijo tudi druge vrste neprimerne ali žaljive vsebine, zaradi česar je njihova uporaba v občutljivih kontekstih brez dodatnih ukrepov, prilagojenih specifični uporabi, neprimerna.  
Zanesljivost informacij: Jezikovni modeli lahko ustvarijo nesmiselno vsebino ali izmišljajo informacije, ki se morda zdijo verjetne, a so netočne ali zastarele.

**Omejen obseg za kodo:** Večina učnih podatkov Phi-3 temelji na Pythonu in uporablja pogoste pakete, kot so "typing, math, random, collections, datetime, itertools". Če model generira Python skripte, ki uporabljajo druge pakete ali skripte v drugih jezikih, močno priporočamo, da uporabniki ročno preverijo vse uporabe API-jev.

Razvijalci naj uporabljajo najboljše prakse odgovorne umetne inteligence in so odgovorni za zagotavljanje, da specifičen primer uporabe spoštuje veljavne zakone in predpise (npr. zasebnost, trgovina itd.). Pomembna področja za razmislek vključujejo:

**Dodeljevanje:** Modeli morda niso primerni za scenarije, ki bi lahko imeli pomemben vpliv na pravni status ali dodeljevanje virov ali življenjskih priložnosti (npr. stanovanje, zaposlitev, kredit itd.) brez dodatnih ocen in tehnik odpravljanja pristranskosti.

**Visoko tvegani scenariji:** Razvijalci naj ocenijo primernost uporabe modelov v visokotvegani scenarijih, kjer bi nepravični, nezanesljivi ali žaljivi izhodi lahko povzročili velike stroške ali škodo. To vključuje svetovanje na občutljivih ali strokovnih področjih, kjer sta natančnost in zanesljivost ključni (npr. pravni ali zdravstveni nasveti). Dodatni varnostni ukrepi naj se izvajajo na ravni aplikacije glede na kontekst uporabe.

**Dezinformacije:** Modeli lahko ustvarijo netočne informacije. Razvijalci naj sledijo najboljšim praksam preglednosti in obvestijo končne uporabnike, da komunicirajo z AI sistemom. Na ravni aplikacije lahko razvijalci zgradijo mehanizme povratnih informacij in procese, ki temeljijo na specifičnih, kontekstualnih informacijah za primer uporabe, tehniko, znano kot Retrieval Augmented Generation (RAG).

**Generiranje škodljive vsebine:** Razvijalci naj ocenijo izhode glede na njihov kontekst in uporabijo razpoložljive varnostne klasifikatorje ali prilagojene rešitve, primerne za njihov primer uporabe.

**Zloraba:** Možne so tudi druge oblike zlorabe, kot so prevara, neželena pošta ali ustvarjanje zlonamerne programske opreme, zato naj razvijalci zagotovijo, da njihove aplikacije ne kršijo veljavnih zakonov in predpisov.

### Dodatno usposabljanje in varnost AI vsebin

Po dodatnem usposabljanju modela močno priporočamo uporabo ukrepov [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) za spremljanje vsebine, ki jo ustvarjajo modeli, prepoznavanje in blokiranje morebitnih tveganj, groženj in težav s kakovostjo.

![Phi3AISafety](../../../../../translated_images/01.phi3aisafety.c0d7fc42f5a5c40507c5e8be556615b8377a63b8764865d057d4faac3757a478.sl.png)

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) podpira tako besedilno kot slikovno vsebino. Lahko se izvaja v oblaku, v odklopljenih vsebnikih in na robnih/vgrajenih napravah.

## Pregled Azure AI Content Safety

Azure AI Content Safety ni univerzalna rešitev; lahko se prilagodi, da ustreza specifičnim politikam podjetij. Poleg tega njegovi večjezični modeli omogočajo razumevanje več jezikov hkrati.

![AIContentSafety](../../../../../translated_images/01.AIcontentsafety.a288819b8ce8da1a56cf708aff010a541799d002ae7ae84bb819b19ab8950591.sl.png)

- **Azure AI Content Safety**  
- **Microsoft Developer**  
- **5 videoposnetkov**

Storitev Azure AI Content Safety zaznava škodljivo vsebino, ki jo ustvarijo uporabniki in AI, v aplikacijah in storitvah. Vključuje API-je za besedilo in slike, ki omogočajo zaznavanje škodljive ali neprimerne vsebine.

[AI Content Safety Playlist](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo AI prevajalske storitve [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas opozarjamo, da avtomatizirani prevodi lahko vsebujejo napake ali netočnosti. Izvirni dokument v njegovem izvirnem jeziku velja za avtoritativni vir. Za ključne informacije priporočamo strokovni človeški prevod. Za morebitna nesporazume ali napačne interpretacije, ki izhajajo iz uporabe tega prevoda, ne odgovarjamo.