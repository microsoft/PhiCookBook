# Začni z Phi-3 lokalno

Ta vodič ti bo pomagal nastaviti lokalno okolje za zagon modela Phi-3 z uporabo Ollama. Model lahko zaženeš na več načinov, vključno z GitHub Codespaces, VS Code Dev Containers ali v svojem lokalnem okolju.

## Nastavitev okolja

### GitHub Codespaces

Ta predlogo lahko zaženeš virtualno z uporabo GitHub Codespaces. Gumb bo odprl spletno različico VS Code v tvojem brskalniku:

1. Odpri predlogo (to lahko traja nekaj minut):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Odpri terminal

### VS Code Dev Containers

⚠️ Ta možnost deluje le, če ima tvoj Docker Desktop dodeljenih vsaj 16 GB RAM-a. Če imaš manj kot 16 GB RAM-a, lahko poskusiš [GitHub Codespaces možnost](../../../../../md/01.Introduction/01) ali [nastaviš lokalno](../../../../../md/01.Introduction/01).

Sorodna možnost so VS Code Dev Containers, ki bodo odprli projekt v tvojem lokalnem VS Code z uporabo [Dev Containers razširitve](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Zaženi Docker Desktop (namesti ga, če še ni nameščen)
2. Odpri projekt:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. V odprtem oknu VS Code, ko se prikažejo datoteke projekta (to lahko traja nekaj minut), odpri terminal.
4. Nadaljuj z [koraki za namestitev](../../../../../md/01.Introduction/01)

### Lokalno okolje

1. Prepričaj se, da so nameščena naslednja orodja:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Preizkusi model

1. Prosi Ollama, naj prenese in zažene model phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Prenos modela bo trajal nekaj minut.

2. Ko v izhodu vidiš "success", lahko modelu pošlješ sporočilo iz ukazne vrstice.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Po nekaj sekundah bi moral prejeti odgovor modela v obliki toka.

4. Če želiš spoznati različne tehnike uporabe jezikovnih modelov, odpri Python zvezek [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) in zaženi vsako celico. Če si uporabil model, ki ni 'phi3:mini', spremeni `MODEL_NAME` v prvi celici.

5. Za pogovor z modelom phi3:mini iz Pythona odpri Python datoteko [chat.py](../../../../../code/01.Introduce/chat.py) in jo zaženi. Po potrebi lahko spremeniš `MODEL_NAME` na vrhu datoteke, prav tako lahko prilagodiš sistemsko sporočilo ali dodaš nekaj primerov za učenje.

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo AI prevajalske storitve [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas opozarjamo, da avtomatizirani prevodi lahko vsebujejo napake ali netočnosti. Izvirni dokument v njegovem izvirnem jeziku velja za avtoritativni vir. Za ključne informacije priporočamo strokovni človeški prevod. Nismo odgovorni za morebitna nesporazume ali napačne interpretacije, ki izhajajo iz uporabe tega prevoda.