<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-09T07:20:48+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "sl"
}
-->
# Get started with Phi-3 locally

This guide will help you set up your local environment to run the Phi-3 model using Ollama. You can run the model in a few different ways, including using GitHub Codespaces, VS Code Dev Containers, or your local environment.

## Environment setup

### GitHub Codespaces

You can run this template virtually by using GitHub Codespaces. The button will open a web-based VS Code instance in your browser:

1. Open the template (this may take several minutes):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Open a terminal window

### VS Code Dev Containers

⚠️ This option will only work if your Docker Desktop is allocated at least 16 GB of RAM. If you have less than 16 GB of RAM, you can try the [GitHub Codespaces option](../../../../../md/01.Introduction/01) or [set it up locally](../../../../../md/01.Introduction/01).

A related option is VS Code Dev Containers, which will open the project in your local VS Code using the [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Start Docker Desktop (install it if not already installed)  
2. Open the project:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. In the VS Code window that opens, once the project files show up (this may take several minutes), open a terminal window.  
4. Continue with the [deployment steps](../../../../../md/01.Introduction/01)

### Local Environment

1. Make sure the following tools are installed:

    * [Ollama](https://ollama.com/)  
    * [Python 3.10+](https://www.python.org/downloads/)  
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Test the model

1. Ask Ollama to download and run the phi3:mini model:

    ```shell
    ollama run phi3:mini
    ```

    This will take a few minutes to download the model.

2. Once you see "success" in the output, you can send a message to that model from the prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. After a few seconds, you should see a response streaming from the model.

4. To explore different techniques used with language models, open the Python notebook [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) and run each cell. If you used a model other than 'phi3:mini', update the `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` at the top of the file as needed. You can also modify the system message or add few-shot examples if you want.

**Izjava o omejitvi odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za prevajanje z umetno inteligenco [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas prosimo, da upoštevate, da avtomatizirani prevodi lahko vsebujejo napake ali netočnosti. Izvirni dokument v njegovem maternem jeziku velja za avtoritativni vir. Za kritične informacije priporočamo strokovni človeški prevod. Nismo odgovorni za kakršnekoli nesporazume ali napačne razlage, ki izhajajo iz uporabe tega prevoda.