<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:33:28+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "hr"
}
-->
# Ključne tehnologije koje su spomenute uključuju

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - niskorazinski API za hardverski ubrzano strojno učenje izgrađen na vrhu DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - paralelna računalna platforma i model API-ja koji je razvio Nvidia, omogućujući opću obradu na grafičkim procesorima (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - otvoreni format dizajniran za predstavljanje modela strojnog učenja koji omogućuje interoperabilnost između različitih ML okvira.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - format koji se koristi za predstavljanje i ažuriranje modela strojnog učenja, posebno koristan za manje jezične modele koji mogu učinkovito raditi na CPU-ima s 4-8 bitnom kvantizacijom.

## DirectML

DirectML je niskorazinski API koji omogućuje hardverski ubrzano strojno učenje. Izgrađen je na vrhu DirectX 12 kako bi iskoristio ubrzanje GPU-a i nije vezan za određenog proizvođača, što znači da ne zahtijeva promjene koda da bi radio na različitim GPU-ima. Prvenstveno se koristi za treniranje modela i izvođenje inferencija na GPU-ima.

Što se tiče hardverske podrške, DirectML je dizajniran za rad s širokim spektrom GPU-ova, uključujući AMD integrirane i diskretne GPU-ove, Intel integrirane GPU-ove i NVIDIA diskretne GPU-ove. Dio je Windows AI Platforme i podržava se na Windows 10 i 11, omogućujući treniranje modela i izvođenje inferencija na bilo kojem Windows uređaju.

Bilo je ažuriranja i prilika vezanih uz DirectML, poput podrške za do 150 ONNX operatora i korištenja od strane ONNX runtime-a i WinML-a. Podržavaju ga glavni integrirani proizvođači hardvera (IHVs), svaki implementirajući različite metanaredbe.

## CUDA

CUDA, što znači Compute Unified Device Architecture, paralelna je računalna platforma i model API-ja koji je razvio Nvidia. Omogućuje programerima softvera da koriste GPU s podrškom za CUDA za opću obradu – pristup poznat kao GPGPU (General-Purpose computing on Graphics Processing Units). CUDA je ključni pokretač Nvidia GPU ubrzanja i široko se koristi u raznim područjima, uključujući strojno učenje, znanstvene izračune i obradu videozapisa.

Hardverska podrška za CUDA je specifična za Nvidia GPU-ove, jer je to vlasnička tehnologija koju je razvio Nvidia. Svaka arhitektura podržava određene verzije CUDA toolkit-a, koji pruža potrebne biblioteke i alate za razvoj i izvođenje CUDA aplikacija.

## ONNX

ONNX (Open Neural Network Exchange) je otvoreni format dizajniran za predstavljanje modela strojnog učenja. Pruža definiciju proširivog modela računalnog grafa, kao i definicije ugrađenih operatora i standardnih tipova podataka. ONNX omogućuje programerima da prenose modele između različitih ML okvira, što omogućuje interoperabilnost i olakšava stvaranje i implementaciju AI aplikacija.

Phi3 mini može raditi s ONNX Runtime-om na CPU-u i GPU-u na različitim uređajima, uključujući serverske platforme, Windows, Linux i Mac desktop računala te mobilne CPU-e.
Optimizirane konfiguracije koje smo dodali su

- ONNX modeli za int4 DML: kvantizirani u int4 putem AWQ
- ONNX model za fp16 CUDA
- ONNX model za int4 CUDA: kvantiziran u int4 putem RTN
- ONNX model za int4 CPU i Mobile: kvantiziran u int4 putem RTN

## Llama.cpp

Llama.cpp je open-source softverska biblioteka napisana u C++. Izvodi inferenciju na različitim velikim jezičnim modelima (LLM), uključujući Llama. Razvijena zajedno s ggml bibliotekom (općom tensor bibliotekom), llama.cpp ima za cilj pružiti bržu inferenciju i manju potrošnju memorije u usporedbi s izvornom Python implementacijom. Podržava hardversku optimizaciju, kvantizaciju te nudi jednostavan API i primjere. Ako vas zanima učinkovita inferencija LLM modela, llama.cpp vrijedi istražiti jer Phi3 može pokretati Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) je format koji se koristi za predstavljanje i ažuriranje modela strojnog učenja. Posebno je koristan za manje jezične modele (SLM) koji mogu učinkovito raditi na CPU-ima s 4-8 bitnom kvantizacijom. GGUF je koristan za brzo prototipiranje i pokretanje modela na edge uređajima ili u batch zadacima poput CI/CD pipeline-a.

**Odricanje od odgovornosti**:  
Ovaj je dokument preveden pomoću AI usluge za prijevod [Co-op Translator](https://github.com/Azure/co-op-translator). Iako težimo točnosti, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za kritične informacije preporučuje se profesionalni ljudski prijevod. Ne snosimo odgovornost za bilo kakva nesporazuma ili pogrešna tumačenja koja proizlaze iz korištenja ovog prijevoda.