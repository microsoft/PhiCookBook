<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:48:35+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "hr"
}
-->
# Ključne tehnologije koje se spominju uključuju

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - niskorazinska API za hardverski ubrzano strojno učenje izgrađena na DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - platforma za paralelno računarstvo i model API-ja razvijen od strane Nvidije, koja omogućuje opću obradu na grafičkim procesorima (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - otvoreni format dizajniran za predstavljanje modela strojnog učenja koji omogućuje interoperabilnost između različitih ML okvira.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - format koji se koristi za predstavljanje i ažuriranje modela strojnog učenja, posebno koristan za manje jezične modele koji mogu učinkovito raditi na CPU-ima s 4-8bitnom kvantizacijom.

## DirectML

DirectML je niskorazinska API koja omogućuje hardverski ubrzano strojno učenje. Izgrađena je na DirectX 12 kako bi iskoristila ubrzanje GPU-a i neovisna je o proizvođaču, što znači da ne zahtijeva promjene u kodu za rad na različitim GPU proizvođačima. Primarno se koristi za treniranje modela i izvođenje inferencije na GPU-ima.

Što se tiče hardverske podrške, DirectML je dizajniran za rad s širokim spektrom GPU-a, uključujući AMD integrirane i diskretne GPU-e, Intel integrirane GPU-e i NVIDIA diskretne GPU-e. Dio je Windows AI Platforme i podržan je na Windows 10 i 11, omogućujući treniranje modela i inferenciju na bilo kojem Windows uređaju.

Bilo je ažuriranja i prilika vezanih uz DirectML, poput podrške za do 150 ONNX operatora i korištenja od strane ONNX runtime-a i WinML-a. Podržavaju ga glavni proizvođači hardvera (IHV-ovi), svaki implementirajući različite metanaredbe.

## CUDA

CUDA, što znači Compute Unified Device Architecture, je platforma za paralelno računarstvo i model API-ja koji je razvio Nvidia. Omogućuje programerima da koriste GPU s podrškom za CUDA za opću obradu – pristup poznat kao GPGPU (General-Purpose computing on Graphics Processing Units). CUDA je ključni pokretač Nvidia GPU ubrzanja i široko se koristi u raznim područjima, uključujući strojno učenje, znanstveno računarstvo i obradu videa.

Hardverska podrška za CUDA specifična je za Nvidia GPU-e, budući da je to vlasnička tehnologija razvijena od strane Nvidije. Svaka arhitektura podržava određene verzije CUDA toolkita, koji pruža potrebne biblioteke i alate za razvoj i pokretanje CUDA aplikacija.

## ONNX

ONNX (Open Neural Network Exchange) je otvoreni format dizajniran za predstavljanje modela strojnog učenja. Pruža definiciju proširivog modela računalnog grafa, kao i definicije ugrađenih operatora i standardnih tipova podataka. ONNX omogućuje programerima da prenose modele između različitih ML okvira, što omogućuje interoperabilnost i olakšava stvaranje i implementaciju AI aplikacija.

Phi3 mini može raditi s ONNX Runtime-om na CPU-u i GPU-u na različitim uređajima, uključujući serverske platforme, Windows, Linux i Mac desktop računala te mobilne CPU-e.
Optimizirane konfiguracije koje smo dodali su

- ONNX modeli za int4 DML: kvantizirani u int4 putem AWQ
- ONNX model za fp16 CUDA
- ONNX model za int4 CUDA: kvantiziran u int4 putem RTN
- ONNX model za int4 CPU i Mobile: kvantiziran u int4 putem RTN

## Llama.cpp

Llama.cpp je open-source softverska biblioteka napisana u C++. Izvodi inferenciju na različitim velikim jezičnim modelima (LLM), uključujući Llama. Razvijena zajedno s ggml bibliotekom (opća tensor biblioteka), llama.cpp ima za cilj pružiti bržu inferenciju i manju potrošnju memorije u usporedbi s izvornom Python implementacijom. Podržava hardversku optimizaciju, kvantizaciju i nudi jednostavan API i primjere. Ako vas zanima učinkovita inferencija LLM modela, llama.cpp vrijedi istražiti jer Phi3 može pokretati Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) je format koji se koristi za predstavljanje i ažuriranje modela strojnog učenja. Posebno je koristan za manje jezične modele (SLM) koji mogu učinkovito raditi na CPU-ima s 4-8bitnom kvantizacijom. GGUF je koristan za brzo prototipiranje i pokretanje modela na edge uređajima ili u batch poslovima poput CI/CD pipeline-a.

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden korištenjem AI usluge za prevođenje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo postići točnost, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za kritične informacije preporučuje se profesionalni ljudski prijevod. Ne snosimo odgovornost za bilo kakva nesporazume ili pogrešna tumačenja koja proizlaze iz korištenja ovog prijevoda.