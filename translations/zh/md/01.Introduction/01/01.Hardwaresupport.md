<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8cdc17ce0f10535da30b53d23fe1a795",
  "translation_date": "2025-05-07T14:56:51+00:00",
  "source_file": "md/01.Introduction/01/01.Hardwaresupport.md",
  "language_code": "zh"
}
-->
# Phi 硬件支持

Microsoft Phi 已针对 ONNX Runtime 进行了优化，并支持 Windows DirectML。它在各种硬件类型上表现良好，包括 GPU、CPU，甚至移动设备。

## 设备硬件  
具体支持的硬件包括：

- GPU 型号：RTX 4090（DirectML）
- GPU 型号：1 个 A100 80GB（CUDA）
- CPU 型号：Standard F64s v2（64 个 vCPU，128 GiB 内存）

## 移动设备型号

- 安卓 - Samsung Galaxy S21
- Apple iPhone 14 或更高版本，搭载 A16/A17 处理器

## Phi 硬件规格

- 最低配置要求。
- Windows：支持 DirectX 12 的 GPU，且总内存至少 4GB

CUDA：计算能力 >= 7.02 的 NVIDIA GPU

![HardwareSupport](../../../../../translated_images/01.phihardware.5d51b2377cba18afc6949074542f290c56bb278dac3f4f86302aca6d80fffeb9.zh.png)

## 在多 GPU 上运行 onnxruntime

目前可用的 Phi ONNX 模型仅支持单 GPU。虽然理论上可以支持 Phi 模型的多 GPU，但使用 2 个 GPU 的 ORT 并不保证其吞吐量会比运行 2 个 ORT 实例更高。请参阅 [ONNX Runtime](https://onnxruntime.ai/) 以获取最新信息。

在 [Build 2024 the GenAI ONNX Team](https://youtu.be/WLW4SE8M9i8?si=EtG04UwDvcjunyfC) 上宣布，他们已为 Phi 模型启用了多实例支持，而非多 GPU。

目前，这允许你通过设置 CUDA_VISIBLE_DEVICES 环境变量运行一个 onnxruntime 或 onnxruntime-genai 实例，如下所示。

```Python
CUDA_VISIBLE_DEVICES=0 python infer.py
CUDA_VISIBLE_DEVICES=1 python infer.py
```

欢迎在 [Azure AI Foundry](https://ai.azure.com) 中进一步探索 Phi。

**免责声明**：  
本文件使用 AI 翻译服务 [Co-op Translator](https://github.com/Azure/co-op-translator) 进行翻译。虽然我们力求准确，但请注意，自动翻译可能包含错误或不准确之处。原始文件的母语版本应被视为权威来源。对于重要信息，建议采用专业人工翻译。对于因使用本翻译而产生的任何误解或误释，我们概不负责。