<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:41:17+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "zh"
}
-->
# 主要提到的关键技术包括

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - 基于 DirectX 12 的硬件加速机器学习低级 API。
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - 由 Nvidia 开发的并行计算平台和应用程序接口（API）模型，支持在图形处理单元（GPU）上进行通用计算。
3. [ONNX](https://onnx.ai/)（开放神经网络交换格式）- 一种用于表示机器学习模型的开放格式，支持不同机器学习框架之间的互操作性。
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)（通用图更新格式）- 用于表示和更新机器学习模型的格式，特别适合在 CPU 上通过 4-8 位量化高效运行的小型语言模型。

## DirectML

DirectML 是一个低级 API，支持硬件加速的机器学习。它基于 DirectX 12 构建，利用 GPU 加速，且与厂商无关，意味着无需修改代码即可在不同 GPU 厂商的设备上运行。它主要用于 GPU 上的模型训练和推理任务。

在硬件支持方面，DirectML 设计兼容多种 GPU，包括 AMD 集成和独立 GPU、Intel 集成 GPU 以及 NVIDIA 独立 GPU。它是 Windows AI 平台的一部分，支持 Windows 10 和 11，可在任何 Windows 设备上进行模型训练和推理。

DirectML 也有一些更新和发展机会，比如支持多达 150 个 ONNX 操作符，并被 ONNX runtime 和 WinML 采用。它得到了主要集成硬件厂商（IHV）的支持，每家厂商都实现了各种元命令。

## CUDA

CUDA（Compute Unified Device Architecture）是 Nvidia 创建的并行计算平台和应用程序接口（API）模型。它允许软件开发者利用支持 CUDA 的 GPU 进行通用计算，这种方法称为 GPGPU（图形处理单元通用计算）。CUDA 是 Nvidia GPU 加速的关键技术，广泛应用于机器学习、科学计算和视频处理等领域。

CUDA 的硬件支持仅限于 Nvidia 的 GPU，因为它是 Nvidia 的专有技术。每个架构支持特定版本的 CUDA 工具包，提供开发者构建和运行 CUDA 应用所需的库和工具。

## ONNX

ONNX（开放神经网络交换格式）是一种用于表示机器学习模型的开放格式。它定义了可扩展的计算图模型，以及内置操作符和标准数据类型的定义。ONNX 使开发者能够在不同的机器学习框架之间迁移模型，实现互操作性，简化 AI 应用的创建和部署。

Phi3 mini 可以在 CPU 和 GPU 上使用 ONNX Runtime 运行，支持服务器平台、Windows、Linux、Mac 桌面以及移动 CPU。
我们添加的优化配置包括：

- 用于 int4 DML 的 ONNX 模型：通过 AWQ 量化为 int4
- 用于 fp16 CUDA 的 ONNX 模型
- 用于 int4 CUDA 的 ONNX 模型：通过 RTN 量化为 int4
- 用于 int4 CPU 和移动设备的 ONNX 模型：通过 RTN 量化为 int4

## Llama.cpp

Llama.cpp 是一个用 C++ 编写的开源软件库。它支持对多种大型语言模型（LLM）进行推理，包括 Llama。该项目与通用张量库 ggml 一起开发，旨在比原始 Python 实现提供更快的推理速度和更低的内存占用。它支持硬件优化、量化，并提供简单的 API 和示例。如果你对高效的 LLM 推理感兴趣，llama.cpp 非常值得探索，因为 Phi3 可以运行 Llama.cpp。

## GGUF

GGUF（通用图更新格式）是一种用于表示和更新机器学习模型的格式。它特别适合通过 4-8 位量化在 CPU 上高效运行的小型语言模型（SLM）。GGUF 适合快速原型开发以及在边缘设备或批处理任务（如 CI/CD 流水线）中运行模型。

**免责声明**：  
本文件使用 AI 翻译服务 [Co-op Translator](https://github.com/Azure/co-op-translator) 进行翻译。虽然我们力求准确，但请注意，自动翻译可能包含错误或不准确之处。原始文件的母语版本应被视为权威来源。对于重要信息，建议采用专业人工翻译。因使用本翻译而产生的任何误解或误释，我们概不负责。