<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "583e1ebd3884b47b43c883072eb8fa03",
  "translation_date": "2025-04-03T06:39:59+00:00",
  "source_file": "md\\01.Introduction\\01\\01.Understandingtech.md",
  "language_code": "zh"
}
-->
# 提到的关键技术包括

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - 基于 DirectX 12 构建的低级 API，用于硬件加速的机器学习。
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia 开发的并行计算平台和应用程序接口 (API) 模型，支持在图形处理单元 (GPU) 上进行通用计算。
3. [ONNX](https://onnx.ai/) (开放神经网络交换格式) - 一种开放格式，用于表示机器学习模型，提供不同 ML 框架之间的互操作性。
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (通用图更新格式) - 一种用于表示和更新机器学习模型的格式，特别适用于能够在 CPU 上高效运行的小型语言模型，支持 4-8bit 量化。

## DirectML

DirectML 是一个低级 API，支持硬件加速的机器学习。它基于 DirectX 12 构建，利用 GPU 加速，并且与硬件厂商无关，这意味着无需代码更改即可在不同 GPU 厂商之间工作。它主要用于 GPU 上的模型训练和推理工作负载。

在硬件支持方面，DirectML 设计用于支持各种 GPU，包括 AMD 集成和独立 GPU、Intel 集成 GPU 以及 NVIDIA 独立 GPU。它是 Windows AI 平台的一部分，支持 Windows 10 和 11，可以在任何 Windows 设备上进行模型训练和推理。

DirectML 有一些更新和机会，例如支持多达 150 个 ONNX 操作符，并被 ONNX runtime 和 WinML 使用。它得到了主要集成硬件厂商 (IHVs) 的支持，每个厂商都实现了不同的元命令。

## CUDA

CUDA，全称 Compute Unified Device Architecture，是由 Nvidia 创建的并行计算平台和应用程序接口 (API) 模型。它允许软件开发人员使用支持 CUDA 的图形处理单元 (GPU) 进行通用计算，这种方法称为 GPGPU（图形处理单元上的通用计算）。CUDA 是 Nvidia GPU 加速的关键推动力，广泛应用于多个领域，包括机器学习、科学计算和视频处理。

CUDA 的硬件支持专属于 Nvidia 的 GPU，因为它是由 Nvidia 开发的专有技术。每种架构支持特定版本的 CUDA 工具包，该工具包为开发人员提供必要的库和工具，以构建和运行 CUDA 应用程序。

## ONNX

ONNX（开放神经网络交换格式）是一种开放格式，用于表示机器学习模型。它定义了可扩展的计算图模型，以及内置操作符和标准数据类型的定义。ONNX 允许开发人员在不同的 ML 框架之间迁移模型，提供互操作性，并简化 AI 应用的创建和部署。

Phi3 mini 可以通过 ONNX Runtime 在 CPU 和 GPU 上运行，支持包括服务器平台、Windows、Linux 和 Mac 桌面，以及移动 CPU 等设备。我们优化的配置包括：

- 用于 int4 DML 的 ONNX 模型：通过 AWQ 量化为 int4
- 用于 fp16 CUDA 的 ONNX 模型
- 用于 int4 CUDA 的 ONNX 模型：通过 RTN 量化为 int4
- 用于 int4 CPU 和移动端的 ONNX 模型：通过 RTN 量化为 int4

## Llama.cpp

Llama.cpp 是一个用 C++ 编写的开源软件库，用于对各种大型语言模型 (LLMs)（包括 Llama）进行推理。它与 ggml 库（一个通用张量库）一起开发，旨在提供比原始 Python 实现更快的推理速度和更低的内存使用。Llama.cpp 支持硬件优化、量化，并提供简单的 API 和示例。如果您对高效的 LLM 推理感兴趣，Llama.cpp 值得探索，因为 Phi3 可以运行 Llama.cpp。

## GGUF

GGUF（通用图更新格式）是一种用于表示和更新机器学习模型的格式。它特别适用于能够在 CPU 上高效运行的小型语言模型 (SLMs)，支持 4-8bit 量化。GGUF 在快速原型开发以及在边缘设备或批处理作业（如 CI/CD 管道）中运行模型方面非常有用。

**免责声明**：  
本文档使用AI翻译服务 [Co-op Translator](https://github.com/Azure/co-op-translator) 进行翻译。尽管我们努力确保翻译的准确性，但请注意，自动翻译可能会包含错误或不准确之处。应以原始语言的文档作为权威来源。对于关键信息，建议寻求专业人工翻译。对于因使用本翻译而导致的任何误解或误读，我们不承担责任。