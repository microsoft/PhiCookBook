<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-07T14:59:19+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "zh"
}
-->
# 主要提到的关键技术包括

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - 一个基于 DirectX 12 的低级硬件加速机器学习 API。
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - 由 Nvidia 开发的并行计算平台和应用程序编程接口（API）模型，支持在图形处理单元（GPU）上进行通用计算。
3. [ONNX](https://onnx.ai/)（开放神经网络交换格式）- 一种用于表示机器学习模型的开放格式，支持不同机器学习框架之间的互操作性。
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)（通用图更新格式）- 用于表示和更新机器学习模型的格式，特别适合可在 CPU 上通过 4-8 位量化高效运行的小型语言模型。

## DirectML

DirectML 是一个低级 API，支持硬件加速的机器学习。它构建于 DirectX 12 之上，利用 GPU 加速，且与厂商无关，这意味着它无需修改代码即可在不同 GPU 厂商的设备上运行。它主要用于 GPU 上的模型训练和推理工作负载。

在硬件支持方面，DirectML 设计兼容多种 GPU，包括 AMD 的集成和独立显卡、Intel 集成显卡以及 NVIDIA 独立显卡。它是 Windows AI 平台的一部分，支持 Windows 10 和 11，允许在任何 Windows 设备上进行模型训练和推理。

DirectML 也不断更新和扩展功能，比如支持多达 150 个 ONNX 操作符，并被 ONNX runtime 和 WinML 使用。它得到了主要集成硬件供应商（IHV）的支持，每家厂商都实现了不同的元命令。

## CUDA

CUDA（Compute Unified Device Architecture）是 Nvidia 开发的并行计算平台和 API 模型。它允许软件开发者利用支持 CUDA 的 GPU 进行通用计算，这种方法称为 GPGPU（图形处理单元通用计算）。CUDA 是 Nvidia GPU 加速的关键技术，被广泛应用于机器学习、科学计算和视频处理等领域。

CUDA 的硬件支持仅限于 Nvidia 的 GPU，因为它是 Nvidia 的专有技术。每个架构支持特定版本的 CUDA 工具包，该工具包为开发者提供构建和运行 CUDA 应用所需的库和工具。

## ONNX

ONNX（开放神经网络交换格式）是一种用于表示机器学习模型的开放格式。它定义了可扩展的计算图模型，以及内置操作符和标准数据类型的定义。ONNX 允许开发者在不同机器学习框架之间迁移模型，实现互操作性，从而简化 AI 应用的创建和部署。

Phi3 mini 可在包括服务器平台、Windows、Linux 和 Mac 桌面以及移动 CPU 在内的多种设备上，使用 ONNX Runtime 在 CPU 和 GPU 上运行。
我们添加的优化配置包括：

- 针对 int4 DML 的 ONNX 模型：通过 AWQ 量化为 int4
- 针对 fp16 CUDA 的 ONNX 模型
- 针对 int4 CUDA 的 ONNX 模型：通过 RTN 量化为 int4
- 针对 int4 CPU 和移动端的 ONNX 模型：通过 RTN 量化为 int4

## Llama.cpp

Llama.cpp 是一个用 C++ 编写的开源软件库。它支持对多种大型语言模型（LLM）进行推理，包括 Llama。该项目与通用张量库 ggml 一起开发，旨在比原始 Python 实现提供更快的推理速度和更低的内存占用。它支持硬件优化和量化，并提供简单的 API 和示例。如果你对高效的 LLM 推理感兴趣，llama.cpp 非常值得探索，Phi3 也能运行 Llama.cpp。

## GGUF

GGUF（通用图更新格式）是一种用于表示和更新机器学习模型的格式。它特别适合可通过 4-8 位量化在 CPU 上高效运行的小型语言模型（SLM）。GGUF 适用于快速原型开发以及在边缘设备或批处理任务（如 CI/CD 流水线）中运行模型。

**免责声明**：  
本文件使用 AI 翻译服务 [Co-op Translator](https://github.com/Azure/co-op-translator) 进行翻译。虽然我们力求准确，但请注意，自动翻译可能包含错误或不准确之处。原始语言的文件应被视为权威来源。对于重要信息，建议采用专业人工翻译。对于因使用本翻译而产生的任何误解或误释，我们不承担任何责任。