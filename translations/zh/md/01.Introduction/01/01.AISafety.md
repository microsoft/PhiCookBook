<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c8273672cc57df2be675407a1383aaf0",
  "translation_date": "2025-05-07T14:55:26+00:00",
  "source_file": "md/01.Introduction/01/01.AISafety.md",
  "language_code": "zh"
}
-->
# Phi 模型的 AI 安全

Phi 系列模型的开发遵循了[Microsoft Responsible AI Standard](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFl)，这是微软公司基于以下六大原则制定的一套通用要求：问责制、透明度、公平性、可靠性与安全性、隐私与安全性以及包容性，这些构成了[微软的负责任 AI 原则](https://www.microsoft.com/ai/responsible-ai)。

与之前的 Phi 模型类似，我们采用了多方面的安全评估和安全后训练方法，并针对本次发布的多语言能力采取了额外措施。我们在[Phi Safety Post-Training Paper](https://arxiv.org/abs/2407.13833)中详细介绍了安全训练和评估的方法，包括跨多语言和风险类别的测试。虽然 Phi 模型受益于这一方法，开发者仍需遵循负责任 AI 的最佳实践，针对具体用例及其文化和语言环境进行风险映射、测量和缓解。

## 最佳实践

与其他模型一样，Phi 系列模型也可能出现不公平、不可靠或冒犯性的行为。

需要注意的一些 SLM 和 LLM 限制行为包括：

- **服务质量：** Phi 模型主要基于英文文本训练。非英语语言的表现会较差，训练数据中代表性较少的英语变体表现可能不及标准美式英语。
- **伤害表现与刻板印象延续：** 这些模型可能会对某些群体进行过度或不足的表现，抹去部分群体的存在，或强化贬低性或负面刻板印象。尽管经过安全后训练，这些限制仍可能存在，因为训练数据中不同群体的代表程度不同，且负面刻板印象的例子反映了现实世界的模式和社会偏见。
- **不当或冒犯性内容：** 这些模型可能生成其他类型的不当或冒犯性内容，因此在敏感场景下部署时，若无针对具体用例的额外缓解措施，可能不适合使用。
- **信息可靠性：** 语言模型可能生成无意义内容，或捏造听起来合理但不准确或过时的信息。
- **代码范围有限：** 大部分 Phi-3 训练数据基于 Python，使用了如 "typing, math, random, collections, datetime, itertools" 等常用包。如果模型生成的 Python 脚本使用了其他包，或生成了其他语言的脚本，强烈建议用户手动验证所有 API 的使用。

开发者应遵循负责任 AI 的最佳实践，确保具体用例符合相关法律法规（如隐私、贸易等）。

## 负责任 AI 考量

与其他语言模型一样，Phi 系列模型可能表现出不公平、不可靠或冒犯性的行为。需要注意的限制行为包括：

**服务质量：** Phi 模型主要基于英文文本训练。非英语语言的表现会较差。训练数据中代表性较少的英语变体表现可能不及标准美式英语。

**伤害表现与刻板印象延续：** 这些模型可能会对某些群体进行过度或不足的表现，抹去部分群体的存在，或强化贬低性或负面刻板印象。尽管经过安全后训练，这些限制仍可能存在，因为训练数据中不同群体的代表程度不同，且负面刻板印象的例子反映了现实世界的模式和社会偏见。

**不当或冒犯性内容：** 这些模型可能生成其他类型的不当或冒犯性内容，因此在敏感场景下部署时，若无针对具体用例的额外缓解措施，可能不适合使用。
信息可靠性：语言模型可能生成无意义内容，或捏造听起来合理但不准确或过时的信息。

**代码范围有限：** 大部分 Phi-3 训练数据基于 Python，使用了如 "typing, math, random, collections, datetime, itertools" 等常用包。如果模型生成的 Python 脚本使用了其他包，或生成了其他语言的脚本，强烈建议用户手动验证所有 API 的使用。

开发者应遵循负责任 AI 的最佳实践，确保具体用例符合相关法律法规（如隐私、贸易等）。重要考量领域包括：

**分配：** 在可能对法律地位、资源或生活机会分配产生重大影响的场景（如住房、就业、信贷等），模型可能不适用，除非经过进一步评估和额外的去偏技术。

**高风险场景：** 开发者应评估模型在高风险场景中的适用性，这些场景中不公平、不可靠或冒犯性输出可能带来极大代价或伤害。这包括在对准确性和可靠性要求极高的敏感或专业领域提供建议（如法律或健康建议）。应根据部署环境在应用层面实施额外保护措施。

**错误信息：** 模型可能生成不准确的信息。开发者应遵循透明度最佳实践，告知终端用户他们正在与 AI 系统交互。在应用层面，开发者可以构建反馈机制和管道，将响应基于具体用例的上下文信息，这一技术称为检索增强生成（RAG）。

**有害内容生成：** 开发者应根据上下文评估输出，使用现有安全分类器或针对用例的定制解决方案。

**滥用：** 可能存在欺诈、垃圾信息或恶意软件生成等滥用形式，开发者应确保其应用不违反适用法律法规。

### 微调与 AI 内容安全

微调模型后，我们强烈建议利用[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview)措施，监控模型生成的内容，识别并阻断潜在风险、威胁及质量问题。

![Phi3AISafety](../../../../../translated_images/01.phi3aisafety.c0d7fc42f5a5c40507c5e8be556615b8377a63b8764865d057d4faac3757a478.zh.png)

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview)支持文本和图像内容，可部署在云端、离线容器及边缘/嵌入式设备上。

## Azure AI 内容安全概览

Azure AI 内容安全并非一刀切的解决方案；它可以根据企业的具体政策进行定制。此外，其多语言模型使其能够同时理解多种语言。

![AIContentSafety](../../../../../translated_images/01.AIcontentsafety.a288819b8ce8da1a56cf708aff010a541799d002ae7ae84bb819b19ab8950591.zh.png)

- **Azure AI Content Safety**
- **Microsoft Developer**
- **5 个视频**

Azure AI 内容安全服务能够检测应用和服务中有害的用户生成和 AI 生成内容。它包含文本和图像 API，帮助检测有害或不当的材料。

[AI Content Safety 播放列表](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

**免责声明**：  
本文件已使用 AI 翻译服务 [Co-op Translator](https://github.com/Azure/co-op-translator) 进行翻译。虽然我们力求准确，但请注意，自动翻译可能存在错误或不准确之处。原始文件的原文版本应被视为权威来源。对于重要信息，建议采用专业人工翻译。对于因使用本翻译而产生的任何误解或误释，我们不承担任何责任。