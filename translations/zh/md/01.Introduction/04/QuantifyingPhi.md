<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-05-07T14:49:22+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "zh"
}
-->
# **量化Phi系列**

模型量化是指将神经网络模型中的参数（如权重和激活值）从较大的数值范围（通常是连续值范围）映射到较小的有限数值范围的过程。这项技术可以减少模型的大小和计算复杂度，提高模型在资源受限环境（如移动设备或嵌入式系统）中的运行效率。模型量化通过降低参数的精度来实现压缩，但也会带来一定的精度损失。因此，在量化过程中，需要在模型大小、计算复杂度和精度之间进行权衡。常见的量化方法包括定点量化、浮点量化等。您可以根据具体场景和需求选择合适的量化策略。

我们希望将GenAI模型部署到边缘设备，使更多设备能够进入GenAI场景，如移动设备、AI PC/Copilot+PC以及传统的物联网设备。通过量化模型，可以基于不同设备将其部署到不同的边缘设备上。结合硬件厂商提供的模型加速框架和量化模型，可以构建更好的SLM应用场景。

在量化场景中，我们有不同的精度（INT4、INT8、FP16、FP32）。以下是常用量化精度的说明：

### **INT4**

INT4量化是一种激进的量化方法，将模型的权重和激活值量化为4位整数。由于表示范围较小且精度较低，INT4量化通常会带来较大的精度损失。但与INT8量化相比，INT4量化可以进一步降低模型的存储需求和计算复杂度。需要注意的是，INT4量化在实际应用中相对较少见，因为过低的精度可能导致模型性能显著下降。此外，并非所有硬件都支持INT4操作，因此在选择量化方法时需要考虑硬件兼容性。

### **INT8**

INT8量化是将模型的权重和激活值从浮点数转换为8位整数的过程。虽然INT8整数表示的数值范围较小且精度较低，但它可以显著降低存储和计算需求。在INT8量化中，模型的权重和激活值经过量化处理，包括缩放和偏移，以尽可能保留原始浮点信息。在推理过程中，这些量化值会被反量化回浮点数进行计算，然后再量化回INT8以进行下一步操作。这种方法在大多数应用中可以提供足够的精度，同时保持较高的计算效率。

### **FP16**

FP16格式，即16位浮点数（float16），相比32位浮点数（float32）将内存占用减半，在大规模深度学习应用中具有显著优势。FP16格式允许在相同GPU内存限制下加载更大的模型或处理更多数据。随着现代GPU硬件对FP16操作的持续支持，使用FP16格式也可能带来计算速度的提升。然而，FP16格式也有其固有缺点，即精度较低，可能在某些情况下导致数值不稳定或精度损失。

### **FP32**

FP32格式提供更高的精度，能够准确表示更广泛的数值范围。在执行复杂数学运算或需要高精度结果的场景中，优先选择FP32格式。但高精度也意味着更大的内存使用和更长的计算时间。对于大型深度学习模型，尤其是模型参数众多且数据量巨大的情况下，FP32格式可能导致GPU内存不足或推理速度下降。

在移动设备或物联网设备上，我们可以将Phi-3.x模型转换为INT4，而AI PC / Copilot PC可以使用更高精度的INT8、FP16、FP32。

目前，不同硬件厂商提供了支持生成模型的不同框架，如Intel的OpenVINO、Qualcomm的QNN、Apple的MLX以及Nvidia的CUDA等，结合模型量化实现本地部署。

在技术方面，量化后我们支持不同的格式，如PyTorch / Tensorflow格式、GGUF和ONNX。我已经做过GGUF和ONNX之间的格式对比及应用场景推荐。这里推荐ONNX量化格式，它从模型框架到硬件都有良好的支持。本章将重点介绍如何使用ONNX Runtime for GenAI、OpenVINO和Apple MLX进行模型量化（如果您有更好的方法，也欢迎通过提交PR的方式提供给我们）。

**本章内容包括**

1. [使用llama.cpp量化Phi-3.5 / 4](./UsingLlamacppQuantifyingPhi.md)

2. [使用onnxruntime生成式AI扩展量化Phi-3.5 / 4](./UsingORTGenAIQuantifyingPhi.md)

3. [使用Intel OpenVINO量化Phi-3.5 / 4](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [使用Apple MLX框架量化Phi-3.5 / 4](./UsingAppleMLXQuantifyingPhi.md)

**免责声明**：  
本文件由AI翻译服务[Co-op Translator](https://github.com/Azure/co-op-translator)翻译而成。尽管我们力求准确，但请注意自动翻译可能存在错误或不准确之处。原始语言的原文应被视为权威来源。对于重要信息，建议使用专业人工翻译。对于因使用本翻译而产生的任何误解或误释，我们不承担任何责任。