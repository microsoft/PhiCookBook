<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "418c693c63cc0e817dc560558f730a7a",
  "translation_date": "2025-04-03T07:01:12+00:00",
  "source_file": "md\\01.Introduction\\04\\QuantifyingPhi.md",
  "language_code": "zh"
}
-->
# **量化 Phi 系列**

模型量化是指将神经网络模型中的参数（如权重和激活值）从一个较大的数值范围（通常是连续的数值范围）映射到一个较小的有限数值范围的过程。这项技术可以减小模型的大小和计算复杂度，并提高模型在资源受限环境（如移动设备或嵌入式系统）中的运行效率。通过降低参数的精度，模型量化实现了压缩，但也会引入一定的精度损失。因此，在量化过程中，需要在模型大小、计算复杂度和精度之间找到平衡。常见的量化方法包括定点量化、浮点量化等。可以根据具体场景和需求选择合适的量化策略。

我们希望将生成式 AI（GenAI）模型部署到边缘设备上，让更多设备能够进入 GenAI 场景，例如移动设备、AI PC / Copilot+PC，以及传统的 IoT 设备。通过量化模型，我们可以根据不同的设备需求，将其部署到不同的边缘设备上。结合硬件厂商提供的模型加速框架和量化模型，我们可以构建更好的 SLM 应用场景。

在量化场景中，我们有不同的精度选项（INT4、INT8、FP16、FP32）。以下是常用量化精度的解释：

### **INT4**

INT4 量化是一种激进的量化方法，它将模型的权重和激活值量化为 4 位整数。由于表示范围较小且精度较低，INT4 量化通常会导致较大的精度损失。然而，与 INT8 量化相比，INT4 量化可以进一步降低模型的存储需求和计算复杂度。需要注意的是，在实际应用中，INT4 量化相对较少使用，因为过低的精度可能会导致模型性能显著下降。此外，并非所有硬件都支持 INT4 操作，因此在选择量化方法时需要考虑硬件的兼容性。

### **INT8**

INT8 量化是将模型的权重和激活值从浮点数转换为 8 位整数的过程。虽然 INT8 整数表示的数值范围较小且精度较低，但它可以显著减少存储和计算需求。在 INT8 量化中，模型的权重和激活值会经过量化处理，包括缩放和偏移，以尽可能保留原始浮点信息。在推理过程中，这些量化值会被反量化回浮点数进行计算，然后再量化回 INT8 以进入下一步操作。这种方法可以在大多数应用中提供足够的精度，同时保持较高的计算效率。

### **FP16**

FP16 格式，即 16 位浮点数（float16），相比 32 位浮点数（float32），将内存占用减少了一半，在大规模深度学习应用中具有显著优势。FP16 格式允许在相同的 GPU 内存限制下加载更大的模型或处理更多的数据。随着现代 GPU 硬件持续支持 FP16 操作，使用 FP16 格式还可能带来计算速度的提升。然而，FP16 格式也有其固有的缺点，即较低的精度，可能在某些情况下导致数值不稳定或精度损失。

### **FP32**

FP32 格式提供更高的精度，可以精确表示更广泛的数值范围。在需要执行复杂数学运算或高精度结果的场景中，FP32 格式是首选。然而，更高的精度也意味着更多的内存使用和更长的计算时间。对于大规模深度学习模型，尤其是在模型参数众多且数据量巨大的情况下，FP32 格式可能导致 GPU 内存不足或推理速度下降。

在移动设备或 IoT 设备上，我们可以将 Phi-3.x 模型转换为 INT4，而 AI PC / Copilot PC 则可以使用更高的精度，例如 INT8、FP16 或 FP32。

目前，不同的硬件厂商有不同的框架来支持生成式模型，例如 Intel 的 OpenVINO、Qualcomm 的 QNN、Apple 的 MLX 和 Nvidia 的 CUDA 等，结合模型量化可以完成本地部署。

在技术层面上，量化后我们有不同的格式支持，例如 PyTorch / Tensorflow 格式、GGUF 和 ONNX。我对 GGUF 和 ONNX 的格式及应用场景进行了比较。在此推荐 ONNX 量化格式，它从模型框架到硬件都具有良好的支持。在本章中，我们将重点介绍如何使用 ONNX Runtime for GenAI、OpenVINO 和 Apple MLX 进行模型量化（如果你有更好的方法，也可以通过提交 PR 提供给我们）。

**本章内容包括**

1. [使用 llama.cpp 对 Phi-3.5 / 4 进行量化](./UsingLlamacppQuantifyingPhi.md)

2. [使用 ONNX Runtime 的生成式 AI 扩展对 Phi-3.5 / 4 进行量化](./UsingORTGenAIQuantifyingPhi.md)

3. [使用 Intel OpenVINO 对 Phi-3.5 / 4 进行量化](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [使用 Apple MLX 框架对 Phi-3.5 / 4 进行量化](./UsingAppleMLXQuantifyingPhi.md)

**免责声明**：  
本文档通过 AI 翻译服务 [Co-op Translator](https://github.com/Azure/co-op-translator) 进行翻译。虽然我们努力确保翻译的准确性，但请注意，自动翻译可能包含错误或不准确之处。原始语言版本的文档应被视为权威来源。对于关键信息，建议使用专业人工翻译。我们对于因使用此翻译而导致的任何误解或误读不承担责任。