<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "624fe133fba62773979d45f54519f7bb",
  "translation_date": "2025-05-07T15:09:35+00:00",
  "source_file": "md/01.Introduction/02/01.HF.md",
  "language_code": "zh"
}
-->
# **在 Hugging Face 中使用 Phi 家族**

[Hugging Face](https://huggingface.co/) 是一个非常受欢迎的 AI 社区，拥有丰富的数据和开源模型资源。不同厂商会通过 Hugging Face 发布开源的 LLM 和 SLM，比如 Microsoft、Meta、Mistral、Apple、Google 等。

Microsoft Phi 家族已在 Hugging Face 发布。开发者可以根据场景和业务需求下载对应的 Phi 家族模型。除了在 Hugging Face 上部署 Phi Pytorch 模型外，我们还发布了量化模型，使用 GGUF 和 ONNX 格式，为终端用户提供更多选择。

## **在 Hugging Face 下载模型**

你可以通过以下链接下载 Phi 家族模型

[Microsoft Models on Hugging Face](https://huggingface.co/microsoft)

-  **Phi-1 / 1.5** https://huggingface.co/collections/microsoft/phi-1-6626e29134744e94e222d572

-  **Phi-3 / 3.5** https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3

-  **Phi-4** https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4

- **Phi-4-reasoning** https://huggingface.co/microsoft/Phi-4-reasoning

- **Phi-4-reasoning Plus** https://huggingface.co/microsoft/Phi-4-reasoning-plus 

- **Phi-4-mini-reasoning** https://huggingface.co/microsoft/Phi-4-mini-reasoning

你可以通过多种方式下载模型，比如安装 ***Hugging Face CLI SDK*** 或使用 ***git clone***。

### **使用 Hugging Face CLI 下载 Phi 家族模型**

- 安装 Hugging Face CLI

```bash

pip install -U "huggingface_hub[cli]"

```

- 使用 huggingface-cli 登录

使用你的 [用户访问令牌](https://huggingface.co/docs/hub/security-tokens)，在你的 [设置页面](https://huggingface.co/settings/tokens) 获取

```bash

huggingface-cli login --token $HF_TOKEN --add-to-git-credential

```

- 下载

你可以下载模型并保存到缓存中

```bash

huggingface-cli download microsoft/phi-4

```

你也可以将模型保存到指定位置

```bash

huggingface-cli download microsoft/phi-4 --local-dir $YOUR_PATH

```

### **使用 git clone 下载 Phi 家族模型**

你也可以使用 ***git clone*** 来下载模型

```bash

git lfs install

git clone https://huggingface.co/microsoft/phi-4

```

## **示例 - 推理 Microsoft Phi-4**

- **安装 transformers 库**

```bash

pip install transformers -U

```

- **在 VSCode 中运行此代码**

```python

import transformers

pipeline = transformers.pipeline(
    "text-generation",
    model="microsoft/phi-4",
    model_kwargs={"torch_dtype": "auto"},
    device_map="auto",
)

messages = [
    {"role": "user", "content": "I have $20,000 in my savings account, where I receive a 4% profit per year and payments twice a year. Can you please tell me how long it will take for me to become a millionaire? Also, can you please explain the math step by step as if you were explaining it to an uneducated person?"},
]

outputs = pipeline(messages, max_new_tokens=2048)
print(outputs[0]["generated_text"][-1])

```

**免责声明**：  
本文件已使用 AI 翻译服务 [Co-op Translator](https://github.com/Azure/co-op-translator) 进行翻译。尽管我们力求准确，但请注意，自动翻译可能存在错误或不准确之处。原始语言的文档应被视为权威来源。对于重要信息，建议使用专业人工翻译。对于因使用本翻译而产生的任何误解或误释，我们不承担任何责任。