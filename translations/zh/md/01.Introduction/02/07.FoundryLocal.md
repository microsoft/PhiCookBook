<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "52973a5680a65a810aa80b7036afd31f",
  "translation_date": "2025-06-27T13:34:09+00:00",
  "source_file": "md/01.Introduction/02/07.FoundryLocal.md",
  "language_code": "zh"
}
-->
## 在 Foundry Local 中快速上手 Phi-Family 模型

### Foundry Local 简介

Foundry Local 是一款强大的本地设备 AI 推理解决方案，将企业级 AI 能力直接带到您的本地硬件上。本教程将引导您如何在 Foundry Local 中设置和使用 Phi-Family 模型，让您完全掌控 AI 任务，同时保障隐私并降低成本。

Foundry Local 通过在设备本地运行 AI 模型，提供性能、隐私、自定义和成本优势。它通过直观的 CLI、SDK 和 REST API 无缝集成到您现有的工作流程和应用中。

![arch](../../../../../translated_images/foundry-local-arch.8823e321dd8258d7d68815ddb0153503587142ff32e6997041c7cf0c9df24b49.zh.png)

### 为什么选择 Foundry Local？

了解 Foundry Local 的优势，帮助您做出明智的 AI 部署决策：

- **本地推理：** 在您自己的硬件上运行模型，降低成本，同时所有数据都保留在设备上。
- **模型定制：** 可选择预设模型或使用自定义模型，满足特定需求和应用场景。
- **成本效益：** 利用现有硬件，避免持续的云服务费用，让 AI 更加普及。
- **无缝集成：** 通过 SDK、API 端点或 CLI 连接应用，且随着需求增长，可轻松扩展到 Azure AI Foundry。

> **开始提示：** 本教程重点介绍如何通过 CLI 和 SDK 使用 Foundry Local，帮助您选择最适合的使用方式。

## 第 1 部分：设置 Foundry Local CLI

### 第 1 步：安装

Foundry Local CLI 是您管理和本地运行 AI 模型的入口。让我们先在系统上安装它。

**支持平台：** Windows 和 macOS

详细安装说明，请参阅[官方 Foundry Local 文档](https://github.com/microsoft/Foundry-Local/blob/main/README.md)。

### 第 2 步：查看可用模型

安装完成后，您可以查看适合您使用场景的模型。以下命令会列出所有支持的模型：

```bash
foundry model list
```

### 第 3 步：了解 Phi Family 模型

Phi Family 提供了针对不同用例和硬件配置优化的一系列模型。以下是在 Foundry Local 中可用的 Phi 模型：

**可用 Phi 模型：**

- **phi-3.5-mini** - 适用于基础任务的紧凑型模型
- **phi-3-mini-128k** - 支持更长对话的扩展上下文版本
- **phi-3-mini-4k** - 通用的标准上下文模型
- **phi-4** - 功能更强大的高级模型
- **phi-4-mini** - Phi-4 的轻量版
- **phi-4-mini-reasoning** - 专门针对复杂推理任务设计

> **硬件兼容性：** 每个模型可根据系统能力配置不同的硬件加速（CPU、GPU）。

### 第 4 步：运行您的第一个 Phi 模型

让我们通过一个实际例子开始。这里运行的是擅长逐步解决复杂问题的 `phi-4-mini-reasoning` 模型。

**运行模型的命令：**

```bash
foundry model run Phi-4-mini-reasoning-generic-cpu
```

> **首次运行提示：** 第一次运行模型时，Foundry Local 会自动下载模型到本地设备。下载时间取决于网络速度，初次设置时请耐心等待。

### 第 5 步：用实际问题测试模型

现在用一个经典的逻辑问题测试模型的逐步推理能力：

**示例问题：**

```txt
Please calculate the following step by step: Now there are pheasants and rabbits in the same cage, there are thirty-five heads on top and ninety-four legs on the bottom, how many pheasants and rabbits are there?
```

**预期表现：** 模型应将问题拆解为逻辑步骤，利用野鸡有 2 条腿、兔子有 4 条腿的事实，求解方程组。

**结果展示：**

![cli](../../../../../translated_images/cli.862ec6b55c2b5d916093866d4df99190150d4198fd33ab79e586f9d6f5403089.zh.png)

## 第 2 部分：使用 Foundry Local SDK 构建应用

### 为什么使用 SDK？

虽然 CLI 适合测试和快速交互，SDK 让您能够以编程方式将 Foundry Local 集成到应用中。这样您可以：

- 构建定制的 AI 驱动应用
- 创建自动化工作流
- 将 AI 能力集成到现有系统
- 开发聊天机器人和交互式工具

### 支持的编程语言

Foundry Local 提供多种语言的 SDK，满足不同开发偏好：

**📦 可用 SDK：**

- **C# (.NET)：** [SDK 文档与示例](https://github.com/microsoft/Foundry-Local/tree/main/sdk/cs)
- **Python：** [SDK 文档与示例](https://github.com/microsoft/Foundry-Local/tree/main/sdk/python)
- **JavaScript：** [SDK 文档与示例](https://github.com/microsoft/Foundry-Local/tree/main/sdk/js)
- **Rust：** [SDK 文档与示例](https://github.com/microsoft/Foundry-Local/tree/main/sdk/rust)

### 下一步

1. **根据开发环境选择合适的 SDK**
2. **参考对应 SDK 的详细文档进行实现**
3. **从简单示例开始，逐步构建复杂应用**
4. **探索各 SDK 仓库中的示例代码**

## 总结

您已经学会如何：

- ✅ 安装并设置 Foundry Local CLI
- ✅ 发现并运行 Phi Family 模型
- ✅ 用实际问题测试模型
- ✅ 了解用于应用开发的 SDK 选项

Foundry Local 为您提供了强大的基础，将 AI 能力直接带入本地环境，让您掌控性能、隐私和成本，同时保持灵活性，必要时可扩展至云端解决方案。

**免责声明**：  
本文件由 AI 翻译服务 [Co-op Translator](https://github.com/Azure/co-op-translator) 翻译而成。虽然我们力求准确，但请注意，自动翻译可能包含错误或不准确之处。原始文件的母语版本应被视为权威来源。对于重要信息，建议采用专业人工翻译。对于因使用本翻译而产生的任何误解或误释，我们不承担任何责任。