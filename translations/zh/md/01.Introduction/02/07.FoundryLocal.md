<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "52973a5680a65a810aa80b7036afd31f",
  "translation_date": "2025-07-16T19:42:58+00:00",
  "source_file": "md/01.Introduction/02/07.FoundryLocal.md",
  "language_code": "zh"
}
-->
## 在 Foundry Local 中快速上手 Phi-Family 模型

### Foundry Local 简介

Foundry Local 是一款强大的本地设备 AI 推理解决方案，将企业级 AI 能力直接带到您的本地硬件上。本教程将指导您如何在 Foundry Local 中设置和使用 Phi-Family 模型，让您在保持隐私和降低成本的同时，全面掌控 AI 任务。

Foundry Local 通过在设备本地运行 AI 模型，提供性能、隐私、自定义和成本优势。它通过直观的 CLI、SDK 和 REST API 无缝集成到您现有的工作流程和应用中。

![arch](../../../../../translated_images/zh-CN/foundry-local-arch.8823e321dd8258d7.webp)

### 为什么选择 Foundry Local？

了解 Foundry Local 的优势，有助于您做出更明智的 AI 部署决策：

- **本地推理：** 在您自己的硬件上运行模型，降低成本，同时确保所有数据留在设备上。

- **模型定制：** 可选择预设模型或使用自定义模型，满足特定需求和应用场景。

- **成本效益：** 利用现有硬件，免除持续的云服务费用，让 AI 更加普及。

- **无缝集成：** 通过 SDK、API 端点或 CLI 连接您的应用，随着需求增长轻松扩展到 Azure AI Foundry。

> **入门提示：** 本教程重点介绍通过 CLI 和 SDK 使用 Foundry Local。您将学习两种方式，帮助您选择最适合的方案。

## 第 1 部分：设置 Foundry Local CLI

### 第 1 步：安装

Foundry Local CLI 是管理和运行本地 AI 模型的入口。让我们先在系统上安装它。

**支持平台：** Windows 和 macOS

详细安装说明请参阅[官方 Foundry Local 文档](https://github.com/microsoft/Foundry-Local/blob/main/README.md)。

### 第 2 步：查看可用模型

安装 Foundry Local CLI 后，您可以查看适合您需求的可用模型。以下命令将列出所有支持的模型：

```bash
foundry model list
```

### 第 3 步：了解 Phi Family 模型

Phi Family 提供了一系列针对不同应用场景和硬件配置优化的模型。以下是 Foundry Local 中可用的 Phi 模型：

**可用 Phi 模型：**

- **phi-3.5-mini** - 适合基础任务的紧凑型模型
- **phi-3-mini-128k** - 支持更长对话的扩展上下文版本
- **phi-3-mini-4k** - 通用标准上下文模型
- **phi-4** - 功能更强大的高级模型
- **phi-4-mini** - Phi-4 的轻量级版本
- **phi-4-mini-reasoning** - 专门针对复杂推理任务设计

> **硬件兼容性：** 每个模型可根据系统能力配置不同的硬件加速（CPU、GPU）。

### 第 4 步：运行您的第一个 Phi 模型

让我们通过一个实际示例开始。我们将运行擅长逐步解决复杂问题的 `phi-4-mini-reasoning` 模型。

**运行模型的命令：**

```bash
foundry model run Phi-4-mini-reasoning-generic-cpu
```

> **首次运行提示：** 第一次运行模型时，Foundry Local 会自动将模型下载到本地设备。下载时间取决于您的网络速度，初次设置时请耐心等待。

### 第 5 步：用实际问题测试模型

现在让我们用一个经典的逻辑问题测试模型，看看它如何进行逐步推理：

**示例问题：**

```txt
Please calculate the following step by step: Now there are pheasants and rabbits in the same cage, there are thirty-five heads on top and ninety-four legs on the bottom, how many pheasants and rabbits are there?
```

**预期表现：** 模型应将问题拆解为逻辑步骤，利用野鸡有 2 条腿、兔子有 4 条腿的事实，求解方程组。

**结果展示：**

![cli](../../../../../translated_images/zh-CN/cli.862ec6b55c2b5d91.webp)

## 第 2 部分：使用 Foundry Local SDK 构建应用

### 为什么使用 SDK？

虽然 CLI 适合测试和快速交互，SDK 让您能够以编程方式将 Foundry Local 集成到应用中，开启更多可能：

- 构建定制的 AI 应用
- 创建自动化工作流
- 将 AI 功能集成到现有系统
- 开发聊天机器人和交互工具

### 支持的编程语言

Foundry Local 提供多种编程语言的 SDK，满足您的开发偏好：

**📦 可用 SDK：**

- **C# (.NET)：** [SDK 文档与示例](https://github.com/microsoft/Foundry-Local/tree/main/sdk/cs)
- **Python：** [SDK 文档与示例](https://github.com/microsoft/Foundry-Local/tree/main/sdk/python)
- **JavaScript：** [SDK 文档与示例](https://github.com/microsoft/Foundry-Local/tree/main/sdk/js)
- **Rust：** [SDK 文档与示例](https://github.com/microsoft/Foundry-Local/tree/main/sdk/rust)

### 后续步骤

1. **根据开发环境选择合适的 SDK**
2. **参考 SDK 相关文档，了解详细实现指南**
3. **从简单示例开始，逐步构建复杂应用**
4. **探索各 SDK 仓库中的示例代码**

## 结语

您已经学会了如何：
- ✅ 安装并设置 Foundry Local CLI
- ✅ 发现并运行 Phi Family 模型
- ✅ 用实际问题测试模型
- ✅ 了解用于应用开发的 SDK 选项

Foundry Local 为您提供了强大的基础，将 AI 能力直接带入本地环境，让您掌控性能、隐私和成本，同时保持向云端解决方案扩展的灵活性。

**免责声明**：  
本文件使用 AI 翻译服务 [Co-op Translator](https://github.com/Azure/co-op-translator) 进行翻译。虽然我们力求准确，但请注意，自动翻译可能包含错误或不准确之处。原始文件的母语版本应被视为权威来源。对于重要信息，建议使用专业人工翻译。对于因使用本翻译而产生的任何误解或误释，我们概不负责。