{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaktyvus Phi 3 Mini 4K Instruct Chatbot su Whisper\n",
    "\n",
    "### Įvadas:\n",
    "Interaktyvus Phi 3 Mini 4K Instruct Chatbot yra įrankis, leidžiantis vartotojams bendrauti su Microsoft Phi 3 Mini 4K instruct demonstracija naudojant tekstinį arba garso įvedimą. Šis pokalbių robotas gali būti naudojamas įvairiems uždaviniams, pavyzdžiui, vertimui, orų atnaujinimams ir bendram informacijos rinkimui.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[Sukurkite savo Huggingface prieigos žetoną](https://huggingface.co/settings/tokens)\n",
    "\n",
    "Sukurkite naują žetoną \n",
    "Nurodykite naują pavadinimą \n",
    "Pasirinkite rašymo teises\n",
    "nukopijuokite žetoną ir išsaugokite jį saugioje vietoje\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Žemiau pateiktas Python kodas atlieka dvi pagrindines užduotis: importuoja `os` modulį ir nustato aplinkos kintamąjį.\n",
    "\n",
    "1. Importing the `os` module:\n",
    "   - `os` modulis Python suteikia būdą bendrauti su operacine sistema. Leidžia atlikti įvairias su operacine sistema susijusias užduotis, tokias kaip prieiga prie aplinkos kintamųjų, darbas su failais ir katalogais ir kt.\n",
    "   - Šiame kode `os` modulis yra importuojamas naudojant `import` sakinį. Šis sakinys padaro `os` modulio funkcionalumą prieinamą naudoti esamame Python skripte.\n",
    "\n",
    "2. Setting an environment variable:\n",
    "   - Aplinkos kintamasis yra reikšmė, prie kurios gali prieiti operacinėje sistemoje veikiančios programos. Tai būdas saugoti konfigūracijos nustatymus ar kitą informaciją, kurią gali naudoti kelios programos.\n",
    "   - Šiame kode naujas aplinkos kintamasis nustatomas naudojant `os.environ` žodyną. Žodyno raktas yra `'HF_TOKEN'`, o reikšmė priskiriama iš kintamojo `HUGGINGFACE_TOKEN`.\n",
    "   - Kintamasis `HUGGINGFACE_TOKEN` apibrėžtas kiek aukščiau nei šis kodo fragmentas, jam priskiriama eilutės reikšmė `\"hf_**************\"` naudojant `#@param` sintaksę. Ši sintaksė dažnai naudojama Jupyter užrašinėse (notebook) leidžiant vartotojo įvestį ir parametrų konfigūravimą tiesiogiai užrašinės aplinkoje.\n",
    "   - Nustačius aplinkos kintamąjį `'HF_TOKEN'`, prie jo gali prieiti kitos programos dalys arba kitos toje pačioje operacinėje sistemoje veikiančios programos.\n",
    "\n",
    "Apskritai, šis kodas importuoja `os` modulį ir nustato aplinkos kintamąjį pavadinimu `'HF_TOKEN'` su reikšme, pateikta kintamajame `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Šis kodo fragmentas apibrėžia funkciją pavadinimu clear_output, kuri naudojama išvalyti esamos ląstelės išvestį Jupyter Notebook arba IPython. Paaiškinkime kodą ir supraskime jo funkcionalumą:\n",
    "\n",
    "Funkcija clear_output priima vieną parametrą pavadinimu wait, kuris yra loginė reikšmė. Pagal numatytuosius nustatymus wait nustatytas į False. Šis parametras nurodo, ar funkcija turėtų palaukti, kol bus pasiekiama nauja išvestis, kuri pakeistų esamą išvestį prieš ją išvalant.\n",
    "\n",
    "Sama funkcija naudojama išvalyti esamos ląstelės išvestį. Jupyter Notebook arba IPython, kai ląstelė generuoja išvestį, pavyzdžiui atspausdintą tekstą arba grafikus, ji rodoma po ląstele. Funkcija clear_output leidžia jums išvalyti tą išvestį.\n",
    "\n",
    "Funkcijos įgyvendinimas nebuvo pateiktas kodo fragmente, kaip nurodo elipsė (...). Elipsė reiškia vietos rezervavimo ženklą faktiniam kodui, kuris atlieka išvesties išvalymą. Funkcijos įgyvendinimas gali apimti sąveiką su Jupyter Notebook arba IPython API, kad būtų pašalinta esama ląstelės išvestis.\n",
    "\n",
    "Apskritai ši funkcija suteikia patogų būdą išvalyti esamos ląstelės išvestį Jupyter Notebook arba IPython, todėl lengviau valdyti ir atnaujinti rodomą išvestį interaktyvių programavimo sesijų metu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform text-to-speech (TTS) using the Edge TTS service. Let's go through the relevant function implementations one by one:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Ši funkcija priima įvesties reikšmę ir paskaičiuoja greičio eilutę (rate string) TTS balsui. Įvesties reikšmė reiškia pageidaujamą kalbos greitį, kur reikšmė 1 reiškia normalų greitį. Funkcija apskaičiuoja greičio eilutę atimdama 1 iš įvesties reikšmės, padauginama ją iš 100, o tada nustato ženklą pagal tai, ar įvesties reikšmė yra didesnė arba lygi 1. Funkcija grąžina greičio eilutę formatu \"{sign}{rate}\".\n",
    "\n",
    "2.`make_chunks(input_text, language)`: Ši funkcija priima įvesties tekstą ir kalbą kaip parametrus. Ji skiria įvesties tekstą į gabalus pagal kalbai specifines taisykles. Šioje implementacijoje, jei kalba yra \"anglų\", funkcija suskaido tekstą ties kiekvienu tašku (\".\") ir pašalina bet kokius priekyje arba gale esančius tarpelius. Tada ji prideda tašką prie kiekvieno gabalo ir grąžina filtruotą gabalų sąrašą.\n",
    "\n",
    "3. `tts_file_name(text)`: Ši funkcija sugeneruoja failo pavadinimą TTS garso failui remdamasi įvesties tekstu. Ji atlieka kelias transformacijas su tekstu: pašalina galinį tašką (jei yra), paverčia tekstą mažosiomis raidėmis, nukerpa priekinį ir galinį tarpą ir pakeičia tarpus pabraukais (_). Tada ji sutrumpina tekstą iki maksimumo 25 simbolių (jei jis ilgesnis) arba naudoja visą tekstą, jei jis yra tuščias. Galiausiai ji sugeneruoja atsitiktinę eilutę naudodama [`uuid`] modulį ir sujungia ją su sutrumpintu tekstu, kad sukurtų failo pavadinimą formatu \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Ši funkcija sujungia kelis garso failus į vieną garso failą. Ji priima garso failų kelių sąrašą ir išvesties kelią kaip parametrus. Funkcija inicijuoja tuščią `AudioSegment` objektą pavadintą [`merged_audio`]. Tada ji iteruoja per kiekvieną garso failo kelią, įkelia garso failą naudodama `AudioSegment.from_file()` metodą iš `pydub` bibliotekos ir prideda esamą garso failą prie objekto [`merged_audio`]. Galiausiai ji eksportuoja sujungtą garsą į nurodytą išvesties kelią MP3 formatu.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path): This function performs the TTS operation using the Edge TTS service. It takes a list of text chunks, the speed of the speech, the voice name, and the save path as parameters. If the number of chunks is greater than 1, the function creates a directory for storing the individual chunk audio files. It then iterates through each chunk, constructs an Edge TTS command using the `calculate_rate_string()' function, the voice name, and the chunk text, and executes the command using the `os.system()` function. If the command execution is successful, it appends the path of the generated audio file to a list. After processing all the chunks, it merges the individual audio files using the `merge_audio_files()` function and saves the merged audio to the specified save path. If there is only one chunk, it directly generates the Edge TTS command and saves the audio to the save path. Finally, it returns the save path of the generated audio file.\n",
    "\n",
    "6. `random_audio_name_generate()`: Ši funkcija sugeneruoja atsitiktinį garso failo pavadinimą naudodama [`uuid`] modulį. Ji sugeneruoja atsitiktinį UUID, paverčia jį eilute, paima pirmus 8 simbolius, prideda \".mp3\" plėtinį ir grąžina atsitiktinį garso failo pavadinimą.\n",
    "\n",
    "7. `talk(input_text)`: Ši funkcija yra pagrindinis įėjimo taškas TTS operacijai atlikti. Ji priima įvesties tekstą kaip parametrą. Pirmiausia ji patikrina įvesties teksto ilgį, kad nustatytų, ar tai ilgas tekstas (didesnis arba lygus 600 simbolių). Remiantis ilgiu ir `translate_text_flag` kintamojo reikšme, ji nustato kalbą ir sugeneruoja teksto gabalų sąrašą naudodama `make_chunks()` funkciją. Tada ji sugeneruoja išsaugojimo kelią garso failui naudodama `random_audio_name_generate()` funkciją. Galiausiai ji iškviečia `edge_free_tts()` funkciją, kad atliktų TTS operaciją, ir grąžina sugeneruoto garso failo išsaugojimo kelią.\n",
    "\n",
    "Overall, these functions work together to split the input text into chunks, generate a file name for the audio file, perform the TTS operation using the Edge TTS service, and merge the individual audio files into a single audio file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dviejų funkcijų implementacija: convert_to_text ir run_text_prompt, taip pat dviejų klasių deklaracija: str ir Audio.\n",
    "\n",
    "Funkcija convert_to_text priima audio_path kaip įvestį ir transkribuoją garso įrašą į tekstą naudodama modelį whisper_model. Funkcija pirmiausia patikrina, ar gpu flag yra nustatytas į True. Jei taip, whisper_model naudojamas su tam tikrais parametrais, tokiais kaip word_timestamps=True, fp16=True, language='English', ir task='translate'. Jei gpu flag yra False, whisper_model naudojamas su fp16=False. Gautas transkriptas vėliau išsaugomas faile 'scan.txt' ir grąžinamas kaip tekstas.\n",
    "\n",
    "Funkcija run_text_prompt priima message ir chat_history kaip įvestį. Ji naudoja phi_demo funkciją sugeneruoti atsakymą iš pokalbių roboto pagal įvestą message. Sugeneruotas atsakymas tuomet perduodamas talk funkcijai, kuri konvertuoja atsakymą į garso failą ir grąžina failo kelią. Audio klasė naudojama garso failui rodyti ir leisti. Garsas rodomas naudojant display funkciją iš IPython.display modulio, o Audio objektas sukuriamas su parametru autoplay=True, todėl garsas pradeda groti automatiškai. chat_history atnaujinamas su įvestu message ir sugeneruotu atsakymu, o grąžinami tuščias eilutė ir atnaujintas chat_history.\n",
    "\n",
    "str klasė yra integruota Python klasė, kuri atstovauja simbolių seką. Ji suteikia įvairius metodus darbui su eilutėmis, tokius kaip capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill ir kt. Šie metodai leidžia atlikti operacijas, tokias kaip paieška, keitimas, formatavimas ir eilutžių manipuliavimas.\n",
    "\n",
    "Audio klasė yra pasirinktinė klasė, kuri atstovauja garso objektą. Ji naudojama sukurti garso grotuvo Jupyter Notebook aplinkoje. Klasė priima įvairius parametrus, tokius kaip data, filename, url, embed, rate, autoplay ir normalize. Parametras data gali būti numpy array, mėginių sąrašas, eilutė, nurodanti failo pavadinimą arba URL, arba žali PCM duomenys. Parametras filename naudojamas nurodyti vietinį failą, iš kurio įkraunami garso duomenys, o parametras url naudojamas nurodyti URL, iš kurio atsisiunčiami garso duomenys. Parametras embed lemia, ar garso duomenys turėtų būti įterpti naudojant data URI, ar nuorodą į originalų šaltinį. Parametras rate nurodo garso duomenų ėmimo dažnį. Parametras autoplay nusako, ar garsas turėtų pradėti groti automatiškai. Parametras normalize nurodo, ar garso duomenys turėtų būti normalizuoti (permatuoti) iki didžiausio galimo diapazono. Audio klasė taip pat suteikia metodus, tokius kaip reload, skirtus perkrauti garso duomenis iš failo ar URL, ir atributus tokias kaip src_attr, autoplay_attr ir element_id_attr, kad gauti atitinkamus atributus garso elemento HTML.\n",
    "\n",
    "Apskritai, šios funkcijos ir klasės naudojamos garso transkripcijai į tekstą, garso atsakymų generavimui iš pokalbių roboto ir garso rodymui bei atkūrimui Jupyter Notebook aplinkoje.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Atsakomybės pareiškimas**:\nŠis dokumentas buvo išverstas naudojant dirbtinio intelekto vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors stengiamės užtikrinti tikslumą, atkreipkite dėmesį, kad automatizuoti vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turi būti laikomas autoritetingu šaltiniu. Kritinei informacijai rekomenduojame kreiptis į profesionalų vertėją. Mes neatsakome už jokius nesusipratimus ar neteisingus aiškinimus, kilusius naudojant šį vertimą.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-12-22T04:52:43+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "lt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}