# Pradėkite naudotis Phi-3 lokaliai

Šis vadovas padės jums paruošti savo vietinę aplinką, kad galėtumėte paleisti Phi-3 modelį naudodami Ollama. Modelį galite paleisti keliais skirtingais būdais, įskaitant GitHub Codespaces, VS Code Dev Containers arba savo vietinę aplinką.

## Aplinkos paruošimas

### GitHub Codespaces

Šį šabloną galite paleisti virtualiai naudodami GitHub Codespaces. Mygtukas atidarys naršyklėje veikiančią VS Code instanciją:

1. Atidarykite šabloną (tai gali užtrukti kelias minutes):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Atidarykite terminalo langą.

### VS Code Dev Containers

⚠️ Ši parinktis veiks tik tuo atveju, jei jūsų Docker Desktop yra paskirta bent 16 GB RAM. Jei turite mažiau nei 16 GB RAM, galite išbandyti [GitHub Codespaces parinktį](../../../../../md/01.Introduction/01) arba [paruošti aplinką lokaliai](../../../../../md/01.Introduction/01).

Kita susijusi parinktis yra VS Code Dev Containers, kuri atidarys projektą jūsų vietinėje VS Code naudojant [Dev Containers plėtinį](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Paleiskite Docker Desktop (įdiekite, jei dar neįdiegta).
2. Atidarykite projektą:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Kai atsidarys VS Code langas ir pasirodys projekto failai (tai gali užtrukti kelias minutes), atidarykite terminalo langą.
4. Tęskite su [diegimo žingsniais](../../../../../md/01.Introduction/01).

### Vietinė aplinka

1. Įsitikinkite, kad įdiegti šie įrankiai:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Modelio testavimas

1. Paprašykite Ollama atsisiųsti ir paleisti phi3:mini modelį:

    ```shell
    ollama run phi3:mini
    ```

    Tai užtruks kelias minutes, kol modelis bus atsisiųstas.

2. Kai išvestyje pamatysite „success“, galite išsiųsti žinutę modeliui iš komandinės eilutės.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Po kelių sekundžių turėtumėte pamatyti, kaip modelis pradeda siųsti atsakymą.

4. Norėdami sužinoti apie skirtingas technikas, naudojamas su kalbos modeliais, atidarykite Python užrašų knygelę [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) ir vykdykite kiekvieną langelį. Jei naudojote kitą modelį nei „phi3:mini“, pakeiskite `MODEL_NAME` pirmajame langelyje.

5. Norėdami bendrauti su phi3:mini modeliu iš Python, atidarykite Python failą [chat.py](../../../../../code/01.Introduce/chat.py) ir paleiskite jį. Jei reikia, galite pakeisti `MODEL_NAME` failo viršuje, taip pat galite modifikuoti sistemos žinutę arba pridėti pavyzdžių, jei norite.

---

**Atsakomybės apribojimas**:  
Šis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Kritinei informacijai rekomenduojama profesionali žmogaus vertimo paslauga. Mes neprisiimame atsakomybės už nesusipratimus ar klaidingus interpretavimus, atsiradusius naudojant šį vertimą.