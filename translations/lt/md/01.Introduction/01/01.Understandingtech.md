<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-09-12T14:47:55+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "lt"
}
-->
# Pagrindinės technologijos, kurios paminėtos

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) – žemo lygio API, skirtas aparatūros pagreitintam mašininio mokymosi procesui, sukurtas ant DirectX 12 pagrindo.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) – lygiagretaus skaičiavimo platforma ir programavimo sąsajos (API) modelis, sukurtas Nvidia, leidžiantis bendros paskirties apdorojimą grafikos procesoriuose (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) – atviras formatas, skirtas mašininio mokymosi modeliams atvaizduoti, užtikrinantis skirtingų ML sistemų tarpusavio suderinamumą.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) – formatas, naudojamas mašininio mokymosi modeliams atvaizduoti ir atnaujinti, ypač naudingas mažesniems kalbos modeliams, kurie efektyviai veikia CPU su 4-8 bitų kvantavimu.

## DirectML

DirectML yra žemo lygio API, leidžiantis aparatūros pagreitintą mašininį mokymąsi. Jis sukurtas ant DirectX 12 pagrindo, kad būtų galima pasinaudoti GPU pagreitinimu, ir yra nepriklausomas nuo tiekėjo, todėl nereikia keisti kodo, kad jis veiktų su skirtingais GPU tiekėjais. DirectML daugiausia naudojamas modelių mokymui ir inferencijai GPU.

Kalbant apie aparatūros palaikymą, DirectML yra sukurtas veikti su įvairiais GPU, įskaitant AMD integruotus ir diskretinius GPU, Intel integruotus GPU ir NVIDIA diskretinius GPU. Jis yra Windows AI Platformos dalis ir palaikomas Windows 10 ir 11, leidžiant modelių mokymą ir inferenciją bet kuriame Windows įrenginyje.

DirectML buvo atnaujintas ir suteikė naujų galimybių, pavyzdžiui, palaikant iki 150 ONNX operatorių ir naudojant tiek ONNX runtime, tiek WinML. Jį palaiko pagrindiniai integruotos aparatūros tiekėjai (IHV), kiekvienas įgyvendinantis įvairius metakomandas.

## CUDA

CUDA, arba Compute Unified Device Architecture, yra lygiagretaus skaičiavimo platforma ir programavimo sąsajos (API) modelis, sukurtas Nvidia. Jis leidžia programinės įrangos kūrėjams naudoti CUDA palaikantį grafikos procesorių (GPU) bendros paskirties apdorojimui – tai vadinama GPGPU (bendros paskirties skaičiavimas grafikos procesoriuose). CUDA yra pagrindinis Nvidia GPU pagreitinimo įrankis ir plačiai naudojamas įvairiose srityse, įskaitant mašininį mokymąsi, mokslinius skaičiavimus ir vaizdo apdorojimą.

Aparatūros palaikymas CUDA yra specifinis Nvidia GPU, nes tai yra patentuota Nvidia technologija. Kiekviena architektūra palaiko specifines CUDA įrankių rinkinio versijas, kurios suteikia reikalingas bibliotekas ir įrankius kūrėjams kurti ir vykdyti CUDA programas.

## ONNX

ONNX (Open Neural Network Exchange) yra atviras formatas, skirtas mašininio mokymosi modeliams atvaizduoti. Jis apibrėžia išplečiamą skaičiavimo grafų modelį, taip pat įmontuotų operatorių ir standartinių duomenų tipų apibrėžimus. ONNX leidžia kūrėjams perkelti modelius tarp skirtingų ML sistemų, užtikrinant tarpusavio suderinamumą ir palengvinant AI programų kūrimą bei diegimą.

Phi3 mini gali veikti su ONNX Runtime CPU ir GPU įvairiuose įrenginiuose, įskaitant serverių platformas, Windows, Linux ir Mac stalinius kompiuterius bei mobiliuosius CPU.
Optimizuotos konfigūracijos, kurias pridėjome, yra:

- ONNX modeliai int4 DML: Kvantuoti į int4 per AWQ
- ONNX modelis fp16 CUDA
- ONNX modelis int4 CUDA: Kvantuoti į int4 per RTN
- ONNX modelis int4 CPU ir mobiliesiems: Kvantuoti į int4 per RTN

## Llama.cpp

Llama.cpp yra atvirojo kodo programinės įrangos biblioteka, parašyta C++. Ji vykdo inferenciją įvairiuose dideliuose kalbos modeliuose (LLM), įskaitant Llama. Sukurta kartu su ggml biblioteka (bendros paskirties tensorų biblioteka), llama.cpp siekia užtikrinti greitesnę inferenciją ir mažesnį atminties naudojimą, palyginti su originalia Python implementacija. Ji palaiko aparatūros optimizaciją, kvantavimą ir siūlo paprastą API bei pavyzdžius. Jei jus domina efektyvi LLM inferencija, verta išbandyti llama.cpp, nes Phi3 gali veikti su Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) yra formatas, naudojamas mašininio mokymosi modeliams atvaizduoti ir atnaujinti. Jis ypač naudingas mažesniems kalbos modeliams (SLM), kurie efektyviai veikia CPU su 4-8 bitų kvantavimu. GGUF yra naudingas greitam prototipų kūrimui ir modelių vykdymui kraštiniuose įrenginiuose arba partijų užduotyse, tokiose kaip CI/CD procesai.

---

**Atsakomybės apribojimas**:  
Šis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Kritinei informacijai rekomenduojama profesionali žmogaus vertimo paslauga. Mes neprisiimame atsakomybės už nesusipratimus ar klaidingus interpretavimus, atsiradusius naudojant šį vertimą.