<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0b38834693bb497f96bf53f0d941f9a1",
  "translation_date": "2025-09-12T14:56:44+00:00",
  "source_file": "md/01.Introduction/02/04.Ollama.md",
  "language_code": "lt"
}
-->
## Phi šeima Ollama

[Ollama](https://ollama.com) leidžia daugiau žmonių tiesiogiai diegti atvirojo kodo LLM arba SLM naudojant paprastus scenarijus, taip pat kurti API, kurie padeda vietinėse Copilot taikymo situacijose.

## **1. Įdiegimas**

Ollama palaiko veikimą Windows, macOS ir Linux operacinėse sistemose. Ollama galite įdiegti per šią nuorodą ([https://ollama.com/download](https://ollama.com/download)). Po sėkmingo įdiegimo galite tiesiogiai naudoti Ollama scenarijų, kad terminalo lange iškviestumėte Phi-3. Visas [galimas bibliotekas Ollama](https://ollama.com/library) galite peržiūrėti čia. Jei atidarysite šį saugyklą Codespace aplinkoje, Ollama jau bus įdiegta.

```bash

ollama run phi4

```

> [!NOTE]
> Modelis bus atsisiųstas pirmą kartą jį paleidus. Žinoma, galite tiesiogiai nurodyti atsisiųstą Phi-4 modelį. Kaip pavyzdį naudojame WSL, kad paleistume komandą. Po sėkmingo modelio atsisiuntimo galite tiesiogiai sąveikauti terminale.

![run](../../../../../imgs/01/02/04/ollama_run.png)

## **2. Phi-4 API iškvietimas per Ollama**

Jei norite iškviesti Phi-4 API, sukurtą Ollama, terminale galite naudoti šią komandą, kad paleistumėte Ollama serverį.

```bash

ollama serve

```

> [!NOTE]
> Jei naudojate macOS arba Linux, atkreipkite dėmesį, kad galite susidurti su šia klaida **"Error: listen tcp 127.0.0.1:11434: bind: address already in use"**. Ši klaida gali pasirodyti paleidžiant komandą. Galite ignoruoti šią klaidą, nes ji paprastai reiškia, kad serveris jau veikia, arba galite sustabdyti ir iš naujo paleisti Ollama:

**macOS**

```bash

brew services restart ollama

```

**Linux**

```bash

sudo systemctl stop ollama

```

Ollama palaiko du API: generate ir chat. Galite iškviesti Ollama teikiamą modelio API pagal savo poreikius, siųsdami užklausas į vietinę paslaugą, veikiančią 11434 prievade.

**Chat**

```bash

curl http://127.0.0.1:11434/api/chat -d '{
  "model": "phi3",
  "messages": [
    {
      "role": "system",
      "content": "Your are a python developer."
    },
    {
      "role": "user",
      "content": "Help me generate a bubble algorithm"
    }
  ],
  "stream": false
  
}'

This is the result in Postman

![Screenshot of JSON results for generate request](../../../../../imgs/01/02/04/ollama_gen.png)

## Additional Resources

Check the list of available models in Ollama in [their library](https://ollama.com/library).

Pull your model from the Ollama server using this command

```bash
ollama pull phi4
```

Run the model using this command

```bash
ollama run phi4
```

***Note:*** Visit this link [https://github.com/ollama/ollama/blob/main/docs/api.md](https://github.com/ollama/ollama/blob/main/docs/api.md) to learn more

## Calling Ollama from Python

You can use `requests` or `urllib3` to make requests to the local server endpoints used above. However, a popular way to use Ollama in Python is via the [openai](https://pypi.org/project/openai/) SDK, since Ollama provides OpenAI-compatible server endpoints as well.

Here is an example for phi3-mini:

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="nokeyneeded",
)

response = client.chat.completions.create(
    model="phi4",
    temperature=0.7,
    n=1,
    messages=[
        {"role": "system", "content": "Jūs esate naudingas asistentas."},
        {"role": "user", "content": "Parašyk haiku apie alkaną katę"},
    ],
)

print("Atsakymas:")
print(response.choices[0].message.content)
```

## Calling Ollama from JavaScript 

```javascript
// Pavyzdys: failo santrauka su Phi-4
script({
    model: "ollama:phi4",
    title: "Santrauka su Phi-4",
    system: ["system"],
})

// Santraukos pavyzdys
const file = def("FILE", env.files)
$`Santrauka ${file} viename paragrafu.`
```

## Calling Ollama from C#

Create a new C# Console application and add the following NuGet package:

```bash
dotnet add package Microsoft.SemanticKernel --version 1.34.0
```

Then replace this code in the `Program.cs` file

```csharp
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.ChatCompletion;

// pridėti pokalbio užbaigimo paslaugą naudojant vietinį Ollama serverio adresą
#pragma warning disable SKEXP0001, SKEXP0003, SKEXP0010, SKEXP0011, SKEXP0050, SKEXP0052
builder.AddOpenAIChatCompletion(
    modelId: "phi4",
    endpoint: new Uri("http://localhost:11434/"),
    apiKey: "non required");

// pateikti paprastą užklausą pokalbio paslaugai
string prompt = "Parašyk pokštą apie kačiukus";
var response = await kernel.InvokePromptAsync(prompt);
Console.WriteLine(response.GetValue<string>());
```

Run the app with the command:

```bash
dotnet run

---

**Atsakomybės apribojimas**:  
Šis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, atkreipkite dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Kritinei informacijai rekomenduojama profesionali žmogaus vertimo paslauga. Mes neprisiimame atsakomybės už nesusipratimus ar klaidingus interpretavimus, atsiradusius naudojant šį vertimą.