{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaktywny Chatbot Phi 3 Mini 4K Instruct z Whisper\n",
    "\n",
    "### Wprowadzenie:\n",
    "Interaktywny Chatbot Phi 3 Mini 4K Instruct to narzędzie, które umożliwia użytkownikom interakcję z demonstracją Microsoft Phi 3 Mini 4K Instruct za pomocą tekstu lub dźwięku. Chatbot może być używany do różnych zadań, takich jak tłumaczenie, aktualizacje pogodowe czy zbieranie ogólnych informacji.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "Utwórz swój Huggingface Access Token\n",
    "\n",
    "Utwórz nowy token  \n",
    "Podaj nową nazwę  \n",
    "Wybierz uprawnienia do zapisu  \n",
    "Skopiuj token i zapisz go w bezpiecznym miejscu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższy kod w Pythonie wykonuje dwa główne zadania: importowanie modułu `os` oraz ustawianie zmiennej środowiskowej.\n",
    "\n",
    "1. Importowanie modułu `os`:\n",
    "   - Moduł `os` w Pythonie umożliwia interakcję z systemem operacyjnym. Pozwala na wykonywanie różnych zadań związanych z systemem operacyjnym, takich jak dostęp do zmiennych środowiskowych, operacje na plikach i katalogach itp.\n",
    "   - W tym kodzie moduł `os` jest importowany za pomocą instrukcji `import`. Ta instrukcja udostępnia funkcjonalność modułu `os` do użycia w bieżącym skrypcie Pythona.\n",
    "\n",
    "2. Ustawianie zmiennej środowiskowej:\n",
    "   - Zmienna środowiskowa to wartość, do której mogą mieć dostęp programy działające w systemie operacyjnym. Jest to sposób przechowywania ustawień konfiguracyjnych lub innych informacji, które mogą być używane przez wiele programów.\n",
    "   - W tym kodzie nowa zmienna środowiskowa jest ustawiana za pomocą słownika `os.environ`. Kluczem w tym słowniku jest `'HF_TOKEN'`, a wartość jest przypisywana z zmiennej `HUGGINGFACE_TOKEN`.\n",
    "   - Zmienna `HUGGINGFACE_TOKEN` jest zdefiniowana tuż nad tym fragmentem kodu i przypisywana jest jej wartość tekstowa `\"hf_**************\"` za pomocą składni `#@param`. Ta składnia jest często używana w notebookach Jupyter, aby umożliwić wprowadzanie danych przez użytkownika oraz konfigurację parametrów bezpośrednio w interfejsie notebooka.\n",
    "   - Ustawiając zmienną środowiskową `'HF_TOKEN'`, można uzyskać do niej dostęp w innych częściach programu lub w innych programach działających na tym samym systemie operacyjnym.\n",
    "\n",
    "Podsumowując, ten kod importuje moduł `os` i ustawia zmienną środowiskową o nazwie `'HF_TOKEN'` z wartością podaną w zmiennej `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten fragment kodu definiuje funkcję o nazwie clear_output, która służy do czyszczenia wyników w bieżącej komórce w Jupyter Notebook lub IPython. Przyjrzyjmy się kodowi i zrozummy jego działanie:\n",
    "\n",
    "Funkcja clear_output przyjmuje jeden parametr o nazwie wait, który jest wartością logiczną (boolean). Domyślnie wait jest ustawiony na False. Ten parametr określa, czy funkcja powinna poczekać, aż nowe wyniki będą dostępne do zastąpienia istniejących, zanim je wyczyści.\n",
    "\n",
    "Sama funkcja służy do czyszczenia wyników w bieżącej komórce. W Jupyter Notebook lub IPython, gdy komórka generuje wyniki, takie jak tekst wyjściowy czy wykresy, są one wyświetlane poniżej komórki. Funkcja clear_output pozwala na usunięcie tych wyników.\n",
    "\n",
    "Implementacja funkcji nie została podana w fragmencie kodu, co wskazuje na obecność wielokropka (...). Wielokropek reprezentuje miejsce na właściwy kod, który realizuje czyszczenie wyników. Implementacja funkcji może polegać na interakcji z API Jupyter Notebook lub IPython w celu usunięcia istniejących wyników z komórki.\n",
    "\n",
    "Podsumowując, funkcja ta zapewnia wygodny sposób na czyszczenie wyników w bieżącej komórce w Jupyter Notebook lub IPython, co ułatwia zarządzanie i aktualizowanie wyświetlanych wyników podczas interaktywnych sesji kodowania.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykonaj syntezę mowy (TTS) za pomocą usługi Edge TTS. Przejdźmy przez odpowiednie implementacje funkcji krok po kroku:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Ta funkcja przyjmuje wartość wejściową i oblicza ciąg szybkości dla głosu TTS. Wartość wejściowa reprezentuje pożądaną prędkość mowy, gdzie wartość 1 oznacza normalną prędkość. Funkcja oblicza ciąg szybkości, odejmując 1 od wartości wejściowej, mnożąc wynik przez 100, a następnie określając znak na podstawie tego, czy wartość wejściowa jest większa lub równa 1. Funkcja zwraca ciąg szybkości w formacie \"{sign}{rate}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: Ta funkcja przyjmuje tekst wejściowy i język jako parametry. Dzieli tekst wejściowy na fragmenty na podstawie reguł specyficznych dla języka. W tej implementacji, jeśli język to \"English\", funkcja dzieli tekst na kropkach (\".\"), usuwając wszelkie wiodące lub końcowe spacje. Następnie dodaje kropkę do każdego fragmentu i zwraca przefiltrowaną listę fragmentów.\n",
    "\n",
    "3. `tts_file_name(text)`: Ta funkcja generuje nazwę pliku dla pliku audio TTS na podstawie tekstu wejściowego. Wykonuje kilka transformacji na tekście: usuwa końcową kropkę (jeśli występuje), konwertuje tekst na małe litery, usuwa wiodące i końcowe spacje oraz zastępuje spacje podkreśleniami. Następnie skraca tekst do maksymalnie 25 znaków (jeśli jest dłuższy) lub używa pełnego tekstu, jeśli jest pusty. Na końcu generuje losowy ciąg za pomocą modułu [`uuid`] i łączy go ze skróconym tekstem, aby utworzyć nazwę pliku w formacie \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Ta funkcja łączy wiele plików audio w jeden plik audio. Przyjmuje listę ścieżek plików audio oraz ścieżkę wyjściową jako parametry. Funkcja inicjalizuje pusty obiekt `AudioSegment` o nazwie [`merged_audio`]. Następnie iteruje przez każdą ścieżkę pliku audio, ładuje plik audio za pomocą metody `AudioSegment.from_file()` z biblioteki `pydub` i dodaje bieżący plik audio do obiektu [`merged_audio`]. Na końcu eksportuje połączony plik audio do określonej ścieżki wyjściowej w formacie MP3.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: Ta funkcja wykonuje operację TTS za pomocą usługi Edge TTS. Przyjmuje listę fragmentów tekstu, prędkość mowy, nazwę głosu oraz ścieżkę zapisu jako parametry. Jeśli liczba fragmentów jest większa niż 1, funkcja tworzy katalog do przechowywania indywidualnych plików audio fragmentów. Następnie iteruje przez każdy fragment, konstruuje polecenie Edge TTS za pomocą funkcji `calculate_rate_string()`, nazwy głosu i tekstu fragmentu, a następnie wykonuje polecenie za pomocą funkcji `os.system()`. Jeśli wykonanie polecenia zakończy się sukcesem, dodaje ścieżkę wygenerowanego pliku audio do listy. Po przetworzeniu wszystkich fragmentów, łączy indywidualne pliki audio za pomocą funkcji `merge_audio_files()` i zapisuje połączony plik audio w określonej ścieżce zapisu. Jeśli jest tylko jeden fragment, bezpośrednio generuje polecenie Edge TTS i zapisuje audio w ścieżce zapisu. Na końcu zwraca ścieżkę zapisu wygenerowanego pliku audio.\n",
    "\n",
    "6. `random_audio_name_generate()`: Ta funkcja generuje losową nazwę pliku audio za pomocą modułu [`uuid`]. Generuje losowy UUID, konwertuje go na ciąg znaków, pobiera pierwsze 8 znaków, dodaje rozszerzenie \".mp3\" i zwraca losową nazwę pliku audio.\n",
    "\n",
    "7. `talk(input_text)`: Ta funkcja jest głównym punktem wejścia do wykonania operacji TTS. Przyjmuje tekst wejściowy jako parametr. Najpierw sprawdza długość tekstu wejściowego, aby określić, czy jest to długie zdanie (większe lub równe 600 znaków). Na podstawie długości i wartości zmiennej `translate_text_flag` określa język i generuje listę fragmentów tekstu za pomocą funkcji `make_chunks()`. Następnie generuje ścieżkę zapisu dla pliku audio za pomocą funkcji `random_audio_name_generate()`. Na końcu wywołuje funkcję `edge_free_tts()`, aby wykonać operację TTS, i zwraca ścieżkę zapisu wygenerowanego pliku audio.\n",
    "\n",
    "Podsumowując, te funkcje współpracują ze sobą, aby podzielić tekst wejściowy na fragmenty, wygenerować nazwę pliku dla pliku audio, wykonać operację TTS za pomocą usługi Edge TTS oraz połączyć indywidualne pliki audio w jeden plik audio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementacja dwóch funkcji: convert_to_text i run_text_prompt, oraz deklaracja dwóch klas: str i Audio.\n",
    "\n",
    "Funkcja convert_to_text przyjmuje jako argument audio_path i transkrybuje dźwięk na tekst za pomocą modelu o nazwie whisper_model. Funkcja najpierw sprawdza, czy flaga gpu jest ustawiona na True. Jeśli tak, whisper_model jest używany z określonymi parametrami, takimi jak word_timestamps=True, fp16=True, language='English' i task='translate'. Jeśli flaga gpu jest ustawiona na False, whisper_model jest używany z fp16=False. Wynikowa transkrypcja jest następnie zapisywana do pliku o nazwie 'scan.txt' i zwracana jako tekst.\n",
    "\n",
    "Funkcja run_text_prompt przyjmuje jako argumenty message i chat_history. Wykorzystuje funkcję phi_demo do wygenerowania odpowiedzi od chatbota na podstawie podanej wiadomości. Wygenerowana odpowiedź jest następnie przekazywana do funkcji talk, która konwertuje odpowiedź na plik audio i zwraca ścieżkę do tego pliku. Klasa Audio jest używana do wyświetlania i odtwarzania pliku audio. Dźwięk jest wyświetlany za pomocą funkcji display z modułu IPython.display, a obiekt Audio jest tworzony z parametrem autoplay=True, dzięki czemu dźwięk zaczyna się odtwarzać automatycznie. chat_history jest aktualizowany o podaną wiadomość i wygenerowaną odpowiedź, a następnie zwracane są pusty ciąg znaków oraz zaktualizowana historia czatu.\n",
    "\n",
    "Klasa str to wbudowana klasa w Pythonie, która reprezentuje sekwencję znaków. Udostępnia różne metody do manipulacji i pracy z ciągami znaków, takie jak capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill i inne. Te metody pozwalają na wykonywanie operacji takich jak wyszukiwanie, zastępowanie, formatowanie i manipulowanie ciągami znaków.\n",
    "\n",
    "Klasa Audio to niestandardowa klasa, która reprezentuje obiekt audio. Jest używana do tworzenia odtwarzacza audio w środowisku Jupyter Notebook. Klasa akceptuje różne parametry, takie jak data, filename, url, embed, rate, autoplay i normalize. Parametr data może być tablicą numpy, listą próbek, ciągiem znaków reprezentującym nazwę pliku lub URL, albo surowymi danymi PCM. Parametr filename służy do określenia lokalnego pliku, z którego mają być załadowane dane audio, a parametr url służy do określenia URL, z którego mają być pobrane dane audio. Parametr embed określa, czy dane audio powinny być osadzone za pomocą URI danych, czy odwoływać się do oryginalnego źródła. Parametr rate określa częstotliwość próbkowania danych audio. Parametr autoplay decyduje, czy dźwięk powinien zacząć się odtwarzać automatycznie. Parametr normalize określa, czy dane audio powinny być normalizowane (przeskalowane) do maksymalnego możliwego zakresu. Klasa Audio udostępnia również metody, takie jak reload do ponownego załadowania danych audio z pliku lub URL, oraz atrybuty, takie jak src_attr, autoplay_attr i element_id_attr do pobierania odpowiednich atrybutów dla elementu audio w HTML.\n",
    "\n",
    "Podsumowując, te funkcje i klasy są używane do transkrypcji dźwięku na tekst, generowania odpowiedzi audio od chatbota oraz wyświetlania i odtwarzania dźwięku w środowisku Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Zastrzeżenie**:  \nTen dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy wszelkich starań, aby tłumaczenie było precyzyjne, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być uznawany za źródło autorytatywne. W przypadku informacji o kluczowym znaczeniu zaleca się skorzystanie z profesjonalnego tłumaczenia przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T16:39:45+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "pl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}