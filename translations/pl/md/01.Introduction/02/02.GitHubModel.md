<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "fb67a08b9fc911a10ed58081fadef416",
  "translation_date": "2025-05-09T08:49:02+00:00",
  "source_file": "md/01.Introduction/02/02.GitHubModel.md",
  "language_code": "pl"
}
-->
## Rodzina Phi w GitHub Models

Witamy w [GitHub Models](https://github.com/marketplace/models)! Mamy wszystko gotowe, abyś mógł eksplorować modele AI hostowane na Azure AI.

![GitHubModel](../../../../../translated_images/GitHub_ModelCatalog.4fc858ab26afe64c43f5e423ad0c5c733878bb536fdb027a5bcf1f80c41b0633.pl.png)

Aby uzyskać więcej informacji o dostępnych modelach w GitHub Models, zajrzyj na [GitHub Model Marketplace](https://github.com/marketplace/models)

## Dostępne modele

Każdy model ma dedykowane środowisko testowe i przykładowy kod

![Phi-4Model_Github](../../../../../translated_images/GitHub_ModelPlay.998e294f6ee69c3ca174c880b32af9feec4221d0d787de899ad9bb2da3b58981.pl.png)

### Rodzina Phi w katalogu GitHub Model

- [Phi-4](https://github.com/marketplace/models/azureml/Phi-4)

- [Phi-3.5-MoE instruct (128k)](https://github.com/marketplace/models/azureml/Phi-3-5-MoE-instruct)

- [Phi-3.5-vision instruct (128k)](https://github.com/marketplace/models/azureml/Phi-3-5-vision-instruct)

- [Phi-3.5-mini instruct (128k)](https://github.com/marketplace/models/azureml/Phi-3-5-mini-instruct)

- [Phi-3-Medium-128k-Instruct](https://github.com/marketplace/models/azureml/Phi-3-medium-128k-instruct)

- [Phi-3-medium-4k-instruct](https://github.com/marketplace/models/azureml/Phi-3-medium-4k-instruct)

- [Phi-3-mini-128k-instruct](https://github.com/marketplace/models/azureml/Phi-3-mini-128k-instruct)

- [Phi-3-mini-4k-instruct](https://github.com/marketplace/models/azureml/Phi-3-mini-4k-instruct)

- [Phi-3-small-128k-instruct](https://github.com/marketplace/models/azureml/Phi-3-small-128k-instruct)

- [Phi-3-small-8k-instruct](https://github.com/marketplace/models/azureml/Phi-3-small-8k-instruct)

## Pierwsze kroki

Dostępne są podstawowe przykłady gotowe do uruchomienia. Znajdziesz je w katalogu samples. Jeśli chcesz od razu przejść do swojego ulubionego języka, przykłady są dostępne w następujących językach:

- Python
- JavaScript
- C#
- Java
- cURL

Dostępne jest również dedykowane środowisko Codespaces do uruchamiania przykładów i modeli.

![Getting Started](../../../../../translated_images/GitHub_ModelGetStarted.b4b839a081583da39bc976c2f0d8ac4603d3b8c23194b16cc9e0a1014f5611d0.pl.png)


## Przykładowy kod

Poniżej znajdziesz fragmenty przykładowego kodu dla kilku zastosowań. Aby uzyskać dodatkowe informacje o Azure AI Inference SDK, zobacz pełną dokumentację i przykłady.

## Konfiguracja

1. Utwórz personalny token dostępu  
Nie musisz nadawać żadnych uprawnień tokenowi. Pamiętaj, że token będzie wysyłany do usługi Microsoft.

Aby użyć poniższych fragmentów kodu, utwórz zmienną środowiskową i ustaw w niej swój token jako klucz dla kodu klienta.

Jeśli korzystasz z bash:  
```
export GITHUB_TOKEN="<your-github-token-goes-here>"
```  
Jeśli używasz powershell:  

```
$Env:GITHUB_TOKEN="<your-github-token-goes-here>"
```  

Jeśli korzystasz z Windows command prompt:  

```
set GITHUB_TOKEN=<your-github-token-goes-here>
```  

## Przykład w Pythonie

### Instalacja zależności  
Zainstaluj Azure AI Inference SDK za pomocą pip (wymagane: Python >=3.8):

```
pip install azure-ai-inference
```  
### Uruchom podstawowy przykład kodu

Ten przykład pokazuje podstawowe wywołanie API chat completion. Wykorzystuje punkt końcowy inferencji modelu AI GitHub oraz Twój token GitHub. Wywołanie jest synchroniczne.

```python
import os
from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.models import SystemMessage, UserMessage
from azure.core.credentials import AzureKeyCredential

endpoint = "https://models.inference.ai.azure.com"
model_name = "Phi-4"
token = os.environ["GITHUB_TOKEN"]

client = ChatCompletionsClient(
    endpoint=endpoint,
    credential=AzureKeyCredential(token),
)

response = client.complete(
    messages=[
        UserMessage(content="I have $20,000 in my savings account, where I receive a 4% profit per year and payments twice a year. Can you please tell me how long it will take for me to become a millionaire? Also, can you please explain the math step by step as if you were explaining it to an uneducated person?"),
    ],
    temperature=0.4,
    top_p=1.0,
    max_tokens=2048,
    model=model_name
)

print(response.choices[0].message.content)
```

### Uruchom konwersację wieloetapową

Ten przykład pokazuje wieloetapową rozmowę z API chat completion. Korzystając z modelu w aplikacji czatu, musisz zarządzać historią rozmowy i wysyłać do modelu najnowsze wiadomości.

```
import os
from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage
from azure.core.credentials import AzureKeyCredential

token = os.environ["GITHUB_TOKEN"]
endpoint = "https://models.inference.ai.azure.com"
# Replace Model_Name
model_name = "Phi-4"

client = ChatCompletionsClient(
    endpoint=endpoint,
    credential=AzureKeyCredential(token),
)

messages = [
    SystemMessage(content="You are a helpful assistant."),
    UserMessage(content="What is the capital of France?"),
    AssistantMessage(content="The capital of France is Paris."),
    UserMessage(content="What about Spain?"),
]

response = client.complete(messages=messages, model=model_name)

print(response.choices[0].message.content)
```

### Strumieniowanie odpowiedzi

Dla lepszego doświadczenia użytkownika warto strumieniować odpowiedź modelu, aby pierwszy token pojawił się szybko i uniknąć oczekiwania na długie odpowiedzi.

```
import os
from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.models import SystemMessage, UserMessage
from azure.core.credentials import AzureKeyCredential

token = os.environ["GITHUB_TOKEN"]
endpoint = "https://models.inference.ai.azure.com"
# Replace Model_Name
model_name = "Phi-4"

client = ChatCompletionsClient(
    endpoint=endpoint,
    credential=AzureKeyCredential(token),
)

response = client.complete(
    stream=True,
    messages=[
        SystemMessage(content="You are a helpful assistant."),
        UserMessage(content="Give me 5 good reasons why I should exercise every day."),
    ],
    model=model_name,
)

for update in response:
    if update.choices:
        print(update.choices[0].delta.content or "", end="")

client.close()
```

## Darmowe użycie i limity dla GitHub Models

![Model Catalog](../../../../../translated_images/GitHub_Model.0c2abb992151c5407046e2b763af51505ff709f04c0950785e0300fdc8c55a0c.pl.png)

[Limity szybkości dla środowiska testowego i darmowego API](https://docs.github.com/en/github-models/prototyping-with-ai-models#rate-limits) mają na celu umożliwić eksperymentowanie z modelami i prototypowanie aplikacji AI. Aby korzystać poza tymi limitami i skalować swoją aplikację, musisz założyć zasoby w koncie Azure i uwierzytelnić się tam zamiast używać personalnego tokena GitHub. Nie musisz zmieniać niczego w swoim kodzie. Skorzystaj z tego linku, aby dowiedzieć się, jak wyjść poza darmowe limity w Azure AI.

### Informacje dodatkowe

Pamiętaj, że eksperymentując z modelem, korzystasz z AI, więc mogą pojawić się błędy w treści.

Funkcjonalność podlega różnym ograniczeniom (w tym liczbie zapytań na minutę, na dzień, liczbie tokenów na zapytanie oraz liczbie równoczesnych zapytań) i nie jest przeznaczona do zastosowań produkcyjnych.

GitHub Models korzysta z Azure AI Content Safety. Te filtry nie mogą być wyłączone w ramach doświadczenia GitHub Models. Jeśli zdecydujesz się korzystać z modeli poprzez płatną usługę, skonfiguruj filtry treści zgodnie z własnymi wymaganiami.

Ta usługa jest objęta warunkami wstępnej wersji GitHub.

**Zastrzeżenie**:  
Niniejszy dokument został przetłumaczony przy użyciu usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mimo że dążymy do dokładności, prosimy mieć na uwadze, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w języku źródłowym powinien być uznawany za źródło autorytatywne. W przypadku istotnych informacji zalecane jest skorzystanie z profesjonalnego tłumaczenia wykonanego przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z korzystania z tego tłumaczenia.