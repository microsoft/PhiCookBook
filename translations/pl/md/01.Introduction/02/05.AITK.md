<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4951d458c0b60c02cd1e751b40903877",
  "translation_date": "2025-07-16T19:25:42+00:00",
  "source_file": "md/01.Introduction/02/05.AITK.md",
  "language_code": "pl"
}
-->
# Rodzina Phi w AITK

[AI Toolkit for VS Code](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio) ułatwia tworzenie aplikacji generatywnej AI, łącząc najnowocześniejsze narzędzia i modele AI z katalogu Azure AI Foundry oraz innych katalogów, takich jak Hugging Face. Będziesz mógł przeglądać katalog modeli AI zasilany przez GitHub Models i Azure AI Foundry Model Catalogs, pobierać je lokalnie lub zdalnie, dostrajać, testować i używać w swojej aplikacji.

Podgląd AI Toolkit będzie działał lokalnie. Lokalna inferencja lub dostrajanie zależy od wybranego modelu, może być wymagane posiadanie GPU, takiego jak NVIDIA CUDA GPU. Możesz również uruchamiać modele GitHub bezpośrednio za pomocą AITK.

## Pierwsze kroki

[Dowiedz się więcej, jak zainstalować Windows Subsystem for Linux](https://learn.microsoft.com/windows/wsl/install?WT.mc_id=aiml-137032-kinfeylo)

oraz [jak zmienić domyślną dystrybucję](https://learn.microsoft.com/windows/wsl/install#change-the-default-linux-distribution-installed).

[Repozytorium AI Toolkit na GitHub](https://github.com/microsoft/vscode-ai-toolkit/)

- Windows, Linux, macOS
  
- Do dostrajania na Windows i Linux potrzebujesz karty Nvidia GPU. Dodatkowo, **Windows** wymaga subsystemu Linux z dystrybucją Ubuntu 18.4 lub nowszą. [Dowiedz się więcej, jak zainstalować Windows Subsystem for Linux](https://learn.microsoft.com/windows/wsl/install) oraz [jak zmienić domyślną dystrybucję](https://learn.microsoft.com/windows/wsl/install#change-the-default-linux-distribution-installed).

### Instalacja AI Toolkit

AI Toolkit jest dostarczany jako [rozszerzenie Visual Studio Code](https://code.visualstudio.com/docs/setup/additional-components#_vs-code-extensions), więc najpierw musisz zainstalować [VS Code](https://code.visualstudio.com/docs/setup/windows?WT.mc_id=aiml-137032-kinfeylo), a następnie pobrać AI Toolkit z [VS Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio).  
[AI Toolkit jest dostępny w Visual Studio Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio) i można go zainstalować jak każde inne rozszerzenie VS Code.

Jeśli nie masz doświadczenia z instalacją rozszerzeń VS Code, wykonaj następujące kroki:

### Logowanie

1. W pasku aktywności w VS Code wybierz **Extensions**  
1. W polu wyszukiwania rozszerzeń wpisz "AI Toolkit"  
1. Wybierz "AI Toolkit for Visual Studio code"  
1. Kliknij **Install**

Teraz możesz korzystać z rozszerzenia!

Zostaniesz poproszony o zalogowanie się do GitHub, kliknij "Allow", aby kontynuować. Zostaniesz przekierowany na stronę logowania GitHub.

Zaloguj się i postępuj zgodnie z instrukcjami. Po pomyślnym zalogowaniu zostaniesz przekierowany z powrotem do VS Code.

Po zainstalowaniu rozszerzenia w pasku aktywności pojawi się ikona AI Toolkit.

Poznajmy dostępne funkcje!

### Dostępne funkcje

Główny pasek boczny AI Toolkit jest podzielony na  

- **Models**  
- **Resources**  
- **Playground**  
- **Fine-tuning**  
- **Evaluation**

Znajdziesz je w sekcji Resources. Aby zacząć, wybierz **Model Catalog**.

### Pobieranie modelu z katalogu

Po uruchomieniu AI Toolkit z paska bocznego VS Code możesz wybrać jedną z następujących opcji:

![AI toolkit model catalog](../../../../../translated_images/AItoolkitmodel_catalog.7a7be6a7d8468d310ae1dc2cdb2d42add99d7607b5e0e838db7924d4d25e8475.pl.png)

- Znajdź obsługiwany model w **Model Catalog** i pobierz go lokalnie  
- Przetestuj inferencję modelu w **Model Playground**  
- Dostrój model lokalnie lub zdalnie w **Model Fine-tuning**  
- Wdróż dostrojone modele do chmury za pomocą palety poleceń AI Toolkit  
- Ewaluacja modeli

> [!NOTE]
>
> **GPU kontra CPU**
>
> Zauważysz, że karty modeli pokazują rozmiar modelu, platformę i typ akceleratora (CPU, GPU). Dla zoptymalizowanej wydajności na **urządzeniach Windows z co najmniej jednym GPU**, wybierz wersje modeli przeznaczone wyłącznie dla Windows.
>
> Zapewnia to model zoptymalizowany pod akcelerator DirectML.
>
> Nazwy modeli mają format
>
> - `{model_name}-{accelerator}-{quantization}-{format}`.
>
>Aby sprawdzić, czy masz GPU na swoim urządzeniu z Windows, otwórz **Menedżer zadań** i przejdź do zakładki **Wydajność**. Jeśli masz GPU, będą one widoczne pod nazwami takimi jak "GPU 0" lub "GPU 1".

### Uruchamianie modelu w playground

Po ustawieniu wszystkich parametrów kliknij **Generate Project**.

Gdy model zostanie pobrany, wybierz **Load in Playground** na karcie modelu w katalogu:

- Rozpocznij pobieranie modelu  
- Zainstaluj wszystkie wymagane zależności  
- Utwórz środowisko pracy w VS Code

![Load model in playground](../../../../../translated_images/AItoolkitload_model_into_playground.dcef5355b1653b52e1f675d80cd429100cfe0c5d6a316ff331f3ae10923bca38.pl.png)

### Korzystanie z REST API w Twojej aplikacji

AI Toolkit zawiera lokalny serwer REST API **na porcie 5272**, który korzysta z [formatu OpenAI chat completions](https://platform.openai.com/docs/api-reference/chat/create).

Pozwala to testować aplikację lokalnie bez konieczności korzystania z chmurowej usługi AI. Na przykład poniższy plik JSON pokazuje, jak skonfigurować ciało żądania:

```json
{
    "model": "Phi-4",
    "messages": [
        {
            "role": "user",
            "content": "what is the golden ratio?"
        }
    ],
    "temperature": 0.7,
    "top_p": 1,
    "top_k": 10,
    "max_tokens": 100,
    "stream": true
}
```

REST API możesz testować za pomocą (np.) [Postman](https://www.postman.com/) lub narzędzia CURL (Client URL):

```bash
curl -vX POST http://127.0.0.1:5272/v1/chat/completions -H 'Content-Type: application/json' -d @body.json
```

### Korzystanie z biblioteki OpenAI dla Pythona

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://127.0.0.1:5272/v1/", 
    api_key="x" # required for the API but not used
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "what is the golden ratio?",
        }
    ],
    model="Phi-4",
)

print(chat_completion.choices[0].message.content)
```

### Korzystanie z biblioteki Azure OpenAI dla .NET

Dodaj [bibliotekę Azure OpenAI dla .NET](https://www.nuget.org/packages/Azure.AI.OpenAI/) do swojego projektu za pomocą NuGet:

```bash
dotnet add {project_name} package Azure.AI.OpenAI --version 1.0.0-beta.17
```

Dodaj do projektu plik C# o nazwie **OverridePolicy.cs** i wklej następujący kod:

```csharp
// OverridePolicy.cs
using Azure.Core.Pipeline;
using Azure.Core;

internal partial class OverrideRequestUriPolicy(Uri overrideUri)
    : HttpPipelineSynchronousPolicy
{
    private readonly Uri _overrideUri = overrideUri;

    public override void OnSendingRequest(HttpMessage message)
    {
        message.Request.Uri.Reset(_overrideUri);
    }
}
```

Następnie wklej poniższy kod do pliku **Program.cs**:

```csharp
// Program.cs
using Azure.AI.OpenAI;

Uri localhostUri = new("http://localhost:5272/v1/chat/completions");

OpenAIClientOptions clientOptions = new();
clientOptions.AddPolicy(
    new OverrideRequestUriPolicy(localhostUri),
    Azure.Core.HttpPipelinePosition.BeforeTransport);
OpenAIClient client = new(openAIApiKey: "unused", clientOptions);

ChatCompletionsOptions options = new()
{
    DeploymentName = "Phi-4",
    Messages =
    {
        new ChatRequestSystemMessage("You are a helpful assistant. Be brief and succinct."),
        new ChatRequestUserMessage("What is the golden ratio?"),
    }
};

StreamingResponse<StreamingChatCompletionsUpdate> streamingChatResponse
    = await client.GetChatCompletionsStreamingAsync(options);

await foreach (StreamingChatCompletionsUpdate chatChunk in streamingChatResponse)
{
    Console.Write(chatChunk.ContentUpdate);
}
```


## Dostrajanie z AI Toolkit

- Zacznij od odkrywania modeli i playground  
- Dostrajanie modeli i inferencja z wykorzystaniem lokalnych zasobów obliczeniowych  
- Zdalne dostrajanie i inferencja z użyciem zasobów Azure

[Dostrajanie z AI Toolkit](../../03.FineTuning/Finetuning_VSCodeaitoolkit.md)

## Zasoby Q&A AI Toolkit

Zapoznaj się z naszą [stroną Q&A](https://github.com/microsoft/vscode-ai-toolkit/blob/main/archive/QA.md) dotyczącą najczęstszych problemów i ich rozwiązań.

**Zastrzeżenie**:  
Niniejszy dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mimo że dążymy do dokładności, prosimy mieć na uwadze, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w języku źródłowym powinien być uznawany za źródło autorytatywne. W przypadku informacji o kluczowym znaczeniu zalecane jest skorzystanie z profesjonalnego tłumaczenia wykonanego przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z korzystania z tego tłumaczenia.