<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4951d458c0b60c02cd1e751b40903877",
  "translation_date": "2025-05-09T09:32:47+00:00",
  "source_file": "md/01.Introduction/02/05.AITK.md",
  "language_code": "pl"
}
-->
# Rodzina Phi w AITK

[AI Toolkit for VS Code](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio) ułatwia tworzenie aplikacji generatywnej AI, łącząc nowoczesne narzędzia i modele AI z katalogu Azure AI Foundry oraz innych źródeł, takich jak Hugging Face. Będziesz mógł przeglądać katalog modeli AI zasilany przez GitHub Models i Azure AI Foundry Model Catalogs, pobierać je lokalnie lub zdalnie, dostrajać, testować i używać w swojej aplikacji.

AI Toolkit Preview działa lokalnie. Lokalna inferencja lub dostrajanie zależy od wybranego modelu, może być wymagany GPU, np. NVIDIA CUDA GPU. Możesz też uruchamiać modele GitHub bezpośrednio za pomocą AITK.

## Pierwsze kroki

[Dowiedz się więcej, jak zainstalować Windows subsystem for Linux](https://learn.microsoft.com/windows/wsl/install?WT.mc_id=aiml-137032-kinfeylo)

oraz [jak zmienić domyślną dystrybucję](https://learn.microsoft.com/windows/wsl/install#change-the-default-linux-distribution-installed).

[Repozytorium AI Toolkit na GitHub](https://github.com/microsoft/vscode-ai-toolkit/)

- Windows, Linux, macOS
  
- Do fine-tuningu na Windows i Linux potrzebna jest karta Nvidia GPU. Dodatkowo, **Windows** wymaga subsystemu Linux z dystrybucją Ubuntu 18.4 lub nowszą. [Dowiedz się więcej, jak zainstalować Windows subsystem for Linux](https://learn.microsoft.com/windows/wsl/install) oraz [jak zmienić domyślną dystrybucję](https://learn.microsoft.com/windows/wsl/install#change-the-default-linux-distribution-installed).

### Instalacja AI Toolkit

AI Toolkit jest dostarczany jako [rozszerzenie Visual Studio Code](https://code.visualstudio.com/docs/setup/additional-components#_vs-code-extensions), więc najpierw musisz zainstalować [VS Code](https://code.visualstudio.com/docs/setup/windows?WT.mc_id=aiml-137032-kinfeylo), a następnie pobrać AI Toolkit z [VS Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio).  
[AI Toolkit jest dostępny w Visual Studio Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio) i można go zainstalować jak każde inne rozszerzenie VS Code.

Jeśli nie masz doświadczenia z instalacją rozszerzeń VS Code, wykonaj następujące kroki:

### Logowanie

1. W pasku aktywności VS Code wybierz **Extensions**
1. W polu wyszukiwania rozszerzeń wpisz "AI Toolkit"
1. Wybierz "AI Toolkit for Visual Studio code"
1. Kliknij **Install**

Teraz możesz korzystać z rozszerzenia!

Zostaniesz poproszony o zalogowanie się do GitHub, kliknij "Allow", aby kontynuować. Nastąpi przekierowanie na stronę logowania GitHub.

Zaloguj się i postępuj zgodnie z instrukcjami. Po pomyślnym zalogowaniu zostaniesz przekierowany z powrotem do VS Code.

Po zainstalowaniu rozszerzenia w pasku aktywności pojawi się ikona AI Toolkit.

Poznajmy dostępne funkcje!

### Dostępne funkcje

Główna boczna belka AI Toolkit jest podzielona na:

- **Models**
- **Resources**
- **Playground**  
- **Fine-tuning**
- **Evaluation**

Znajdziesz je w sekcji Resources. Aby zacząć, wybierz **Model Catalog**.

### Pobieranie modelu z katalogu

Po uruchomieniu AI Toolkit z paska bocznego VS Code możesz wybrać jedną z opcji:

![AI toolkit model catalog](../../../../../translated_images/AItoolkitmodel_catalog.eee6b38a71f628501d730ffe9c2ae69b8f18706e7492ac2371423b045485996e.pl.png)

- Znajdź obsługiwany model w **Model Catalog** i pobierz go lokalnie
- Przetestuj inferencję modelu w **Model Playground**
- Dostrój model lokalnie lub zdalnie w **Model Fine-tuning**
- Wdróż dostrojone modele do chmury za pomocą palety poleceń AI Toolkit
- Ewaluacja modeli

> [!NOTE]
>
> **GPU a CPU**
>
> Na kartach modeli zobaczysz rozmiar modelu, platformę i typ akceleratora (CPU, GPU). Aby uzyskać optymalną wydajność na **urządzeniach Windows z co najmniej jednym GPU**, wybierz wersje modeli dedykowane tylko Windows.
>
> Zapewnia to optymalizację pod akcelerator DirectML.
>
> Nazwy modeli mają format
>
> - `{model_name}-{accelerator}-{quantization}-{format}`.
>
>Aby sprawdzić, czy na Twoim urządzeniu Windows jest GPU, otwórz **Menedżera zadań** i przejdź do zakładki **Wydajność**. Jeśli masz GPU, pojawią się pod nazwami takimi jak "GPU 0" lub "GPU 1".

### Uruchom model w playground

Po ustawieniu wszystkich parametrów kliknij **Generate Project**.

Po pobraniu modelu wybierz **Load in Playground** na karcie modelu w katalogu:

- Rozpocznij pobieranie modelu
- Zainstaluj wszystkie wymagane zależności i komponenty
- Utwórz przestrzeń roboczą VS Code

![Load model in playground](../../../../../translated_images/AItoolkitload_model_into_playground.e442d8013c65406e69471fb4f8e4e3800505255fe1bd7aa9422f02ee715bad57.pl.png)

### Używanie REST API w Twojej aplikacji

AI Toolkit zawiera lokalny serwer REST API **na porcie 5272**, który korzysta z [formatu OpenAI chat completions](https://platform.openai.com/docs/api-reference/chat/create).

Pozwala to testować aplikację lokalnie, bez konieczności korzystania z chmurowych usług AI. Przykładowo, poniższy plik JSON pokazuje, jak skonfigurować ciało żądania:

```json
{
    "model": "Phi-4",
    "messages": [
        {
            "role": "user",
            "content": "what is the golden ratio?"
        }
    ],
    "temperature": 0.7,
    "top_p": 1,
    "top_k": 10,
    "max_tokens": 100,
    "stream": true
}
```

REST API możesz testować za pomocą np. [Postman](https://www.postman.com/) lub narzędzia CURL (Client URL):

```bash
curl -vX POST http://127.0.0.1:5272/v1/chat/completions -H 'Content-Type: application/json' -d @body.json
```

### Korzystanie z biblioteki klienta OpenAI dla Pythona

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://127.0.0.1:5272/v1/", 
    api_key="x" # required for the API but not used
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "what is the golden ratio?",
        }
    ],
    model="Phi-4",
)

print(chat_completion.choices[0].message.content)
```

### Korzystanie z biblioteki Azure OpenAI dla .NET

Dodaj [Azure OpenAI client library dla .NET](https://www.nuget.org/packages/Azure.AI.OpenAI/) do swojego projektu za pomocą NuGet:

```bash
dotnet add {project_name} package Azure.AI.OpenAI --version 1.0.0-beta.17
```

Dodaj plik C# o nazwie **OverridePolicy.cs** do projektu i wklej poniższy kod:

```csharp
// OverridePolicy.cs
using Azure.Core.Pipeline;
using Azure.Core;

internal partial class OverrideRequestUriPolicy(Uri overrideUri)
    : HttpPipelineSynchronousPolicy
{
    private readonly Uri _overrideUri = overrideUri;

    public override void OnSendingRequest(HttpMessage message)
    {
        message.Request.Uri.Reset(_overrideUri);
    }
}
```

Następnie wklej poniższy kod do pliku **Program.cs**:

```csharp
// Program.cs
using Azure.AI.OpenAI;

Uri localhostUri = new("http://localhost:5272/v1/chat/completions");

OpenAIClientOptions clientOptions = new();
clientOptions.AddPolicy(
    new OverrideRequestUriPolicy(localhostUri),
    Azure.Core.HttpPipelinePosition.BeforeTransport);
OpenAIClient client = new(openAIApiKey: "unused", clientOptions);

ChatCompletionsOptions options = new()
{
    DeploymentName = "Phi-4",
    Messages =
    {
        new ChatRequestSystemMessage("You are a helpful assistant. Be brief and succinct."),
        new ChatRequestUserMessage("What is the golden ratio?"),
    }
};

StreamingResponse<StreamingChatCompletionsUpdate> streamingChatResponse
    = await client.GetChatCompletionsStreamingAsync(options);

await foreach (StreamingChatCompletionsUpdate chatChunk in streamingChatResponse)
{
    Console.Write(chatChunk.ContentUpdate);
}
```


## Fine tuning z AI Toolkit

- Zacznij od odkrywania modeli i playground.
- Dostrzeż i inferencja modeli z użyciem lokalnych zasobów obliczeniowych.
- Zdalne dostrajanie i inferencja przy użyciu zasobów Azure.

[Fine Tuning with AI Toolkit](../../03.FineTuning/Finetuning_VSCodeaitoolkit.md)

## Zasoby Q&A dla AI Toolkit

Zapoznaj się z naszą [stroną Q&A](https://github.com/microsoft/vscode-ai-toolkit/blob/main/archive/QA.md) z najczęściej występującymi problemami i ich rozwiązaniami.

**Zastrzeżenie**:  
Niniejszy dokument został przetłumaczony przy użyciu automatycznej usługi tłumaczeniowej AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mimo że dokładamy starań, aby tłumaczenie było jak najbardziej precyzyjne, prosimy mieć na uwadze, że automatyczne przekłady mogą zawierać błędy lub nieścisłości. Oryginalny dokument w języku źródłowym należy traktować jako źródło wiążące. W przypadku informacji o krytycznym znaczeniu zalecane jest skorzystanie z profesjonalnego tłumaczenia wykonanego przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z korzystania z tego tłumaczenia.