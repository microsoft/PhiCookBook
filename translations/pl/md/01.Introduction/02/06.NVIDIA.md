<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7b08e277df2a9307f861ae54bc30c772",
  "translation_date": "2025-07-16T19:36:21+00:00",
  "source_file": "md/01.Introduction/02/06.NVIDIA.md",
  "language_code": "pl"
}
-->
## Rodzina Phi w NVIDIA NIM

NVIDIA NIM to zestaw łatwych w użyciu mikrousług zaprojektowanych, aby przyspieszyć wdrażanie modeli generatywnej sztucznej inteligencji w chmurze, centrach danych i na stacjach roboczych. NIM-y są podzielone według rodziny modeli oraz na poziomie poszczególnych modeli. Na przykład NVIDIA NIM dla dużych modeli językowych (LLM) dostarcza moc najnowocześniejszych LLM do zastosowań korporacyjnych, oferując niezrównane możliwości przetwarzania i rozumienia języka naturalnego.

NIM ułatwia zespołom IT i DevOps samodzielne hostowanie dużych modeli językowych (LLM) we własnych zarządzanych środowiskach, jednocześnie zapewniając programistom standardowe API branżowe, które pozwalają tworzyć potężne kopiloty, chatboty i asystentów AI, mogące zrewolucjonizować ich biznes. Wykorzystując najnowsze przyspieszenie GPU od NVIDIA oraz skalowalne wdrożenia, NIM oferuje najszybszą ścieżkę do inferencji z niezrównaną wydajnością.

Możesz użyć NVIDIA NIM do inferencji modeli z rodziny Phi

![nim](../../../../../translated_images/Phi-NIM.09bebb743387ee4a5028d7d4f8fed55e619711b26c8937526b43a2af980f7dcf.pl.png)

### **Przykłady - Phi-3-Vision w NVIDIA NIM**

Wyobraź sobie, że masz obraz (`demo.png`) i chcesz wygenerować kod w Pythonie, który przetworzy ten obraz i zapisze jego nową wersję (`phi-3-vision.jpg`).

Powyższy kod automatyzuje ten proces poprzez:

1. Konfigurację środowiska i niezbędnych ustawień.
2. Utworzenie promptu, który instruuje model, aby wygenerował potrzebny kod w Pythonie.
3. Wysłanie promptu do modelu i zebranie wygenerowanego kodu.
4. Wyodrębnienie i uruchomienie wygenerowanego kodu.
5. Wyświetlenie oryginalnego i przetworzonego obrazu.

To podejście wykorzystuje moc AI do automatyzacji zadań związanych z przetwarzaniem obrazów, co ułatwia i przyspiesza osiąganie zamierzonych celów.

[Przykładowe rozwiązanie kodu](../../../../../code/06.E2E/E2E_Nvidia_NIM_Phi3_Vision.ipynb)

Przeanalizujmy krok po kroku, co robi cały kod:

1. **Instalacja wymaganego pakietu**:
    ```python
    !pip install langchain_nvidia_ai_endpoints -U
    ```
    To polecenie instaluje pakiet `langchain_nvidia_ai_endpoints`, zapewniając, że jest to najnowsza wersja.

2. **Importowanie niezbędnych modułów**:
    ```python
    from langchain_nvidia_ai_endpoints import ChatNVIDIA
    import getpass
    import os
    import base64
    ```
    Te importy wprowadzają potrzebne moduły do interakcji z punktami końcowymi NVIDIA AI, bezpiecznego obsługiwania haseł, pracy z systemem operacyjnym oraz kodowania/odkodowywania danych w formacie base64.

3. **Ustawienie klucza API**:
    ```python
    if not os.getenv("NVIDIA_API_KEY"):
        os.environ["NVIDIA_API_KEY"] = getpass.getpass("Enter your NVIDIA API key: ")
    ```
    Ten fragment sprawdza, czy zmienna środowiskowa `NVIDIA_API_KEY` jest ustawiona. Jeśli nie, prosi użytkownika o bezpieczne wprowadzenie klucza API.

4. **Definicja modelu i ścieżki do obrazu**:
    ```python
    model = 'microsoft/phi-3-vision-128k-instruct'
    chat = ChatNVIDIA(model=model)
    img_path = './imgs/demo.png'
    ```
    Tutaj ustawia się model do użycia, tworzy instancję `ChatNVIDIA` z wybranym modelem oraz definiuje ścieżkę do pliku z obrazem.

5. **Utworzenie tekstowego promptu**:
    ```python
    text = "Please create Python code for image, and use plt to save the new picture under imgs/ and name it phi-3-vision.jpg."
    ```
    Definiuje prompt tekstowy, który instruuje model, aby wygenerował kod Pythona do przetwarzania obrazu.

6. **Kodowanie obrazu w base64**:
    ```python
    with open(img_path, "rb") as f:
        image_b64 = base64.b64encode(f.read()).decode()
    image = f'<img src="data:image/png;base64,{image_b64}" />'
    ```
    Ten kod odczytuje plik obrazu, koduje go w base64 i tworzy tag HTML obrazu z zakodowanymi danymi.

7. **Połączenie tekstu i obrazu w jeden prompt**:
    ```python
    prompt = f"{text} {image}"
    ```
    Łączy prompt tekstowy i tag HTML obrazu w jeden ciąg znaków.

8. **Generowanie kodu za pomocą ChatNVIDIA**:
    ```python
    code = ""
    for chunk in chat.stream(prompt):
        print(chunk.content, end="")
        code += chunk.content
    ```
    Ten fragment wysyła prompt do modelu `ChatNVIDIA` i zbiera generowany kod w kawałkach, drukując i dopisując każdy fragment do zmiennej `code`.

9. **Wyodrębnienie kodu Pythona z wygenerowanej zawartości**:
    ```python
    begin = code.index('```python') + 9
    code = code[begin:]
    end = code.index('```')
    code = code[:end]
    ```
    Ten krok wyciąga właściwy kod Pythona z wygenerowanej zawartości, usuwając formatowanie markdown.

10. **Uruchomienie wygenerowanego kodu**:
    ```python
    import subprocess
    result = subprocess.run(["python", "-c", code], capture_output=True)
    ```
    Uruchamia wyodrębniony kod Pythona jako podproces i przechwytuje jego wyjście.

11. **Wyświetlenie obrazów**:
    ```python
    from IPython.display import Image, display
    display(Image(filename='./imgs/phi-3-vision.jpg'))
    display(Image(filename='./imgs/demo.png'))
    ```
    Te linie wyświetlają obrazy za pomocą modułu `IPython.display`.

**Zastrzeżenie**:  
Niniejszy dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mimo że dążymy do dokładności, prosimy mieć na uwadze, że tłumaczenia automatyczne mogą zawierać błędy lub nieścisłości. Oryginalny dokument w języku źródłowym powinien być uznawany za źródło autorytatywne. W przypadku informacji o kluczowym znaczeniu zalecane jest skorzystanie z profesjonalnego tłumaczenia wykonanego przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z korzystania z tego tłumaczenia.