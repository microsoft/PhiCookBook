<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:44:03+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "pl"
}
-->
# Kluczowe technologie wymienione to

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) – niskopoziomowe API do sprzętowo przyspieszonego uczenia maszynowego, oparte na DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) – platforma obliczeń równoległych i model API opracowany przez Nvidię, umożliwiający ogólne przetwarzanie na procesorach graficznych (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) – otwarty format zaprojektowany do reprezentacji modeli uczenia maszynowego, zapewniający interoperacyjność między różnymi frameworkami ML.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) – format używany do reprezentacji i aktualizacji modeli uczenia maszynowego, szczególnie przydatny dla mniejszych modeli językowych, które mogą efektywnie działać na CPU z kwantyzacją 4-8 bitów.

## DirectML

DirectML to niskopoziomowe API umożliwiające sprzętowo przyspieszone uczenie maszynowe. Jest zbudowane na bazie DirectX 12, aby wykorzystać akcelerację GPU i jest niezależne od producenta, co oznacza, że nie wymaga zmian w kodzie, aby działać na różnych kartach graficznych. Jest głównie wykorzystywane do trenowania modeli i wykonywania inferencji na GPU.

Jeśli chodzi o wsparcie sprzętowe, DirectML jest zaprojektowane do współpracy z szeroką gamą GPU, w tym zintegrowanymi i dedykowanymi kartami AMD, zintegrowanymi GPU Intela oraz dedykowanymi GPU Nvidii. Jest częścią Windows AI Platform i jest obsługiwane na Windows 10 i 11, co pozwala na trenowanie modeli i inferencję na dowolnym urządzeniu z Windows.

Pojawiły się aktualizacje i nowe możliwości związane z DirectML, takie jak wsparcie do 150 operatorów ONNX oraz wykorzystanie zarówno przez ONNX runtime, jak i WinML. Za projektem stoją główni producenci sprzętu (IHV), z których każdy implementuje różne metakomendy.

## CUDA

CUDA, czyli Compute Unified Device Architecture, to platforma obliczeń równoległych i model API stworzony przez Nvidię. Pozwala programistom na wykorzystanie GPU z obsługą CUDA do ogólnego przetwarzania – podejście to nazywa się GPGPU (General-Purpose computing on Graphics Processing Units). CUDA jest kluczowym elementem przyspieszenia GPU Nvidii i jest szeroko stosowana w różnych dziedzinach, w tym w uczeniu maszynowym, obliczeniach naukowych i przetwarzaniu wideo.

Wsparcie sprzętowe dla CUDA jest specyficzne dla GPU Nvidii, ponieważ jest to technologia własnościowa opracowana przez tę firmę. Każda architektura obsługuje określone wersje zestawu narzędzi CUDA, który dostarcza niezbędne biblioteki i narzędzia dla programistów do tworzenia i uruchamiania aplikacji CUDA.

## ONNX

ONNX (Open Neural Network Exchange) to otwarty format zaprojektowany do reprezentacji modeli uczenia maszynowego. Definiuje rozszerzalny model grafu obliczeniowego, a także wbudowane operatory i standardowe typy danych. ONNX pozwala programistom przenosić modele między różnymi frameworkami ML, co umożliwia interoperacyjność i ułatwia tworzenie oraz wdrażanie aplikacji AI.

Phi3 mini może działać z ONNX Runtime na CPU i GPU na różnych urządzeniach, w tym na platformach serwerowych, Windows, Linux, Mac desktopach oraz mobilnych CPU.  
Dodane przez nas zoptymalizowane konfiguracje to:

- modele ONNX dla int4 DML: kwantyzowane do int4 za pomocą AWQ  
- model ONNX dla fp16 CUDA  
- model ONNX dla int4 CUDA: kwantyzowane do int4 za pomocą RTN  
- model ONNX dla int4 CPU i Mobile: kwantyzowane do int4 za pomocą RTN  

## Llama.cpp

Llama.cpp to otwartoźródłowa biblioteka napisana w C++. Wykonuje inferencję na różnych dużych modelach językowych (LLM), w tym Llama. Opracowana równolegle z biblioteką ggml (ogólnego przeznaczenia biblioteką tensorową), llama.cpp ma na celu zapewnienie szybszej inferencji i mniejszego zużycia pamięci w porównaniu do oryginalnej implementacji w Pythonie. Wspiera optymalizację sprzętową, kwantyzację oraz oferuje prosty API i przykłady3. Jeśli interesuje Cię efektywna inferencja LLM, warto zapoznać się z llama.cpp, ponieważ Phi3 może uruchamiać Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) to format używany do reprezentacji i aktualizacji modeli uczenia maszynowego. Jest szczególnie przydatny dla mniejszych modeli językowych (SLM), które mogą efektywnie działać na CPU z kwantyzacją 4-8 bitów. GGUF jest korzystny do szybkiego prototypowania oraz uruchamiania modeli na urządzeniach brzegowych lub w zadaniach wsadowych, takich jak pipeline’y CI/CD.

**Zastrzeżenie**:  
Niniejszy dokument został przetłumaczony przy użyciu usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy starań, aby tłumaczenie było jak najbardziej precyzyjne, prosimy mieć na uwadze, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w języku źródłowym należy traktować jako źródło wiążące. W przypadku informacji o kluczowym znaczeniu zalecane jest skorzystanie z profesjonalnego tłumaczenia wykonanego przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z korzystania z tego tłumaczenia.