<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:21:13+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "pl"
}
-->
# Kluczowe technologie wymienione to

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) – niskopoziomowe API do sprzętowo przyspieszonego uczenia maszynowego oparte na DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) – platforma obliczeniowa i model API opracowany przez Nvidia, umożliwiający przetwarzanie ogólnego przeznaczenia na procesorach graficznych (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) – otwarty format zaprojektowany do reprezentacji modeli uczenia maszynowego, zapewniający interoperacyjność między różnymi frameworkami ML.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) – format służący do reprezentacji i aktualizacji modeli uczenia maszynowego, szczególnie przydatny dla mniejszych modeli językowych działających efektywnie na CPU z kwantyzacją 4-8 bitów.

## DirectML

DirectML to niskopoziomowe API umożliwiające sprzętowo przyspieszone uczenie maszynowe. Jest zbudowane na bazie DirectX 12, by wykorzystać akcelerację GPU i jest niezależne od producenta, co oznacza, że nie wymaga zmian w kodzie, aby działać na różnych kartach graficznych. Głównie stosowane jest do trenowania modeli i wykonywania inferencji na GPU.

Jeśli chodzi o wsparcie sprzętowe, DirectML jest zaprojektowane do współpracy z szeroką gamą GPU, w tym zintegrowanymi i dedykowanymi kartami AMD, zintegrowanymi GPU Intela oraz dedykowanymi GPU NVIDIA. Jest częścią platformy Windows AI i jest obsługiwane na Windows 10 i 11, umożliwiając trenowanie i inferencję modeli na dowolnym urządzeniu z Windows.

Pojawiły się aktualizacje i nowe możliwości związane z DirectML, takie jak wsparcie do 150 operatorów ONNX oraz wykorzystanie go przez ONNX runtime i WinML. Jest wspierane przez głównych producentów sprzętu (IHVs), z których każdy implementuje różne metakomendy.

## CUDA

CUDA, czyli Compute Unified Device Architecture, to platforma obliczeniowa i model API stworzony przez Nvidia. Pozwala programistom korzystać z GPU obsługujących CUDA do przetwarzania ogólnego przeznaczenia – podejście to nazywa się GPGPU (General-Purpose computing on Graphics Processing Units). CUDA jest kluczowym elementem akceleracji GPU Nvidii i jest szeroko stosowane w różnych dziedzinach, w tym w uczeniu maszynowym, obliczeniach naukowych i przetwarzaniu wideo.

Wsparcie sprzętowe dla CUDA dotyczy wyłącznie GPU Nvidii, ponieważ jest to technologia własnościowa. Każda architektura obsługuje określone wersje zestawu narzędzi CUDA, który dostarcza niezbędne biblioteki i narzędzia dla programistów do tworzenia i uruchamiania aplikacji CUDA.

## ONNX

ONNX (Open Neural Network Exchange) to otwarty format zaprojektowany do reprezentacji modeli uczenia maszynowego. Definiuje rozszerzalny model grafu obliczeniowego oraz wbudowane operatory i standardowe typy danych. ONNX pozwala programistom przenosić modele między różnymi frameworkami ML, ułatwiając interoperacyjność i tworzenie oraz wdrażanie aplikacji AI.

Phi3 mini może działać z ONNX Runtime na CPU i GPU na różnych urządzeniach, w tym na platformach serwerowych, Windows, Linux, Mac desktopach oraz mobilnych CPU.  
Dodane przez nas zoptymalizowane konfiguracje to:

- modele ONNX dla int4 DML: kwantyzowane do int4 za pomocą AWQ  
- model ONNX dla fp16 CUDA  
- model ONNX dla int4 CUDA: kwantyzowane do int4 za pomocą RTN  
- model ONNX dla int4 CPU i Mobile: kwantyzowane do int4 za pomocą RTN  

## Llama.cpp

Llama.cpp to otwartoźródłowa biblioteka napisana w C++. Wykonuje inferencję na różnych dużych modelach językowych (LLM), w tym na Llama. Stworzona razem z biblioteką ggml (ogólnego przeznaczenia biblioteka tensorów), llama.cpp ma na celu zapewnienie szybszej inferencji i mniejszego zużycia pamięci w porównaniu do oryginalnej implementacji w Pythonie. Obsługuje optymalizację sprzętową, kwantyzację oraz oferuje prosty API i przykłady3. Jeśli interesujesz się efektywną inferencją LLM, warto przyjrzeć się llama.cpp, ponieważ Phi3 może uruchamiać Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) to format służący do reprezentacji i aktualizacji modeli uczenia maszynowego. Jest szczególnie przydatny dla mniejszych modeli językowych (SLM), które mogą działać efektywnie na CPU z kwantyzacją 4-8 bitów. GGUF jest korzystny do szybkiego prototypowania oraz uruchamiania modeli na urządzeniach brzegowych lub w zadaniach wsadowych, takich jak pipeline CI/CD.

**Zastrzeżenie**:  
Niniejszy dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mimo że dokładamy starań, aby tłumaczenie było jak najbardziej precyzyjne, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w języku źródłowym powinien być uznawany za źródło autorytatywne. W przypadku informacji krytycznych zalecane jest skorzystanie z profesjonalnego tłumaczenia wykonanego przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z korzystania z tego tłumaczenia.