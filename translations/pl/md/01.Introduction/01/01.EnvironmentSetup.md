# Rozpocznij pracę z Phi-3 lokalnie

Ten przewodnik pomoże Ci skonfigurować lokalne środowisko do uruchomienia modelu Phi-3 za pomocą Ollama. Model możesz uruchomić na kilka sposobów, w tym korzystając z GitHub Codespaces, VS Code Dev Containers lub lokalnie.

## Konfiguracja środowiska

### GitHub Codespaces

Możesz uruchomić ten szablon praktycznie, korzystając z GitHub Codespaces. Przycisk otworzy w przeglądarce instancję VS Code działającą w chmurze:

1. Otwórz szablon (to może potrwać kilka minut):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Otwórz okno terminala

### VS Code Dev Containers

⚠️ Ta opcja zadziała tylko, jeśli Docker Desktop ma przydzielone co najmniej 16 GB RAM. Jeśli masz mniej niż 16 GB RAM, możesz spróbować opcji [GitHub Codespaces](../../../../../md/01.Introduction/01) lub [ustawić środowisko lokalnie](../../../../../md/01.Introduction/01).

Powiązaną opcją są VS Code Dev Containers, które otworzą projekt w lokalnym VS Code z użyciem [rozszerzenia Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Uruchom Docker Desktop (zainstaluj go, jeśli jeszcze nie masz)
2. Otwórz projekt:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. W oknie VS Code, które się otworzy, gdy pojawią się pliki projektu (to może potrwać kilka minut), otwórz terminal.
4. Kontynuuj zgodnie z [krokami wdrożenia](../../../../../md/01.Introduction/01)

### Środowisko lokalne

1. Upewnij się, że masz zainstalowane następujące narzędzia:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Testowanie modelu

1. Poproś Ollama o pobranie i uruchomienie modelu phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Pobranie modelu zajmie kilka minut.

2. Gdy w wyjściu pojawi się "success", możesz wysłać wiadomość do modelu z poziomu prompta.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Po kilku sekundach powinieneś zobaczyć strumień odpowiedzi od modelu.

4. Aby poznać różne techniki stosowane z modelami językowymi, otwórz notatnik Pythona [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) i uruchom każdą komórkę. Jeśli używałeś innego modelu niż 'phi3:mini', zmień `MODEL_NAME` w pierwszej komórce.

5. Aby porozmawiać z modelem phi3:mini z poziomu Pythona, otwórz plik [chat.py](../../../../../code/01.Introduce/chat.py) i go uruchom. W razie potrzeby możesz zmienić `MODEL_NAME` na początku pliku, a także zmodyfikować komunikat systemowy lub dodać przykłady few-shot.

**Zastrzeżenie**:  
Niniejszy dokument został przetłumaczony przy użyciu automatycznej usługi tłumaczeniowej AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mimo że dokładamy starań, aby tłumaczenie było jak najbardziej precyzyjne, prosimy mieć na uwadze, że tłumaczenia automatyczne mogą zawierać błędy lub nieścisłości. Oryginalny dokument w języku źródłowym należy traktować jako źródło wiążące. W przypadku informacji o kluczowym znaczeniu zalecane jest skorzystanie z profesjonalnego tłumaczenia wykonanego przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z korzystania z tego tłumaczenia.