<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-09T07:07:19+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "pl"
}
-->
# Zacznij lokalnie z Phi-3

Ten przewodnik pomoże Ci skonfigurować lokalne środowisko do uruchomienia modelu Phi-3 za pomocą Ollama. Model możesz uruchomić na kilka sposobów, w tym korzystając z GitHub Codespaces, VS Code Dev Containers lub lokalnie.

## Konfiguracja środowiska

### GitHub Codespaces

Możesz uruchomić ten szablon praktycznie, korzystając z GitHub Codespaces. Przycisk otworzy w przeglądarce instancję VS Code działającą w chmurze:

1. Otwórz szablon (to może potrwać kilka minut):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Otwórz okno terminala

### VS Code Dev Containers

⚠️ Ta opcja zadziała tylko, jeśli Docker Desktop ma przydzielone co najmniej 16 GB pamięci RAM. Jeśli masz mniej niż 16 GB RAM, możesz spróbować opcji [GitHub Codespaces](../../../../../md/01.Introduction/01) lub [ustawić środowisko lokalnie](../../../../../md/01.Introduction/01).

Powiązaną opcją są VS Code Dev Containers, które otworzą projekt w Twoim lokalnym VS Code z użyciem rozszerzenia [Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Uruchom Docker Desktop (jeśli nie masz, zainstaluj)
2. Otwórz projekt:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. W oknie VS Code, które się otworzy, gdy pojawią się pliki projektu (to może potrwać kilka minut), otwórz terminal.
4. Kontynuuj zgodnie z [krokami wdrożenia](../../../../../md/01.Introduction/01)

### Środowisko lokalne

1. Upewnij się, że masz zainstalowane następujące narzędzia:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Testowanie modelu

1. Poproś Ollama o pobranie i uruchomienie modelu phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Pobranie modelu zajmie kilka minut.

2. Gdy w wyjściu pojawi się "success", możesz wysłać wiadomość do modelu z poziomu promptu.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Po kilku sekundach powinieneś zobaczyć strumień odpowiedzi od modelu.

4. Aby poznać różne techniki stosowane z modelami językowymi, otwórz notatnik Pythona [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) i uruchom każdą komórkę. Jeśli używałeś innego modelu niż 'phi3:mini', zmień `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` na początku pliku według potrzeb, a także możesz zmodyfikować wiadomość systemową lub dodać przykłady few-shot, jeśli chcesz.

**Zastrzeżenie**:  
Niniejszy dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy starań, aby tłumaczenie było jak najdokładniejsze, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w języku źródłowym powinien być uznawany za wiarygodne źródło. W przypadku informacji o krytycznym znaczeniu zalecane jest skorzystanie z profesjonalnego tłumaczenia wykonanego przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z korzystania z tego tłumaczenia.