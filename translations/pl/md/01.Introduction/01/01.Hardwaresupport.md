<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8cdc17ce0f10535da30b53d23fe1a795",
  "translation_date": "2025-07-16T18:25:02+00:00",
  "source_file": "md/01.Introduction/01/01.Hardwaresupport.md",
  "language_code": "pl"
}
-->
# Obsługa sprzętu Phi

Microsoft Phi został zoptymalizowany pod kątem ONNX Runtime i obsługuje Windows DirectML. Działa dobrze na różnych typach sprzętu, w tym na GPU, CPU, a nawet urządzeniach mobilnych.

## Sprzęt urządzenia  
Konkretnie, obsługiwany sprzęt obejmuje:

- GPU SKU: RTX 4090 (DirectML)
- GPU SKU: 1 A100 80GB (CUDA)
- CPU SKU: Standard F64s v2 (64 vCPU, 128 GiB pamięci)

## Mobile SKU

- Android - Samsung Galaxy S21
- Apple iPhone 14 lub nowszy z procesorem A16/A17

## Specyfikacja sprzętu Phi

- Wymagana minimalna konfiguracja.
- Windows: GPU obsługujący DirectX 12 oraz co najmniej 4 GB łącznej pamięci RAM

CUDA: GPU NVIDIA z Compute Capability >= 7.02

![HardwareSupport](../../../../../translated_images/01.phihardware.5d51b2377cba18afc6949074542f290c56bb278dac3f4f86302aca6d80fffeb9.pl.png)

## Uruchamianie onnxruntime na wielu GPU

Obecnie dostępne modele Phi ONNX działają tylko na 1 GPU. Możliwe jest wsparcie dla wielu GPU w modelu Phi, jednak ORT z 2 GPU nie gwarantuje większej przepustowości w porównaniu do 2 instancji ort. Prosimy o zapoznanie się z [ONNX Runtime](https://onnxruntime.ai/) w celu uzyskania najnowszych informacji.

Podczas [Build 2024 zespół GenAI ONNX](https://youtu.be/WLW4SE8M9i8?si=EtG04UwDvcjunyfC) ogłosił, że włączyli obsługę wielu instancji zamiast wielu GPU dla modeli Phi.

Obecnie pozwala to na uruchomienie jednej instancji onnxruntime lub onnxruntime-genai z użyciem zmiennej środowiskowej CUDA_VISIBLE_DEVICES w następujący sposób.

```Python
CUDA_VISIBLE_DEVICES=0 python infer.py
CUDA_VISIBLE_DEVICES=1 python infer.py
```

Zachęcamy do dalszego eksplorowania Phi w [Azure AI Foundry](https://ai.azure.com)

**Zastrzeżenie**:  
Niniejszy dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mimo że dążymy do dokładności, prosimy mieć na uwadze, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w języku źródłowym powinien być uznawany za źródło autorytatywne. W przypadku informacji o kluczowym znaczeniu zalecane jest skorzystanie z profesjonalnego tłumaczenia wykonanego przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z korzystania z tego tłumaczenia.