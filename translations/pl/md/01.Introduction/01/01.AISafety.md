<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c8273672cc57df2be675407a1383aaf0",
  "translation_date": "2025-07-16T17:47:30+00:00",
  "source_file": "md/01.Introduction/01/01.AISafety.md",
  "language_code": "pl"
}
-->
# Bezpieczeństwo AI dla modeli Phi  
Rodzina modeli Phi została opracowana zgodnie z [Microsoft Responsible AI Standard](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFl), czyli firmowym zestawem wymagań opartym na sześciu zasadach: odpowiedzialność, przejrzystość, sprawiedliwość, niezawodność i bezpieczeństwo, prywatność i ochrona oraz inkluzywność, które tworzą [zasady odpowiedzialnej AI Microsoft](https://www.microsoft.com/ai/responsible-ai).

Podobnie jak w przypadku wcześniejszych modeli Phi, zastosowano wieloaspektową ocenę bezpieczeństwa oraz podejście do bezpieczeństwa po treningu, z dodatkowymi środkami uwzględniającymi wielojęzyczne możliwości tej wersji. Nasze podejście do treningu i oceny bezpieczeństwa, w tym testy w wielu językach i kategoriach ryzyka, zostało opisane w [Phi Safety Post-Training Paper](https://arxiv.org/abs/2407.13833). Choć modele Phi korzystają z tego podejścia, deweloperzy powinni stosować najlepsze praktyki odpowiedzialnej AI, w tym mapowanie, mierzenie i łagodzenie ryzyk związanych z ich konkretnym przypadkiem użycia oraz kontekstem kulturowym i językowym.

## Najlepsze praktyki

Podobnie jak inne modele, rodzina modeli Phi może potencjalnie zachowywać się w sposób niesprawiedliwy, zawodny lub obraźliwy.

Niektóre ograniczenia zachowań SLM i LLM, o których warto wiedzieć, to:

- **Jakość usługi:** Modele Phi są trenowane głównie na tekstach w języku angielskim. Języki inne niż angielski mogą wykazywać gorszą wydajność. Warianty języka angielskiego, które są mniej reprezentowane w danych treningowych, mogą działać gorzej niż standardowy amerykański angielski.  
- **Reprezentacja szkód i utrwalanie stereotypów:** Modele te mogą nadmiernie lub niedostatecznie reprezentować grupy ludzi, usuwać reprezentację niektórych grup lub wzmacniać poniżające lub negatywne stereotypy. Pomimo treningu bezpieczeństwa po treningu, te ograniczenia mogą nadal występować z powodu różnego poziomu reprezentacji grup lub obecności przykładów negatywnych stereotypów w danych treningowych, które odzwierciedlają rzeczywiste wzorce i społeczne uprzedzenia.  
- **Nieodpowiednie lub obraźliwe treści:** Modele te mogą generować inne rodzaje nieodpowiednich lub obraźliwych treści, co może sprawić, że ich wdrożenie w wrażliwych kontekstach będzie nieodpowiednie bez dodatkowych środków zaradczych specyficznych dla danego przypadku użycia.  
- **Wiarygodność informacji:** Modele językowe mogą generować bezsensowne treści lub fabrykować informacje, które mogą brzmieć wiarygodnie, ale są nieścisłe lub nieaktualne.  
- **Ograniczony zakres dla kodu:** Większość danych treningowych Phi-3 opiera się na Pythonie i korzysta z popularnych pakietów takich jak "typing, math, random, collections, datetime, itertools". Jeśli model generuje skrypty Pythona wykorzystujące inne pakiety lub skrypty w innych językach, zdecydowanie zalecamy ręczną weryfikację wszystkich użyć API przez użytkowników.

Deweloperzy powinni stosować najlepsze praktyki odpowiedzialnej AI i są odpowiedzialni za zapewnienie, że konkretny przypadek użycia jest zgodny z obowiązującymi przepisami prawa (np. prywatność, handel itp.).

## Rozważania dotyczące odpowiedzialnej AI

Podobnie jak inne modele językowe, modele z serii Phi mogą potencjalnie zachowywać się w sposób niesprawiedliwy, zawodny lub obraźliwy. Niektóre ograniczenia, o których warto pamiętać, to:

**Jakość usługi:** Modele Phi są trenowane głównie na tekstach w języku angielskim. Języki inne niż angielski mogą wykazywać gorszą wydajność. Warianty języka angielskiego, które są mniej reprezentowane w danych treningowych, mogą działać gorzej niż standardowy amerykański angielski.

**Reprezentacja szkód i utrwalanie stereotypów:** Modele te mogą nadmiernie lub niedostatecznie reprezentować grupy ludzi, usuwać reprezentację niektórych grup lub wzmacniać poniżające lub negatywne stereotypy. Pomimo treningu bezpieczeństwa po treningu, te ograniczenia mogą nadal występować z powodu różnego poziomu reprezentacji grup lub obecności przykładów negatywnych stereotypów w danych treningowych, które odzwierciedlają rzeczywiste wzorce i społeczne uprzedzenia.

**Nieodpowiednie lub obraźliwe treści:** Modele te mogą generować inne rodzaje nieodpowiednich lub obraźliwych treści, co może sprawić, że ich wdrożenie w wrażliwych kontekstach będzie nieodpowiednie bez dodatkowych środków zaradczych specyficznych dla danego przypadku użycia.  
Wiarygodność informacji: Modele językowe mogą generować bezsensowne treści lub fabrykować informacje, które mogą brzmieć wiarygodnie, ale są nieścisłe lub nieaktualne.

**Ograniczony zakres dla kodu:** Większość danych treningowych Phi-3 opiera się na Pythonie i korzysta z popularnych pakietów takich jak "typing, math, random, collections, datetime, itertools". Jeśli model generuje skrypty Pythona wykorzystujące inne pakiety lub skrypty w innych językach, zdecydowanie zalecamy ręczną weryfikację wszystkich użyć API przez użytkowników.

Deweloperzy powinni stosować najlepsze praktyki odpowiedzialnej AI i są odpowiedzialni za zapewnienie, że konkretny przypadek użycia jest zgodny z obowiązującymi przepisami prawa (np. prywatność, handel itp.). Ważne obszary do rozważenia to:

**Alokacja:** Modele mogą nie być odpowiednie do scenariuszy, które mogą mieć istotny wpływ na status prawny lub przydział zasobów czy szans życiowych (np. mieszkanie, zatrudnienie, kredyt itp.) bez dalszych ocen i dodatkowych technik usuwania uprzedzeń.

**Scenariusze wysokiego ryzyka:** Deweloperzy powinni ocenić przydatność modeli w scenariuszach wysokiego ryzyka, gdzie niesprawiedliwe, zawodnie lub obraźliwe wyniki mogą być bardzo kosztowne lub prowadzić do szkody. Dotyczy to m.in. udzielania porad w wrażliwych lub specjalistycznych dziedzinach, gdzie dokładność i niezawodność są kluczowe (np. porady prawne lub zdrowotne). Dodatkowe zabezpieczenia powinny być wdrażane na poziomie aplikacji zgodnie z kontekstem wdrożenia.

**Dezinformacja:** Modele mogą generować nieścisłe informacje. Deweloperzy powinni stosować najlepsze praktyki przejrzystości i informować użytkowników końcowych, że mają do czynienia z systemem AI. Na poziomie aplikacji deweloperzy mogą tworzyć mechanizmy feedbacku i procesy, które opierają odpowiedzi na specyficznych dla przypadku użycia, kontekstowych informacjach, technikę znaną jako Retrieval Augmented Generation (RAG).

**Generowanie szkodliwych treści:** Deweloperzy powinni oceniać wyniki pod kątem kontekstu i korzystać z dostępnych klasyfikatorów bezpieczeństwa lub własnych rozwiązań odpowiednich dla ich przypadku użycia.

**Niewłaściwe użycie:** Inne formy nadużyć, takie jak oszustwa, spam czy produkcja złośliwego oprogramowania, mogą być możliwe, dlatego deweloperzy powinni zapewnić, że ich aplikacje nie naruszają obowiązujących przepisów prawa i regulacji.

### Dostosowywanie i bezpieczeństwo treści AI

Po dostrojeniu modelu zdecydowanie zalecamy korzystanie z [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) do monitorowania generowanych przez modele treści, identyfikowania i blokowania potencjalnych zagrożeń, ryzyk i problemów z jakością.

![Phi3AISafety](../../../../../translated_images/01.phi3aisafety.c0d7fc42f5a5c40507c5e8be556615b8377a63b8764865d057d4faac3757a478.pl.png)

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) obsługuje zarówno treści tekstowe, jak i obrazowe. Może być wdrażany w chmurze, w odłączonych kontenerach oraz na urządzeniach brzegowych/wbudowanych.

## Przegląd Azure AI Content Safety

Azure AI Content Safety nie jest rozwiązaniem uniwersalnym; można je dostosować do specyficznych polityk firm. Dodatkowo, jego wielojęzyczne modele pozwalają na jednoczesne rozumienie wielu języków.

![AIContentSafety](../../../../../translated_images/01.AIcontentsafety.a288819b8ce8da1a56cf708aff010a541799d002ae7ae84bb819b19ab8950591.pl.png)

- **Azure AI Content Safety**  
- **Microsoft Developer**  
- **5 filmów**

Usługa Azure AI Content Safety wykrywa szkodliwe treści generowane przez użytkowników i AI w aplikacjach i usługach. Obejmuje API tekstowe i obrazowe, które pozwalają wykrywać szkodliwe lub nieodpowiednie materiały.

[AI Content Safety Playlist](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

**Zastrzeżenie**:  
Niniejszy dokument został przetłumaczony przy użyciu usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż dokładamy starań, aby tłumaczenie było jak najbardziej precyzyjne, prosimy mieć na uwadze, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w języku źródłowym należy traktować jako źródło autorytatywne. W przypadku informacji o kluczowym znaczeniu zalecane jest skorzystanie z profesjonalnego tłumaczenia wykonanego przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z korzystania z tego tłumaczenia.