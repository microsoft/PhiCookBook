<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7b08e277df2a9307f861ae54bc30c772",
  "translation_date": "2025-05-09T09:52:23+00:00",
  "source_file": "md/01.Introduction/02/06.NVIDIA.md",
  "language_code": "it"
}
-->
## Famiglia Phi in NVIDIA NIM

NVIDIA NIM è un insieme di microservizi facili da usare progettati per accelerare il deployment di modelli di intelligenza artificiale generativa su cloud, data center e workstation. I NIM sono categorizzati per famiglia di modelli e per singolo modello. Ad esempio, NVIDIA NIM per modelli di linguaggio di grandi dimensioni (LLM) porta la potenza degli LLM all’avanguardia nelle applicazioni aziendali, offrendo capacità di elaborazione e comprensione del linguaggio naturale senza pari.

NIM rende semplice per i team IT e DevOps ospitare autonomamente grandi modelli di linguaggio (LLM) nei propri ambienti gestiti, fornendo allo stesso tempo agli sviluppatori API standard di settore che permettono di creare potenti copiloti, chatbot e assistenti AI in grado di trasformare il business. Sfruttando l’accelerazione GPU all’avanguardia di NVIDIA e un deployment scalabile, NIM offre il percorso più veloce per l’inferenza con prestazioni senza eguali.

Puoi utilizzare NVIDIA NIM per inferire modelli della Famiglia Phi

![nim](../../../../../translated_images/Phi-NIM.45af94d89220fbbbc85f8da0379150a29cc88c3dd8ec417b1d3b7237bbe1c58a.it.png)

### **Esempi - Phi-3-Vision in NVIDIA NIM**

Immagina di avere un’immagine (`demo.png`) e di voler generare codice Python che elabori questa immagine e ne salvi una nuova versione (`phi-3-vision.jpg`).

Il codice sopra automatizza questo processo:

1. Configura l’ambiente e le impostazioni necessarie.
2. Crea un prompt che istruisce il modello a generare il codice Python richiesto.
3. Invia il prompt al modello e raccoglie il codice generato.
4. Estrae ed esegue il codice generato.
5. Mostra le immagini originale ed elaborata.

Questo approccio sfrutta la potenza dell’AI per automatizzare compiti di elaborazione delle immagini, rendendo più facile e veloce raggiungere i tuoi obiettivi.

[Soluzione codice di esempio](../../../../../code/06.E2E/E2E_Nvidia_NIM_Phi3_Vision.ipynb)

Analizziamo passo dopo passo cosa fa l’intero codice:

1. **Installa il pacchetto richiesto**:  
    ```python
    !pip install langchain_nvidia_ai_endpoints -U
    ```  
    Questo comando installa il pacchetto `langchain_nvidia_ai_endpoints`, assicurandosi che sia l’ultima versione.

2. **Importa i moduli necessari**:  
    ```python
    from langchain_nvidia_ai_endpoints import ChatNVIDIA
    import getpass
    import os
    import base64
    ```  
    Questi import includono i moduli necessari per interagire con gli endpoint NVIDIA AI, gestire password in modo sicuro, interagire con il sistema operativo e codificare/decodificare dati in base64.

3. **Configura la chiave API**:  
    ```python
    if not os.getenv("NVIDIA_API_KEY"):
        os.environ["NVIDIA_API_KEY"] = getpass.getpass("Enter your NVIDIA API key: ")
    ```  
    Questo codice verifica se la variabile d’ambiente `NVIDIA_API_KEY` è impostata. In caso contrario, chiede all’utente di inserire la chiave API in modo sicuro.

4. **Definisce modello e percorso immagine**:  
    ```python
    model = 'microsoft/phi-3-vision-128k-instruct'
    chat = ChatNVIDIA(model=model)
    img_path = './imgs/demo.png'
    ```  
    Qui si imposta il modello da usare, si crea un’istanza di `ChatNVIDIA` con il modello specificato e si definisce il percorso del file immagine.

5. **Crea il prompt testuale**:  
    ```python
    text = "Please create Python code for image, and use plt to save the new picture under imgs/ and name it phi-3-vision.jpg."
    ```  
    Questo definisce un prompt testuale che istruisce il modello a generare codice Python per elaborare un’immagine.

6. **Codifica l’immagine in base64**:  
    ```python
    with open(img_path, "rb") as f:
        image_b64 = base64.b64encode(f.read()).decode()
    image = f'<img src="data:image/png;base64,{image_b64}" />'
    ```  
    Questo codice legge il file immagine, lo codifica in base64 e crea un tag HTML `<img>` con i dati codificati.

7. **Combina testo e immagine nel prompt**:  
    ```python
    prompt = f"{text} {image}"
    ```  
    Qui si uniscono il prompt testuale e il tag immagine HTML in un’unica stringa.

8. **Genera codice usando ChatNVIDIA**:  
    ```python
    code = ""
    for chunk in chat.stream(prompt):
        print(chunk.content, end="")
        code += chunk.content
    ```  
    Questo codice invia il prompt a `ChatNVIDIA` model and collects the generated code in chunks, printing and appending each chunk to the `code`.

9. **Estrae il codice Python dal contenuto generato**:  
    ```python
    begin = code.index('```python') + 9  
    code = code[begin:]  
    end = code.index('```')
    code = code[:end]
    ```  
    Questo estrae il codice Python vero e proprio dal contenuto generato, rimuovendo la formattazione markdown.

10. **Esegue il codice generato**:  
    ```python
    import subprocess
    result = subprocess.run(["python", "-c", code], capture_output=True)
    ```  
    Qui il codice Python estratto viene eseguito come processo secondario e ne viene catturato l’output.

11. **Mostra le immagini**:  
    ```python
    from IPython.display import Image, display
    display(Image(filename='./imgs/phi-3-vision.jpg'))
    display(Image(filename='./imgs/demo.png'))
    ```  
    Queste righe mostrano le immagini usando il modulo `IPython.display`.

**Disclaimer**:  
Questo documento è stato tradotto utilizzando il servizio di traduzione automatica AI [Co-op Translator](https://github.com/Azure/co-op-translator). Pur impegnandoci per l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o inesattezze. Il documento originale nella sua lingua madre deve essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda la traduzione professionale effettuata da un umano. Non siamo responsabili per eventuali fraintendimenti o interpretazioni errate derivanti dall'uso di questa traduzione.