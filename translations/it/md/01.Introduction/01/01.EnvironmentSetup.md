# Inizia con Phi-3 in locale

Questa guida ti aiuterà a configurare il tuo ambiente locale per eseguire il modello Phi-3 usando Ollama. Puoi eseguire il modello in diversi modi, inclusi GitHub Codespaces, VS Code Dev Containers o il tuo ambiente locale.

## Configurazione dell'ambiente

### GitHub Codespaces

Puoi eseguire questo template virtualmente utilizzando GitHub Codespaces. Il pulsante aprirà un'istanza di VS Code basata sul web nel tuo browser:

1. Apri il template (potrebbe richiedere alcuni minuti):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Apri una finestra del terminale

### VS Code Dev Containers

⚠️ Questa opzione funziona solo se Docker Desktop ha almeno 16 GB di RAM allocati. Se hai meno di 16 GB di RAM, puoi provare l'[opzione GitHub Codespaces](../../../../../md/01.Introduction/01) o [configurarlo in locale](../../../../../md/01.Introduction/01).

Un'opzione correlata è VS Code Dev Containers, che aprirà il progetto nel tuo VS Code locale usando l'[estensione Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Avvia Docker Desktop (installalo se non è già presente)
2. Apri il progetto:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Nella finestra di VS Code che si apre, una volta che i file del progetto sono caricati (potrebbe richiedere alcuni minuti), apri una finestra del terminale.
4. Prosegui con i [passaggi di deployment](../../../../../md/01.Introduction/01)

### Ambiente Locale

1. Assicurati che i seguenti strumenti siano installati:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Testa il modello

1. Chiedi a Ollama di scaricare ed eseguire il modello phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Il download del modello richiederà qualche minuto.

2. Quando vedi "success" nell'output, puoi inviare un messaggio a quel modello dal prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Dopo alcuni secondi, dovresti vedere arrivare una risposta in streaming dal modello.

4. Per scoprire le diverse tecniche usate con i modelli di linguaggio, apri il notebook Python [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) ed esegui ogni cella. Se hai usato un modello diverso da 'phi3:mini', modifica `MODEL_NAME` nella prima cella.

5. Per conversare con il modello phi3:mini da Python, apri il file Python [chat.py](../../../../../code/01.Introduce/chat.py) ed eseguilo. Puoi cambiare `MODEL_NAME` in cima al file secondo necessità, e puoi anche modificare il messaggio di sistema o aggiungere esempi few-shot se vuoi.

**Disclaimer**:  
Questo documento è stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Pur impegnandoci per garantire accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa deve essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un umano. Non siamo responsabili per eventuali malintesi o interpretazioni errate derivanti dall’uso di questa traduzione.