<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-09T07:06:45+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "it"
}
-->
# Inizia con Phi-3 in locale

Questa guida ti aiuterà a configurare il tuo ambiente locale per eseguire il modello Phi-3 utilizzando Ollama. Puoi eseguire il modello in diversi modi, incluso l’uso di GitHub Codespaces, VS Code Dev Containers o il tuo ambiente locale.

## Configurazione dell’ambiente

### GitHub Codespaces

Puoi eseguire questo template virtualmente utilizzando GitHub Codespaces. Il pulsante aprirà un’istanza di VS Code basata su web nel tuo browser:

1. Apri il template (potrebbe richiedere alcuni minuti):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Apri una finestra del terminale

### VS Code Dev Containers

⚠️ Questa opzione funziona solo se Docker Desktop ha almeno 16 GB di RAM allocati. Se hai meno di 16 GB di RAM, puoi provare l’[opzione GitHub Codespaces](../../../../../md/01.Introduction/01) o [configurarlo in locale](../../../../../md/01.Introduction/01).

Un’opzione correlata è VS Code Dev Containers, che aprirà il progetto nel tuo VS Code locale utilizzando l’[estensione Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Avvia Docker Desktop (installalo se non è già presente)
2. Apri il progetto:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Nella finestra di VS Code che si apre, una volta che i file del progetto sono visibili (potrebbe richiedere alcuni minuti), apri una finestra del terminale.
4. Continua con i [passaggi di deployment](../../../../../md/01.Introduction/01)

### Ambiente locale

1. Assicurati che i seguenti strumenti siano installati:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Testa il modello

1. Chiedi a Ollama di scaricare ed eseguire il modello phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Il download del modello richiederà alcuni minuti.

2. Quando vedi "success" nell’output, puoi inviare un messaggio a quel modello dal prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Dopo qualche secondo, dovresti vedere un flusso di risposta dal modello.

4. Per approfondire le diverse tecniche utilizzate con i modelli linguistici, apri il notebook Python [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) ed esegui ogni cella. Se hai usato un modello diverso da 'phi3:mini', modifica il `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` all’inizio del file secondo necessità, e puoi anche modificare il messaggio di sistema o aggiungere esempi few-shot se vuoi.

**Disclaimer**:  
Questo documento è stato tradotto utilizzando il servizio di traduzione automatica AI [Co-op Translator](https://github.com/Azure/co-op-translator). Pur impegnandoci per garantire l'accuratezza, si prega di considerare che le traduzioni automatiche possono contenere errori o inesattezze. Il documento originale nella sua lingua nativa deve essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda la traduzione professionale umana. Non ci assumiamo alcuna responsabilità per eventuali malintesi o interpretazioni errate derivanti dall'uso di questa traduzione.