<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:20:43+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "it"
}
-->
# Tecnologie chiave menzionate includono

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - un'API a basso livello per l'apprendimento automatico accelerato dall'hardware basata su DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - una piattaforma di calcolo parallelo e modello di interfaccia di programmazione delle applicazioni (API) sviluppata da Nvidia, che consente l'elaborazione general-purpose su unità di elaborazione grafica (GPU).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - un formato aperto progettato per rappresentare modelli di machine learning che facilita l'interoperabilità tra diversi framework ML.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - un formato utilizzato per rappresentare e aggiornare modelli di machine learning, particolarmente utile per modelli linguistici più piccoli che possono funzionare efficacemente su CPU con quantizzazione a 4-8 bit.

## DirectML

DirectML è un'API a basso livello che permette l'apprendimento automatico accelerato dall'hardware. È costruita sopra DirectX 12 per sfruttare l'accelerazione GPU ed è indipendente dal fornitore, il che significa che non richiede modifiche al codice per funzionare su GPU di diversi produttori. È principalmente utilizzata per carichi di lavoro di addestramento e inferenza di modelli su GPU.

Per quanto riguarda il supporto hardware, DirectML è progettata per funzionare con un'ampia gamma di GPU, incluse GPU integrate e discrete AMD, GPU integrate Intel e GPU discrete NVIDIA. Fa parte della Windows AI Platform ed è supportata su Windows 10 e 11, consentendo l'addestramento e l'inferenza di modelli su qualsiasi dispositivo Windows.

Sono stati fatti aggiornamenti e ci sono opportunità legate a DirectML, come il supporto fino a 150 operatori ONNX ed è utilizzata sia da ONNX runtime che da WinML. È supportata dai principali Integrated Hardware Vendors (IHVs), ognuno dei quali implementa vari metacomandi.

## CUDA

CUDA, acronimo di Compute Unified Device Architecture, è una piattaforma di calcolo parallelo e modello di API sviluppato da Nvidia. Permette agli sviluppatori software di utilizzare una GPU abilitata CUDA per l'elaborazione general-purpose, un approccio chiamato GPGPU (General-Purpose computing on Graphics Processing Units). CUDA è un elemento chiave per l'accelerazione GPU di Nvidia ed è ampiamente utilizzata in vari settori, tra cui machine learning, calcolo scientifico e elaborazione video.

Il supporto hardware per CUDA è specifico per le GPU Nvidia, essendo una tecnologia proprietaria sviluppata da Nvidia. Ogni architettura supporta versioni specifiche del toolkit CUDA, che fornisce le librerie e gli strumenti necessari agli sviluppatori per costruire ed eseguire applicazioni CUDA.

## ONNX

ONNX (Open Neural Network Exchange) è un formato aperto progettato per rappresentare modelli di machine learning. Fornisce una definizione di un modello di grafo computazionale estensibile, oltre a definizioni di operatori integrati e tipi di dati standard. ONNX permette agli sviluppatori di spostare i modelli tra diversi framework ML, facilitando l'interoperabilità e semplificando la creazione e il deployment di applicazioni AI.

Phi3 mini può funzionare con ONNX Runtime su CPU e GPU su diversi dispositivi, inclusi server, desktop Windows, Linux e Mac, e CPU mobili.  
Le configurazioni ottimizzate che abbiamo aggiunto sono:

- Modelli ONNX per int4 DML: quantizzati a int4 tramite AWQ  
- Modello ONNX per fp16 CUDA  
- Modello ONNX per int4 CUDA: quantizzato a int4 tramite RTN  
- Modello ONNX per int4 CPU e Mobile: quantizzato a int4 tramite RTN

## Llama.cpp

Llama.cpp è una libreria software open-source scritta in C++. Esegue inferenza su vari Large Language Models (LLM), incluso Llama. Sviluppata insieme alla libreria ggml (una libreria tensor generale), llama.cpp punta a fornire inferenza più veloce e un uso di memoria inferiore rispetto all'implementazione originale in Python. Supporta ottimizzazioni hardware, quantizzazione e offre una semplice API con esempi3. Se sei interessato a un'inferenza efficiente di LLM, llama.cpp vale la pena di essere esplorato, dato che Phi3 può eseguire Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) è un formato utilizzato per rappresentare e aggiornare modelli di machine learning. È particolarmente utile per modelli linguistici più piccoli (SLM) che possono funzionare efficacemente su CPU con quantizzazione a 4-8 bit. GGUF è vantaggioso per prototipazione rapida e per eseguire modelli su dispositivi edge o in batch come pipeline CI/CD.

**Disclaimer**:  
Questo documento è stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Pur impegnandoci per garantire l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa deve essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda la traduzione professionale umana. Non siamo responsabili per eventuali malintesi o interpretazioni errate derivanti dall'uso di questa traduzione.