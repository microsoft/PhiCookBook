<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:43:50+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "it"
}
-->
# Tecnologie chiave menzionate includono

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - un'API di basso livello per il machine learning accelerato dall'hardware, basata su DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - una piattaforma di calcolo parallelo e un modello di API sviluppato da Nvidia, che consente l'elaborazione general-purpose su unità di elaborazione grafica (GPU).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - un formato aperto progettato per rappresentare modelli di machine learning che garantisce interoperabilità tra diversi framework ML.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - un formato utilizzato per rappresentare e aggiornare modelli di machine learning, particolarmente utile per modelli linguistici più piccoli che possono funzionare efficacemente su CPU con quantizzazione a 4-8 bit.

## DirectML

DirectML è un'API di basso livello che permette il machine learning accelerato dall'hardware. È costruita sopra DirectX 12 per sfruttare l'accelerazione GPU ed è indipendente dal fornitore, il che significa che non richiede modifiche al codice per funzionare con GPU di diversi produttori. Viene utilizzata principalmente per carichi di lavoro di addestramento e inferenza su GPU.

Per quanto riguarda il supporto hardware, DirectML è progettata per funzionare con un'ampia gamma di GPU, incluse GPU integrate e discrete AMD, GPU integrate Intel e GPU discrete NVIDIA. Fa parte della Windows AI Platform ed è supportata su Windows 10 e 11, permettendo l'addestramento e l'inferenza di modelli su qualsiasi dispositivo Windows.

Sono stati introdotti aggiornamenti e opportunità legate a DirectML, come il supporto fino a 150 operatori ONNX ed è utilizzata sia da ONNX runtime che da WinML. È supportata dai principali Integrated Hardware Vendors (IHV), ognuno dei quali implementa vari metacomandi.

## CUDA

CUDA, acronimo di Compute Unified Device Architecture, è una piattaforma di calcolo parallelo e un modello di API creato da Nvidia. Consente agli sviluppatori software di utilizzare una GPU abilitata CUDA per l'elaborazione general-purpose, un approccio noto come GPGPU (General-Purpose computing on Graphics Processing Units). CUDA è un elemento chiave per l'accelerazione GPU di Nvidia ed è ampiamente utilizzata in vari campi, tra cui machine learning, calcolo scientifico e elaborazione video.

Il supporto hardware per CUDA è specifico per le GPU Nvidia, essendo una tecnologia proprietaria sviluppata da Nvidia. Ogni architettura supporta versioni specifiche del toolkit CUDA, che fornisce le librerie e gli strumenti necessari agli sviluppatori per creare ed eseguire applicazioni CUDA.

## ONNX

ONNX (Open Neural Network Exchange) è un formato aperto progettato per rappresentare modelli di machine learning. Fornisce una definizione di un modello di grafo computazionale estensibile, oltre a definizioni di operatori integrati e tipi di dati standard. ONNX permette agli sviluppatori di spostare modelli tra diversi framework ML, favorendo l'interoperabilità e semplificando la creazione e il deployment di applicazioni AI.

Phi3 mini può funzionare con ONNX Runtime su CPU e GPU su diversi dispositivi, inclusi server, desktop Windows, Linux e Mac, e CPU mobili.  
Le configurazioni ottimizzate che abbiamo aggiunto sono:

- modelli ONNX per int4 DML: quantizzati a int4 tramite AWQ  
- modello ONNX per fp16 CUDA  
- modello ONNX per int4 CUDA: quantizzato a int4 tramite RTN  
- modello ONNX per int4 CPU e Mobile: quantizzato a int4 tramite RTN

## Llama.cpp

Llama.cpp è una libreria software open-source scritta in C++. Esegue inferenza su vari Large Language Models (LLM), incluso Llama. Sviluppata insieme alla libreria ggml (una libreria tensoriale general-purpose), llama.cpp mira a fornire inferenza più veloce e un uso della memoria inferiore rispetto all'implementazione originale in Python. Supporta l'ottimizzazione hardware, la quantizzazione e offre un'API semplice con esempi. Se sei interessato a un'inferenza efficiente di LLM, llama.cpp vale la pena di essere esplorata, dato che Phi3 può eseguire Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) è un formato utilizzato per rappresentare e aggiornare modelli di machine learning. È particolarmente utile per modelli linguistici più piccoli (SLM) che possono funzionare efficacemente su CPU con quantizzazione a 4-8 bit. GGUF è vantaggioso per il rapido prototipaggio e per l'esecuzione di modelli su dispositivi edge o in batch job come pipeline CI/CD.

**Disclaimer**:  
Questo documento è stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Pur impegnandoci per garantire l’accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa deve essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un umano. Non ci assumiamo alcuna responsabilità per eventuali malintesi o interpretazioni errate derivanti dall’uso di questa traduzione.