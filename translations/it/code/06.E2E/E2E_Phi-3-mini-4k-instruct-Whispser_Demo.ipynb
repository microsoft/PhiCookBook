{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot Interattivo Phi 3 Mini 4K Instruct con Whisper\n",
    "\n",
    "### Introduzione:\n",
    "Il Chatbot Interattivo Phi 3 Mini 4K Instruct è uno strumento che consente agli utenti di interagire con la demo Microsoft Phi 3 Mini 4K Instruct utilizzando input testuali o audio. Il chatbot può essere utilizzato per una varietà di attività, come traduzioni, aggiornamenti meteo e raccolta di informazioni generali.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[Crea il tuo Huggingface Access Token](https://huggingface.co/settings/tokens)\n",
    "\n",
    "Crea un nuovo token  \n",
    "Fornisci un nuovo nome  \n",
    "Seleziona i permessi di scrittura  \n",
    "Copia il token e salvalo in un luogo sicuro  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il seguente codice Python esegue due compiti principali: importare il modulo `os` e impostare una variabile d'ambiente.\n",
    "\n",
    "1. Importazione del modulo `os`:\n",
    "   - Il modulo `os` in Python fornisce un modo per interagire con il sistema operativo. Permette di svolgere varie operazioni legate al sistema operativo, come accedere alle variabili d'ambiente, lavorare con file e directory, ecc.\n",
    "   - In questo codice, il modulo `os` viene importato utilizzando l'istruzione `import`. Questa istruzione rende disponibile la funzionalità del modulo `os` per l'uso nello script Python corrente.\n",
    "\n",
    "2. Impostazione di una variabile d'ambiente:\n",
    "   - Una variabile d'ambiente è un valore che può essere accessibile dai programmi in esecuzione sul sistema operativo. È un modo per memorizzare configurazioni o altre informazioni che possono essere utilizzate da più programmi.\n",
    "   - In questo codice, una nuova variabile d'ambiente viene impostata utilizzando il dizionario `os.environ`. La chiave del dizionario è `'HF_TOKEN'`, e il valore viene assegnato dalla variabile `HUGGINGFACE_TOKEN`.\n",
    "   - La variabile `HUGGINGFACE_TOKEN` è definita appena sopra questo frammento di codice ed è assegnata a una stringa `\"hf_**************\"` utilizzando la sintassi `#@param`. Questa sintassi è spesso utilizzata nei notebook Jupyter per permettere l'inserimento di dati da parte dell'utente e la configurazione dei parametri direttamente nell'interfaccia del notebook.\n",
    "   - Impostando la variabile d'ambiente `'HF_TOKEN'`, essa può essere accessibile da altre parti del programma o da altri programmi in esecuzione sullo stesso sistema operativo.\n",
    "\n",
    "In sintesi, questo codice importa il modulo `os` e imposta una variabile d'ambiente chiamata `'HF_TOKEN'` con il valore fornito nella variabile `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo frammento di codice definisce una funzione chiamata clear_output, utilizzata per cancellare l'output della cella corrente in Jupyter Notebook o IPython. Analizziamo il codice e comprendiamone la funzionalità:\n",
    "\n",
    "La funzione clear_output accetta un parametro chiamato wait, che è un valore booleano. Di default, wait è impostato su False. Questo parametro determina se la funzione deve attendere che sia disponibile un nuovo output per sostituire quello esistente prima di cancellarlo.\n",
    "\n",
    "La funzione stessa viene utilizzata per cancellare l'output della cella corrente. In Jupyter Notebook o IPython, quando una cella produce un output, come testo stampato o grafici, quell'output viene visualizzato sotto la cella. La funzione clear_output consente di cancellare quell'output.\n",
    "\n",
    "L'implementazione della funzione non è fornita nel frammento di codice, come indicato dall'ellissi (...). L'ellissi rappresenta un segnaposto per il codice effettivo che esegue la cancellazione dell'output. L'implementazione della funzione potrebbe coinvolgere l'interazione con l'API di Jupyter Notebook o IPython per rimuovere l'output esistente dalla cella.\n",
    "\n",
    "In sintesi, questa funzione offre un modo pratico per cancellare l'output della cella corrente in Jupyter Notebook o IPython, facilitando la gestione e l'aggiornamento dell'output visualizzato durante sessioni di codifica interattive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esegui la sintesi vocale (TTS) utilizzando il servizio Edge TTS. Analizziamo le implementazioni delle funzioni rilevanti una per una:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Questa funzione prende un valore di input e calcola la stringa di velocità per la voce TTS. Il valore di input rappresenta la velocità desiderata del discorso, dove un valore di 1 rappresenta la velocità normale. La funzione calcola la stringa di velocità sottraendo 1 dal valore di input, moltiplicandolo per 100 e determinando il segno in base al fatto che il valore di input sia maggiore o uguale a 1. La funzione restituisce la stringa di velocità nel formato \"{segno}{velocità}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: Questa funzione prende un testo di input e una lingua come parametri. Divide il testo di input in frammenti basandosi sulle regole specifiche della lingua. In questa implementazione, se la lingua è \"English\", la funzione divide il testo a ogni punto (\".\") e rimuove eventuali spazi iniziali o finali. Successivamente, aggiunge un punto a ogni frammento e restituisce l'elenco filtrato dei frammenti.\n",
    "\n",
    "3. `tts_file_name(text)`: Questa funzione genera un nome file per il file audio TTS basandosi sul testo di input. Esegue diverse trasformazioni sul testo: rimuove un punto finale (se presente), converte il testo in minuscolo, elimina spazi iniziali e finali e sostituisce gli spazi con trattini bassi. Successivamente, tronca il testo a un massimo di 25 caratteri (se più lungo) o utilizza il testo completo se è vuoto. Infine, genera una stringa casuale utilizzando il modulo [`uuid`] e la combina con il testo troncato per creare il nome file nel formato \"/content/edge_tts_voice/{testo_troncato}_{stringa_casuale}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Questa funzione unisce più file audio in un unico file audio. Prende un elenco di percorsi di file audio e un percorso di output come parametri. La funzione inizializza un oggetto vuoto `AudioSegment` chiamato [`merged_audio`]. Successivamente, itera attraverso ogni percorso di file audio, carica il file audio utilizzando il metodo `AudioSegment.from_file()` della libreria `pydub` e aggiunge il file audio corrente all'oggetto [`merged_audio`]. Infine, esporta l'audio unito nel percorso di output specificato nel formato MP3.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: Questa funzione esegue l'operazione TTS utilizzando il servizio Edge TTS. Prende un elenco di frammenti di testo, la velocità del discorso, il nome della voce e il percorso di salvataggio come parametri. Se il numero di frammenti è maggiore di 1, la funzione crea una directory per memorizzare i file audio dei singoli frammenti. Successivamente, itera attraverso ogni frammento, costruisce un comando Edge TTS utilizzando la funzione `calculate_rate_string()`, il nome della voce e il testo del frammento, ed esegue il comando utilizzando la funzione `os.system()`. Se l'esecuzione del comando ha successo, aggiunge il percorso del file audio generato a un elenco. Dopo aver elaborato tutti i frammenti, unisce i file audio individuali utilizzando la funzione `merge_audio_files()` e salva l'audio unito nel percorso di salvataggio specificato. Se c'è solo un frammento, genera direttamente il comando Edge TTS e salva l'audio nel percorso di salvataggio. Infine, restituisce il percorso di salvataggio del file audio generato.\n",
    "\n",
    "6. `random_audio_name_generate()`: Questa funzione genera un nome file audio casuale utilizzando il modulo [`uuid`]. Genera un UUID casuale, lo converte in una stringa, prende i primi 8 caratteri, aggiunge l'estensione \".mp3\" e restituisce il nome file audio casuale.\n",
    "\n",
    "7. `talk(input_text)`: Questa funzione è il punto di ingresso principale per eseguire l'operazione TTS. Prende un testo di input come parametro. Controlla prima la lunghezza del testo di input per determinare se si tratta di una frase lunga (maggiore o uguale a 600 caratteri). In base alla lunghezza e al valore della variabile `translate_text_flag`, determina la lingua e genera l'elenco dei frammenti di testo utilizzando la funzione `make_chunks()`. Successivamente, genera un percorso di salvataggio per il file audio utilizzando la funzione `random_audio_name_generate()`. Infine, chiama la funzione `edge_free_tts()` per eseguire l'operazione TTS e restituisce il percorso di salvataggio del file audio generato.\n",
    "\n",
    "Nel complesso, queste funzioni lavorano insieme per dividere il testo di input in frammenti, generare un nome file per il file audio, eseguire l'operazione TTS utilizzando il servizio Edge TTS e unire i file audio individuali in un unico file audio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'implementazione di due funzioni: convert_to_text e run_text_prompt, così come la dichiarazione di due classi: str e Audio.\n",
    "\n",
    "La funzione convert_to_text prende un audio_path come input e trascrive l'audio in testo utilizzando un modello chiamato whisper_model. La funzione verifica innanzitutto se il flag gpu è impostato su True. Se lo è, il whisper_model viene utilizzato con determinati parametri come word_timestamps=True, fp16=True, language='English' e task='translate'. Se il flag gpu è False, il whisper_model viene utilizzato con fp16=False. La trascrizione risultante viene quindi salvata in un file chiamato 'scan.txt' e restituita come testo.\n",
    "\n",
    "La funzione run_text_prompt prende un messaggio e una chat_history come input. Utilizza la funzione phi_demo per generare una risposta da un chatbot basata sul messaggio di input. La risposta generata viene poi passata alla funzione talk, che converte la risposta in un file audio e restituisce il percorso del file. La classe Audio viene utilizzata per visualizzare e riprodurre il file audio. L'audio viene visualizzato utilizzando la funzione display del modulo IPython.display, e l'oggetto Audio viene creato con il parametro autoplay=True, in modo che l'audio inizi a suonare automaticamente. La chat_history viene aggiornata con il messaggio di input e la risposta generata, e vengono restituiti una stringa vuota e la chat_history aggiornata.\n",
    "\n",
    "La classe str è una classe integrata in Python che rappresenta una sequenza di caratteri. Fornisce vari metodi per manipolare e lavorare con le stringhe, come capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill e altri. Questi metodi permettono di eseguire operazioni come ricerca, sostituzione, formattazione e manipolazione delle stringhe.\n",
    "\n",
    "La classe Audio è una classe personalizzata che rappresenta un oggetto audio. Viene utilizzata per creare un lettore audio nell'ambiente Jupyter Notebook. La classe accetta vari parametri come data, filename, url, embed, rate, autoplay e normalize. Il parametro data può essere un array numpy, una lista di campioni, una stringa che rappresenta un nome di file o URL, o dati PCM grezzi. Il parametro filename viene utilizzato per specificare un file locale da cui caricare i dati audio, e il parametro url viene utilizzato per specificare un URL da cui scaricare i dati audio. Il parametro embed determina se i dati audio devono essere incorporati utilizzando un URI di dati o referenziati dalla fonte originale. Il parametro rate specifica la frequenza di campionamento dei dati audio. Il parametro autoplay determina se l'audio deve iniziare a suonare automaticamente. Il parametro normalize specifica se i dati audio devono essere normalizzati (riscalati) al massimo intervallo possibile. La classe Audio fornisce anche metodi come reload per ricaricare i dati audio da file o URL, e attributi come src_attr, autoplay_attr e element_id_attr per recuperare gli attributi corrispondenti per l'elemento audio in HTML.\n",
    "\n",
    "In generale, queste funzioni e classi vengono utilizzate per trascrivere audio in testo, generare risposte audio da un chatbot e visualizzare e riprodurre audio nell'ambiente Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Disclaimer**:  \nQuesto documento è stato tradotto utilizzando il servizio di traduzione automatica [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un traduttore umano. Non siamo responsabili per eventuali incomprensioni o interpretazioni errate derivanti dall'uso di questa traduzione.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T19:57:36+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "it"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}