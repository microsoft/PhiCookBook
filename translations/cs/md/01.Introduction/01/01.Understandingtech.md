<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:30:56+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "cs"
}
-->
// Key technologies mentioned include

// 1. DirectML - a low-level API for hardware-accelerated machine learning built on top of DirectX 12.
// 2. CUDA - a parallel computing platform and application programming interface (API) model developed by Nvidia, enabling general-purpose processing on graphics processing units (GPUs).
// 3. ONNX (Open Neural Network Exchange) - an open format designed to represent machine learning models that provides interoperability between different ML frameworks.
// 4. GGUF (Generic Graph Update Format) - a format used for representing and updating machine learning models, particularly useful for smaller language models that can run effectively on CPUs with 4-8bit quantization.

namespace Technologies
{
    /// <summary>
    /// DirectML is a low-level API that enables hardware-accelerated machine learning.
    /// It’s built on top of DirectX 12 to utilize GPU acceleration and is vendor-agnostic,
    /// meaning it doesn’t require code changes to work across different GPU vendors.
    /// It’s primarily used for model training and inferencing workloads on GPUs.
    /// </summary>
    public class DirectML
    {
        // DirectML supports a wide range of GPUs, including AMD integrated and discrete GPUs,
        // Intel integrated GPUs, and NVIDIA discrete GPUs.
        // It’s part of the Windows AI Platform and is supported on Windows 10 & 11,
        // allowing for model training and inferencing on any Windows device.

        // Updates include support for up to 150 ONNX operators and usage by both ONNX runtime and WinML.
        // Major Integrated Hardware Vendors (IHVs) back it, each implementing various metacommands.
    }

    /// <summary>
    /// CUDA (Compute Unified Device Architecture) is a parallel computing platform and API model created by Nvidia.
    /// It allows software developers to use a CUDA-enabled GPU for general purpose processing – known as GPGPU.
    /// CUDA is a key enabler of Nvidia’s GPU acceleration and is widely used in machine learning, scientific computing, and video processing.
    /// </summary>
    public class CUDA
    {
        // Hardware support for CUDA is specific to Nvidia GPUs as it is proprietary technology.
        // Each architecture supports specific CUDA toolkit versions, providing necessary libraries and tools for development.
    }

    /// <summary>
    /// ONNX (Open Neural Network Exchange) is an open format to represent machine learning models.
    /// It defines an extensible computation graph model, built-in operators, and standard data types.
    /// ONNX enables model portability between different ML frameworks, facilitating AI application deployment.
    /// </summary>
    public class ONNX
    {
        // Phi3 mini can run ONNX Runtime on CPU and GPU across platforms including servers, Windows, Linux, Mac, and mobile CPUs.

        // Optimized configurations include:
        // - ONNX models for int4 DML: Quantized to int4 via AWQ
        // - ONNX model for fp16 CUDA
        // - ONNX model for int4 CUDA: Quantized to int4 via RTN
        // - ONNX model for int4 CPU and Mobile: Quantized to int4 via RTN
    }

    /// <summary>
    /// Llama.cpp is an open-source C++ library for inference on various Large Language Models (LLMs), including Llama.
    /// Developed alongside the ggml library, it aims for faster inference and lower memory use than the original Python implementation.
    /// Supports hardware optimization, quantization, and offers a simple API with examples.
    /// Phi3 can run Llama.cpp, making it a good choice for efficient LLM inference.
    /// </summary>
    public class LlamaCpp
    {
    }

    /// <summary>
    /// GGUF (Generic Graph Update Format) is used for representing and updating machine learning models.
    /// It’s especially useful for smaller language models that run efficiently on CPUs with 4-8bit quantization.
    /// GGUF facilitates rapid prototyping and running models on edge devices or batch jobs like CI/CD pipelines.
    /// </summary>
    public class GGUF
    {
    }
}

**Prohlášení o vyloučení odpovědnosti**:  
Tento dokument byl přeložen pomocí AI překladatelské služby [Co-op Translator](https://github.com/Azure/co-op-translator). Přestože usilujeme o přesnost, mějte prosím na paměti, že automatizované překlady mohou obsahovat chyby nebo nepřesnosti. Originální dokument v jeho mateřském jazyce by měl být považován za závazný zdroj. Pro důležité informace se doporučuje profesionální lidský překlad. Nejsme odpovědni za jakékoliv nedorozumění nebo chybné interpretace vzniklé použitím tohoto překladu.