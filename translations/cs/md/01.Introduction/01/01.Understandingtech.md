<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:47:26+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "cs"
}
-->
# Klíčové zmíněné technologie zahrnují

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) – nízkoúrovňové API pro hardwarově akcelerované strojové učení postavené na DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) – paralelní výpočetní platforma a aplikační programovací rozhraní (API) vyvinuté společností Nvidia, umožňující obecné zpracování na grafických procesorech (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) – otevřený formát navržený pro reprezentaci modelů strojového učení, který zajišťuje interoperabilitu mezi různými ML frameworky.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) – formát používaný pro reprezentaci a aktualizaci modelů strojového učení, obzvláště užitečný pro menší jazykové modely, které mohou efektivně běžet na CPU s 4-8bitovou kvantizací.

## DirectML

DirectML je nízkoúrovňové API, které umožňuje hardwarově akcelerované strojové učení. Je postavené na DirectX 12, aby využívalo akceleraci GPU, a je nezávislé na výrobci, což znamená, že nevyžaduje změny kódu pro práci s různými výrobci GPU. Používá se především pro trénink modelů a inferenční úlohy na GPU.

Co se týče hardwarové podpory, DirectML je navržen tak, aby fungoval s širokou škálou GPU, včetně integrovaných a dedikovaných GPU od AMD, integrovaných GPU od Intelu a dedikovaných GPU od NVIDIA. Je součástí Windows AI Platform a je podporován na Windows 10 a 11, což umožňuje trénink a inferenci modelů na jakémkoli zařízení s Windows.

Byly provedeny aktualizace a rozšíření týkající se DirectML, například podpora až 150 ONNX operátorů a jeho využití jak v ONNX runtime, tak ve WinML. Podporují jej hlavní výrobci hardwaru (IHVs), kteří implementují různé metapříkazy.

## CUDA

CUDA, což znamená Compute Unified Device Architecture, je paralelní výpočetní platforma a aplikační programovací rozhraní (API) vytvořené společností Nvidia. Umožňuje vývojářům softwaru využívat grafický procesor (GPU) s podporou CUDA pro obecné výpočty – tento přístup se nazývá GPGPU (General-Purpose computing on Graphics Processing Units). CUDA je klíčovým prvkem akcelerace GPU od Nvidie a široce se používá v různých oblastech, včetně strojového učení, vědeckých výpočtů a zpracování videa.

Hardwarová podpora CUDA je specifická pro GPU od Nvidie, protože se jedná o proprietární technologii vyvinutou touto společností. Každá architektura podporuje specifické verze CUDA toolkit, který poskytuje potřebné knihovny a nástroje pro vývojáře k vytváření a spouštění CUDA aplikací.

## ONNX

ONNX (Open Neural Network Exchange) je otevřený formát navržený pro reprezentaci modelů strojového učení. Poskytuje definici rozšiřitelného modelu výpočetního grafu, stejně jako definice vestavěných operátorů a standardních datových typů. ONNX umožňuje vývojářům přenášet modely mezi různými ML frameworky, což zajišťuje interoperabilitu a usnadňuje tvorbu a nasazení AI aplikací.

Phi3 mini může běžet s ONNX Runtime na CPU i GPU napříč zařízeními, včetně serverových platforem, Windows, Linuxu, Mac desktopů a mobilních CPU.  
Optimalizované konfigurace, které jsme přidali, jsou:

- ONNX modely pro int4 DML: kvantizované na int4 pomocí AWQ  
- ONNX model pro fp16 CUDA  
- ONNX model pro int4 CUDA: kvantizované na int4 pomocí RTN  
- ONNX model pro int4 CPU a Mobile: kvantizované na int4 pomocí RTN  

## Llama.cpp

Llama.cpp je open-source softwarová knihovna napsaná v C++. Provádí inferenci na různých velkých jazykových modelech (LLM), včetně Llama. Vyvinuta společně s knihovnou ggml (obecná knihovna pro tenzory), llama.cpp si klade za cíl nabídnout rychlejší inferenci a nižší spotřebu paměti ve srovnání s původní implementací v Pythonu. Podporuje hardwarovou optimalizaci, kvantizaci a nabízí jednoduché API a příklady3. Pokud vás zajímá efektivní inference LLM, llama.cpp stojí za vyzkoušení, protože Phi3 může spouštět Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) je formát používaný pro reprezentaci a aktualizaci modelů strojového učení. Je obzvláště užitečný pro menší jazykové modely (SLM), které mohou efektivně běžet na CPU s 4-8bitovou kvantizací. GGUF je výhodný pro rychlé prototypování a spouštění modelů na edge zařízeních nebo v dávkových úlohách, jako jsou CI/CD pipeline.

**Prohlášení o vyloučení odpovědnosti**:  
Tento dokument byl přeložen pomocí AI překladatelské služby [Co-op Translator](https://github.com/Azure/co-op-translator). Přestože usilujeme o přesnost, mějte prosím na paměti, že automatické překlady mohou obsahovat chyby nebo nepřesnosti. Původní dokument v jeho mateřském jazyce by měl být považován za autoritativní zdroj. Pro důležité informace se doporučuje profesionální lidský překlad. Nejsme odpovědní za jakékoliv nedorozumění nebo nesprávné výklady vyplývající z použití tohoto překladu.