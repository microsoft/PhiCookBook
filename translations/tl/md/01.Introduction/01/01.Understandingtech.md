# Mga pangunahing teknolohiyang binanggit ay kinabibilangan ng

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - isang low-level API para sa hardware-accelerated machine learning na nakabase sa DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - isang parallel computing platform at application programming interface (API) model na binuo ng Nvidia, na nagpapahintulot ng general-purpose processing sa graphics processing units (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - isang bukas na format na idinisenyo upang kumatawan sa mga machine learning model na nagbibigay-daan sa interoperability sa pagitan ng iba't ibang ML frameworks.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - isang format na ginagamit para sa pagrepresenta at pag-update ng mga machine learning model, lalo na para sa mas maliliit na language models na epektibong tumatakbo sa CPUs gamit ang 4-8bit quantization.

## DirectML

Ang DirectML ay isang low-level API na nagpapagana ng hardware-accelerated machine learning. Nakabase ito sa DirectX 12 upang magamit ang GPU acceleration at vendor-agnostic, ibig sabihin ay hindi kailangan ng pagbabago sa code para gumana ito sa iba't ibang GPU vendors. Pangunahing ginagamit ito para sa model training at inferencing workloads sa GPUs.

Tungkol naman sa hardware support, ang DirectML ay idinisenyo upang gumana sa malawak na hanay ng GPUs, kabilang ang AMD integrated at discrete GPUs, Intel integrated GPUs, at NVIDIA discrete GPUs. Bahagi ito ng Windows AI Platform at sinusuportahan sa Windows 10 at 11, na nagpapahintulot ng model training at inferencing sa anumang Windows device.

May mga update at oportunidad na kaugnay ng DirectML, tulad ng pagsuporta sa hanggang 150 ONNX operators at paggamit nito ng parehong ONNX runtime at WinML. Sinusuportahan ito ng mga pangunahing Integrated Hardware Vendors (IHVs), na bawat isa ay nagpapatupad ng iba't ibang metacommands.

## CUDA

Ang CUDA, na nangangahulugang Compute Unified Device Architecture, ay isang parallel computing platform at application programming interface (API) model na nilikha ng Nvidia. Pinapayagan nito ang mga software developer na gamitin ang CUDA-enabled graphics processing unit (GPU) para sa general purpose processing â€“ isang paraan na tinatawag na GPGPU (General-Purpose computing on Graphics Processing Units). Ang CUDA ay isang mahalagang teknolohiya para sa GPU acceleration ng Nvidia at malawakang ginagamit sa iba't ibang larangan, kabilang ang machine learning, scientific computing, at video processing.

Ang hardware support para sa CUDA ay partikular sa mga GPU ng Nvidia, dahil ito ay proprietary technology na binuo ng Nvidia. Bawat arkitektura ay sumusuporta sa partikular na mga bersyon ng CUDA toolkit, na nagbibigay ng mga kinakailangang libraries at tools para sa mga developer upang makabuo at makapagpatakbo ng CUDA applications.

## ONNX

Ang ONNX (Open Neural Network Exchange) ay isang bukas na format na idinisenyo upang kumatawan sa mga machine learning model. Nagbibigay ito ng depinisyon ng isang extensible computation graph model, pati na rin ng mga built-in operators at standard data types. Pinapayagan ng ONNX ang mga developer na ilipat ang mga modelo sa pagitan ng iba't ibang ML frameworks, na nagpapadali ng interoperability at paggawa at pag-deploy ng mga AI application.

Ang Phi3 mini ay maaaring tumakbo gamit ang ONNX Runtime sa CPU at GPU sa iba't ibang device, kabilang ang mga server platform, Windows, Linux at Mac desktops, at mga mobile CPU.  
Ang mga optimized na configuration na idinagdag namin ay

- ONNX models para sa int4 DML: Quantized sa int4 gamit ang AWQ  
- ONNX model para sa fp16 CUDA  
- ONNX model para sa int4 CUDA: Quantized sa int4 gamit ang RTN  
- ONNX model para sa int4 CPU at Mobile: Quantized sa int4 gamit ang RTN  

## Llama.cpp

Ang Llama.cpp ay isang open-source software library na isinulat sa C++. Nagsasagawa ito ng inference sa iba't ibang Large Language Models (LLMs), kabilang ang Llama. Binuo kasabay ng ggml library (isang general-purpose tensor library), layunin ng llama.cpp na magbigay ng mas mabilis na inference at mas mababang paggamit ng memorya kumpara sa orihinal na implementasyon sa Python. Sinusuportahan nito ang hardware optimization, quantization, at nag-aalok ng simpleng API at mga halimbawa3. Kung interesado ka sa epektibong LLM inference, sulit na subukan ang llama.cpp dahil kayang patakbuhin ng Phi3 ang Llama.cpp.

## GGUF

Ang GGUF (Generic Graph Update Format) ay isang format na ginagamit para sa pagrepresenta at pag-update ng mga machine learning model. Partikular itong kapaki-pakinabang para sa mas maliliit na language models (SLMs) na epektibong tumatakbo sa CPUs gamit ang 4-8bit quantization. Ang GGUF ay mainam para sa mabilisang prototyping at pagpapatakbo ng mga modelo sa mga edge device o sa mga batch jobs tulad ng CI/CD pipelines.

**Paalala**:  
Ang dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagamat nagsusumikap kami para sa katumpakan, pakatandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o di-tumpak na impormasyon. Ang orihinal na dokumento sa kanyang sariling wika ang dapat ituring na pangunahing sanggunian. Para sa mahahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na maaaring magmula sa paggamit ng pagsasaling ito.