<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:28:50+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "tl"
}
-->
# Mga pangunahing teknolohiyang nabanggit ay kinabibilangan ng

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - isang low-level API para sa hardware-accelerated machine learning na nakabase sa DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - isang parallel computing platform at application programming interface (API) model na ginawa ng Nvidia, na nagpapahintulot ng general-purpose processing sa graphics processing units (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - isang bukas na format na idinisenyo para irepresenta ang mga machine learning model na nagbibigay ng interoperability sa pagitan ng iba't ibang ML frameworks.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - isang format na ginagamit para irepresenta at i-update ang mga machine learning model, na partikular na kapaki-pakinabang para sa mas maliliit na language model na epektibong tumatakbo sa CPUs gamit ang 4-8bit quantization.

## DirectML

Ang DirectML ay isang low-level API na nagpapahintulot ng hardware-accelerated machine learning. Nakabase ito sa DirectX 12 upang magamit ang GPU acceleration at vendor-agnostic ito, ibig sabihin hindi kailangan ng pagbabago sa code para gumana ito sa iba't ibang GPU vendors. Pangunahing ginagamit ito para sa model training at inferencing workloads sa GPUs.

Tungkol naman sa hardware support, ang DirectML ay idinisenyo para gumana sa malawak na hanay ng GPUs, kabilang ang AMD integrated at discrete GPUs, Intel integrated GPUs, at NVIDIA discrete GPUs. Bahagi ito ng Windows AI Platform at sinusuportahan sa Windows 10 at 11, kaya puwedeng gamitin para sa model training at inferencing sa anumang Windows device.

May mga updates at oportunidad na kaugnay ng DirectML, tulad ng suporta sa hanggang 150 ONNX operators at paggamit nito ng parehong ONNX runtime at WinML. Sinusuportahan ito ng mga pangunahing Integrated Hardware Vendors (IHVs), na bawat isa ay nagpapatupad ng iba't ibang metacommands.

## CUDA

Ang CUDA, na nangangahulugang Compute Unified Device Architecture, ay isang parallel computing platform at application programming interface (API) model na nilikha ng Nvidia. Pinapahintulutan nito ang mga software developer na gamitin ang CUDA-enabled graphics processing unit (GPU) para sa general purpose processing â€“ isang paraan na tinatawag na GPGPU (General-Purpose computing on Graphics Processing Units). Ang CUDA ay isang pangunahing dahilan ng GPU acceleration ng Nvidia at malawak na ginagamit sa iba't ibang larangan, kabilang ang machine learning, scientific computing, at video processing.

Ang hardware support para sa CUDA ay partikular sa mga GPU ng Nvidia, dahil ito ay proprietary technology na ginawa ng Nvidia. Bawat architecture ay sumusuporta sa partikular na bersyon ng CUDA toolkit, na nagbibigay ng kinakailangang libraries at tools para sa mga developer upang makabuo at makapagpatakbo ng CUDA applications.

## ONNX

Ang ONNX (Open Neural Network Exchange) ay isang bukas na format na idinisenyo para irepresenta ang mga machine learning model. Nagbibigay ito ng depinisyon ng extensible computation graph model, pati na rin ng mga built-in operators at standard data types. Pinapahintulutan ng ONNX ang mga developer na ilipat ang mga modelo sa pagitan ng iba't ibang ML frameworks, kaya nagiging mas madali ang interoperability at paggawa at deployment ng AI applications.

Ang Phi3 mini ay puwedeng tumakbo gamit ang ONNX Runtime sa CPU at GPU sa iba't ibang devices, kabilang ang server platforms, Windows, Linux at Mac desktops, at mobile CPUs.  
Ang mga optimized na configurations na idinagdag namin ay

- ONNX models para sa int4 DML: Quantized sa int4 gamit ang AWQ  
- ONNX model para sa fp16 CUDA  
- ONNX model para sa int4 CUDA: Quantized sa int4 gamit ang RTN  
- ONNX model para sa int4 CPU at Mobile: Quantized sa int4 gamit ang RTN  

## Llama.cpp

Ang Llama.cpp ay isang open-source software library na isinulat sa C++. Gumagawa ito ng inference sa iba't ibang Large Language Models (LLMs), kabilang ang Llama. Ginawa ito kasabay ng ggml library (isang general-purpose tensor library), layunin ng llama.cpp na magbigay ng mas mabilis na inference at mas mababang paggamit ng memorya kumpara sa orihinal na Python implementation. Sinusuportahan nito ang hardware optimization, quantization, at nag-aalok ng simpleng API at mga halimbawa3. Kung interesado ka sa epektibong LLM inference, sulit subukan ang llama.cpp dahil puwedeng patakbuhin ito ng Phi3.

## GGUF

Ang GGUF (Generic Graph Update Format) ay isang format na ginagamit para irepresenta at i-update ang mga machine learning model. Partikular itong kapaki-pakinabang para sa mas maliliit na language model (SLMs) na epektibong tumatakbo sa CPUs gamit ang 4-8bit quantization. Ang GGUF ay mabisa para sa mabilisang prototyping at pagpapatakbo ng mga modelo sa edge devices o sa batch jobs tulad ng CI/CD pipelines.

**Pagtatanggol**:  
Ang dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagamat aming pinagsisikapang maging tumpak ang salin, pakatandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o di-tumpak na impormasyon. Ang orihinal na dokumento sa orihinal nitong wika ang dapat ituring na pangunahing sanggunian. Para sa mahahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaintindihan o maling interpretasyon na maaaring magmula sa paggamit ng pagsasaling ito.