<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7f72d7981ed3640865700f51ae407da4",
  "translation_date": "2026-01-14T15:22:45+00:00",
  "source_file": "md/02.Application/01.TextAndChat/Phi3/E2E_Phi-3-mini_with_whisper.md",
  "language_code": "mr"
}
-->
# इंटरॅक्टिव्ह Phi 3 मिनी 4K इन्स्ट्रक्ट चॅटबॉट विथ व्हिस्पर

## आढावा

इंटरॅक्टिव्ह Phi 3 मिनी 4K इन्स्ट्रक्ट चॅटबॉट हा एक साधन आहे जे वापरकर्त्यांना Microsoft Phi 3 मिनी 4K इन्स्ट्रक्ट डेमोशी टेक्स्ट किंवा ऑडिओ इनपुटच्या माध्यमातून संवाद साधण्याची परवानगी देतो. हा चॅटबॉट अनेक कामांसाठी वापरला जाऊ शकतो, जसे की भाषांतर, हवामान अद्यतने, आणि सामान्य माहिती गोळा करणे.

### सुरूवात कशी करावी

हा चॅटबॉट वापरण्यासाठी, फक्त खालील सूचनांचे पालन करा:

1. नवीन [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) उघडा
2. नोटबुकच्या मुख्य विंडोमध्ये, तुम्हाला टेक्स्ट इनपुट बॉक्स आणि "Send" बटण असलेले चॅटबॉक्स इंटरफेस दिसेल.
3. टेक्स्ट-आधारित चॅटबॉट वापरण्यासाठी, फक्त तुमचा संदेश टेक्स्ट इनपुट बॉक्समध्ये टाका आणि "Send" बटणावर क्लिक करा. चॅटबॉट एक ऑडिओ फाइल तयार करेल जी नोटबुकमध्ये थेट प्ले केली जाऊ शकते.

**टीप**: या साधनासाठी GPU आणि Microsoft Phi-3 व OpenAI Whisper मॉडेल्सची गरज आहे, जे भाषण ओळख आणि भाषांतरासाठी वापरले जातात.

### GPU आवश्यकताः

हा डेमो चालवण्यासाठी तुम्हाला 12Gb GPU मेमरीची गरज आहे.

**Microsoft-Phi-3-Mini-4K instruct** डेमो GPU वर चालवण्यासाठी मेमरीची गरज इनपुट डेटा (ऑडिओ किंवा टेक्स्ट), भाषांतरासाठी वापरलेली भाषा, मॉडेलची गती, आणि GPUवरील उपलब्ध मेमरीसह अनेक घटकांवर अवलंबून असते.

साधारणपणे, Whisper मॉडेल GPUs वर चालण्यासाठी डिझाइन केलेले आहे. Whisper मॉडेल चालवण्यासाठी शिफारस केलेली किमान GPU मेमरी 8 GB आहे, पण आवश्यक असल्यास ती मोठ्या मेमरीची देखील हाताळणी करू शकते.

हे महत्त्वाचे आहे की, मोठ्या प्रमाणातील डेटा किंवा उच्च प्रमाणावर विनंत्या मॉडेलवर चालवल्यास अधिक GPU मेमरीची गरज भासू शकते किंवा कार्यक्षमतेमध्ये अडचणी येऊ शकतात. आपल्या उपयोग प्रकरणाची चाचणी वेगवेगळ्या सेटिंग्जसह करा आणि मेमरी वापरावर लक्ष ठेवा जेणेकरून तुमच्या विशिष्ट गरजांसाठी सर्वोत्तम सेटिंग्ज निश्चित करता येतील.

## इंटरॅक्टिव्ह Phi 3 मिनी 4K इन्स्ट्रक्ट चॅटबॉट विथ व्हिस्पर साठी E2E उदा.

[Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) नावाच्या ज्युपिटर नोटबुकमध्ये दाखवले आहे की Microsoft Phi 3 मिनी 4K इन्स्ट्रक्ट डेमोचा वापर कसा ऑडिओ किंवा लिहिलेल्या टेक्स्ट इनपुटवरून टेक्स्ट तयार करण्यासाठी करता येतो. नोटबुकमध्ये अनेक फंक्शन्स परिभाषित केले आहेत:

1. `tts_file_name(text)`: हा फंक्शन इनपुट टेक्स्टच्या आधारे जेनरेट झालेल्या ऑडिओ फाइलसाठी एक फाइल नाव तयार करतो.
2. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: हा फंक्शन Edge TTS API वापरून इनपुट टेक्स्टच्या टुक्र्यांच्या यादीतून ऑडिओ फाइल तयार करतो. इनपुट पॅरामीटर्समध्ये टुकड्यांची यादी, भाषणाचा वेग, आवाजाचे नाव, आणि ऑडिओ फाइल सेव्ह करण्यासाठी आउटपुट पथ असतात.
3. `talk(input_text)`: हा फंक्शन Edge TTS API चा वापर करून ऑडिओ फाइल तयार करतो आणि ती /content/audio निर्देशिकेत रँडम नावाने सेव्ह करतो. इनपुट म्हणजे रूपांतरित करण्यासाठी टेक्स्ट.
4. `run_text_prompt(message, chat_history)`: हा फंक्शन Microsoft Phi 3 मिनी 4K इन्स्ट्रक्ट डेमो वापरून संदेश इनपुटवरून ऑडिओ फाइल तयार करतो आणि ती चॅट इतिहासात जोडतो.
5. `run_audio_prompt(audio, chat_history)`: हा फंक्शन व्हिस्पर मॉडेल API वापरून ऑडिओ फाइल टेक्स्टमध्ये रूपांतरित करतो आणि नंतर `run_text_prompt()` फंक्शनला पाठवतो.
6. कोड Gradio अॅप सुरू करतो जे वापरकर्त्यांना Phi 3 मिनी 4K इन्स्ट्रक्ट डेमोशी संदेश टाकून किंवा ऑडिओ फाइल्स अपलोड करून संवाद साधण्याची परवानगी देते. आउटपुट अॅपमध्ये टेक्स्ट संदेश म्हणून दाखवले जाते.

## समस्या निवारण

Cuda GPU ड्रायव्हर्स इंस्टॉल करणे

1. तुमची Linux अ‍ॅप्लिकेशन्स अद्ययावत आहेत की नाही याची खात्री करा

    ```bash
    sudo apt update
    ```

1. Cuda ड्रायव्हर्स इंस्टॉल करा

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. cuda ड्रायव्हर स्थान नोंदणी करा

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. Nvidia GPU मेमरी आकार तपासा (आवश्यक 12GB GPU मेमरी)

    ```bash
    nvidia-smi
    ```

1. कॅशे रिकामा करा: जर तुम्ही PyTorch वापरत असाल तर, torch.cuda.empty_cache() कॉल करून सर्व न वापरलेली कॅश केलेली मेमरी मुक्त करू शकता, ज्यामुळे ती अन्य GPU ऍप्लिकेशन्ससाठी उपलब्ध होईल.

    ```python
    torch.cuda.empty_cache() 
    ```

1. Nvidia Cuda तपासणी

    ```bash
    nvcc --version
    ```

1. Hugging Face टोकन तयार करण्यासाठी खालील टास्क करा.

    - [Hugging Face Token Settings page](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo) वर जा.
    - **New token** निवडा.
    - वापरायच्या प्रोजेक्टचे **Name** टाका.
    - **Type** मध्ये **Write** निवडा.

> [!NOTE]
>
> तुम्हाला खालील त्रुटी आढळल्यास:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> हे सोडवण्यासाठी, टर्मिनलमध्ये खालील कमांड टाका.
>
> ```bash
> sudo ldconfig
> ```

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**अस्वीकरण**:
हा दस्तऐवज AI भाषांतर सेवेचा वापर करून अनुवादित केला आहे [Co-op Translator](https://github.com/Azure/co-op-translator). आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी, कृपया लक्षात ठेवा की स्वयंचलित भाषांतरांमध्ये चुका किंवा अपूर्णता असू शकते. मूळ दस्तऐवज त्याच्या स्थानिक भाषेत अधिकृत स्रोत म्हणून पाहिला जावा. अत्यंत महत्त्वाच्या माहितीकरिता व्यावसायिक मानवी भाषांतर करण्याचा सल्ला दिला जातो. हा भाषांतर वापरल्यामुळे होणाऱ्या कोणत्याही गैरसमजुती किंवा चुकीच्या अर्थग्रहणासाठी आम्ही जबाबदार नाही.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->