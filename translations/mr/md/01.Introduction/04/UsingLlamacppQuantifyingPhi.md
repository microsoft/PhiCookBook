<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "462bddc47427d8785f3c9fd817b346fe",
  "translation_date": "2025-07-16T22:07:53+00:00",
  "source_file": "md/01.Introduction/04/UsingLlamacppQuantifyingPhi.md",
  "language_code": "mr"
}
-->
# **llama.cpp वापरून Phi कुटुंबाचे क्वांटायझेशन**

## **llama.cpp म्हणजे काय**

llama.cpp ही मुख्यतः C++ मध्ये लिहिलेली एक मुक्त स्रोत सॉफ्टवेअर लायब्ररी आहे जी Llama सारख्या विविध मोठ्या भाषा मॉडेल्स (LLMs) वर इन्फरन्स करते. याचा मुख्य उद्देश कमी सेटअपसह विविध हार्डवेअरवर अत्याधुनिक LLM इन्फरन्स कार्यक्षमता प्रदान करणे आहे. शिवाय, या लायब्ररीसाठी Python बाइंडिंग्स देखील उपलब्ध आहेत, जे टेक्स्ट पूर्ण करण्यासाठी उच्च-स्तरीय API आणि OpenAI सुसंगत वेब सर्व्हर ऑफर करतात.

llama.cpp चे मुख्य उद्दिष्ट म्हणजे कमी सेटअपसह आणि विविध हार्डवेअरवर - स्थानिक तसेच क्लाउडमध्ये - अत्याधुनिक LLM इन्फरन्स सक्षम करणे.

- कोणत्याही अवलंबित्वांशिवाय साधा C/C++ अंमलबजावणी
- Apple सिलिकॉनला प्रथम दर्जाचा मान - ARM NEON, Accelerate आणि Metal फ्रेमवर्कद्वारे ऑप्टिमाइझ केलेले
- x86 आर्किटेक्चरसाठी AVX, AVX2 आणि AVX512 समर्थन
- जलद इन्फरन्स आणि कमी मेमरी वापरासाठी 1.5-बिट, 2-बिट, 3-बिट, 4-बिट, 5-बिट, 6-बिट आणि 8-बिट पूर्णांक क्वांटायझेशन
- NVIDIA GPU वर LLM चालवण्यासाठी कस्टम CUDA कर्नल्स (AMD GPU साठी HIP द्वारे समर्थन)
- Vulkan आणि SYCL बॅकएंड समर्थन
- CPU+GPU हायब्रिड इन्फरन्स ज्यामुळे एकूण VRAM क्षमतेपेक्षा मोठ्या मॉडेल्सचा अंशतः वेग वाढवता येतो

## **llama.cpp वापरून Phi-3.5 चे क्वांटायझेशन**

Phi-3.5-Instruct मॉडेलला llama.cpp वापरून क्वांटायझेशन करता येते, पण Phi-3.5-Vision आणि Phi-3.5-MoE अजून समर्थित नाहीत. llama.cpp द्वारे रूपांतरित फॉरमॅट gguf आहे, जो सर्वाधिक वापरला जाणारा क्वांटायझेशन फॉरमॅट देखील आहे.

Hugging Face वर क्वांटायझ्ड GGUF फॉरमॅटमध्ये बरेच मॉडेल्स उपलब्ध आहेत. AI Foundry, Ollama, आणि LlamaEdge हे llama.cpp वर अवलंबून आहेत, त्यामुळे GGUF मॉडेल्स देखील बर्‍याचदा वापरले जातात.

### **GGUF म्हणजे काय**

GGUF हा एक बायनरी फॉरमॅट आहे जो मॉडेल्स लवकर लोड आणि सेव्ह करण्यासाठी ऑप्टिमाइझ केलेला आहे, ज्यामुळे इन्फरन्ससाठी अत्यंत कार्यक्षम ठरतो. GGUF GGML आणि इतर एक्झिक्युटर्ससाठी डिझाइन केलेला आहे. GGUF ची निर्मिती @ggerganov यांनी केली आहे, जे llama.cpp चे देखील विकसक आहेत, जे एक लोकप्रिय C/C++ LLM इन्फरन्स फ्रेमवर्क आहे. PyTorch सारख्या फ्रेमवर्कमध्ये सुरुवातीला विकसित केलेले मॉडेल्स GGUF फॉरमॅटमध्ये रूपांतरित करून त्या इंजिन्ससह वापरता येतात.

### **ONNX विरुद्ध GGUF**

ONNX हा पारंपरिक मशीन लर्निंग/डीप लर्निंग फॉरमॅट आहे, जो विविध AI फ्रेमवर्कमध्ये चांगल्या प्रकारे समर्थित आहे आणि एज डिव्हाइसेसमध्ये चांगल्या वापराच्या परिस्थितींसाठी उपयुक्त आहे. GGUF हे llama.cpp वर आधारित आहे आणि ते GenAI युगात तयार झालेले म्हणता येईल. दोन्हीचे वापर समान आहेत. जर तुम्हाला एम्बेडेड हार्डवेअर आणि अ‍ॅप्लिकेशन स्तरांवर चांगली कार्यक्षमता हवी असेल, तर ONNX तुमचा पर्याय असू शकतो. जर तुम्ही llama.cpp च्या व्युत्पन्न फ्रेमवर्क आणि तंत्रज्ञानाचा वापर करत असाल, तर GGUF चांगले ठरू शकते.

### **llama.cpp वापरून Phi-3.5-Instruct चे क्वांटायझेशन**

**1. पर्यावरण सेटअप**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. क्वांटायझेशन**

llama.cpp वापरून Phi-3.5-Instruct चे FP16 GGUF मध्ये रूपांतर करा


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Phi-3.5 चे INT4 मध्ये क्वांटायझेशन करा


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. चाचणी**

llama-cpp-python इंस्टॉल करा


```bash

pip install llama-cpp-python -U

```

***टीप*** 

जर तुम्ही Apple Silicon वापरत असाल, तर llama-cpp-python असे इंस्टॉल करा


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

चाचणी 


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **संसाधने**

1. llama.cpp बद्दल अधिक जाणून घ्या [https://github.com/ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)
2. onnxruntime बद्दल अधिक जाणून घ्या [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)
3. GGUF बद्दल अधिक जाणून घ्या [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**अस्वीकरण**:  
हा दस्तऐवज AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) वापरून अनुवादित केला आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी, कृपया लक्षात घ्या की स्वयंचलित अनुवादांमध्ये चुका किंवा अचूकतेची कमतरता असू शकते. मूळ दस्तऐवज त्याच्या स्थानिक भाषेत अधिकृत स्रोत मानला जावा. महत्त्वाच्या माहितीसाठी व्यावसायिक मानवी अनुवाद करण्याची शिफारस केली जाते. या अनुवादाच्या वापरामुळे उद्भवणाऱ्या कोणत्याही गैरसमजुती किंवा चुकीच्या अर्थलागी आम्ही जबाबदार नाही.