<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-05-09T13:20:37+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "mr"
}
-->
# **फाय फॅमिलीचे प्रमाणितीकरण**

मॉडेल क्वांटायझेशन म्हणजे न्यूरल नेटवर्क मॉडेलमधील पॅरामीटर्स (जसे की वेट्स आणि अॅक्टिव्हेशन व्हॅल्यूज) मोठ्या मूल्य श्रेणीतून (सामान्यतः सतत मूल्य श्रेणी) लहान मर्यादित मूल्य श्रेणीमध्ये मॅप करण्याची प्रक्रिया होय. ही तंत्रज्ञान मॉडेलचा आकार आणि गणनात्मक गुंतागुंती कमी करू शकते आणि मोबाइल डिव्हाइसेस किंवा एम्बेडेड सिस्टीम्स सारख्या संसाधन मर्यादित वातावरणात मॉडेलची कार्यक्षमता वाढवू शकते. मॉडेल क्वांटायझेशन पॅरामीटर्सची अचूकता कमी करून कम्प्रेशन साधते, पण त्यात काही प्रमाणात अचूकतेची हानी होते. त्यामुळे, क्वांटायझेशन प्रक्रियेत मॉडेलचा आकार, गणनात्मक गुंतागुंती आणि अचूकता यांचा संतुलन राखणे आवश्यक आहे. सामान्य क्वांटायझेशन पद्धतींमध्ये फिक्स्ड-पॉइंट क्वांटायझेशन, फ्लोटिंग-पॉइंट क्वांटायझेशन इत्यादी समाविष्ट आहेत. तुम्ही विशिष्ट परिस्थिती आणि गरजेनुसार योग्य क्वांटायझेशन धोरण निवडू शकता.

आम्हाला GenAI मॉडेल एज डिव्हाइसेसवर तैनात करायचे आहे आणि अधिक डिव्हाइसेसना GenAI परिस्थितीत आणायचे आहे, जसे की मोबाइल डिव्हाइसेस, AI PC/Copilot+PC, आणि पारंपरिक IoT डिव्हाइसेस. क्वांटायझेशन मॉडेलच्या माध्यमातून, वेगवेगळ्या डिव्हाइसेसवर त्याची तैनाती करता येईल. हार्डवेअर उत्पादकांनी दिलेल्या मॉडेल अॅक्सेलरेशन फ्रेमवर्क आणि क्वांटायझेशन मॉडेलसह, आम्ही अधिक चांगल्या SLM अनुप्रयोग परिस्थिती तयार करू शकतो.

क्वांटायझेशन परिस्थितीत, आमच्याकडे वेगवेगळ्या अचूकता पातळ्या (INT4, INT8, FP16, FP32) आहेत. खाली सामान्यतः वापरल्या जाणाऱ्या क्वांटायझेशन अचूकतेबद्दल स्पष्टीकरण दिले आहे.

### **INT4**

INT4 क्वांटायझेशन ही एक कठोर क्वांटायझेशन पद्धत आहे, ज्यात मॉडेलचे वेट्स आणि अॅक्टिव्हेशन व्हॅल्यूज 4-बिट पूर्णांकांमध्ये रूपांतरित केले जातात. INT4 क्वांटायझेशनमध्ये कमी प्रतिनिधित्व श्रेणी आणि कमी अचूकतेमुळे अधिक अचूकतेची हानी होण्याची शक्यता असते. मात्र, INT8 क्वांटायझेशनच्या तुलनेत, INT4 क्वांटायझेशन मॉडेलच्या स्टोरेज गरजा आणि गणनात्मक गुंतागुंत आणखी कमी करू शकते. लक्षात ठेवण्यासारखे म्हणजे INT4 क्वांटायझेशन प्रत्यक्ष वापरात तुलनेने कमी दिसते, कारण खूप कमी अचूकता मॉडेलच्या कार्यक्षमतेत मोठी घट करू शकते. तसेच, सर्व हार्डवेअर INT4 ऑपरेशन्सना समर्थन देत नाही, त्यामुळे क्वांटायझेशन पद्धत निवडताना हार्डवेअर सुसंगतता विचारात घ्यावी लागते.

### **INT8**

INT8 क्वांटायझेशन म्हणजे मॉडेलचे वेट्स आणि अॅक्टिव्हेशन्स फ्लोटिंग पॉइंट नंबरमधून 8-बिट पूर्णांकांमध्ये रूपांतरित करणे. INT8 पूर्णांकांनी दर्शवलेली संख्या श्रेणी लहान आणि कमी अचूक असली तरी, ती स्टोरेज आणि गणनात्मक गरजा मोठ्या प्रमाणावर कमी करू शकते. INT8 क्वांटायझेशनमध्ये, मॉडेलचे वेट्स आणि अॅक्टिव्हेशन व्हॅल्यूज स्केलिंग आणि ऑफसेटसह क्वांटायझेशन प्रक्रियेतून जातात, ज्यामुळे मूळ फ्लोटिंग पॉइंट माहिती शक्य तितकी जपली जाते. इन्फरन्स दरम्यान, हे क्वांटाइज्ड व्हॅल्यूज पुन्हा फ्लोटिंग पॉइंट नंबरमध्ये डी-क्वांटाइज्ड होतात आणि नंतर पुढील टप्प्यासाठी INT8 मध्ये क्वांटाइज्ड केले जातात. ही पद्धत बहुतेक अनुप्रयोगांसाठी पुरेशी अचूकता आणि उच्च गणनात्मक कार्यक्षमता प्रदान करते.

### **FP16**

FP16 फॉरमॅट, म्हणजे 16-बिट फ्लोटिंग पॉइंट नंबर (float16), 32-बिट फ्लोटिंग पॉइंट नंबर (float32) च्या तुलनेत अर्धा मेमरी वापर करते, जे मोठ्या प्रमाणावर डीप लर्निंग अनुप्रयोगांसाठी महत्त्वाचे फायदे देतो. FP16 फॉरमॅट वापरल्याने त्याच GPU मेमरी मर्यादेत अधिक मोठे मॉडेल लोड करता येतात किंवा अधिक डेटा प्रक्रिया करता येतो. आधुनिक GPU हार्डवेअर FP16 ऑपरेशन्सना सतत समर्थन देत असल्याने, FP16 फॉरमॅट वापरल्याने गणनात्मक वेगातही सुधारणा होऊ शकते. मात्र, FP16 फॉरमॅटची inherent तोट्यांपैकी एक म्हणजे कमी अचूकता, ज्यामुळे काही वेळेस संख्यात्मक अस्थिरता किंवा अचूकतेची हानी होऊ शकते.

### **FP32**

FP32 फॉरमॅट उच्च अचूकता प्रदान करतो आणि मोठ्या प्रमाणात मूल्ये अचूकपणे दर्शवू शकतो. जिथे जटिल गणितीय ऑपरेशन्स कराव्या लागतात किंवा उच्च अचूकतेची आवश्यकता असते, तिथे FP32 फॉरमॅट प्राधान्य दिले जाते. मात्र, उच्च अचूकता म्हणजे अधिक मेमरी वापर आणि जास्त गणनावेला लागतो. मोठ्या प्रमाणावर डीप लर्निंग मॉडेल्ससाठी, विशेषतः जेथे बरेच मॉडेल पॅरामीटर्स आणि प्रचंड डेटा असतो, FP32 फॉरमॅटमुळे GPU मेमरी अपुरी पडू शकते किंवा इन्फरन्सचा वेग कमी होऊ शकतो.

मोबाइल डिव्हाइसेस किंवा IoT डिव्हाइसेसवर, आम्ही Phi-3.x मॉडेल्स INT4 मध्ये रूपांतरित करू शकतो, तर AI PC / Copilot PC वर जास्त अचूकता जसे की INT8, FP16, FP32 वापरता येते.

सध्या, वेगवेगळ्या हार्डवेअर उत्पादकांकडे जनरेटिव्ह मॉडेल्सना समर्थन देणारे वेगवेगळे फ्रेमवर्क्स आहेत, जसे की Intel चे OpenVINO, Qualcomm चे QNN, Apple चे MLX, आणि Nvidia चे CUDA, इत्यादी, जे मॉडेल क्वांटायझेशनसह स्थानिक तैनाती पूर्ण करतात.

तंत्रज्ञानाच्या दृष्टीने, क्वांटायझेशन नंतर वेगवेगळ्या फॉरमॅट्सना समर्थन आहे, जसे की PyTorch / Tensorflow फॉरमॅट, GGUF, आणि ONNX. मी GGUF आणि ONNX यांच्यात फॉरमॅट तुलना आणि अनुप्रयोग परिस्थिती केली आहे. येथे मी ONNX क्वांटायझेशन फॉरमॅटची शिफारस करतो, ज्याला मॉडेल फ्रेमवर्कपासून हार्डवेअरपर्यंत चांगले समर्थन आहे. या अध्यायात, आम्ही ONNX Runtime for GenAI, OpenVINO, आणि Apple MLX वापरून मॉडेल क्वांटायझेशनवर लक्ष केंद्रित करू (जर तुमच्याकडे चांगला मार्ग असेल तर PR सबमिट करून आम्हाला ते देखील देता येईल).

**हा अध्याय समाविष्ट करतो**

1. [llama.cpp वापरून Phi-3.5 / 4 चे क्वांटायझेशन](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntime साठी Generative AI एक्स्टेंशन्स वापरून Phi-3.5 / 4 चे क्वांटायझेशन](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO वापरून Phi-3.5 / 4 चे क्वांटायझेशन](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX Framework वापरून Phi-3.5 / 4 चे क्वांटायझेशन](./UsingAppleMLXQuantifyingPhi.md)

**अस्वीकरण**:  
हा दस्तऐवज AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) चा वापर करून अनुवादित केला आहे. आम्ही अचूकतेसाठी प्रयत्नशील आहोत, परंतु कृपया लक्षात ठेवा की स्वयंचलित अनुवादांमध्ये चुका किंवा अचूकतेचा अभाव असू शकतो. मूळ दस्तऐवज त्याच्या स्थानिक भाषेत अधिकृत स्रोत मानला जावा. महत्त्वाच्या माहिती साठी व्यावसायिक मानवी अनुवादाची शिफारस केली जाते. या अनुवादाचा वापर केल्यामुळे उद्भवणाऱ्या कोणत्याही गैरसमजुती किंवा चुकीच्या अर्थलाभासाठी आम्ही जबाबदार नाही.