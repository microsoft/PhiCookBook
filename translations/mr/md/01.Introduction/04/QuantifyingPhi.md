# **फाय कुटुंबाचे प्रमाणित करणे**

मॉडेल क्वांटायझेशन म्हणजे न्यूरल नेटवर्क मॉडेलमधील पॅरामीटर्स (जसे वेट्स आणि अॅक्टिव्हेशन व्हॅल्यूज) मोठ्या मूल्य श्रेणीतून (साधारणपणे सतत मूल्य श्रेणी) लहान निश्चित मूल्य श्रेणीमध्ये नकाशित करण्याची प्रक्रिया. ही तंत्रज्ञान मॉडेलचा आकार आणि संगणकीय गुंतागुंत कमी करू शकते आणि मोबाइल डिव्हाइसेस किंवा एम्बेडेड सिस्टीम्ससारख्या संसाधन-कमी असलेल्या वातावरणात मॉडेलच्या कार्यक्षमतेत सुधारणा करू शकते. मॉडेल क्वांटायझेशन पॅरामीटर्सच्या अचूकतेत कपात करून संपीडन साध्य करते, पण यामुळे अचूकतेत काही प्रमाणात तोटा होतो. म्हणून, क्वांटायझेशन प्रक्रियेत मॉडेलचा आकार, संगणकीय गुंतागुंत आणि अचूकता यांचा समतोल राखणे आवश्यक आहे. सामान्य क्वांटायझेशन पद्धतींमध्ये फिक्स्ड-पॉइंट क्वांटायझेशन, फ्लोटिंग-पॉइंट क्वांटायझेशन वगैरे आहेत. आपण विशिष्ट परिस्थिती आणि गरजेनुसार योग्य क्वांटायझेशन धोरण निवडू शकता.

आम्हाला जनरेटिव्ह AI मॉडेल एज डिव्हाइसेसवर तैनात करण्याची इच्छा आहे आणि मोबाइल उपकरणे, AI PC/Copilot+PC आणि पारंपरिक IoT उपकरणे यांसारख्या अधिक उपकरणांना GenAI परिस्थितीत आणण्याची इच्छा आहे. क्वांटायझेशन मॉडेलमुळे आम्ही वेगवेगळ्या उपकरणांवर त्याला अनुकूलित करून तैनात करू शकतो. हार्डवेयर उत्पादकांनी पुरवलेल्या मॉडेल ऍक्सेलेरेशन फ्रेमवर्क आणि क्वांटायझेशन मॉडेलशी मिळून, आम्ही चांगल्या SLM अनुप्रयोग परिस्थिती तयार करू शकतो.

क्वांटायझेशन परिस्थितीत, आमच्याकडे वेगवेगळ्या अचूकतांचे प्रकार (INT4, INT8, FP16, FP32) आहेत. खाली सामान्यपणे वापरल्या जाणाऱ्या क्वांटायझेशन अचूकता प्रकारांचे स्पष्टीकरण दिले आहे.

### **INT4**

INT4 क्वांटायझेशन हा एक कठोर क्वांटायझेशन पद्धत आहे ज्यात मॉडेलचे वेट्स आणि अॅक्टिव्हेशन व्हॅल्यूज 4-बिट पूर्णांकांमध्ये क्वांटाइझ केल्या जातात. INT4 क्वांटायझेशनमध्ये कमी प्रतिनिधित्व श्रेणी आणि कमी अचूकतेमुळे प्रामुख्याने जास्त अचूकतेचा तोटा होतो. तरीही, INT8 क्वांटायझेशनच्या तुलनेत, INT4 क्वांटायझेशन मॉडेलच्या संग्रहण गरजा आणि संगणकीय गुंतागुंत अधिक कमी करू शकते. मात्र, व्यावहारिक वापरात INT4 क्वांटायझेशन तुलनेने विरळ आहे, कारण फार कमी अचूकता मॉडेल कामगिरीत मोठ्या प्रमाणात घट करू शकते. शिवाय, सर्व हार्डवेअर INT4 ऑपरेशन्सना समर्थन देत नाही, त्यामुळे क्वांटायझेशन पद्धत निवडताना हार्डवेअर सुसंगतता लक्षात घेणे आवश्यक आहे.

### **INT8**

INT8 क्वांटायझेशन म्हणजे मॉडेलचे वेट्स आणि अॅक्टिव्हेशन फ्लोटिंग पॉइंट नंबरमधून 8-बिट पूर्णांकांमध्ये रूपांतरित करणे. जरी INT8 पूर्णांकाद्वारे दर्शविला जाणारा संख्यात्मक श्रेणी लहान आणि कमी अचूक असला तरी, तो संग्रहण आणि गणना आवश्यकतांमध्ये मोठी कपात करू शकतो. INT8 क्वांटायझेशनमध्ये मॉडेलचे वेट्स आणि अॅक्टिव्हेशन मूल्ये स्केलिंग आणि ऑफसेटसह क्वांटाइझ करण्याची प्रक्रिया पार करतात, ज्यामुळे मूळ फ्लोटिंग पॉइंट माहिती शक्य तितकी राखली जाते. इन्फरन्स दरम्यान, ही क्वांटाइझ केलेली मूल्ये परत फ्लोटिंग पॉइंट नंबरमध्ये डी-क्वांटाइज केली जातात आणि नंतर पुढील टप्प्यासाठी पुन्हा INT8 मध्ये क्वांटाइझ केली जातात. हे माध्यम बहुतेक अनुप्रयोगांमध्ये पुरेशी अचूकता प्रदान करते आणि उच्च संगणकीय कार्यक्षमता राखते.

### **FP16**

FP16 फॉर्मॅट म्हणजे 16-बिट फ्लोटिंग पॉइंट नंबर (float16), जो 32-बिट फ्लोटिंग पॉइंट नंबर (float32) पेक्षा स्मृती वापर अर्धा करतो, ज्यामुळे मोठ्या प्रमाणावर डीप लर्निंगमध्ये महत्त्वपूर्ण फायदे मिळतात. FP16 फॉर्मॅट GPU च्या मर्यादित स्मृतीमध्ये मोठे मॉडेल लोड करण्याची किंवा अधिक डेटा प्रक्रिया करण्याची अनुमती देते. आधुनिक GPU हार्डवेअर FP16 ऑपरेशन्सना समर्थन करत असल्यामुळे, FP16 वापरल्याने संगणकीय वेगात देखील सुधारणा होऊ शकते. मात्र, FP16 चे एक inherent तोटे म्हणजे कमी अचूकता, ज्यामुळे काही प्रसंगी संख्यात्मक अस्थिरता किंवा अचूकतेत तोटा होऊ शकतो.

### **FP32**

FP32 फॉर्मॅट उच्च अचूकता पुरवतो आणि विस्तृत मूल्यांसाठी अचूक प्रतिनिधित्व करतो. ज्या परिस्थितीत गुंतागुंतीची गणितीय क्रिया केली जाते किंवा उच्च अचूकतेचे परिणाम आवश्यक असतात, तेथे FP32 फॉर्मॅट पसंत केला जातो. मात्र, अधिक अचूकता म्हणजे अधिक स्मृती वापर आणि जास्त गणना वेळ. मोठ्या प्रमाणातील डीप लर्निंग मॉडेलसाठी, विशेषत: जेव्हा अनेक मॉडेल पॅरामीटर्स आणि फार मोठ्या डेटाचा वापर असतो, FP32 फॉर्मॅट GPU स्मृती अपुरी पडण्यास कारणीभूत होऊ शकतो किंवा इन्फरन्स वेग मंदावू शकतो.

मोबाइल डिव्हाइसेस किंवा IoT उपकरणांवर, आम्ही Phi-3.x मॉडेल्सना INT4 मध्ये रूपांतर करू शकतो, तर AI PC / Copilot PC वर INT8, FP16, FP32 सारख्या अधिक अचूकतेचा वापर करू शकतो.

सध्या, विविध हार्डवेअर उत्पादकांकडे जनरेटिव्ह मॉडेल्ससाठी वेगवेगळ्या फ्रेमवर्क्स आहेत, जसे की Intel चा OpenVINO, Qualcomm चा QNN, Apple चा MLX, आणि Nvidia चा CUDA इत्यादी, जे मॉडेल क्वांटायझेशनसह स्थानिक तैनाती पूर्ण करतात.

तांत्रिकदृष्ट्या, क्वांटायझेशननंतर आमच्याकडे वेगवेगळ्या फॉर्मॅट्सचे समर्थन आहे, जसे PyTorch / TensorFlow फॉर्मॅट, GGUF, आणि ONNX. मी GGUF आणि ONNX मधील फॉर्मॅट तुलना आणि अनुप्रयोग परिस्थिती केली आहे. येथे मी ONNX क्वांटायझेशन फॉर्मॅटची शिफारस करतो, ज्याला मॉडेल फ्रेमवर्कपासून हार्डवेअरपर्यंत चांगले समर्थन आहे. या प्रकरणात, आम्ही GenAI साठी ONNX रनटाइम, OpenVINO, आणि Apple MLX वापरून मॉडेल क्वांटायझेशनवर लक्ष केंद्रित करू (जर आपल्याकडे चांगली पद्धत असेल तर तुम्ही PR सबमिट करून ती आम्हाला देऊ शकता).

**या प्रकरणात समाविष्ट आहे**

1. [llama.cpp वापरून Phi-3.5 / 4 चे क्वांटायझेशन](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntime साठी Generative AI विस्तार वापरून Phi-3.5 / 4 चे क्वांटायझेशन](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO वापरून Phi-3.5 / 4 चे क्वांटायझेशन](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX फ्रेमवर्क वापरून Phi-3.5 / 4 चे क्वांटायझेशन](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**अस्वीकरण**:
हा दस्तऐवज AI भाषांतर सेवेचा वापर करून अनुवादित केला आहे [Co-op Translator](https://github.com/Azure/co-op-translator). आम्ही अचूकतेसाठी प्रयत्न करतो, पण कृपया लक्षात ठेवा की स्वयंचलित भाषांतरांमध्ये त्रुटी किंवा अचूकतेच्या चुका असू शकतात. मूळ दस्तऐवज त्याच्या स्थानिक भाषेत अधिकृत स्रोत मानला जावा. महत्त्वाच्या माहितीसाठी व्यावसायिक मानवी भाषांतर शिफारस केले जाते. या भाषांतराच्या वापरामुळे उद्भवणाऱ्या कोणत्याही गैरसमजुती किंवा चुकीच्या अर्थलाभांसाठी आम्ही जबाबदार नाही.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->