<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-05T02:12:50+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "mr"
}
-->
# **Phi कुटुंबाचे संख्यात्मक मापन**

मॉडेल क्वांटायझेशन म्हणजे न्यूरल नेटवर्क मॉडेलमधील पॅरामीटर्स (उदा. वजन आणि अ‍ॅक्टिव्हेशन मूल्ये) मोठ्या मूल्य श्रेणीतून (सामान्यतः सतत मूल्य श्रेणी) लहान मर्यादित मूल्य श्रेणीमध्ये नकाशित करण्याची प्रक्रिया आहे. ही तंत्रज्ञान मॉडेलचा आकार आणि संगणकीय जटिलता कमी करू शकते आणि मोबाइल डिव्हाइसेस किंवा एम्बेडेड सिस्टीम सारख्या संसाधन-मर्यादित वातावरणात मॉडेलचे ऑपरेशन अधिक कार्यक्षम बनवू शकते. मॉडेल क्वांटायझेशन पॅरामीटर्सची अचूकता कमी करून संकुचन साध्य करते, परंतु त्यातून काही प्रमाणात अचूकतेचा तोटा देखील होतो. म्हणूनच, क्वांटायझेशन प्रक्रियेत मॉडेलचा आकार, संगणकीय जटिलता आणि अचूकता यांचा समतोल राखणे आवश्यक असते. सामान्य क्वांटायझेशन पद्धतींमध्ये fixed-point quantization, floating-point quantization इत्यादींचा समावेश होतो. आपण विशिष्ट परिस्थिती आणि गरजेनुसार योग्य क्वांटायझेशन रणनीती निवडू शकता.

आम्हाला GenAI मॉडेलना edge devices वर तैनात करायचे आहे आणि अधिक उपकरणांना GenAI परिस्थितींमध्ये आणायचे आहे, जसे की मोबाइल डिव्हाइसेस, AI PC/Copilot+PC, आणि पारंपरिक IoT उपकरणे. क्वांटायझेशन मॉडेलद्वारे, आपण ते विविध उपकरणांनुसार भिन्न edge devices वर तैनात करू शकतो. हार्डवेअर उत्पादकांनी पुरवलेल्या मॉडेल अ‍ॅक्सेलरेशन फ्रेमवर्क आणि क्वांटायझेशन मॉडेलसह संयुक्तपणे, आपण चांगले SLM अनुप्रयोग परिदृश्य निर्माण करू शकतो.

क्वांटायझेशन परिस्थितीत, आपल्याकडे वेगवेगळ्या अचूकता पातळ्या (INT4, INT8, FP16, FP32) असतात. खाली सामान्यतः वापरल्या जाणाऱ्या क्वांटायझेशन अचूकता पातळ्यांचे स्पष्टीकरण आहे

### **INT4**

INT4 क्वांटायझेशन ही एक कट्टर क्वांटायझेशन पद्धत आहे जी मॉडेलची वजनं आणि अ‍ॅक्टिव्हेशन मूल्ये 4-बिट पूर्णांकांमध्ये क्वांटाइझ करते. प्रतिनिधित्व श्रेणी कमी आणि अचूकता कमी असल्यामुळे INT4 क्वांटायझेशन साधारणपणे जास्त अचूकतेचा तोटा निर्माण करते. तथापि, INT8 क्वांटायझेशनच्या तुलनेत, INT4 क्वांटायझेशन मॉडेलच्या संग्रहण आवश्यकता आणि संगणकीय जटिलता आणखी कमी करू शकते. लक्षात घेण्यासारखे आहे की व्यावहारिक अनुप्रयोगांमध्ये INT4 क्वांटायझेशन तुलनेने क्वचित वापरले जाते, कारण अतिशय कमी अचूकतेमुळे मॉडेलची कामगिरी लक्षणीयरीत्या कमी होऊ शकते. याशिवाय, सर्व हार्डवेअर्स INT4 ऑपरेशन्सला समर्थन देत नाहीत, त्यामुळे क्वांटायझेशन पद्धत निवडताना हार्डवेअर सुसंगतता विचारात घ्यावी लागते.

### **INT8**

INT8 क्वांटायझेशन ही अशी प्रक्रिया आहे ज्यात मॉडेलची वजनं आणि अ‍ॅक्टिव्हेशन्स फ्लोटिंग पॉईंट संख्यांमधून 8-बिट पूर्णांकांमध्ये रूपांतरित केली जातात. जरी INT8 पूर्णांकांनी दर्शविलेले संख्यात्मक श्रेणी कमी आणि कमी अचूक असली तरी, ते संग्रहण आणि गणना आवश्यकतांना लक्षणीयरीत्या कमी करू शकते. INT8 क्वांटायझेशनमध्ये, मॉडेलची वजनं आणि अ‍ॅक्टिव्हेशन मूल्ये स्केलिंग आणि ऑफसेट यांसारख्या क्वांटायझेशन प्रक्रियेतून जातात जेणेकरून मूळ फ्लोटिंग पॉईंट माहिती शक्य तितकी जपता येईल. इन्फरन्सदरम्यान, ही क्वांटाइज्ड मूल्ये गणनेसाठी परत फ्लोटिंग पॉईंट संख्यांमध्ये डी-क्वांटाइज करण्यात येतात, आणि नंतर पुढील टप्प्यासाठी परत INT8 मध्ये क्वांटाइझ केली जातात. ही पद्धत बहुतेक अनुप्रयोगांमध्ये पुरेशी अचूकता प्रदान करू शकते आणि उच्च संगणकीय कार्यक्षमतेचे संरक्षण करते.

### **FP16**

FP16 फॉरमॅट, म्हणजेच 16-बिट फ्लोटिंग पॉईंट संख्यां (float16), 32-बिट फ्लोटिंग पॉईंट संख्यां (float32) च्या तुलनेत मेमरीच्या वापरात अर्धा कपात करते, जे मोठ्या प्रमाणावरील डीप लर्निंग अनुप्रयोगांसाठी मोठे फायदे प्रदान करते. FP16 फॉरमॅट एकाच GPU मेमरी मर्यादेत मोठी मॉडेल लोड करणे किंवा अधिक डेटा प्रक्रिया करणे याची परवानगी देतो. आधुनिक GPU हार्डवेअर FP16 ऑपरेशन्सना सतत समर्थन देत असल्याने FP16 फॉरमॅटचा वापर केल्याने संगणकीय वेगातही सुधारणा होऊ शकते. तथापि, FP16 फॉरमॅटची अंतर्निहित कमतरता म्हणजे कमी अचूकता, जी काही परिस्थितींमध्ये संख्यात्मक अस्थिरता किंवा अचूकतेचा तोटा निर्माण करू शकते.

### **FP32**

FP32 फॉरमॅट उच्च अचूकता प्रदान करते आणि विस्तृत श्रेणीतील मूल्ये अचूकपणे दर्शवू शकते. जिथे क्लिष्ट गणितीय ऑपरेशन्स केले जातात किंवा उच्च-अचूकतेचे परिणाम आवश्यक असतात, तिथे FP32 फॉरमॅटला प्राधान्य दिले जाते. तथापि, उच्च अचूकता म्हणजे अधिक मेमरी वापर आणि गणनेसाठी जास्त वेळ लागणे. मोठ्या प्रमाणातील डीप लर्निंग मॉडेलांसाठी, विशेषतः जेथे बरेच मॉडेल पॅरामीटर्स आणि प्रचंड प्रमाणात डेटा असतो, FP32 फॉरमॅटमुळे GPU मेमरी अपुरी पडू शकते किंवा इन्फरन्स स्पीड कमी होऊ शकते.

मोबाइल डिव्हाइसेस किंवा IoT उपकरणांवर, आपण Phi-3.x मॉडेल्सना INT4 मध्ये रूपांतरित करू शकतो, तर AI PC / Copilot PC सारखी उपकरणे उच्च अचूकता जसे की INT8, FP16, FP32 वापरू शकतात.

सध्या, वेगवेगळे हार्डवेअर उत्पादक जनरेटिव्ह मॉडेल्सला समर्थन देण्यासाठी विविध फ्रेमवर्क्स वापरतात, जसे Intel चे OpenVINO, Qualcomm चे QNN, Apple चे MLX, आणि Nvidia चे CUDA इत्यादी, आणि हे मॉडेल क्वांटायझेशनसह एकत्र करून लोकल तैनाती पूर्ण करतात.

तांत्रिकदृष्ट्या, क्वांटायझेशननंतर आपल्याकडे विविध फॉरमॅट समर्थन असते, जसे PyTorch / TensorFlow फॉरमॅट, GGUF आणि ONNX. मी GGUF आणि ONNX मधील फॉरमॅट तुलना आणि अनुप्रयोग परिदृश्ये केली आहेत. येथे मी ONNX क्वांटायझेशन फॉरमॅटची शिफारस करतो, कारण मॉडेल फ्रेमवर्कपासून हार्डवेअरपर्यंत त्याला चांगले समर्थन आहे. या प्रकरणात, आपण ONNX Runtime for GenAI, OpenVINO, आणि Apple MLX वर मॉडेल क्वांटायझेशनवर लक्ष केंद्रित करू (जर तुमच्याकडे चांगला मार्ग असेल तर तुम्ही PR सबमिट करून ते आम्हाला देऊ शकता).

**हा अध्याय समाविष्ट करतो**

1. [llama.cpp वापरून Phi-3.5 / 4 चे क्वांटायझेशन](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntime साठी Generative AI एक्स्टेंशन्स वापरून Phi-3.5 / 4 चे क्वांटायझेशन](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO वापरून Phi-3.5 / 4 चे क्वांटायझेशन](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX Framework वापरून Phi-3.5 / 4 चे क्वांटायझेशन](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**अस्वीकरण**:
हा दस्तऐवज एआय भाषांतर सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) वापरून भाषांतरित केला आहे. आमची अचूकतेची काळजी असली तरी, कृपया लक्षात ठेवा की स्वयंचलित भाषांतरांमध्ये चुका किंवा अचूकतेच्या त्रुटी असू शकतात. त्याच्या मूळ भाषेतील मूळ दस्तऐवजास अधिकृत स्रोत मानले पाहिजे. महत्वाच्या माहितीच्या बाबतीत व्यावसायिक मानवी भाषांतर करणे शिफारसीय आहे. या भाषांतराच्या वापरामुळे झालेल्या कोणत्याही गैरसमजुती किंवा चुकीच्या अर्थान्वयाबद्दल आम्ही जबाबदार नाही.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->