<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2aa35f3c8b437fd5dc9995d53909d495",
  "translation_date": "2025-12-21T11:05:32+00:00",
  "source_file": "md/01.Introduction/02/04.Ollama.md",
  "language_code": "mr"
}
-->
## Ollama मधील Phi कुटुंब


[Ollama](https://ollama.com) अधिक लोकांना सोप्या स्क्रिप्टद्वारे ओपन सोर्स LLM किंवा SLM थेट तैनात करण्यास परवानगी देते, आणि स्थानिक Copilot अनुप्रयोग परिस्थितींसाठी API तयार देखील करू शकते.

## **1. स्थापना**

Ollama Windows, macOS आणि Linux वर चालते. आपण या दुव्याद्वारे Ollama इन्स्टॉल करू शकता ([https://ollama.com/download](https://ollama.com/download)). यशस्वी स्थापना नंतर, आपण टर्मिनल विंडोमधून Ollama स्क्रिप्ट वापरून थेट Phi-3 कॉल करू शकता. आपण सर्व [Ollama मधील उपलब्ध लायब्ररी](https://ollama.com/library) पाहू शकता. जर आपण हा रेपॉझिटरी Codespace मध्ये उघडला तर त्यात Ollama आधीच स्थापित असेल.

```bash

ollama run phi4

```

> [!NOTE]
> मॉडेल प्रथम जेव्हा आपण ते प्रथमच चालवाल तेव्हा डाउनलोड केले जाईल. अर्थात, आपण आधी डाउनलोड केलेले Phi-4 मॉडेल थेट निर्दिष्ट देखील करू शकता. आम्ही कमांड चालवण्यासाठी WSL चे उदाहरण घेतो. मॉडेल यशस्वीरित्या डाउनलोड झाल्यानंतर, आपण थेट टर्मिनलवर संवाद साधू शकता.

![चालवा](../../../../../translated_images/mr/ollama_run.e9755172b162b381.png)

## **2. Ollama मधून phi-4 API कॉल करणे**

जर आपण Ollama ने तयार केलेली Phi-4 API कॉल करू इच्छित असाल, तर Ollama सर्व्हर सुरू करण्यासाठी टर्मिनलमध्ये हा कमांड वापरू शकता.

```bash

ollama serve

```

> [!NOTE]
> जर MacOS किंवा Linux चालवत असाल, तर कृपया लक्षात घ्या की आपल्याला पुढील त्रुटी येऊ शकते **"Error: listen tcp 127.0.0.1:11434: bind: address already in use"** ही त्रुटी कमांड चालवताना मिळू शकते. आपण ती त्रुटी दुर्लक्षित करू शकता, कारण ती सामान्यतः दर्शवते की सर्व्हर आधीपासूनच चालू आहे, किंवा आपण Ollama थांबवून पुन्हा सुरू करू शकता:

**macOS**

```bash

brew services restart ollama

```

**Linux**

```bash

sudo systemctl stop ollama

```

Ollama दोन API ला समर्थन करते: generate आणि chat. आपण आपल्या गरजेनुसार Ollama द्वारे पुरवलेल्या मॉडेल API ला स्थानिक सेवेला (पोर्ट 11434 वर चालणाऱ्या) विनंत्या पाठवून कॉल करू शकता.

**Chat**

```bash

curl http://127.0.0.1:11434/api/chat -d '{
  "model": "phi3",
  "messages": [
    {
      "role": "system",
      "content": "Your are a python developer."
    },
    {
      "role": "user",
      "content": "Help me generate a bubble algorithm"
    }
  ],
  "stream": false
  
}'
```

हा Postman मधील निकाल आहे

![generate विनंतीसाठी JSON निकालाचा स्क्रीनशॉट](../../../../../translated_images/mr/ollama_gen.bda5d4e715366cc9.png)

## अतिरिक्त संसाधने

Ollama मधील उपलब्ध मॉडेल्सची यादी त्यांच्या [लायब्ररी](https://ollama.com/library) मध्ये तपासा.

Ollama सर्व्हरवरून आपले मॉडेल ओढण्यासाठी हा कमांड वापरा

```bash
ollama pull phi4
```

या कमांडचा उपयोग करून मॉडेल चालवा

```bash
ollama run phi4
```

***टीप:*** अधिक जाणून घेण्यासाठी हा दुवा भेट द्या [https://github.com/ollama/ollama/blob/main/docs/api.md](https://github.com/ollama/ollama/blob/main/docs/api.md)

## Python मधून Ollama कॉल करणे

वरील स्थानिक सर्व्हर एन्डपॉइंट्सवर विनंत्या करण्यासाठी आपण `requests` किंवा `urllib3` वापरू शकता. तथापि, Python मध्ये Ollama वापरण्याचा लोकप्रिय मार्ग म्हणजे [openai](https://pypi.org/project/openai/) SDK वापरणे, कारण Ollama OpenAI-सुसंगत सर्व्हर एन्डपॉइंटस देखील पुरवतो.

येथे phi3-mini साठी एक उदाहरण आहे:

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="nokeyneeded",
)

response = client.chat.completions.create(
    model="phi4",
    temperature=0.7,
    n=1,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a haiku about a hungry cat"},
    ],
)

print("Response:")
print(response.choices[0].message.content)
```

## JavaScript मधून Ollama कॉल करणे 

```javascript
// Phi-4 वापरून फाइलचा सारांश करण्याचे उदाहरण
script({
    model: "ollama:phi4",
    title: "Summarize with Phi-4",
    system: ["system"],
})

// सारांश करण्याचे उदाहरण
const file = def("FILE", env.files)
$`Summarize ${file} in a single paragraph.`
```

## C# मधून Ollama कॉल करणे

एक नवीन C# Console अ‍ॅप्लिकेशन तयार करा आणि खालील NuGet पॅकेज जोडा:

```bash
dotnet add package Microsoft.SemanticKernel --version 1.34.0
```

त्यानंतर `Program.cs` फाइलमधील हा कोड बदला

```csharp
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.ChatCompletion;

// add chat completion service using the local ollama server endpoint
#pragma warning disable SKEXP0001, SKEXP0003, SKEXP0010, SKEXP0011, SKEXP0050, SKEXP0052
builder.AddOpenAIChatCompletion(
    modelId: "phi4",
    endpoint: new Uri("http://localhost:11434/"),
    apiKey: "non required");

// invoke a simple prompt to the chat service
string prompt = "Write a joke about kittens";
var response = await kernel.InvokePromptAsync(prompt);
Console.WriteLine(response.GetValue<string>());
```

खालील कमांडने अॅप चालवा:

```bash
dotnet run
```

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
अस्वीकरण:
हा दस्तऐवज AI अनुवाद सेवा Co‑op Translator चा वापर करून अनुवादित केला आहे. आम्ही नेमक्याचा प्रयत्न करतो, परंतु कृत्रिम अनुवादांमध्ये चुका किंवा अचूकतेची त्रुटी असू शकतात हे कृपया लक्षात ठेवा. मूळ दस्तऐवज त्याच्या मूळ भाषेत अधिकृत स्रोत म्हणून समजले जावे. महत्त्वाच्या माहितीसाठी व्यावसायिक मानवी अनुवादाची शिफारस केली जाते. या अनुवादाच्या वापरामुळे उद्भवलेल्या कोणत्याही गैरसमजुतींबद्दल किंवा चुकीच्या अर्थ लागल्याबद्दल आम्ही जबाबदार नाही.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->