<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:42:47+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "mr"
}
-->
# मुख्य तंत्रज्ञानांमध्ये समाविष्ट आहे

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12 वर आधारित हार्डवेअर-त्वरित मशीन लर्निंगसाठी एक लो-लेव्हल API.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia द्वारे विकसित केलेले एक समांतर संगणन प्लॅटफॉर्म आणि API मॉडेल, जे GPU वर सामान्य उद्देशीय प्रक्रिया सक्षम करते.
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - मशीन लर्निंग मॉडेल्स सादर करण्यासाठी एक खुला फॉरमॅट, जो वेगवेगळ्या ML फ्रेमवर्क्समध्ये इंटरऑपरेबिलिटी प्रदान करतो.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - मशीन लर्निंग मॉडेल्स सादर करण्यासाठी आणि अपडेट करण्यासाठी वापरला जाणारा फॉरमॅट, विशेषतः छोटे भाषा मॉडेल्स जे CPU वर 4-8bit क्वांटायझेशनसह प्रभावीपणे चालू शकतात.

## DirectML

DirectML हा एक लो-लेव्हल API आहे जो हार्डवेअर-त्वरित मशीन लर्निंग सक्षम करतो. तो DirectX 12 वर आधारित असून GPU त्वरणाचा वापर करतो आणि विक्रेता-निरपेक्ष आहे, म्हणजे वेगवेगळ्या GPU विक्रेत्यांवर काम करण्यासाठी कोडमध्ये बदल करण्याची गरज नाही. तो मुख्यतः GPU वर मॉडेल प्रशिक्षण आणि इन्फरन्सिंगसाठी वापरला जातो.

हार्डवेअर समर्थनाबाबत, DirectML अनेक GPU सोबत काम करण्यासाठी डिझाइन केलेला आहे, ज्यात AMD चे इंटिग्रेटेड आणि डिस्क्रीट GPU, Intel चे इंटिग्रेटेड GPU, आणि NVIDIA चे डिस्क्रीट GPU यांचा समावेश आहे. तो Windows AI प्लॅटफॉर्मचा भाग आहे आणि Windows 10 व 11 वर समर्थित आहे, ज्यामुळे कोणत्याही Windows डिव्हाइसवर मॉडेल प्रशिक्षण आणि इन्फरन्सिंग करता येते.

DirectML शी संबंधित अद्यतने आणि संधी देखील आहेत, जसे की 150 ONNX ऑपरेटरपर्यंत समर्थन आणि ONNX runtime व WinML द्वारे वापर. हे प्रमुख Integrated Hardware Vendors (IHVs) द्वारे समर्थित आहे, जे विविध मेटाकमांड्स अंमलात आणतात.

## CUDA

CUDA म्हणजे Compute Unified Device Architecture, Nvidia द्वारे तयार केलेले एक समांतर संगणन प्लॅटफॉर्म आणि API मॉडेल आहे. हे सॉफ्टवेअर डेव्हलपर्सना CUDA-सक्षम GPU वापरून सामान्य उद्देशीय प्रक्रिया करण्याची परवानगी देते – ज्याला GPGPU (General-Purpose computing on Graphics Processing Units) म्हणतात. CUDA हे Nvidia च्या GPU त्वरणासाठी महत्त्वाचे आहे आणि मशीन लर्निंग, वैज्ञानिक संगणन, व्हिडिओ प्रक्रिया यांसारख्या विविध क्षेत्रांमध्ये मोठ्या प्रमाणावर वापरले जाते.

CUDA चे हार्डवेअर समर्थन Nvidia च्या GPU पर्यंत मर्यादित आहे, कारण ही Nvidia ची मालकीची तंत्रज्ञान आहे. प्रत्येक आर्किटेक्चर CUDA टूलकिटच्या विशिष्ट आवृत्त्यांना समर्थन देते, जे डेव्हलपर्सना CUDA अ‍ॅप्लिकेशन्स तयार करण्यासाठी आणि चालवण्यासाठी आवश्यक लायब्ररी आणि साधने पुरवते.

## ONNX

ONNX (Open Neural Network Exchange) हा मशीन लर्निंग मॉडेल्स सादर करण्यासाठी तयार केलेला खुला फॉरमॅट आहे. तो एक विस्तृत संगणकीय ग्राफ मॉडेलची व्याख्या करतो, तसेच अंगभूत ऑपरेटर आणि मानक डेटा प्रकारांची व्याख्या करतो. ONNX डेव्हलपर्सना वेगवेगळ्या ML फ्रेमवर्क्समध्ये मॉडेल्स हलवण्याची परवानगी देतो, ज्यामुळे इंटरऑपरेबिलिटी सुलभ होते आणि AI अ‍ॅप्लिकेशन्स तयार करणे व तैनात करणे सोपे होते.

Phi3 mini ONNX Runtime वापरून CPU आणि GPU वर विविध डिव्हाइसेसवर चालू शकतो, ज्यात सर्व्हर प्लॅटफॉर्म्स, Windows, Linux आणि Mac डेस्कटॉप्स, तसेच मोबाइल CPU यांचा समावेश आहे.  
आम्ही जोडलेल्या ऑप्टिमाइझ्ड कॉन्फिगरेशनमध्ये आहेत:

- int4 DML साठी ONNX मॉडेल: AWQ द्वारे int4 मध्ये क्वांटायझेशन
- fp16 CUDA साठी ONNX मॉडेल
- int4 CUDA साठी ONNX मॉडेल: RTN द्वारे int4 मध्ये क्वांटायझेशन
- int4 CPU आणि मोबाइल साठी ONNX मॉडेल: RTN द्वारे int4 मध्ये क्वांटायझेशन

## Llama.cpp

Llama.cpp हा C++ मध्ये लिहिलेला एक मुक्त स्रोत सॉफ्टवेअर लायब्ररी आहे. तो Llama सह विविध Large Language Models (LLMs) वर इन्फरन्स करतो. ggml लायब्ररी (एक सामान्य उद्देशीय टेन्सर लायब्ररी) सोबत विकसित केलेला, llama.cpp मूळ Python अंमलबजावणीच्या तुलनेत जलद इन्फरन्स आणि कमी मेमरी वापर प्रदान करण्याचा उद्देश ठेवतो. तो हार्डवेअर ऑप्टिमायझेशन, क्वांटायझेशनला समर्थन देतो आणि सोपा API व उदाहरणे पुरवतो. जर तुम्हाला कार्यक्षम LLM इन्फरन्समध्ये रस असेल, तर llama.cpp चा अभ्यास करणे फायदेशीर आहे कारण Phi3 Llama.cpp चालवू शकतो.

## GGUF

GGUF (Generic Graph Update Format) हा मशीन लर्निंग मॉडेल्स सादर करण्यासाठी आणि अपडेट करण्यासाठी वापरला जाणारा फॉरमॅट आहे. तो विशेषतः छोटे भाषा मॉडेल्स (SLMs) साठी उपयुक्त आहे जे CPU वर 4-8bit क्वांटायझेशनसह प्रभावीपणे चालू शकतात. GGUF जलद प्रोटोटायपिंगसाठी आणि एज डिव्हाइसेसवर किंवा CI/CD पाइपलाइन्ससारख्या बॅच जॉब्समध्ये मॉडेल्स चालवण्यासाठी फायदेशीर आहे.

**अस्वीकरण**:  
हा दस्तऐवज AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) वापरून अनुवादित केला आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी, कृपया लक्षात घ्या की स्वयंचलित अनुवादांमध्ये चुका किंवा अचूकतेत त्रुटी असू शकतात. मूळ दस्तऐवज त्याच्या स्थानिक भाषेत अधिकृत स्रोत मानला जावा. महत्त्वाच्या माहितीसाठी व्यावसायिक मानवी अनुवाद करण्याची शिफारस केली जाते. या अनुवादाच्या वापरामुळे उद्भवलेल्या कोणत्याही गैरसमजुती किंवा चुकीच्या अर्थलागी आम्ही जबाबदार नाही.