<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:17:55+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "mr"
}
-->
# मुख्य तंत्रज्ञान ज्यांचा उल्लेख केला आहे

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - हार्डवेअर-त्वरित मशीन लर्निंगसाठी एक लो-लेवल API, जो DirectX 12 वर आधारित आहे.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia द्वारा विकसित केलेली एक समांतर संगणकीय प्लॅटफॉर्म आणि API मॉडेल, जी GPU वर सामान्य उद्देशीय प्रक्रिया सक्षम करते.
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - एक खुला फॉरमॅट जो मशीन लर्निंग मॉडेल्सचे प्रतिनिधित्व करण्यासाठी डिझाइन केला आहे आणि वेगवेगळ्या ML फ्रेमवर्क्समधील इंटरऑपरेबिलिटी प्रदान करतो.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - मशीन लर्निंग मॉडेल्सचे प्रतिनिधित्व आणि अद्ययावत करण्यासाठी वापरला जाणारा फॉरमॅट, विशेषतः लहान भाषा मॉडेल्ससाठी उपयुक्त जे CPU वर 4-8bit क्वांटायझेशनसह प्रभावीपणे चालू शकतात.

## DirectML

DirectML हा एक लो-लेवल API आहे जो हार्डवेअर-त्वरित मशीन लर्निंग सक्षम करतो. तो DirectX 12 वर आधारित असून GPU त्वरेचा वापर करतो आणि विक्रेता-निरपेक्ष आहे, म्हणजे वेगवेगळ्या GPU विक्रेत्यांवर कोडमध्ये बदल न करता काम करतो. तो मुख्यतः GPU वर मॉडेल ट्रेनिंग आणि इन्फरन्ससाठी वापरला जातो.

हार्डवेअर सपोर्टसाठी, DirectML विविध GPU सोबत काम करण्यासाठी डिझाइन केला आहे, ज्यात AMD चे इंटिग्रेटेड आणि डिस्क्रीट GPU, Intel चे इंटिग्रेटेड GPU, आणि NVIDIA चे डिस्क्रीट GPU यांचा समावेश आहे. तो Windows AI Platform चा भाग आहे आणि Windows 10 व 11 वर समर्थित आहे, ज्यामुळे कोणत्याही Windows डिव्हाइसवर मॉडेल ट्रेनिंग आणि इन्फरन्स करता येतो.

DirectML शी संबंधित अपडेट्स आणि संधी आहेत, जसे की 150 पेक्षा जास्त ONNX ऑपरेटरसाठी सपोर्ट आणि ONNX runtime तसेच WinML द्वारे वापर. हे मुख्य Integrated Hardware Vendors (IHVs) द्वारे समर्थित आहे, जे विविध मेटाकमांड्स लागू करतात.

## CUDA

CUDA म्हणजे Compute Unified Device Architecture, ही Nvidia कडून तयार केलेली समांतर संगणकीय प्लॅटफॉर्म आणि API मॉडेल आहे. यामुळे सॉफ्टवेअर डेव्हलपर्सना CUDA-सक्षम GPU वापरून सामान्य उद्देशीय प्रक्रिया करता येते, ज्याला GPGPU (General-Purpose computing on Graphics Processing Units) म्हणतात. CUDA हा Nvidia च्या GPU त्वरेचा मुख्य आधार आहे आणि मशीन लर्निंग, वैज्ञानिक संगणन, व्हिडिओ प्रोसेसिंगसह विविध क्षेत्रांमध्ये मोठ्या प्रमाणावर वापरला जातो.

CUDA साठी हार्डवेअर सपोर्ट Nvidia च्या GPU पर्यंतच मर्यादित आहे, कारण हा Nvidia कडून विकसित केलेला खासगी तंत्रज्ञान आहे. प्रत्येक आर्किटेक्चर CUDA टूलकिटच्या विशिष्ट आवृत्त्यांना सपोर्ट करतो, जे डेव्हलपर्सना CUDA अ‍ॅप्लिकेशन्स तयार आणि चालवण्यासाठी आवश्यक लायब्ररी आणि साधने पुरवते.

## ONNX

ONNX (Open Neural Network Exchange) हा एक खुला फॉरमॅट आहे जो मशीन लर्निंग मॉडेल्सचे प्रतिनिधित्व करण्यासाठी डिझाइन केला आहे. तो एक विस्तारित संगणकीय ग्राफ मॉडेलची व्याख्या करतो, तसेच बिल्ट-इन ऑपरेटर आणि स्टँडर्ड डेटा टाइप्सची व्याख्या करतो. ONNX डेव्हलपर्सना वेगवेगळ्या ML फ्रेमवर्क्समधून मॉडेल्स हलवण्याची मुभा देतो, ज्यामुळे इंटरऑपरेबिलिटी सुलभ होते आणि AI अ‍ॅप्लिकेशन्स तयार व तैनात करणे सोपे होते.

Phi3 mini ONNX Runtime सह CPU आणि GPU वर विविध डिव्हाइसवर चालू शकतो, ज्यात सर्व्हर प्लॅटफॉर्म, Windows, Linux, Mac डेस्कटॉप आणि मोबाइल CPU यांचा समावेश आहे.
आम्ही जोडलेल्या ऑप्टिमाइझ्ड कॉन्फिगरेशनमध्ये आहेत

- int4 DML साठी ONNX मॉडेल्स: AWQ द्वारे int4 मध्ये क्वांटायझेशन
- fp16 CUDA साठी ONNX मॉडेल
- int4 CUDA साठी ONNX मॉडेल: RTN द्वारे int4 मध्ये क्वांटायझेशन
- int4 CPU आणि मोबाइल साठी ONNX मॉडेल: RTN द्वारे int4 मध्ये क्वांटायझेशन

## Llama.cpp

Llama.cpp हा C++ मध्ये लिहिलेला एक मुक्त स्रोत सॉफ्टवेअर लायब्ररी आहे. तो Llama सहित विविध Large Language Models (LLMs) वर इन्फरन्स करतो. ggml लायब्ररी (एक सामान्य उद्देशीय टेन्सर लायब्ररी) सोबत विकसित, llama.cpp मूळ Python अंमलबजावणीच्या तुलनेत जलद इन्फरन्स आणि कमी मेमरी वापर देण्याचा उद्देश आहे. तो हार्डवेअर ऑप्टिमायझेशन, क्वांटायझेशनला सपोर्ट करतो आणि सोपी API व उदाहरणे देतो. जर तुम्हाला कार्यक्षम LLM इन्फरन्समध्ये रस असेल, तर llama.cpp वापरून पाहणे फायदेशीर आहे कारण Phi3 Llama.cpp चालवू शकतो.

## GGUF

GGUF (Generic Graph Update Format) हा मशीन लर्निंग मॉडेल्सचे प्रतिनिधित्व आणि अद्ययावत करण्यासाठी वापरला जाणारा फॉरमॅट आहे. तो विशेषतः लहान भाषा मॉडेल्ससाठी उपयुक्त आहे जे CPU वर 4-8bit क्वांटायझेशनसह प्रभावीपणे चालू शकतात. GGUF जलद प्रोटोटायपिंगसाठी आणि एज डिव्हाइसेस किंवा CI/CD पाइपलाइन्ससारख्या बॅच जॉब्समध्ये मॉडेल्स चालवण्यासाठी फायदेशीर आहे.

**अस्वीकरण**:  
हा दस्तऐवज AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) चा वापर करून अनुवादित केला आहे. आम्ही अचूकतेसाठी प्रयत्न करतो, तरी कृपया लक्षात ठेवा की स्वयंचलित अनुवादांमध्ये चुका किंवा अचूकतेची कमतरता असू शकते. मूळ दस्तऐवज त्याच्या स्थानिक भाषेत अधिकृत स्रोत म्हणून मानला पाहिजे. महत्त्वाच्या माहितीसाठी व्यावसायिक मानवी अनुवादाची शिफारस केली जाते. या अनुवादाच्या वापरामुळे उद्भवलेल्या कोणत्याही गैरसमजुती किंवा चुकीच्या अर्थलागीसाठी आम्ही जबाबदार नाही.