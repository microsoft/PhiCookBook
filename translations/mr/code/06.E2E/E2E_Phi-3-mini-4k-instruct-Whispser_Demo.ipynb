{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## इंटरएक्टिव Phi 3 Mini 4K इंस्ट्रक्ट चॅटबॉट विथ व्हिस्पर\n",
    "\n",
    "### परिचय:\n",
    "इंटरएक्टिव Phi 3 Mini 4K इंस्ट्रक्ट चॅटबॉट हे एक साधन आहे जे वापरकर्त्यांना Microsoft Phi 3 Mini 4K इंस्ट्रक्ट डेमोशी मजकूर किंवा ऑडिओ इनपुटद्वारे संवाद साधण्याची परवानगी देते. चॅटबॉट विविध कार्यांसाठी वापरले जाऊ शकते, जसे की भाषांतर, हवामान अद्यतने, आणि सामान्य माहिती गोळा करणे.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[तुमचा Huggingface प्रवेश टोकन तयार करा](https://huggingface.co/settings/tokens)\n",
    "\n",
    "नवीन टोकन तयार करा  \n",
    "नवीन नाव द्या  \n",
    "लेखन परवानग्या निवडा  \n",
    "टोकन कॉपी करा आणि सुरक्षित ठिकाणी जतन करा  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "पायथॉन कोड खालील दोन मुख्य कार्ये पार पाडतो: `os` मॉड्यूल आयात करणे आणि एक पर्यावरणीय व्हेरिएबल सेट करणे.\n",
    "\n",
    "1. `os` मॉड्यूल आयात करणे:\n",
    "   - पायथॉनमधील `os` मॉड्यूल ऑपरेटिंग सिस्टमशी संवाद साधण्याचा मार्ग प्रदान करते. हे तुम्हाला विविध ऑपरेटिंग सिस्टम-संबंधित कार्ये करण्यास सक्षम करते, जसे की पर्यावरणीय व्हेरिएबल्समध्ये प्रवेश करणे, फाइल्स आणि डिरेक्टरीजसह काम करणे इत्यादी.\n",
    "   - या कोडमध्ये, `os` मॉड्यूल `import` स्टेटमेंट वापरून आयात केले जाते. हे स्टेटमेंट `os` मॉड्यूलची कार्यक्षमता सध्याच्या पायथॉन स्क्रिप्टमध्ये वापरण्यासाठी उपलब्ध करते.\n",
    "\n",
    "2. पर्यावरणीय व्हेरिएबल सेट करणे:\n",
    "   - पर्यावरणीय व्हेरिएबल म्हणजे एक मूल्य जे ऑपरेटिंग सिस्टमवर चालणाऱ्या प्रोग्राम्सद्वारे प्रवेशयोग्य असते. हे कॉन्फिगरेशन सेटिंग्ज किंवा इतर माहिती साठवण्याचा मार्ग आहे, ज्याचा उपयोग अनेक प्रोग्राम्सद्वारे केला जाऊ शकतो.\n",
    "   - या कोडमध्ये, एक नवीन पर्यावरणीय व्हेरिएबल `os.environ` डिक्शनरी वापरून सेट केले जात आहे. डिक्शनरीची की `'HF_TOKEN'` आहे, आणि मूल्य `HUGGINGFACE_TOKEN` व्हेरिएबलमधून असाइन केले जाते.\n",
    "   - `HUGGINGFACE_TOKEN` व्हेरिएबल या कोड स्निपेटच्या वर परिभाषित केले जाते, आणि त्याला `#@param` सिंटॅक्स वापरून `\"hf_**************\"` स्ट्रिंग मूल्य असाइन केले जाते. हा सिंटॅक्स सहसा जुपिटर नोटबुकमध्ये वापरला जातो, जेणेकरून वापरकर्त्याला नोटबुक इंटरफेसमध्ये थेट इनपुट आणि पॅरामीटर कॉन्फिगरेशन करण्याची परवानगी मिळते.\n",
    "   - `'HF_TOKEN'` पर्यावरणीय व्हेरिएबल सेट केल्याने, ते प्रोग्रामच्या इतर भागांद्वारे किंवा त्याच ऑपरेटिंग सिस्टमवर चालणाऱ्या इतर प्रोग्राम्सद्वारे प्रवेशयोग्य होऊ शकते.\n",
    "\n",
    "संपूर्णपणे, हा कोड `os` मॉड्यूल आयात करतो आणि `'HF_TOKEN'` नावाचे पर्यावरणीय व्हेरिएबल सेट करतो, ज्याचे मूल्य `HUGGINGFACE_TOKEN` व्हेरिएबलमध्ये दिले जाते.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ही कोड स्निपेट clear_output नावाची एक फंक्शन परिभाषित करते जी Jupyter Notebook किंवा IPython मधील वर्तमान सेलचा आउटपुट साफ करण्यासाठी वापरली जाते. चला कोडचे विश्लेषण करूया आणि त्याची कार्यक्षमता समजून घेऊया:\n",
    "\n",
    "clear_output फंक्शन एक wait नावाचा पॅरामीटर घेतो, जो एक बूलियन मूल्य आहे. डीफॉल्टनुसार, wait False सेट केले जाते. हा पॅरामीटर ठरवतो की फंक्शनने विद्यमान आउटपुट साफ करण्यापूर्वी नवीन आउटपुट उपलब्ध होईपर्यंत थांबावे की नाही.\n",
    "\n",
    "हे फंक्शन स्वतःच वर्तमान सेलचा आउटपुट साफ करण्यासाठी वापरले जाते. Jupyter Notebook किंवा IPython मध्ये, जेव्हा एखादा सेल आउटपुट तयार करतो, जसे की प्रिंट केलेला मजकूर किंवा ग्राफिकल प्लॉट्स, तो आउटपुट सेलच्या खाली प्रदर्शित केला जातो. clear_output फंक्शन तुम्हाला तो आउटपुट साफ करण्याची परवानगी देते.\n",
    "\n",
    "कोड स्निपेटमध्ये फंक्शनची अंमलबजावणी दिलेली नाही, ज्याचे संकेत एलिप्सिस (...) द्वारे दिले आहेत. एलिप्सिस वास्तविक कोडसाठी एक प्लेसहोल्डर दर्शवतो जो आउटपुट साफ करण्याचे कार्य करतो. फंक्शनची अंमलबजावणी Jupyter Notebook किंवा IPython API शी संवाद साधून सेलमधून विद्यमान आउटपुट काढून टाकण्याचा समावेश असू शकतो.\n",
    "\n",
    "संपूर्णपणे पाहता, हे फंक्शन Jupyter Notebook किंवा IPython मधील वर्तमान सेलचा आउटपुट साफ करण्याचा एक सोयीस्कर मार्ग प्रदान करते, ज्यामुळे इंटरॅक्टिव कोडिंग सत्रांदरम्यान प्रदर्शित आउटपुट व्यवस्थापित करणे आणि अपडेट करणे सोपे होते.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "एज टीटीएस सेवा वापरून टेक्स्ट-टू-स्पीच (TTS) करण्यासाठी खालील फंक्शनची अंमलबजावणी एक-एक करून पाहूया:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: हे फंक्शन इनपुट व्हॅल्यू घेते आणि TTS आवाजासाठी गती स्ट्रिंगची गणना करते. इनपुट व्हॅल्यू भाषणाच्या इच्छित वेगाचे प्रतिनिधित्व करते, जिथे 1 सामान्य वेग दर्शवतो. फंक्शन इनपुट व्हॅल्यूमधून 1 वजा करते, त्याला 100 ने गुणाकार करते आणि नंतर इनपुट व्हॅल्यू 1 पेक्षा जास्त किंवा समान आहे का यावर आधारित चिन्ह ठरवते. फंक्शन \"{sign}{rate}\" या स्वरूपात गती स्ट्रिंग परत करते.\n",
    "\n",
    "2. `make_chunks(input_text, language)`: हे फंक्शन इनपुट टेक्स्ट आणि भाषा पॅरामीटर्स म्हणून घेते. ते भाषा-विशिष्ट नियमांनुसार इनपुट टेक्स्टला तुकड्यांमध्ये विभागते. या अंमलबजावणीत, जर भाषा \"English\" असेल, तर फंक्शन प्रत्येक पूर्णविरामावर (\".\") टेक्स्ट विभागते आणि कोणतेही सुरुवातीचे किंवा शेवटचे रिकामे जागा काढून टाकते. नंतर प्रत्येक तुकड्याला पूर्णविराम जोडते आणि फिल्टर केलेल्या तुकड्यांची यादी परत करते.\n",
    "\n",
    "3. `tts_file_name(text)`: हे फंक्शन इनपुट टेक्स्टच्या आधारे TTS ऑडिओ फाइलसाठी नाव तयार करते. ते टेक्स्टवर अनेक रूपांतरणे करते: शेवटचा पूर्णविराम (जर उपस्थित असेल) काढून टाकणे, टेक्स्ट लहान अक्षरात रूपांतरित करणे, सुरुवातीचे आणि शेवटचे रिकामे जागा काढून टाकणे, आणि जागा अंडरस्कोरने बदलणे. नंतर टेक्स्ट 25 वर्णांपर्यंत (जर लांब असेल) कापते किंवा टेक्स्ट रिकामे असल्यास पूर्ण टेक्स्ट वापरते. शेवटी, [`uuid`] मॉड्यूल वापरून एक रँडम स्ट्रिंग तयार करते आणि कापलेल्या टेक्स्टसह एकत्र करून \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\" या स्वरूपात फाइल नाव तयार करते.\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: हे फंक्शन अनेक ऑडिओ फाइल्स एका ऑडिओ फाइलमध्ये विलीन करते. ते ऑडिओ फाइल पथांची यादी आणि आउटपुट पथ पॅरामीटर्स म्हणून घेते. फंक्शन [`merged_audio`] नावाचा रिकामा `AudioSegment` ऑब्जेक्ट सुरू करते. नंतर प्रत्येक ऑडिओ फाइल पथावर पुनरावृत्ती करते, `pydub` लायब्ररीच्या `AudioSegment.from_file()` पद्धतीचा वापर करून ऑडिओ फाइल लोड करते आणि सध्याच्या ऑडिओ फाइलला [`merged_audio`] ऑब्जेक्टमध्ये जोडते. शेवटी, ते विलीन केलेले ऑडिओ निर्दिष्ट आउटपुट पथावर MP3 स्वरूपात निर्यात करते.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: हे फंक्शन एज टीटीएस सेवा वापरून TTS ऑपरेशन करते. ते टेक्स्ट तुकड्यांची यादी, भाषणाचा वेग, आवाजाचे नाव, आणि सेव्ह पथ पॅरामीटर्स म्हणून घेते. जर तुकड्यांची संख्या 1 पेक्षा जास्त असेल, तर फंक्शन वैयक्तिक तुकड्यांच्या ऑडिओ फाइल्स साठवण्यासाठी एक डिरेक्टरी तयार करते. नंतर प्रत्येक तुकड्यावर पुनरावृत्ती करते, `calculate_rate_string()` फंक्शन, आवाजाचे नाव, आणि तुकड्याच्या टेक्स्टचा वापर करून एज टीटीएस कमांड तयार करते आणि `os.system()` फंक्शन वापरून कमांड अंमलात आणते. जर कमांड अंमलबजावणी यशस्वी झाली, तर ते तयार केलेल्या ऑडिओ फाइलचा पथ यादीत जोडते. सर्व तुकड्यांवर प्रक्रिया केल्यानंतर, ते `merge_audio_files()` फंक्शन वापरून वैयक्तिक ऑडिओ फाइल्स विलीन करते आणि विलीन केलेले ऑडिओ निर्दिष्ट सेव्ह पथावर साठवते. जर फक्त एक तुकडा असेल, तर ते थेट एज टीटीएस कमांड तयार करते आणि ऑडिओ सेव्ह पथावर साठवते. शेवटी, ते तयार केलेल्या ऑडिओ फाइलचा सेव्ह पथ परत करते.\n",
    "\n",
    "6. `random_audio_name_generate()`: हे फंक्शन [`uuid`] मॉड्यूल वापरून एक रँडम ऑडिओ फाइल नाव तयार करते. ते एक रँडम UUID तयार करते, त्याला स्ट्रिंगमध्ये रूपांतरित करते, पहिले 8 वर्ण घेतो, \".mp3\" विस्तार जोडतो, आणि रँडम ऑडिओ फाइल नाव परत करते.\n",
    "\n",
    "7. `talk(input_text)`: हे फंक्शन TTS ऑपरेशन करण्यासाठी मुख्य प्रवेश बिंदू आहे. ते इनपुट टेक्स्ट पॅरामीटर म्हणून घेते. ते प्रथम इनपुट टेक्स्टची लांबी तपासते आणि लांब वाक्य (600 वर्णांपेक्षा जास्त किंवा समान) आहे का ते ठरवते. लांबी आणि `translate_text_flag` व्हेरिएबलच्या मूल्यावर आधारित, ते भाषा ठरवते आणि `make_chunks()` फंक्शन वापरून टेक्स्ट तुकड्यांची यादी तयार करते. नंतर `random_audio_name_generate()` फंक्शन वापरून ऑडिओ फाइलसाठी सेव्ह पथ तयार करते. शेवटी, ते `edge_free_tts()` फंक्शन कॉल करते आणि तयार केलेल्या ऑडिओ फाइलचा सेव्ह पथ परत करते.\n",
    "\n",
    "एकूणच, ही फंक्शन्स इनपुट टेक्स्ट तुकड्यांमध्ये विभागण्यासाठी, ऑडिओ फाइलसाठी नाव तयार करण्यासाठी, एज टीटीएस सेवा वापरून TTS ऑपरेशन करण्यासाठी, आणि वैयक्तिक ऑडिओ फाइल्स एका ऑडिओ फाइलमध्ये विलीन करण्यासाठी एकत्र काम करतात.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "दोन फंक्शन्स: convert_to_text आणि run_text_prompt यांची अंमलबजावणी, तसेच दोन क्लासेस: str आणि Audio यांची घोषणा.\n",
    "\n",
    "convert_to_text फंक्शन audio_path इनपुट म्हणून घेतो आणि whisper_model नावाच्या मॉडेलचा वापर करून ऑडिओचे मजकुरात रूपांतर करतो. फंक्शन प्रथम तपासतो की gpu फ्लॅग True आहे का. जर gpu फ्लॅग True असेल, तर whisper_model काही विशिष्ट पॅरामीटर्ससह वापरला जातो जसे की word_timestamps=True, fp16=True, language='English', आणि task='translate'. जर gpu फ्लॅग False असेल, तर whisper_model fp16=False सह वापरला जातो. परिणामी ट्रान्सक्रिप्शन 'scan.txt' नावाच्या फाईलमध्ये सेव्ह केले जाते आणि मजकूर म्हणून परत केले जाते.\n",
    "\n",
    "run_text_prompt फंक्शन message आणि chat_history इनपुट म्हणून घेतो. हे phi_demo फंक्शनचा वापर करून इनपुट message वर आधारित चॅटबॉटकडून प्रतिसाद निर्माण करते. तयार केलेला प्रतिसाद talk फंक्शनला दिला जातो, जो प्रतिसादाचे ऑडिओ फाईलमध्ये रूपांतर करतो आणि फाईलचा पथ परत करतो. Audio क्लास ऑडिओ फाईल दाखवण्यासाठी आणि प्ले करण्यासाठी वापरला जातो. ऑडिओ IPython.display मॉड्यूलमधील display फंक्शनचा वापर करून दाखवला जातो, आणि Audio ऑब्जेक्ट autoplay=True पॅरामीटरसह तयार केला जातो, त्यामुळे ऑडिओ आपोआप प्ले होतो. chat_history इनपुट message आणि तयार केलेल्या प्रतिसादासह अपडेट केला जातो, आणि रिक्त स्ट्रिंग आणि अपडेट केलेला chat_history परत केला जातो.\n",
    "\n",
    "str क्लास हा Python मधील एक बिल्ट-इन क्लास आहे जो अक्षरांच्या अनुक्रमाचे प्रतिनिधित्व करतो. तो स्ट्रिंग्सचे व्यवस्थापन आणि काम करण्यासाठी विविध पद्धती प्रदान करतो, जसे की capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill, आणि इतर. या पद्धती तुम्हाला शोधणे, बदलणे, स्वरूपित करणे, आणि स्ट्रिंग्सचे व्यवस्थापन करण्यास परवानगी देतात.\n",
    "\n",
    "Audio क्लास हा एक कस्टम क्लास आहे जो ऑडिओ ऑब्जेक्टचे प्रतिनिधित्व करतो. तो Jupyter Notebook वातावरणात ऑडिओ प्लेयर तयार करण्यासाठी वापरला जातो. क्लास विविध पॅरामीटर्स स्वीकारतो जसे की data, filename, url, embed, rate, autoplay, आणि normalize. data पॅरामीटर numpy array, सॅम्पल्सची यादी, filename किंवा URL दर्शवणारी स्ट्रिंग, किंवा raw PCM data असू शकते. filename पॅरामीटर स्थानिक फाईलमधून ऑडिओ डेटा लोड करण्यासाठी वापरला जातो, आणि url पॅरामीटर URL वरून ऑडिओ डेटा डाउनलोड करण्यासाठी वापरला जातो. embed पॅरामीटर ठरवतो की ऑडिओ डेटा data URI वापरून एम्बेड केला जावा किंवा मूळ स्रोतावरून संदर्भित केला जावा. rate पॅरामीटर ऑडिओ डेटाचा सॅम्पलिंग दर निर्दिष्ट करतो. autoplay पॅरामीटर ठरवतो की ऑडिओ आपोआप प्ले व्हावा. normalize पॅरामीटर ठरवतो की ऑडिओ डेटा जास्तीत जास्त श्रेणीसाठी पुनर्प्रशिक्षित (rescaled) केला जावा. Audio क्लास reload सारख्या पद्धती प्रदान करतो ज्यामुळे फाईल किंवा URL वरून ऑडिओ डेटा पुन्हा लोड करता येतो, आणि src_attr, autoplay_attr, आणि element_id_attr सारखे गुणधर्म HTML मधील ऑडिओ एलिमेंटसाठी संबंधित गुणधर्म मिळवण्यासाठी वापरले जातात.\n",
    "\n",
    "संपूर्णपणे, हे फंक्शन्स आणि क्लासेस ऑडिओचे मजकुरात रूपांतर करण्यासाठी, चॅटबॉटकडून ऑडिओ प्रतिसाद निर्माण करण्यासाठी, आणि Jupyter Notebook वातावरणात ऑडिओ दाखवण्यासाठी आणि प्ले करण्यासाठी वापरले जातात.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**अस्वीकरण**:  \nहा दस्तऐवज AI भाषांतर सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) वापरून भाषांतरित करण्यात आला आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी, कृपया लक्षात ठेवा की स्वयंचलित भाषांतरांमध्ये त्रुटी किंवा अचूकतेचा अभाव असू शकतो. मूळ भाषेतील दस्तऐवज हा अधिकृत स्रोत मानला जावा. महत्त्वाच्या माहितीसाठी व्यावसायिक मानवी भाषांतराची शिफारस केली जाते. या भाषांतराचा वापर करून निर्माण होणाऱ्या कोणत्याही गैरसमज किंवा चुकीच्या अर्थासाठी आम्ही जबाबदार राहणार नाही.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T19:54:44+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "mr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}