<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7b08e277df2a9307f861ae54bc30c772",
  "translation_date": "2025-05-09T09:55:31+00:00",
  "source_file": "md/01.Introduction/02/06.NVIDIA.md",
  "language_code": "da"
}
-->
## Phi-familien i NVIDIA NIM

NVIDIA NIM er et sæt brugervenlige mikrotjenester designet til at accelerere udrulningen af generative AI-modeller på tværs af cloud, datacentre og arbejdsstationer. NIM’er er kategoriseret efter modelfamilie og pr. model. For eksempel bringer NVIDIA NIM for store sprogmodeller (LLMs) kraften fra de nyeste LLM’er til virksomhedsapplikationer og leverer uovertrufne evner inden for naturlig sprogbehandling og forståelse.

NIM gør det nemt for IT- og DevOps-teams at selvhoste store sprogmodeller (LLMs) i deres egne styrede miljøer, samtidig med at udviklere får adgang til branchestandard-API’er, som gør det muligt at bygge kraftfulde copiloter, chatbots og AI-assistenter, der kan transformere deres forretning. Ved at udnytte NVIDIA’s banebrydende GPU-acceleration og skalerbare udrulning tilbyder NIM den hurtigste vej til inferens med uovertruffen ydeevne.

Du kan bruge NVIDIA NIM til at inferere Phi Family Models

![nim](../../../../../translated_images/Phi-NIM.45af94d89220fbbbc85f8da0379150a29cc88c3dd8ec417b1d3b7237bbe1c58a.da.png)

### **Eksempler - Phi-3-Vision i NVIDIA NIM**

Forestil dig, at du har et billede (`demo.png`), og du ønsker at generere Python-kode, som behandler dette billede og gemmer en ny version af det (`phi-3-vision.jpg`).

Koden ovenfor automatiserer denne proces ved at:

1. Sætte miljøet op og konfigurere det nødvendige.
2. Oprette en prompt, som instruerer modellen til at generere den nødvendige Python-kode.
3. Sende prompten til modellen og indsamle den genererede kode.
4. Udtrække og køre den genererede kode.
5. Vise det originale og det behandlede billede.

Denne tilgang udnytter AI’s kraft til at automatisere billedbehandlingsopgaver, hvilket gør det lettere og hurtigere at nå dine mål.

[Sample Code Solution](../../../../../code/06.E2E/E2E_Nvidia_NIM_Phi3_Vision.ipynb)

Lad os gennemgå, hvad hele koden gør trin for trin:

1. **Installer nødvendigt pakke**:  
    ```python
    !pip install langchain_nvidia_ai_endpoints -U
    ```  
    Denne kommando installerer pakken `langchain_nvidia_ai_endpoints` og sikrer, at det er den nyeste version.

2. **Importer nødvendige moduler**:  
    ```python
    from langchain_nvidia_ai_endpoints import ChatNVIDIA
    import getpass
    import os
    import base64
    ```  
    Disse imports henter de nødvendige moduler til at interagere med NVIDIA AI-endpoints, håndtere adgangskoder sikkert, arbejde med operativsystemet og kode/dekode data i base64-format.

3. **Opsæt API-nøgle**:  
    ```python
    if not os.getenv("NVIDIA_API_KEY"):
        os.environ["NVIDIA_API_KEY"] = getpass.getpass("Enter your NVIDIA API key: ")
    ```  
    Denne kode tjekker, om miljøvariablen `NVIDIA_API_KEY` er sat. Hvis ikke, beder den brugeren om sikkert at indtaste deres API-nøgle.

4. **Definér model og sti til billede**:  
    ```python
    model = 'microsoft/phi-3-vision-128k-instruct'
    chat = ChatNVIDIA(model=model)
    img_path = './imgs/demo.png'
    ```  
    Her sættes den model, der skal bruges, op, en instans af `ChatNVIDIA` oprettes med den valgte model, og stien til billedfilen defineres.

5. **Opret tekstprompt**:  
    ```python
    text = "Please create Python code for image, and use plt to save the new picture under imgs/ and name it phi-3-vision.jpg."
    ```  
    Denne prompt instruerer modellen til at generere Python-kode til billedbehandling.

6. **Kode billedet i base64**:  
    ```python
    with open(img_path, "rb") as f:
        image_b64 = base64.b64encode(f.read()).decode()
    image = f'<img src="data:image/png;base64,{image_b64}" />'
    ```  
    Koden læser billedfilen, koder den i base64 og opretter et HTML-billedtag med den kodede data.

7. **Kombinér tekst og billede til prompt**:  
    ```python
    prompt = f"{text} {image}"
    ```  
    Tekstprompten og HTML-billedtagget kombineres til én streng.

8. **Generér kode med ChatNVIDIA**:  
    ```python
    code = ""
    for chunk in chat.stream(prompt):
        print(chunk.content, end="")
        code += chunk.content
    ```  
    Prompten sendes til `ChatNVIDIA` model and collects the generated code in chunks, printing and appending each chunk to the `code`-strengen.

9. **Udtræk Python-kode fra genereret indhold**:  
    ```python
    begin = code.index('```python') + 9  
    code = code[begin:]  
    end = code.index('```')
    code = code[:end]
    ```  
    Her udtrækkes den faktiske Python-kode fra det genererede indhold ved at fjerne markdown-formateringen.

10. **Kør den genererede kode**:  
    ```python
    import subprocess
    result = subprocess.run(["python", "-c", code], capture_output=True)
    ```  
    Den udtrukne Python-kode køres som en subprocess, og output fanges.

11. **Vis billederne**:  
    ```python
    from IPython.display import Image, display
    display(Image(filename='./imgs/phi-3-vision.jpg'))
    display(Image(filename='./imgs/demo.png'))
    ```  
    Disse linjer viser billederne ved hjælp af `IPython.display` modulet.

**Ansvarsfraskrivelse**:  
Dette dokument er oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på nøjagtighed, bedes du være opmærksom på, at automatiske oversættelser kan indeholde fejl eller unøjagtigheder. Det oprindelige dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi påtager os intet ansvar for misforståelser eller fejltolkninger, der opstår som følge af brugen af denne oversættelse.