<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:45:13+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "da"
}
-->
# Nøgle teknologier nævnt inkluderer

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - et lavniveau-API til hardwareaccelereret maskinlæring bygget oven på DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - en parallel computing-platform og API-model udviklet af Nvidia, som muliggør generel behandling på grafikkort (GPU’er).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - et åbent format designet til at repræsentere maskinlæringsmodeller, som sikrer interoperabilitet mellem forskellige ML-rammeværk.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - et format til repræsentation og opdatering af maskinlæringsmodeller, særligt nyttigt for mindre sprogmodeller, der kan køre effektivt på CPU’er med 4-8bit kvantisering.

## DirectML

DirectML er et lavniveau-API, der muliggør hardwareaccelereret maskinlæring. Det er bygget oven på DirectX 12 for at udnytte GPU-acceleration og er leverandøruafhængigt, hvilket betyder, at det ikke kræver kodeændringer for at fungere på tværs af forskellige GPU-leverandører. Det bruges primært til modeltræning og inferensarbejdsbelastninger på GPU’er.

Med hensyn til hardwareunderstøttelse er DirectML designet til at fungere med et bredt udvalg af GPU’er, herunder AMD integrerede og diskrete GPU’er, Intel integrerede GPU’er og NVIDIA diskrete GPU’er. Det er en del af Windows AI Platform og understøttes på Windows 10 & 11, hvilket muliggør modeltræning og inferens på enhver Windows-enhed.

Der har været opdateringer og muligheder relateret til DirectML, såsom understøttelse af op til 150 ONNX-operatorer og brug af både ONNX runtime og WinML. Det støttes af store Integrated Hardware Vendors (IHVs), som hver især implementerer forskellige metakommandoer.

## CUDA

CUDA, som står for Compute Unified Device Architecture, er en parallel computing-platform og API-model skabt af Nvidia. Den giver softwareudviklere mulighed for at bruge en CUDA-aktiveret grafikkort (GPU) til generel behandling – en tilgang kaldet GPGPU (General-Purpose computing on Graphics Processing Units). CUDA er en nøglekomponent i Nvidias GPU-acceleration og anvendes bredt inden for forskellige områder, herunder maskinlæring, videnskabelig computing og videobehandling.

Hardwareunderstøttelsen for CUDA er specifik for Nvidias GPU’er, da det er en proprietær teknologi udviklet af Nvidia. Hver arkitektur understøtter specifikke versioner af CUDA-toolkittet, som leverer de nødvendige biblioteker og værktøjer til udviklere til at bygge og køre CUDA-applikationer.

## ONNX

ONNX (Open Neural Network Exchange) er et åbent format designet til at repræsentere maskinlæringsmodeller. Det giver en definition af en udvidelig beregningsgrafmodel samt definitioner af indbyggede operatorer og standard datatyper. ONNX gør det muligt for udviklere at flytte modeller mellem forskellige ML-rammeværk, hvilket sikrer interoperabilitet og gør det lettere at skabe og implementere AI-applikationer.

Phi3 mini kan køre med ONNX Runtime på CPU og GPU på tværs af enheder, herunder serverplatforme, Windows, Linux og Mac desktops samt mobile CPU’er.  
De optimerede konfigurationer, vi har tilføjet, er

- ONNX-modeller til int4 DML: Kvantiseret til int4 via AWQ  
- ONNX-model til fp16 CUDA  
- ONNX-model til int4 CUDA: Kvantiseret til int4 via RTN  
- ONNX-model til int4 CPU og Mobile: Kvantiseret til int4 via RTN  

## Llama.cpp

Llama.cpp er et open source softwarebibliotek skrevet i C++. Det udfører inferens på forskellige store sprogmodeller (LLMs), herunder Llama. Udviklet sideløbende med ggml-biblioteket (et generelt tensorbibliotek), har llama.cpp til formål at levere hurtigere inferens og lavere hukommelsesforbrug sammenlignet med den oprindelige Python-implementering. Det understøtter hardwareoptimering, kvantisering og tilbyder et simpelt API samt eksempler. Hvis du er interesseret i effektiv LLM-inferens, er llama.cpp værd at udforske, da Phi3 kan køre Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) er et format, der bruges til at repræsentere og opdatere maskinlæringsmodeller. Det er særligt nyttigt for mindre sprogmodeller (SLMs), som kan køre effektivt på CPU’er med 4-8bit kvantisering. GGUF er fordelagtigt til hurtig prototyping og til at køre modeller på edge-enheder eller i batchjobs som CI/CD-pipelines.

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på nøjagtighed, bedes du være opmærksom på, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det oprindelige dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi påtager os intet ansvar for misforståelser eller fejltolkninger, der opstår som følge af brugen af denne oversættelse.