<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-07-16T18:10:20+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "da"
}
-->
# Kom godt i gang med Phi-3 lokalt

Denne guide hjælper dig med at sætte dit lokale miljø op til at køre Phi-3 modellen ved hjælp af Ollama. Du kan køre modellen på flere forskellige måder, herunder ved brug af GitHub Codespaces, VS Code Dev Containers eller dit lokale miljø.

## Miljøopsætning

### GitHub Codespaces

Du kan køre denne skabelon virtuelt ved at bruge GitHub Codespaces. Knappen åbner en webbaseret VS Code instans i din browser:

1. Åbn skabelonen (det kan tage flere minutter):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Åbn et terminalvindue

### VS Code Dev Containers

⚠️ Denne mulighed virker kun, hvis din Docker Desktop har tildelt mindst 16 GB RAM. Hvis du har mindre end 16 GB RAM, kan du prøve [GitHub Codespaces-muligheden](../../../../../md/01.Introduction/01) eller [sætte det op lokalt](../../../../../md/01.Introduction/01).

En relateret mulighed er VS Code Dev Containers, som åbner projektet i din lokale VS Code ved hjælp af [Dev Containers-udvidelsen](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Start Docker Desktop (installer det, hvis det ikke allerede er installeret)
2. Åbn projektet:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. I det VS Code-vindue, der åbner, når projektfilerne vises (det kan tage flere minutter), åbner du et terminalvindue.
4. Fortsæt med [deployments-trinene](../../../../../md/01.Introduction/01)

### Lokalt miljø

1. Sørg for, at følgende værktøjer er installeret:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Test modellen

1. Bed Ollama om at downloade og køre phi3:mini modellen:

    ```shell
    ollama run phi3:mini
    ```

    Det vil tage et par minutter at downloade modellen.

2. Når du ser "success" i outputtet, kan du sende en besked til modellen fra prompten.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Efter nogle sekunder burde du se et svar strømme ind fra modellen.

4. For at lære om forskellige teknikker brugt med sprogmodeller, åbn Python-notebooken [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) og kør hver celle. Hvis du brugte en anden model end 'phi3:mini', skal du ændre `MODEL_NAME` i den første celle.

5. For at føre en samtale med phi3:mini modellen fra Python, åbn Python-filen [chat.py](../../../../../code/01.Introduce/chat.py) og kør den. Du kan ændre `MODEL_NAME` øverst i filen efter behov, og du kan også tilpasse systembeskeden eller tilføje few-shot eksempler, hvis du ønsker det.

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på nøjagtighed, bedes du være opmærksom på, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det oprindelige dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi påtager os intet ansvar for misforståelser eller fejltolkninger, der opstår som følge af brugen af denne oversættelse.