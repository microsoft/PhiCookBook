<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-09T07:10:27+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "da"
}
-->
# Kom godt i gang med Phi-3 lokalt

Denne vejledning hjælper dig med at sætte dit lokale miljø op til at køre Phi-3 modellen ved hjælp af Ollama. Du kan køre modellen på flere forskellige måder, herunder ved at bruge GitHub Codespaces, VS Code Dev Containers eller dit lokale miljø.

## Miljøopsætning

### GitHub Codespaces

Du kan køre denne skabelon virtuelt ved at bruge GitHub Codespaces. Knappen åbner en webbaseret VS Code instans i din browser:

1. Åbn skabelonen (det kan tage flere minutter):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Åbn et terminalvindue

### VS Code Dev Containers

⚠️ Denne mulighed virker kun, hvis din Docker Desktop har mindst 16 GB RAM tildelt. Hvis du har mindre end 16 GB RAM, kan du prøve [GitHub Codespaces muligheden](../../../../../md/01.Introduction/01) eller [sætte det op lokalt](../../../../../md/01.Introduction/01).

En relateret mulighed er VS Code Dev Containers, som åbner projektet i din lokale VS Code ved hjælp af [Dev Containers udvidelsen](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Start Docker Desktop (installer det, hvis det ikke allerede er installeret)
2. Åbn projektet:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. I det VS Code vindue, der åbner, når projektfilerne vises (det kan tage flere minutter), åbnes et terminalvindue.
4. Fortsæt med [deploymentskridtene](../../../../../md/01.Introduction/01)

### Lokalt miljø

1. Sørg for, at følgende værktøjer er installeret:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Test modellen

1. Bed Ollama om at downloade og køre phi3:mini modellen:

    ```shell
    ollama run phi3:mini
    ```

    Det vil tage et par minutter at downloade modellen.

2. Når du ser "success" i output, kan du sende en besked til modellen fra prompten.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Efter nogle sekunder bør du se et svarstrøm komme fra modellen.

4. For at lære om forskellige teknikker, der bruges med sprogmodeller, kan du åbne Python-notebooken [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) og køre hver celle. Hvis du brugte en anden model end 'phi3:mini', skal du ændre `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` øverst i filen efter behov, og du kan også ændre systembeskeden eller tilføje few-shot eksempler, hvis du ønsker det.

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på nøjagtighed, bedes du være opmærksom på, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det oprindelige dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi påtager os intet ansvar for eventuelle misforståelser eller fejltolkninger, der opstår som følge af brugen af denne oversættelse.