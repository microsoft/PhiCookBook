<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c8273672cc57df2be675407a1383aaf0",
  "translation_date": "2025-07-16T17:49:10+00:00",
  "source_file": "md/01.Introduction/01/01.AISafety.md",
  "language_code": "da"
}
-->
# AI-sikkerhed for Phi-modeller  
Phi-familien af modeller er udviklet i overensstemmelse med [Microsoft Responsible AI Standard](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFl), som er et virksomheds-dækkende sæt krav baseret på følgende seks principper: ansvarlighed, gennemsigtighed, retfærdighed, pålidelighed og sikkerhed, privatliv og sikkerhed samt inklusivitet, som udgør [Microsofts Responsible AI-principper](https://www.microsoft.com/ai/responsible-ai).

Ligesom de tidligere Phi-modeller er der anvendt en flerstrenget sikkerhedsvurdering og en sikkerhedstræning efter træning, med yderligere tiltag for at tage højde for denne udgivelses flersprogede kapaciteter. Vores tilgang til sikkerhedstræning og evalueringer, herunder test på tværs af flere sprog og risikokategorier, er beskrevet i [Phi Safety Post-Training Paper](https://arxiv.org/abs/2407.13833). Selvom Phi-modellerne drager fordel af denne tilgang, bør udviklere anvende ansvarlige AI bedste praksisser, herunder kortlægning, måling og afbødning af risici forbundet med deres specifikke anvendelsestilfælde samt kulturel og sproglig kontekst.

## Bedste praksis

Ligesom andre modeller kan Phi-familien potentielt opføre sig på måder, der er uretfærdige, upålidelige eller stødende.

Nogle af de begrænsende adfærdsmønstre i SLM og LLM, som du bør være opmærksom på, inkluderer:

- **Servicekvalitet:** Phi-modellerne er primært trænet på engelsksproget tekst. Sprog andre end engelsk vil opleve dårligere ydeevne. Engelske sprogvarianter med mindre repræsentation i træningsdataene kan opleve dårligere ydeevne end standard amerikansk engelsk.  
- **Repræsentation af skader og fastholdelse af stereotyper:** Disse modeller kan over- eller underrepræsentere grupper af mennesker, slette repræsentation af visse grupper eller forstærke nedværdigende eller negative stereotyper. På trods af sikkerhedstræning efter træning kan disse begrænsninger stadig være til stede på grund af forskellige niveauer af repræsentation af forskellige grupper eller forekomsten af eksempler på negative stereotyper i træningsdata, som afspejler virkelige mønstre og samfundsmæssige bias.  
- **Upassende eller stødende indhold:** Disse modeller kan producere andre typer upassende eller stødende indhold, hvilket kan gøre det uhensigtsmæssigt at anvende dem i følsomme sammenhænge uden yderligere afbødende tiltag, der er specifikke for anvendelsestilfældet.  
- **Informationspålidelighed:** Sprogmodeller kan generere meningsløst indhold eller fabrikere indhold, der kan lyde rimeligt, men som er unøjagtigt eller forældet.  
- **Begrænset omfang for kode:** Størstedelen af Phi-3 træningsdata er baseret på Python og bruger almindelige pakker som "typing, math, random, collections, datetime, itertools". Hvis modellen genererer Python-scripts, der anvender andre pakker eller scripts i andre sprog, anbefaler vi kraftigt, at brugere manuelt verificerer alle API-anvendelser.

Udviklere bør anvende ansvarlige AI bedste praksisser og er ansvarlige for at sikre, at et specifikt anvendelsestilfælde overholder gældende love og regler (f.eks. privatliv, handel osv.).

## Overvejelser om ansvarlig AI

Ligesom andre sprogmodeller kan Phi-seriens modeller potentielt opføre sig på måder, der er uretfærdige, upålidelige eller stødende. Nogle af de begrænsende adfærdsmønstre, man bør være opmærksom på, inkluderer:

**Servicekvalitet:** Phi-modellerne er primært trænet på engelsksproget tekst. Sprog andre end engelsk vil opleve dårligere ydeevne. Engelske sprogvarianter med mindre repræsentation i træningsdataene kan opleve dårligere ydeevne end standard amerikansk engelsk.

**Repræsentation af skader og fastholdelse af stereotyper:** Disse modeller kan over- eller underrepræsentere grupper af mennesker, slette repræsentation af visse grupper eller forstærke nedværdigende eller negative stereotyper. På trods af sikkerhedstræning efter træning kan disse begrænsninger stadig være til stede på grund af forskellige niveauer af repræsentation af forskellige grupper eller forekomsten af eksempler på negative stereotyper i træningsdata, som afspejler virkelige mønstre og samfundsmæssige bias.

**Upassende eller stødende indhold:** Disse modeller kan producere andre typer upassende eller stødende indhold, hvilket kan gøre det uhensigtsmæssigt at anvende dem i følsomme sammenhænge uden yderligere afbødende tiltag, der er specifikke for anvendelsestilfældet.  
Informationspålidelighed: Sprogmodeller kan generere meningsløst indhold eller fabrikere indhold, der kan lyde rimeligt, men som er unøjagtigt eller forældet.

**Begrænset omfang for kode:** Størstedelen af Phi-3 træningsdata er baseret på Python og bruger almindelige pakker som "typing, math, random, collections, datetime, itertools". Hvis modellen genererer Python-scripts, der anvender andre pakker eller scripts i andre sprog, anbefaler vi kraftigt, at brugere manuelt verificerer alle API-anvendelser.

Udviklere bør anvende ansvarlige AI bedste praksisser og er ansvarlige for at sikre, at et specifikt anvendelsestilfælde overholder gældende love og regler (f.eks. privatliv, handel osv.). Vigtige områder til overvejelse inkluderer:

**Allokering:** Modeller er muligvis ikke egnede til scenarier, der kan have væsentlig indflydelse på juridisk status eller fordeling af ressourcer eller livsmuligheder (f.eks. bolig, beskæftigelse, kredit osv.) uden yderligere vurderinger og ekstra afbiaseringsteknikker.

**Højt-risiko scenarier:** Udviklere bør vurdere egnetheden af at bruge modeller i højt-risiko scenarier, hvor uretfærdige, upålidelige eller stødende output kan være ekstremt omkostningsfulde eller føre til skade. Dette inkluderer rådgivning i følsomme eller ekspertområder, hvor nøjagtighed og pålidelighed er afgørende (f.eks. juridisk eller sundhedsmæssig rådgivning). Yderligere sikkerhedsforanstaltninger bør implementeres på applikationsniveau i henhold til implementeringskonteksten.

**Fejlinformation:** Modeller kan producere unøjagtige oplysninger. Udviklere bør følge bedste praksis for gennemsigtighed og informere slutbrugere om, at de interagerer med et AI-system. På applikationsniveau kan udviklere opbygge feedbackmekanismer og pipelines for at forankre svar i anvendelsesspecifik, kontekstuel information, en teknik kendt som Retrieval Augmented Generation (RAG).

**Generering af skadeligt indhold:** Udviklere bør vurdere output i forhold til deres kontekst og bruge tilgængelige sikkerhedsklassifikatorer eller skræddersyede løsninger, der passer til deres anvendelsestilfælde.

**Misbrug:** Andre former for misbrug som svindel, spam eller malware-produktion kan være mulige, og udviklere bør sikre, at deres applikationer ikke overtræder gældende love og regler.

### Finetuning og AI-indholdssikkerhed

Efter finetuning af en model anbefaler vi kraftigt at benytte [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) til at overvåge det indhold, som modellerne genererer, identificere og blokere potentielle risici, trusler og kvalitetsproblemer.

![Phi3AISafety](../../../../../translated_images/01.phi3aisafety.c0d7fc42f5a5c40507c5e8be556615b8377a63b8764865d057d4faac3757a478.da.png)

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) understøtter både tekst- og billedindhold. Det kan implementeres i skyen, i isolerede containere og på edge/indlejrede enheder.

## Oversigt over Azure AI Content Safety

Azure AI Content Safety er ikke en løsning, der passer til alle; den kan tilpasses for at matche virksomheders specifikke politikker. Derudover gør dens flersprogede modeller det muligt at forstå flere sprog samtidig.

![AIContentSafety](../../../../../translated_images/01.AIcontentsafety.a288819b8ce8da1a56cf708aff010a541799d002ae7ae84bb819b19ab8950591.da.png)

- **Azure AI Content Safety**  
- **Microsoft Developer**  
- **5 videoer**

Azure AI Content Safety-tjenesten opdager skadeligt bruger- og AI-genereret indhold i applikationer og tjenester. Den inkluderer tekst- og billed-API’er, der gør det muligt at opdage skadeligt eller upassende materiale.

[AI Content Safety Playlist](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på nøjagtighed, bedes du være opmærksom på, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det oprindelige dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi påtager os intet ansvar for misforståelser eller fejltolkninger, der opstår som følge af brugen af denne oversættelse.