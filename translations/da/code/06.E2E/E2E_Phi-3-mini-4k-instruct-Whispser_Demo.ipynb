{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaktiv Phi 3 Mini 4K Instruktionschatbot med Whisper\n",
    "\n",
    "### Introduktion:\n",
    "Den interaktive Phi 3 Mini 4K Instruktionschatbot er et værktøj, der giver brugere mulighed for at interagere med Microsoft Phi 3 Mini 4K instruktionsdemoen ved hjælp af tekst- eller lydinput. Chatbotten kan bruges til en række opgaver, såsom oversættelse, vejropdateringer og generel informationsindsamling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "Opret din Huggingface Access Token\n",
    "\n",
    "Opret en ny token  \n",
    "Angiv et nyt navn  \n",
    "Vælg skrive-tilladelser  \n",
    "Kopiér token og gem den et sikkert sted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den følgende Python-kode udfører to hovedopgaver: import af `os`-modulet og opsætning af en miljøvariabel.\n",
    "\n",
    "1. Import af `os`-modulet:\n",
    "   - `os`-modulet i Python giver en måde at interagere med operativsystemet på. Det gør det muligt at udføre forskellige opgaver relateret til operativsystemet, såsom adgang til miljøvariabler, arbejde med filer og mapper osv.\n",
    "   - I denne kode importeres `os`-modulet ved hjælp af `import`-sætningen. Denne sætning gør funktionaliteten i `os`-modulet tilgængelig til brug i det aktuelle Python-script.\n",
    "\n",
    "2. Opsætning af en miljøvariabel:\n",
    "   - En miljøvariabel er en værdi, som kan tilgås af programmer, der kører på operativsystemet. Det er en måde at gemme konfigurationsindstillinger eller anden information, som kan bruges af flere programmer.\n",
    "   - I denne kode sættes en ny miljøvariabel ved hjælp af `os.environ`-ordbogen. Nøglen i ordbogen er `'HF_TOKEN'`, og værdien tildeles fra variablen `HUGGINGFACE_TOKEN`.\n",
    "   - Variablen `HUGGINGFACE_TOKEN` er defineret lige over denne kode, og den tildeles en strengværdi `\"hf_**************\"` ved hjælp af syntaksen `#@param`. Denne syntaks bruges ofte i Jupyter-notebooks til at tillade brugerinput og parameterkonfiguration direkte i notebook-grænsefladen.\n",
    "   - Ved at sætte miljøvariablen `'HF_TOKEN'` kan den tilgås af andre dele af programmet eller andre programmer, der kører på det samme operativsystem.\n",
    "\n",
    "Samlet set importerer denne kode `os`-modulet og sætter en miljøvariabel med navnet `'HF_TOKEN'` med den værdi, der er angivet i variablen `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denne kodebid viser en funktion kaldet clear_output, som bruges til at rydde outputtet fra den aktuelle celle i Jupyter Notebook eller IPython. Lad os gennemgå koden og forstå dens funktionalitet:\n",
    "\n",
    "Funktionen clear_output tager én parameter kaldet wait, som er en boolsk værdi. Som standard er wait sat til False. Denne parameter afgør, om funktionen skal vente, indtil nyt output er tilgængeligt for at erstatte det eksisterende output, før det ryddes.\n",
    "\n",
    "Selve funktionen bruges til at rydde outputtet fra den aktuelle celle. I Jupyter Notebook eller IPython, når en celle genererer output, såsom udskrevet tekst eller grafiske plots, vises dette output under cellen. Funktionen clear_output giver dig mulighed for at fjerne dette output.\n",
    "\n",
    "Implementeringen af funktionen er ikke angivet i kodebidet, som det fremgår af ellipsen (...). Ellipsen repræsenterer en pladsholder for den faktiske kode, der udfører rydningen af outputtet. Implementeringen af funktionen kan involvere interaktion med Jupyter Notebook- eller IPython-API'en for at fjerne det eksisterende output fra cellen.\n",
    "\n",
    "Samlet set giver denne funktion en praktisk måde at rydde outputtet fra den aktuelle celle i Jupyter Notebook eller IPython, hvilket gør det lettere at administrere og opdatere det viste output under interaktive kodningssessioner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Udfør tekst-til-tale (TTS) ved hjælp af Edge TTS-tjenesten. Lad os gennemgå de relevante funktionsimplementeringer én ad gangen:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Denne funktion tager en inputværdi og beregner hastighedsstrengen for TTS-stemmen. Inputværdien repræsenterer den ønskede talehastighed, hvor en værdi på 1 repræsenterer normal hastighed. Funktionen beregner hastighedsstrengen ved at trække 1 fra inputværdien, multiplicere den med 100 og derefter bestemme tegnet baseret på, om inputværdien er større end eller lig med 1. Funktionen returnerer hastighedsstrengen i formatet \"{sign}{rate}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: Denne funktion tager en inputtekst og et sprog som parametre. Den opdeler inputteksten i stykker baseret på sprogspecifikke regler. I denne implementering, hvis sproget er \"English\", opdeler funktionen teksten ved hver punktum (\".\") og fjerner eventuelle indledende eller afsluttende mellemrum. Den tilføjer derefter et punktum til hvert stykke og returnerer den filtrerede liste af stykker.\n",
    "\n",
    "3. `tts_file_name(text)`: Denne funktion genererer et filnavn til TTS-lydfilen baseret på inputteksten. Den udfører flere transformationer på teksten: fjerner et afsluttende punktum (hvis til stede), konverterer teksten til små bogstaver, fjerner indledende og afsluttende mellemrum og erstatter mellemrum med understregninger. Den afkorter derefter teksten til maksimalt 25 tegn (hvis længere) eller bruger den fulde tekst, hvis den er tom. Til sidst genererer den en tilfældig streng ved hjælp af [`uuid`]-modulet og kombinerer den med den afkortede tekst for at skabe filnavnet i formatet \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Denne funktion samler flere lydfiler i én enkelt lydfil. Den tager en liste over lydfilstier og en outputsti som parametre. Funktionen initialiserer et tomt `AudioSegment`-objekt kaldet [`merged_audio`]. Den itererer derefter gennem hver lydfilsti, indlæser lydfilen ved hjælp af metoden `AudioSegment.from_file()` fra `pydub`-biblioteket og tilføjer den aktuelle lydfil til [`merged_audio`]-objektet. Til sidst eksporterer den den sammensatte lyd til den angivne outputsti i MP3-format.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: Denne funktion udfører TTS-operationen ved hjælp af Edge TTS-tjenesten. Den tager en liste over tekststykker, talehastigheden, stemmens navn og gemme-stien som parametre. Hvis antallet af stykker er større end 1, opretter funktionen en mappe til opbevaring af de individuelle lydfiler. Den itererer derefter gennem hvert stykke, konstruerer en Edge TTS-kommando ved hjælp af funktionen `calculate_rate_string()`, stemmens navn og tekststykket og udfører kommandoen ved hjælp af funktionen `os.system()`. Hvis kommandoens udførelse lykkes, tilføjer den stien til den genererede lydfil til en liste. Efter behandling af alle stykker samler den de individuelle lydfiler ved hjælp af funktionen `merge_audio_files()` og gemmer den sammensatte lyd til den angivne gemme-sti. Hvis der kun er ét stykke, genererer den direkte Edge TTS-kommandoen og gemmer lyden til gemme-stien. Til sidst returnerer den gemme-stien til den genererede lydfil.\n",
    "\n",
    "6. `random_audio_name_generate()`: Denne funktion genererer et tilfældigt lydfilnavn ved hjælp af [`uuid`]-modulet. Den genererer en tilfældig UUID, konverterer den til en streng, tager de første 8 tegn, tilføjer \".mp3\"-udvidelsen og returnerer det tilfældige lydfilnavn.\n",
    "\n",
    "7. `talk(input_text)`: Denne funktion er hovedindgangspunktet for at udføre TTS-operationen. Den tager en inputtekst som parameter. Den kontrollerer først længden af inputteksten for at afgøre, om det er en lang sætning (større end eller lig med 600 tegn). Baseret på længden og værdien af variablen `translate_text_flag` bestemmer den sproget og genererer listen over tekststykker ved hjælp af funktionen `make_chunks()`. Den genererer derefter en gemme-sti til lydfilen ved hjælp af funktionen `random_audio_name_generate()`. Til sidst kalder den funktionen `edge_free_tts()` for at udføre TTS-operationen og returnerer gemme-stien til den genererede lydfil.\n",
    "\n",
    "Samlet set arbejder disse funktioner sammen for at opdele inputteksten i stykker, generere et filnavn til lydfilen, udføre TTS-operationen ved hjælp af Edge TTS-tjenesten og samle de individuelle lydfiler i én enkelt lydfil.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementeringen af to funktioner: convert_to_text og run_text_prompt, samt deklarationen af to klasser: str og Audio.\n",
    "\n",
    "Funktionen convert_to_text tager en audio_path som input og transskriberer lyden til tekst ved hjælp af en model kaldet whisper_model. Funktionen kontrollerer først, om gpu-flaget er sat til True. Hvis det er tilfældet, bruges whisper_model med visse parametre såsom word_timestamps=True, fp16=True, language='English' og task='translate'. Hvis gpu-flaget er False, bruges whisper_model med fp16=False. Den resulterende transskription gemmes derefter i en fil kaldet 'scan.txt' og returneres som tekst.\n",
    "\n",
    "Funktionen run_text_prompt tager en besked og en chat_history som input. Den bruger funktionen phi_demo til at generere et svar fra en chatbot baseret på inputbeskeden. Det genererede svar sendes derefter til funktionen talk, som konverterer svaret til en lydfil og returnerer filstien. Klassen Audio bruges til at vise og afspille lydfilen. Lyden vises ved hjælp af display-funktionen fra modulet IPython.display, og Audio-objektet oprettes med parameteren autoplay=True, så lyden starter automatisk. Chat_history opdateres med inputbeskeden og det genererede svar, og en tom streng samt den opdaterede chat_history returneres.\n",
    "\n",
    "Klassen str er en indbygget klasse i Python, der repræsenterer en sekvens af tegn. Den tilbyder forskellige metoder til at manipulere og arbejde med strenge, såsom capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill og flere. Disse metoder gør det muligt at udføre operationer som søgning, erstatning, formatering og manipulation af strenge.\n",
    "\n",
    "Klassen Audio er en brugerdefineret klasse, der repræsenterer et lydobjekt. Den bruges til at oprette en lydafspiller i Jupyter Notebook-miljøet. Klassen accepterer forskellige parametre såsom data, filename, url, embed, rate, autoplay og normalize. Parameteren data kan være en numpy-array, en liste af samples, en streng, der repræsenterer en filnavn eller URL, eller rå PCM-data. Parameteren filename bruges til at angive en lokal fil, hvorfra lyddataene skal indlæses, og parameteren url bruges til at angive en URL, hvorfra lyddataene skal downloades. Parameteren embed bestemmer, om lyddataene skal indlejres ved hjælp af en data-URI eller refereres fra den originale kilde. Parameteren rate angiver samplingsraten for lyddataene. Parameteren autoplay bestemmer, om lyden skal starte automatisk. Parameteren normalize angiver, om lyddataene skal normaliseres (omskaleres) til det maksimale mulige område. Klassen Audio tilbyder også metoder som reload til at genindlæse lyddataene fra fil eller URL, og attributter som src_attr, autoplay_attr og element_id_attr til at hente de tilsvarende attributter for lydelementet i HTML.\n",
    "\n",
    "Samlet set bruges disse funktioner og klasser til at transskribere lyd til tekst, generere lydsvar fra en chatbot og vise samt afspille lyd i Jupyter Notebook-miljøet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Ansvarsfraskrivelse**:  \nDette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på nøjagtighed, skal du være opmærksom på, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det originale dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi er ikke ansvarlige for eventuelle misforståelser eller fejltolkninger, der opstår som følge af brugen af denne oversættelse.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T23:15:26+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "da"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}