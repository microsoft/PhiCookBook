<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:46:14+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "vi"
}
-->
# Các công nghệ chính được đề cập bao gồm

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - một API cấp thấp cho học máy tăng tốc phần cứng được xây dựng trên DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - một nền tảng tính toán song song và mô hình giao diện lập trình ứng dụng (API) do Nvidia phát triển, cho phép xử lý đa mục đích trên các đơn vị xử lý đồ họa (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - một định dạng mở được thiết kế để biểu diễn các mô hình học máy, cung cấp khả năng tương tác giữa các framework ML khác nhau.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - một định dạng dùng để biểu diễn và cập nhật các mô hình học máy, đặc biệt hữu ích cho các mô hình ngôn ngữ nhỏ hơn có thể chạy hiệu quả trên CPU với lượng tử hóa 4-8 bit.

## DirectML

DirectML là một API cấp thấp cho phép học máy tăng tốc phần cứng. Nó được xây dựng trên DirectX 12 để tận dụng tăng tốc GPU và không phụ thuộc vào nhà cung cấp, nghĩa là không cần thay đổi mã để chạy trên các GPU của các nhà cung cấp khác nhau. DirectML chủ yếu được sử dụng cho các tác vụ huấn luyện và suy luận mô hình trên GPU.

Về hỗ trợ phần cứng, DirectML được thiết kế để làm việc với nhiều loại GPU, bao gồm GPU tích hợp và rời của AMD, GPU tích hợp của Intel, và GPU rời của NVIDIA. Nó là một phần của Windows AI Platform và được hỗ trợ trên Windows 10 & 11, cho phép huấn luyện và suy luận mô hình trên bất kỳ thiết bị Windows nào.

Đã có các cập nhật và cơ hội liên quan đến DirectML, chẳng hạn như hỗ trợ lên đến 150 toán tử ONNX và được sử dụng bởi cả ONNX runtime và WinML. Nó được hỗ trợ bởi các nhà cung cấp phần cứng tích hợp lớn (IHVs), mỗi nhà cung cấp triển khai các metacommand khác nhau.

## CUDA

CUDA, viết tắt của Compute Unified Device Architecture, là một nền tảng tính toán song song và mô hình giao diện lập trình ứng dụng (API) do Nvidia tạo ra. Nó cho phép các nhà phát triển phần mềm sử dụng GPU hỗ trợ CUDA để xử lý đa mục đích – một phương pháp gọi là GPGPU (General-Purpose computing on Graphics Processing Units). CUDA là yếu tố then chốt giúp tăng tốc GPU của Nvidia và được sử dụng rộng rãi trong nhiều lĩnh vực, bao gồm học máy, tính toán khoa học và xử lý video.

Hỗ trợ phần cứng cho CUDA chỉ dành riêng cho GPU của Nvidia, vì đây là công nghệ độc quyền do Nvidia phát triển. Mỗi kiến trúc GPU hỗ trợ các phiên bản cụ thể của bộ công cụ CUDA, cung cấp các thư viện và công cụ cần thiết để các nhà phát triển xây dựng và chạy ứng dụng CUDA.

## ONNX

ONNX (Open Neural Network Exchange) là một định dạng mở được thiết kế để biểu diễn các mô hình học máy. Nó cung cấp định nghĩa về mô hình đồ thị tính toán có thể mở rộng, cũng như định nghĩa các toán tử tích hợp và các kiểu dữ liệu chuẩn. ONNX cho phép các nhà phát triển chuyển đổi mô hình giữa các framework ML khác nhau, giúp tăng khả năng tương tác và làm cho việc tạo và triển khai ứng dụng AI trở nên dễ dàng hơn.

Phi3 mini có thể chạy với ONNX Runtime trên CPU và GPU trên nhiều thiết bị, bao gồm các nền tảng máy chủ, Windows, Linux và Mac desktop, cũng như CPU di động.  
Các cấu hình tối ưu mà chúng tôi đã thêm bao gồm

- Mô hình ONNX cho int4 DML: Lượng tử hóa sang int4 qua AWQ  
- Mô hình ONNX cho fp16 CUDA  
- Mô hình ONNX cho int4 CUDA: Lượng tử hóa sang int4 qua RTN  
- Mô hình ONNX cho int4 CPU và Mobile: Lượng tử hóa sang int4 qua RTN  

## Llama.cpp

Llama.cpp là một thư viện phần mềm mã nguồn mở được viết bằng C++. Nó thực hiện suy luận trên nhiều Mô hình Ngôn ngữ Lớn (LLMs), bao gồm Llama. Được phát triển cùng với thư viện ggml (một thư viện tensor đa mục đích), llama.cpp nhằm cung cấp suy luận nhanh hơn và sử dụng bộ nhớ thấp hơn so với phiên bản Python gốc. Nó hỗ trợ tối ưu phần cứng, lượng tử hóa và cung cấp API đơn giản cùng các ví dụ. Nếu bạn quan tâm đến suy luận LLM hiệu quả, llama.cpp là một lựa chọn đáng khám phá vì Phi3 có thể chạy Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) là một định dạng dùng để biểu diễn và cập nhật các mô hình học máy. Nó đặc biệt hữu ích cho các mô hình ngôn ngữ nhỏ (SLMs) có thể chạy hiệu quả trên CPU với lượng tử hóa 4-8 bit. GGUF rất phù hợp cho việc phát triển nhanh và chạy mô hình trên các thiết bị biên hoặc trong các công việc theo lô như pipeline CI/CD.

**Tuyên bố từ chối trách nhiệm**:  
Tài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ gốc của nó nên được coi là nguồn chính xác và đáng tin cậy. Đối với các thông tin quan trọng, nên sử dụng dịch vụ dịch thuật chuyên nghiệp do con người thực hiện. Chúng tôi không chịu trách nhiệm về bất kỳ sự hiểu lầm hoặc giải thích sai nào phát sinh từ việc sử dụng bản dịch này.