<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c8273672cc57df2be675407a1383aaf0",
  "translation_date": "2025-07-16T17:50:38+00:00",
  "source_file": "md/01.Introduction/01/01.AISafety.md",
  "language_code": "vi"
}
-->
# An toàn AI cho các mô hình Phi  
Dòng mô hình Phi được phát triển theo [Tiêu chuẩn AI có trách nhiệm của Microsoft](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFl), một bộ yêu cầu toàn công ty dựa trên sáu nguyên tắc sau: trách nhiệm, minh bạch, công bằng, độ tin cậy và an toàn, quyền riêng tư và bảo mật, cùng sự bao trùm, tạo thành [nguyên tắc AI có trách nhiệm của Microsoft](https://www.microsoft.com/ai/responsible-ai).

Giống như các mô hình Phi trước đây, một phương pháp đánh giá an toàn đa diện và huấn luyện an toàn sau đào tạo đã được áp dụng, với các biện pháp bổ sung để đảm bảo khả năng đa ngôn ngữ của phiên bản này. Cách tiếp cận huấn luyện và đánh giá an toàn, bao gồm kiểm tra trên nhiều ngôn ngữ và các loại rủi ro, được trình bày trong [Bài báo về An toàn sau đào tạo của Phi](https://arxiv.org/abs/2407.13833). Mặc dù các mô hình Phi được hưởng lợi từ phương pháp này, các nhà phát triển nên áp dụng các thực hành AI có trách nhiệm tốt nhất, bao gồm việc lập bản đồ, đo lường và giảm thiểu các rủi ro liên quan đến trường hợp sử dụng cụ thể cũng như bối cảnh văn hóa và ngôn ngữ.

## Thực hành tốt nhất

Giống như các mô hình khác, dòng mô hình Phi có thể có hành vi không công bằng, không đáng tin cậy hoặc gây khó chịu.

Một số hành vi hạn chế của SLM và LLM mà bạn cần lưu ý bao gồm:

- **Chất lượng dịch vụ:** Các mô hình Phi chủ yếu được huấn luyện trên văn bản tiếng Anh. Các ngôn ngữ khác ngoài tiếng Anh sẽ có hiệu suất kém hơn. Các biến thể tiếng Anh ít được đại diện trong dữ liệu huấn luyện có thể hoạt động kém hơn so với tiếng Anh Mỹ tiêu chuẩn.  
- **Biểu hiện thiệt hại & duy trì định kiến:** Các mô hình này có thể biểu hiện quá mức hoặc thiếu hụt các nhóm người, xóa bỏ sự đại diện của một số nhóm, hoặc củng cố các định kiến tiêu cực hoặc hạ thấp. Mặc dù đã có huấn luyện an toàn sau đào tạo, những hạn chế này vẫn có thể tồn tại do mức độ đại diện khác nhau của các nhóm hoặc sự phổ biến của các ví dụ về định kiến tiêu cực trong dữ liệu huấn luyện phản ánh các mô hình thực tế và thành kiến xã hội.  
- **Nội dung không phù hợp hoặc gây khó chịu:** Các mô hình này có thể tạo ra các loại nội dung không phù hợp hoặc gây khó chịu khác, khiến việc triển khai trong các bối cảnh nhạy cảm trở nên không phù hợp nếu không có các biện pháp giảm thiểu bổ sung phù hợp với trường hợp sử dụng.  
- **Độ tin cậy thông tin:** Các mô hình ngôn ngữ có thể tạo ra nội dung vô nghĩa hoặc bịa đặt nội dung nghe có vẻ hợp lý nhưng không chính xác hoặc lỗi thời.  
- **Phạm vi hạn chế đối với mã:** Phần lớn dữ liệu huấn luyện Phi-3 dựa trên Python và sử dụng các gói phổ biến như "typing, math, random, collections, datetime, itertools". Nếu mô hình tạo ra các đoạn mã Python sử dụng các gói khác hoặc mã trong các ngôn ngữ khác, chúng tôi khuyến nghị người dùng kiểm tra thủ công tất cả các API được sử dụng.

Các nhà phát triển nên áp dụng các thực hành AI có trách nhiệm và chịu trách nhiệm đảm bảo rằng trường hợp sử dụng cụ thể tuân thủ các luật và quy định liên quan (ví dụ: quyền riêng tư, thương mại, v.v.).

## Các cân nhắc về AI có trách nhiệm

Giống như các mô hình ngôn ngữ khác, các mô hình dòng Phi có thể có hành vi không công bằng, không đáng tin cậy hoặc gây khó chịu. Một số hành vi hạn chế cần lưu ý bao gồm:

**Chất lượng dịch vụ:** Các mô hình Phi chủ yếu được huấn luyện trên văn bản tiếng Anh. Các ngôn ngữ khác ngoài tiếng Anh sẽ có hiệu suất kém hơn. Các biến thể tiếng Anh ít được đại diện trong dữ liệu huấn luyện có thể hoạt động kém hơn so với tiếng Anh Mỹ tiêu chuẩn.

**Biểu hiện thiệt hại & duy trì định kiến:** Các mô hình này có thể biểu hiện quá mức hoặc thiếu hụt các nhóm người, xóa bỏ sự đại diện của một số nhóm, hoặc củng cố các định kiến tiêu cực hoặc hạ thấp. Mặc dù đã có huấn luyện an toàn sau đào tạo, những hạn chế này vẫn có thể tồn tại do mức độ đại diện khác nhau của các nhóm hoặc sự phổ biến của các ví dụ về định kiến tiêu cực trong dữ liệu huấn luyện phản ánh các mô hình thực tế và thành kiến xã hội.

**Nội dung không phù hợp hoặc gây khó chịu:** Các mô hình này có thể tạo ra các loại nội dung không phù hợp hoặc gây khó chịu khác, khiến việc triển khai trong các bối cảnh nhạy cảm trở nên không phù hợp nếu không có các biện pháp giảm thiểu bổ sung phù hợp với trường hợp sử dụng.  
Độ tin cậy thông tin: Các mô hình ngôn ngữ có thể tạo ra nội dung vô nghĩa hoặc bịa đặt nội dung nghe có vẻ hợp lý nhưng không chính xác hoặc lỗi thời.

**Phạm vi hạn chế đối với mã:** Phần lớn dữ liệu huấn luyện Phi-3 dựa trên Python và sử dụng các gói phổ biến như "typing, math, random, collections, datetime, itertools". Nếu mô hình tạo ra các đoạn mã Python sử dụng các gói khác hoặc mã trong các ngôn ngữ khác, chúng tôi khuyến nghị người dùng kiểm tra thủ công tất cả các API được sử dụng.

Các nhà phát triển nên áp dụng các thực hành AI có trách nhiệm và chịu trách nhiệm đảm bảo rằng trường hợp sử dụng cụ thể tuân thủ các luật và quy định liên quan (ví dụ: quyền riêng tư, thương mại, v.v.). Các lĩnh vực quan trọng cần xem xét bao gồm:

**Phân bổ:** Mô hình có thể không phù hợp cho các tình huống có thể ảnh hưởng quan trọng đến tình trạng pháp lý hoặc phân bổ tài nguyên, cơ hội sống (ví dụ: nhà ở, việc làm, tín dụng, v.v.) nếu không có các đánh giá thêm và kỹ thuật giảm thiểu định kiến bổ sung.

**Tình huống rủi ro cao:** Các nhà phát triển nên đánh giá tính phù hợp khi sử dụng mô hình trong các tình huống rủi ro cao, nơi các kết quả không công bằng, không đáng tin cậy hoặc gây khó chịu có thể gây tổn thất lớn hoặc dẫn đến thiệt hại. Điều này bao gồm việc cung cấp lời khuyên trong các lĩnh vực nhạy cảm hoặc chuyên môn, nơi độ chính xác và độ tin cậy rất quan trọng (ví dụ: tư vấn pháp lý hoặc y tế). Các biện pháp bảo vệ bổ sung nên được triển khai ở cấp ứng dụng tùy theo bối cảnh triển khai.

**Thông tin sai lệch:** Mô hình có thể tạo ra thông tin không chính xác. Các nhà phát triển nên tuân thủ các thực hành minh bạch và thông báo cho người dùng cuối rằng họ đang tương tác với hệ thống AI. Ở cấp ứng dụng, các nhà phát triển có thể xây dựng cơ chế phản hồi và quy trình để căn cứ câu trả lời vào thông tin cụ thể theo trường hợp sử dụng và bối cảnh, một kỹ thuật được gọi là Retrieval Augmented Generation (RAG).

**Tạo nội dung có hại:** Các nhà phát triển nên đánh giá kết quả đầu ra theo bối cảnh và sử dụng các bộ phân loại an toàn có sẵn hoặc giải pháp tùy chỉnh phù hợp với trường hợp sử dụng của họ.

**Lạm dụng:** Các hình thức lạm dụng khác như gian lận, spam hoặc tạo phần mềm độc hại có thể xảy ra, và các nhà phát triển nên đảm bảo ứng dụng của họ không vi phạm các luật và quy định hiện hành.

### Tinh chỉnh và an toàn nội dung AI

Sau khi tinh chỉnh mô hình, chúng tôi rất khuyến nghị sử dụng các biện pháp của [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) để giám sát nội dung do mô hình tạo ra, nhận diện và chặn các rủi ro, mối đe dọa và các vấn đề về chất lượng.

![Phi3AISafety](../../../../../translated_images/01.phi3aisafety.c0d7fc42f5a5c40507c5e8be556615b8377a63b8764865d057d4faac3757a478.vi.png)

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) hỗ trợ cả nội dung văn bản và hình ảnh. Nó có thể được triển khai trên đám mây, trong các container không kết nối mạng, và trên các thiết bị biên/nhúng.

## Tổng quan về Azure AI Content Safety

Azure AI Content Safety không phải là giải pháp chung cho tất cả; nó có thể được tùy chỉnh để phù hợp với chính sách cụ thể của doanh nghiệp. Ngoài ra, các mô hình đa ngôn ngữ của nó cho phép hiểu nhiều ngôn ngữ cùng lúc.

![AIContentSafety](../../../../../translated_images/01.AIcontentsafety.a288819b8ce8da1a56cf708aff010a541799d002ae7ae84bb819b19ab8950591.vi.png)

- **Azure AI Content Safety**  
- **Microsoft Developer**  
- **5 video**

Dịch vụ Azure AI Content Safety phát hiện nội dung do người dùng và AI tạo ra có hại trong các ứng dụng và dịch vụ. Nó bao gồm các API văn bản và hình ảnh cho phép bạn phát hiện các tài liệu có hại hoặc không phù hợp.

[AI Content Safety Playlist](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

**Tuyên bố từ chối trách nhiệm**:  
Tài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ gốc của nó nên được coi là nguồn chính xác và đáng tin cậy. Đối với các thông tin quan trọng, nên sử dụng dịch vụ dịch thuật chuyên nghiệp do con người thực hiện. Chúng tôi không chịu trách nhiệm về bất kỳ sự hiểu lầm hoặc giải thích sai nào phát sinh từ việc sử dụng bản dịch này.