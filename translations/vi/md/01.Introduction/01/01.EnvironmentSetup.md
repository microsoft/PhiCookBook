<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-09T07:13:20+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "vi"
}
-->
# Bắt đầu với Phi-3 trên máy tính cá nhân

Hướng dẫn này sẽ giúp bạn thiết lập môi trường cục bộ để chạy mô hình Phi-3 bằng Ollama. Bạn có thể chạy mô hình theo nhiều cách khác nhau, bao gồm sử dụng GitHub Codespaces, VS Code Dev Containers, hoặc môi trường cục bộ của bạn.

## Thiết lập môi trường

### GitHub Codespaces

Bạn có thể chạy mẫu này gần như ngay lập tức bằng cách sử dụng GitHub Codespaces. Nút bấm sẽ mở một phiên VS Code trên trình duyệt của bạn:

1. Mở mẫu (có thể mất vài phút):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Mở cửa sổ terminal

### VS Code Dev Containers

⚠️ Tùy chọn này chỉ hoạt động nếu Docker Desktop của bạn được cấp ít nhất 16 GB RAM. Nếu bạn có ít hơn 16 GB RAM, bạn có thể thử [GitHub Codespaces](../../../../../md/01.Introduction/01) hoặc [thiết lập cục bộ](../../../../../md/01.Introduction/01).

Một lựa chọn liên quan là VS Code Dev Containers, nó sẽ mở dự án trong VS Code trên máy bạn sử dụng [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Khởi động Docker Desktop (cài đặt nếu chưa có)
2. Mở dự án:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Trong cửa sổ VS Code mở ra, khi các file dự án hiện lên (có thể mất vài phút), mở cửa sổ terminal.
4. Tiếp tục với các [bước triển khai](../../../../../md/01.Introduction/01)

### Môi trường cục bộ

1. Đảm bảo các công cụ sau đã được cài đặt:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Kiểm tra mô hình

1. Yêu cầu Ollama tải về và chạy mô hình phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Việc tải mô hình sẽ mất vài phút.

2. Khi bạn thấy "success" trong kết quả, bạn có thể gửi tin nhắn đến mô hình từ dòng lệnh.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Sau vài giây, bạn sẽ thấy phản hồi được truyền từ mô hình.

4. Để tìm hiểu về các kỹ thuật khác nhau áp dụng với mô hình ngôn ngữ, mở sổ tay Python [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) và chạy từng ô lệnh. Nếu bạn sử dụng mô hình khác ngoài 'phi3:mini', hãy thay đổi `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` ở đầu file cho phù hợp, và bạn cũng có thể chỉnh sửa thông điệp hệ thống hoặc thêm ví dụ few-shot nếu muốn.

**Tuyên bố miễn trừ trách nhiệm**:  
Tài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc sự không chính xác. Tài liệu gốc bằng ngôn ngữ gốc của nó nên được coi là nguồn tham khảo chính thức. Đối với các thông tin quan trọng, nên sử dụng dịch vụ dịch thuật chuyên nghiệp bởi con người. Chúng tôi không chịu trách nhiệm về bất kỳ sự hiểu lầm hoặc giải thích sai nào phát sinh từ việc sử dụng bản dịch này.