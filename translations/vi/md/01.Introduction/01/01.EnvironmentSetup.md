<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-07-16T18:11:06+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "vi"
}
-->
# Bắt đầu với Phi-3 trên máy tính cá nhân

Hướng dẫn này sẽ giúp bạn thiết lập môi trường cục bộ để chạy mô hình Phi-3 bằng Ollama. Bạn có thể chạy mô hình theo nhiều cách khác nhau, bao gồm sử dụng GitHub Codespaces, VS Code Dev Containers hoặc môi trường cục bộ của bạn.

## Thiết lập môi trường

### GitHub Codespaces

Bạn có thể chạy mẫu này trực tuyến bằng cách sử dụng GitHub Codespaces. Nút bấm sẽ mở một phiên VS Code trên trình duyệt của bạn:

1. Mở mẫu (quá trình này có thể mất vài phút):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Mở cửa sổ terminal

### VS Code Dev Containers

⚠️ Tùy chọn này chỉ hoạt động nếu Docker Desktop của bạn được cấp ít nhất 16 GB RAM. Nếu bạn có ít hơn 16 GB RAM, bạn có thể thử [GitHub Codespaces](../../../../../md/01.Introduction/01) hoặc [thiết lập cục bộ](../../../../../md/01.Introduction/01).

Một lựa chọn liên quan là VS Code Dev Containers, sẽ mở dự án trong VS Code trên máy tính của bạn bằng cách sử dụng [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Khởi động Docker Desktop (cài đặt nếu chưa có)
2. Mở dự án:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Trong cửa sổ VS Code vừa mở, khi các file dự án xuất hiện (có thể mất vài phút), mở một cửa sổ terminal.
4. Tiếp tục với các [bước triển khai](../../../../../md/01.Introduction/01)

### Môi trường cục bộ

1. Đảm bảo các công cụ sau đã được cài đặt:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Kiểm tra mô hình

1. Yêu cầu Ollama tải xuống và chạy mô hình phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Quá trình tải mô hình sẽ mất vài phút.

2. Khi bạn thấy "success" trong kết quả, bạn có thể gửi tin nhắn đến mô hình đó từ prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Sau vài giây, bạn sẽ thấy luồng phản hồi từ mô hình.

4. Để tìm hiểu về các kỹ thuật khác nhau được sử dụng với các mô hình ngôn ngữ, mở notebook Python [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) và chạy từng ô lệnh. Nếu bạn dùng mô hình khác 'phi3:mini', hãy thay đổi `MODEL_NAME` trong ô đầu tiên.

5. Để trò chuyện với mô hình phi3:mini từ Python, mở file Python [chat.py](../../../../../code/01.Introduce/chat.py) và chạy nó. Bạn có thể thay đổi `MODEL_NAME` ở đầu file nếu cần, và cũng có thể chỉnh sửa tin nhắn hệ thống hoặc thêm ví dụ few-shot nếu muốn.

**Tuyên bố từ chối trách nhiệm**:  
Tài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ gốc của nó nên được coi là nguồn chính xác và đáng tin cậy. Đối với các thông tin quan trọng, nên sử dụng dịch vụ dịch thuật chuyên nghiệp do con người thực hiện. Chúng tôi không chịu trách nhiệm về bất kỳ sự hiểu lầm hoặc giải thích sai nào phát sinh từ việc sử dụng bản dịch này.