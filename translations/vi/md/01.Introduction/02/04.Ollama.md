<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0b38834693bb497f96bf53f0d941f9a1",
  "translation_date": "2025-05-09T09:19:41+00:00",
  "source_file": "md/01.Introduction/02/04.Ollama.md",
  "language_code": "vi"
}
-->
## Gia đình Phi trong Ollama


[Ollama](https://ollama.com) cho phép nhiều người dễ dàng triển khai trực tiếp các mô hình LLM hoặc SLM mã nguồn mở thông qua các script đơn giản, đồng thời cũng có thể xây dựng API để hỗ trợ các kịch bản ứng dụng Copilot cục bộ.

## **1. Cài đặt**

Ollama hỗ trợ chạy trên Windows, macOS và Linux. Bạn có thể cài đặt Ollama qua liên kết này ([https://ollama.com/download](https://ollama.com/download)). Sau khi cài đặt thành công, bạn có thể sử dụng script Ollama để gọi Phi-3 ngay trong cửa sổ terminal. Bạn có thể xem tất cả [thư viện có sẵn trong Ollama](https://ollama.com/library). Nếu bạn mở kho lưu trữ này trong Codespace, Ollama đã được cài đặt sẵn.

```bash

ollama run phi4

```

> [!NOTE]
> Mô hình sẽ được tải xuống lần đầu tiên khi bạn chạy. Tất nhiên, bạn cũng có thể chỉ định trực tiếp mô hình Phi-4 đã tải về. Ở đây chúng ta lấy ví dụ chạy lệnh trên WSL. Sau khi mô hình tải xong, bạn có thể tương tác trực tiếp trên terminal.

![run](../../../../../translated_images/ollama_run.b0be611de61f3bb3b42e22205cedf6714b0335ba9288e71d985bf9024f3c20f5.vi.png)

## **2. Gọi API phi-4 từ Ollama**

Nếu bạn muốn gọi API Phi-4 do Ollama tạo ra, bạn có thể dùng lệnh này trong terminal để khởi động server Ollama.

```bash

ollama serve

```

> [!NOTE]
> Nếu chạy trên MacOS hoặc Linux, bạn có thể gặp lỗi sau **"Error: listen tcp 127.0.0.1:11434: bind: address already in use"** khi gọi lệnh. Bạn có thể bỏ qua lỗi này vì thường nghĩa là server đã chạy rồi, hoặc bạn có thể dừng và khởi động lại Ollama:

**macOS**

```bash

brew services restart ollama

```

**Linux**

```bash

sudo systemctl stop ollama

```

Ollama hỗ trợ hai API: generate và chat. Bạn có thể gọi API mô hình do Ollama cung cấp tùy theo nhu cầu, bằng cách gửi yêu cầu tới dịch vụ cục bộ đang chạy trên cổng 11434.

**Chat**

```bash

curl http://127.0.0.1:11434/api/chat -d '{
  "model": "phi3",
  "messages": [
    {
      "role": "system",
      "content": "Your are a python developer."
    },
    {
      "role": "user",
      "content": "Help me generate a bubble algorithm"
    }
  ],
  "stream": false
  
}'

This is the result in Postman

![Screenshot of JSON results for generate request](../../../../../translated_images/ollama_gen.bd58ab69d4004826e8cd31e17a3c59840df127b0a30ac9bb38325ac58c74caa5.vi.png)

## Additional Resources

Check the list of available models in Ollama in [their library](https://ollama.com/library).

Pull your model from the Ollama server using this command

```bash
ollama pull phi4
```

Run the model using this command

```bash
ollama run phi4
```

***Note:*** Visit this link [https://github.com/ollama/ollama/blob/main/docs/api.md](https://github.com/ollama/ollama/blob/main/docs/api.md) to learn more

## Calling Ollama from Python

You can use `requests` or `urllib3` to make requests to the local server endpoints used above. However, a popular way to use Ollama in Python is via the [openai](https://pypi.org/project/openai/) SDK, since Ollama provides OpenAI-compatible server endpoints as well.

Here is an example for phi3-mini:

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="nokeyneeded",
)

response = client.chat.completions.create(
    model="phi4",
    temperature=0.7,
    n=1,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a haiku about a hungry cat"},
    ],
)

print("Response:")
print(response.choices[0].message.content)
```

## Calling Ollama from JavaScript 

```javascript
// Ví dụ tóm tắt một file với Phi-4
script({
    model: "ollama:phi4",
    title: "Summarize with Phi-4",
    system: ["system"],
})

// Ví dụ tóm tắt
const file = def("FILE", env.files)
$`Summarize ${file} in a single paragraph.`
```

## Calling Ollama from C#

Create a new C# Console application and add the following NuGet package:

```bash
dotnet add package Microsoft.SemanticKernel --version 1.34.0
```

Then replace this code in the `Program.cs` file

```csharp
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.ChatCompletion;

// thêm dịch vụ chat completion sử dụng endpoint server ollama cục bộ
#pragma warning disable SKEXP0001, SKEXP0003, SKEXP0010, SKEXP0011, SKEXP0050, SKEXP0052
builder.AddOpenAIChatCompletion(
    modelId: "phi4",
    endpoint: new Uri("http://localhost:11434/"),
    apiKey: "non required");

// gọi prompt đơn giản tới dịch vụ chat
string prompt = "Write a joke about kittens";
var response = await kernel.InvokePromptAsync(prompt);
Console.WriteLine(response.GetValue<string>());
```

Run the app with the command:

```bash
dotnet run

**Tuyên bố từ chối trách nhiệm**:  
Tài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn chính xác và đáng tin cậy. Đối với các thông tin quan trọng, nên sử dụng dịch vụ dịch thuật chuyên nghiệp do con người thực hiện. Chúng tôi không chịu trách nhiệm về bất kỳ sự hiểu nhầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.