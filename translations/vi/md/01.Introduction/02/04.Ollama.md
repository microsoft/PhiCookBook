<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2aa35f3c8b437fd5dc9995d53909d495",
  "translation_date": "2025-12-21T12:38:37+00:00",
  "source_file": "md/01.Introduction/02/04.Ollama.md",
  "language_code": "vi"
}
-->
## Dòng Phi trong Ollama


[Ollama](https://ollama.com) cho phép nhiều người hơn triển khai trực tiếp các LLM hoặc SLM mã nguồn mở thông qua các script đơn giản, và cũng có thể xây dựng các API để hỗ trợ các kịch bản ứng dụng Copilot cục bộ.

## **1. Cài đặt**

Ollama hỗ trợ chạy trên Windows, macOS và Linux. Bạn có thể cài đặt Ollama thông qua liên kết này ([https://ollama.com/download](https://ollama.com/download)). Sau khi cài đặt thành công, bạn có thể trực tiếp sử dụng script Ollama để gọi Phi-3 thông qua cửa sổ terminal. Bạn có thể xem tất cả [thư viện có sẵn trong Ollama](https://ollama.com/library). Nếu bạn mở kho lưu trữ này trong Codespace, nó sẽ đã được cài đặt sẵn Ollama.

```bash

ollama run phi4

```

> [!NOTE]
> Mô hình sẽ được tải xuống trước khi bạn chạy nó lần đầu tiên. Tất nhiên, bạn cũng có thể chỉ định trực tiếp mô hình Phi-4 đã tải xuống. Chúng tôi lấy WSL làm ví dụ để chạy lệnh. Sau khi mô hình được tải xuống thành công, bạn có thể tương tác trực tiếp trên terminal.

![chạy](../../../../../translated_images/vi/ollama_run.e9755172b162b381.png)

## **2. Gọi API phi-4 từ Ollama**

Nếu bạn muốn gọi API Phi-4 do ollama tạo ra, bạn có thể sử dụng lệnh này trong terminal để khởi động server Ollama.

```bash

ollama serve

```

> [!NOTE]
> Nếu chạy MacOS hoặc Linux, xin lưu ý rằng bạn có thể gặp lỗi sau **"Error: listen tcp 127.0.0.1:11434: bind: address already in use"** Bạn có thể nhận lỗi này khi chạy lệnh. Bạn có thể bỏ qua lỗi đó, vì nó thường chỉ ra server đã chạy, hoặc bạn có thể dừng và khởi động lại Ollama:

**macOS**

```bash

brew services restart ollama

```

**Linux**

```bash

sudo systemctl stop ollama

```

Ollama hỗ trợ hai API: generate và chat. Bạn có thể gọi API mô hình do Ollama cung cấp theo nhu cầu của mình, bằng cách gửi yêu cầu tới dịch vụ cục bộ đang chạy trên cổng 11434.

**Chat**

```bash

curl http://127.0.0.1:11434/api/chat -d '{
  "model": "phi3",
  "messages": [
    {
      "role": "system",
      "content": "Your are a python developer."
    },
    {
      "role": "user",
      "content": "Help me generate a bubble algorithm"
    }
  ],
  "stream": false
  
}'
```

Đây là kết quả trong Postman

![Ảnh chụp màn hình kết quả JSON cho yêu cầu generate](../../../../../translated_images/vi/ollama_gen.bda5d4e715366cc9.png)

## Tài nguyên bổ sung

Kiểm tra danh sách các mô hình có sẵn trong Ollama trong [thư viện của họ](https://ollama.com/library).

Kéo mô hình của bạn từ server Ollama bằng lệnh này

```bash
ollama pull phi4
```

Chạy mô hình bằng lệnh này

```bash
ollama run phi4
```

***Lưu ý:*** Visit this link [https://github.com/ollama/ollama/blob/main/docs/api.md](https://github.com/ollama/ollama/blob/main/docs/api.md) để tìm hiểu thêm

## Gọi Ollama từ Python

Bạn có thể sử dụng `requests` hoặc `urllib3` để gửi yêu cầu tới các endpoint server cục bộ đã nêu ở trên. Tuy nhiên, một cách phổ biến để sử dụng Ollama trong Python là qua SDK [openai](https://pypi.org/project/openai/), vì Ollama cũng cung cấp các endpoint tương thích với OpenAI.

Dưới đây là ví dụ cho phi3-mini:

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="nokeyneeded",
)

response = client.chat.completions.create(
    model="phi4",
    temperature=0.7,
    n=1,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a haiku about a hungry cat"},
    ],
)

print("Response:")
print(response.choices[0].message.content)
```

## Gọi Ollama từ JavaScript

```javascript
// Ví dụ về tóm tắt một tệp bằng Phi-4
script({
    model: "ollama:phi4",
    title: "Summarize with Phi-4",
    system: ["system"],
})

// Ví dụ về tóm tắt
const file = def("FILE", env.files)
$`Summarize ${file} in a single paragraph.`
```

## Gọi Ollama từ C#

Tạo một ứng dụng Console C# mới và thêm gói NuGet sau:

```bash
dotnet add package Microsoft.SemanticKernel --version 1.34.0
```

Sau đó thay thế đoạn mã này trong file `Program.cs`

```csharp
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.ChatCompletion;

// add chat completion service using the local ollama server endpoint
#pragma warning disable SKEXP0001, SKEXP0003, SKEXP0010, SKEXP0011, SKEXP0050, SKEXP0052
builder.AddOpenAIChatCompletion(
    modelId: "phi4",
    endpoint: new Uri("http://localhost:11434/"),
    apiKey: "non required");

// invoke a simple prompt to the chat service
string prompt = "Write a joke about kittens";
var response = await kernel.InvokePromptAsync(prompt);
Console.WriteLine(response.GetValue<string>());
```

Chạy ứng dụng với lệnh:

```bash
dotnet run
```

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
Miễn trừ trách nhiệm:
Tài liệu này đã được dịch bằng dịch vụ dịch thuật AI Co-op Translator (https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc thông tin không chính xác. Tài liệu gốc bằng ngôn ngữ ban đầu nên được coi là nguồn tham khảo chính thức. Đối với những thông tin quan trọng, nên sử dụng dịch vụ dịch thuật chuyên nghiệp do người thật thực hiện. Chúng tôi không chịu trách nhiệm về bất kỳ sự hiểu nhầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->