{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot Tương Tác Phi 3 Mini 4K Instruct với Whisper\n",
    "\n",
    "### Giới thiệu:\n",
    "Chatbot Tương Tác Phi 3 Mini 4K Instruct là một công cụ cho phép người dùng tương tác với bản demo Microsoft Phi 3 Mini 4K instruct thông qua đầu vào văn bản hoặc âm thanh. Chatbot này có thể được sử dụng cho nhiều nhiệm vụ khác nhau, chẳng hạn như dịch thuật, cập nhật thời tiết, và thu thập thông tin chung.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "Tạo mã truy cập Huggingface của bạn\n",
    "\n",
    "Tạo một mã mới  \n",
    "Đặt một tên mới  \n",
    "Chọn quyền ghi  \n",
    "Sao chép mã và lưu nó ở nơi an toàn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tệp Python này thực hiện hai nhiệm vụ chính: nhập mô-đun `os` và thiết lập một biến môi trường.\n",
    "\n",
    "1. Nhập mô-đun `os`:\n",
    "   - Mô-đun `os` trong Python cung cấp cách tương tác với hệ điều hành. Nó cho phép bạn thực hiện các tác vụ liên quan đến hệ điều hành, chẳng hạn như truy cập các biến môi trường, làm việc với tệp và thư mục, v.v.\n",
    "   - Trong đoạn mã này, mô-đun `os` được nhập bằng câu lệnh `import`. Câu lệnh này làm cho các chức năng của mô-đun `os` có thể sử dụng trong tập lệnh Python hiện tại.\n",
    "\n",
    "2. Thiết lập một biến môi trường:\n",
    "   - Biến môi trường là một giá trị có thể được truy cập bởi các chương trình chạy trên hệ điều hành. Đây là cách lưu trữ các thiết lập cấu hình hoặc thông tin khác có thể được sử dụng bởi nhiều chương trình.\n",
    "   - Trong đoạn mã này, một biến môi trường mới được thiết lập bằng cách sử dụng từ điển `os.environ`. Khóa của từ điển là `'HF_TOKEN'`, và giá trị được gán từ biến `HUGGINGFACE_TOKEN`.\n",
    "   - Biến `HUGGINGFACE_TOKEN` được định nghĩa ngay phía trên đoạn mã này, và nó được gán một giá trị chuỗi `\"hf_**************\"` bằng cú pháp `#@param`. Cú pháp này thường được sử dụng trong Jupyter notebooks để cho phép người dùng nhập liệu và cấu hình tham số trực tiếp trong giao diện notebook.\n",
    "   - Bằng cách thiết lập biến môi trường `'HF_TOKEN'`, nó có thể được truy cập bởi các phần khác của chương trình hoặc các chương trình khác chạy trên cùng hệ điều hành.\n",
    "\n",
    "Tóm lại, đoạn mã này nhập mô-đun `os` và thiết lập một biến môi trường có tên `'HF_TOKEN'` với giá trị được cung cấp trong biến `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đoạn mã này định nghĩa một hàm có tên là `clear_output`, được sử dụng để xóa nội dung hiển thị của ô hiện tại trong Jupyter Notebook hoặc IPython. Hãy cùng phân tích mã và hiểu rõ chức năng của nó:\n",
    "\n",
    "Hàm `clear_output` nhận một tham số gọi là `wait`, đây là một giá trị boolean. Theo mặc định, `wait` được đặt là `False`. Tham số này xác định liệu hàm có nên chờ cho đến khi có nội dung mới để thay thế nội dung hiện tại trước khi xóa hay không.\n",
    "\n",
    "Bản thân hàm này được sử dụng để xóa nội dung hiển thị của ô hiện tại. Trong Jupyter Notebook hoặc IPython, khi một ô tạo ra nội dung hiển thị, chẳng hạn như văn bản được in ra hoặc biểu đồ, nội dung đó sẽ được hiển thị bên dưới ô. Hàm `clear_output` cho phép bạn xóa nội dung hiển thị đó.\n",
    "\n",
    "Việc triển khai hàm không được cung cấp trong đoạn mã, như được biểu thị bằng dấu ba chấm (...). Dấu ba chấm đại diện cho một chỗ trống dành cho mã thực tế thực hiện việc xóa nội dung hiển thị. Việc triển khai hàm có thể liên quan đến việc tương tác với API của Jupyter Notebook hoặc IPython để loại bỏ nội dung hiện có từ ô.\n",
    "\n",
    "Tóm lại, hàm này cung cấp một cách tiện lợi để xóa nội dung hiển thị của ô hiện tại trong Jupyter Notebook hoặc IPython, giúp dễ dàng quản lý và cập nhật nội dung hiển thị trong các phiên làm việc mã hóa tương tác.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sử dụng dịch vụ Edge TTS để thực hiện chuyển văn bản thành giọng nói (TTS). Hãy cùng tìm hiểu từng chức năng được triển khai:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Hàm này nhận một giá trị đầu vào và tính toán chuỗi tốc độ cho giọng nói TTS. Giá trị đầu vào đại diện cho tốc độ mong muốn của giọng nói, trong đó giá trị 1 là tốc độ bình thường. Hàm tính toán chuỗi tốc độ bằng cách trừ 1 từ giá trị đầu vào, nhân với 100, và sau đó xác định dấu dựa trên việc giá trị đầu vào có lớn hơn hoặc bằng 1 hay không. Hàm trả về chuỗi tốc độ theo định dạng \"{sign}{rate}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: Hàm này nhận văn bản đầu vào và ngôn ngữ làm tham số. Nó chia văn bản đầu vào thành các đoạn nhỏ dựa trên quy tắc ngôn ngữ cụ thể. Trong triển khai này, nếu ngôn ngữ là \"English\", hàm sẽ chia văn bản tại mỗi dấu chấm (\".\") và loại bỏ khoảng trắng ở đầu hoặc cuối. Sau đó, nó thêm dấu chấm vào mỗi đoạn và trả về danh sách các đoạn đã lọc.\n",
    "\n",
    "3. `tts_file_name(text)`: Hàm này tạo tên tệp cho tệp âm thanh TTS dựa trên văn bản đầu vào. Nó thực hiện một số chuyển đổi trên văn bản: loại bỏ dấu chấm ở cuối (nếu có), chuyển văn bản thành chữ thường, loại bỏ khoảng trắng ở đầu và cuối, và thay thế khoảng trắng bằng dấu gạch dưới. Sau đó, nó cắt ngắn văn bản đến tối đa 25 ký tự (nếu dài hơn) hoặc sử dụng toàn bộ văn bản nếu nó rỗng. Cuối cùng, nó tạo một chuỗi ngẫu nhiên bằng cách sử dụng module [`uuid`] và kết hợp với văn bản đã cắt ngắn để tạo tên tệp theo định dạng \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Hàm này hợp nhất nhiều tệp âm thanh thành một tệp âm thanh duy nhất. Nó nhận danh sách các đường dẫn tệp âm thanh và đường dẫn đầu ra làm tham số. Hàm khởi tạo một đối tượng `AudioSegment` rỗng gọi là [`merged_audio`]. Sau đó, nó lặp qua từng đường dẫn tệp âm thanh, tải tệp âm thanh bằng phương pháp `AudioSegment.from_file()` từ thư viện `pydub`, và thêm tệp âm thanh hiện tại vào đối tượng [`merged_audio`]. Cuối cùng, nó xuất tệp âm thanh đã hợp nhất đến đường dẫn đầu ra được chỉ định theo định dạng MP3.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: Hàm này thực hiện thao tác TTS bằng dịch vụ Edge TTS. Nó nhận danh sách các đoạn văn bản, tốc độ giọng nói, tên giọng nói, và đường dẫn lưu làm tham số. Nếu số lượng đoạn lớn hơn 1, hàm tạo một thư mục để lưu trữ các tệp âm thanh của từng đoạn. Sau đó, nó lặp qua từng đoạn, xây dựng lệnh Edge TTS bằng cách sử dụng hàm `calculate_rate_string()`, tên giọng nói, và văn bản đoạn, rồi thực thi lệnh bằng hàm `os.system()`. Nếu lệnh thực thi thành công, nó thêm đường dẫn của tệp âm thanh được tạo vào danh sách. Sau khi xử lý tất cả các đoạn, nó hợp nhất các tệp âm thanh riêng lẻ bằng hàm `merge_audio_files()` và lưu tệp âm thanh đã hợp nhất vào đường dẫn lưu được chỉ định. Nếu chỉ có một đoạn, nó trực tiếp tạo lệnh Edge TTS và lưu âm thanh vào đường dẫn lưu. Cuối cùng, nó trả về đường dẫn lưu của tệp âm thanh được tạo.\n",
    "\n",
    "6. `random_audio_name_generate()`: Hàm này tạo tên tệp âm thanh ngẫu nhiên bằng cách sử dụng module [`uuid`]. Nó tạo một UUID ngẫu nhiên, chuyển đổi thành chuỗi, lấy 8 ký tự đầu tiên, thêm phần mở rộng \".mp3\", và trả về tên tệp âm thanh ngẫu nhiên.\n",
    "\n",
    "7. `talk(input_text)`: Hàm này là điểm khởi đầu chính để thực hiện thao tác TTS. Nó nhận văn bản đầu vào làm tham số. Đầu tiên, nó kiểm tra độ dài của văn bản đầu vào để xác định xem đó có phải là câu dài (lớn hơn hoặc bằng 600 ký tự) hay không. Dựa trên độ dài và giá trị của biến `translate_text_flag`, nó xác định ngôn ngữ và tạo danh sách các đoạn văn bản bằng hàm `make_chunks()`. Sau đó, nó tạo đường dẫn lưu cho tệp âm thanh bằng hàm `random_audio_name_generate()`. Cuối cùng, nó gọi hàm `edge_free_tts()` để thực hiện thao tác TTS và trả về đường dẫn lưu của tệp âm thanh được tạo.\n",
    "\n",
    "Tóm lại, các hàm này phối hợp với nhau để chia văn bản đầu vào thành các đoạn, tạo tên tệp cho tệp âm thanh, thực hiện thao tác TTS bằng dịch vụ Edge TTS, và hợp nhất các tệp âm thanh riêng lẻ thành một tệp âm thanh duy nhất.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Việc triển khai hai hàm: convert_to_text và run_text_prompt, cũng như khai báo hai lớp: str và Audio.\n",
    "\n",
    "Hàm convert_to_text nhận đầu vào là audio_path và chuyển đổi âm thanh thành văn bản bằng cách sử dụng một mô hình gọi là whisper_model. Hàm này đầu tiên kiểm tra xem cờ gpu có được đặt thành True hay không. Nếu đúng, whisper_model được sử dụng với các tham số như word_timestamps=True, fp16=True, language='English', và task='translate'. Nếu cờ gpu là False, whisper_model được sử dụng với fp16=False. Kết quả chuyển đổi sau đó được lưu vào một tệp có tên 'scan.txt' và trả về dưới dạng văn bản.\n",
    "\n",
    "Hàm run_text_prompt nhận đầu vào là một message và một chat_history. Nó sử dụng hàm phi_demo để tạo phản hồi từ chatbot dựa trên message đầu vào. Phản hồi được tạo ra sau đó được truyền vào hàm talk, hàm này chuyển đổi phản hồi thành một tệp âm thanh và trả về đường dẫn của tệp. Lớp Audio được sử dụng để hiển thị và phát tệp âm thanh. Âm thanh được hiển thị bằng cách sử dụng hàm display từ module IPython.display, và đối tượng Audio được tạo với tham số autoplay=True, để âm thanh tự động phát. chat_history được cập nhật với message đầu vào và phản hồi được tạo ra, sau đó trả về một chuỗi rỗng và chat_history đã được cập nhật.\n",
    "\n",
    "Lớp str là một lớp tích hợp sẵn trong Python, đại diện cho một chuỗi ký tự. Nó cung cấp nhiều phương thức để thao tác và làm việc với chuỗi, chẳng hạn như capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill, và nhiều hơn nữa. Các phương thức này cho phép bạn thực hiện các thao tác như tìm kiếm, thay thế, định dạng, và thao tác với chuỗi.\n",
    "\n",
    "Lớp Audio là một lớp tùy chỉnh đại diện cho một đối tượng âm thanh. Nó được sử dụng để tạo trình phát âm thanh trong môi trường Jupyter Notebook. Lớp này chấp nhận nhiều tham số như data, filename, url, embed, rate, autoplay, và normalize. Tham số data có thể là một mảng numpy, một danh sách mẫu, một chuỗi đại diện cho tên tệp hoặc URL, hoặc dữ liệu PCM thô. Tham số filename được sử dụng để chỉ định một tệp cục bộ để tải dữ liệu âm thanh, và tham số url được sử dụng để chỉ định một URL để tải dữ liệu âm thanh. Tham số embed xác định liệu dữ liệu âm thanh có nên được nhúng bằng URI dữ liệu hay tham chiếu từ nguồn gốc ban đầu. Tham số rate chỉ định tốc độ lấy mẫu của dữ liệu âm thanh. Tham số autoplay xác định liệu âm thanh có nên tự động phát hay không. Tham số normalize chỉ định liệu dữ liệu âm thanh có nên được chuẩn hóa (tái định tỷ lệ) đến phạm vi tối đa có thể hay không. Lớp Audio cũng cung cấp các phương thức như reload để tải lại dữ liệu âm thanh từ tệp hoặc URL, và các thuộc tính như src_attr, autoplay_attr, và element_id_attr để truy xuất các thuộc tính tương ứng cho phần tử âm thanh trong HTML.\n",
    "\n",
    "Tóm lại, các hàm và lớp này được sử dụng để chuyển đổi âm thanh thành văn bản, tạo phản hồi âm thanh từ chatbot, và hiển thị cũng như phát âm thanh trong môi trường Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Tuyên bố miễn trừ trách nhiệm**:  \nTài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn thông tin chính thức. Đối với các thông tin quan trọng, khuyến nghị sử dụng dịch vụ dịch thuật chuyên nghiệp bởi con người. Chúng tôi không chịu trách nhiệm cho bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T23:18:29+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "vi"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}