<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "462bddc47427d8785f3c9fd817b346fe",
  "translation_date": "2025-10-11T12:25:27+00:00",
  "source_file": "md/01.Introduction/04/UsingLlamacppQuantifyingPhi.md",
  "language_code": "ta"
}
-->
# **llama.cpp பயன்படுத்தி Phi குடும்பத்தை குவாண்டைஸ் செய்வது**

## **llama.cpp என்றால் என்ன?**

llama.cpp என்பது முதன்மையாக C++ மொழியில் எழுதப்பட்ட ஒரு திறந்த மூல மென்பொருள் நூலகமாகும், இது Llama போன்ற பல பெரிய மொழி மாதிரிகளின் (LLMs) மீதான முன்னறிவிப்புகளைச் செய்கிறது. இதன் முக்கிய நோக்கம் மிகக் குறைந்த அமைப்புடன் பல்வேறு ஹார்ட்வேர் சாதனங்களில் முன்னணி செயல்திறனை வழங்குவதாகும். மேலும், இந்த நூலகத்திற்கான Python பைண்டிங்ஸ்களும் கிடைக்கின்றன, இது உரை நிறைவு மற்றும் OpenAI-க்கு இணக்கமான வலை சேவையகத்திற்கான உயர் நிலை API-யை வழங்குகிறது.

llama.cpp இன் முக்கிய நோக்கம் மிகக் குறைந்த அமைப்புடன், உள்ளூர் மற்றும் மேகத்தில் பல்வேறு ஹார்ட்வேர் சாதனங்களில் முன்னணி செயல்திறனுடன் LLM முன்னறிவிப்புகளை இயக்குவதாகும்.

- எந்தவித சார்புகளும் இல்லாமல் சுத்தமான C/C++ செயல்பாடு
- Apple silicon ஒரு முக்கிய பங்கு வகிக்கிறது - ARM NEON, Accelerate மற்றும் Metal frameworks மூலம் மேம்படுத்தப்பட்டுள்ளது
- x86 கட்டமைப்புகளுக்கான AVX, AVX2 மற்றும் AVX512 ஆதரவு
- 1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit மற்றும் 8-bit முழு எண் குவாண்டைசேஷன், வேகமான முன்னறிவிப்பு மற்றும் குறைந்த நினைவக பயன்பாட்டிற்காக
- NVIDIA GPU-களில் LLM-களை இயக்க தனிப்பயன் CUDA கர்னல்கள் (AMD GPU-களுக்கான HIP ஆதரவு)
- Vulkan மற்றும் SYCL பின்புல ஆதரவு
- மொத்த VRAM திறனை விட பெரிய மாதிரிகளை பகுதியளவில் வேகப்படுத்த CPU+GPU கலப்பு முன்னறிவிப்பு

## **llama.cpp பயன்படுத்தி Phi-3.5 குவாண்டைசேஷன்**

Phi-3.5-Instruct மாதிரியை llama.cpp பயன்படுத்தி குவாண்டைஸ் செய்யலாம், ஆனால் Phi-3.5-Vision மற்றும் Phi-3.5-MoE மாதிரிகள் இன்னும் ஆதரிக்கப்படவில்லை. llama.cpp மூலம் மாற்றப்படும் வடிவம் gguf ஆகும், இது மிகவும் பரவலாக பயன்படுத்தப்படும் குவாண்டைசேஷன் வடிவமாகும்.

Hugging Face இல் பெருமளவில் குவாண்டைஸ் செய்யப்பட்ட GGUF வடிவ மாதிரிகள் உள்ளன. AI Foundry, Ollama மற்றும் LlamaEdge ஆகியவை llama.cpp-ஐ நம்புகின்றன, எனவே GGUF மாதிரிகளும் பெரும்பாலும் பயன்படுத்தப்படுகின்றன.

### **GGUF என்றால் என்ன?**

GGUF என்பது மாதிரிகளை விரைவாக ஏற்றவும் சேமிக்கவும் சிறப்பாக வடிவமைக்கப்பட்ட ஒரு பைனரி வடிவமாகும், இது முன்னறிவிப்பு நோக்கங்களுக்கு மிகவும் திறமையானதாகும். GGUF, GGML மற்றும் பிற செயல்படுத்திகளுடன் பயன்படுத்த வடிவமைக்கப்பட்டுள்ளது. GGUF-ஐ llama.cpp உருவாக்கிய @ggerganov என்பவரால் உருவாக்கப்பட்டது, இது ஒரு பிரபலமான C/C++ LLM முன்னறிவிப்பு கட்டமைப்பாகும். PyTorch போன்ற கட்டமைப்புகளில் முதலில் உருவாக்கப்பட்ட மாதிரிகளை GGUF வடிவத்திற்கு மாற்றி அந்த இயந்திரங்களுடன் பயன்படுத்தலாம்.

### **ONNX vs GGUF**

ONNX என்பது பாரம்பரிய இயந்திரக் கற்றல்/ஆழமான கற்றல் வடிவமாகும், இது பல்வேறு AI கட்டமைப்புகளில் நன்றாக ஆதரிக்கப்படுகிறது மற்றும் எட்ஜ் சாதனங்களில் சிறந்த பயன்பாட்டு சூழல்களை வழங்குகிறது. GGUF llama.cpp அடிப்படையாகக் கொண்டது மற்றும் GenAI காலத்தில் உருவாக்கப்பட்டது என்று கூறலாம். இவை இரண்டுக்கும் ஒரே மாதிரியான பயன்பாடுகள் உள்ளன. எம்பெடெட் ஹார்ட்வேர் மற்றும் பயன்பாட்டு அடுக்குகளில் சிறந்த செயல்திறனை விரும்பினால், ONNX உங்கள் தேர்வாக இருக்கலாம். llama.cpp மற்றும் அதன் சார்ந்த கட்டமைப்புகளைப் பயன்படுத்தினால், GGUF சிறந்ததாக இருக்கலாம்.

### **llama.cpp பயன்படுத்தி Phi-3.5-Instruct குவாண்டைசேஷன்**

**1. சூழல் அமைப்பு**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. குவாண்டைசேஷன்**

llama.cpp பயன்படுத்தி Phi-3.5-Instruct ஐ FP16 GGUF ஆக மாற்றுதல்


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Phi-3.5 ஐ INT4 ஆக குவாண்டைஸ் செய்தல்


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. சோதனை**

llama-cpp-python ஐ நிறுவுதல்


```bash

pip install llama-cpp-python -U

```

***குறிப்பு*** 

Apple Silicon பயன்படுத்தினால், llama-cpp-python ஐ இவ்வாறு நிறுவவும்


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

சோதனை 


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **மூலங்கள்**

1. llama.cpp பற்றி மேலும் அறிக [https://github.com/ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)
2. onnxruntime பற்றி மேலும் அறிக [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)
3. GGUF பற்றி மேலும் அறிக [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

---

**அறிவிப்பு**:  
இந்த ஆவணம் [Co-op Translator](https://github.com/Azure/co-op-translator) என்ற AI மொழிபெயர்ப்பு சேவையை பயன்படுத்தி மொழிபெயர்க்கப்பட்டுள்ளது. நாங்கள் துல்லியத்திற்காக முயற்சிக்கிறோம், ஆனால் தானியங்கி மொழிபெயர்ப்புகளில் பிழைகள் அல்லது தவறுகள் இருக்கக்கூடும் என்பதை கவனத்தில் கொள்ளவும். அதன் சொந்த மொழியில் உள்ள மூல ஆவணம் அதிகாரப்பூர்வ ஆதாரமாக கருதப்பட வேண்டும். முக்கியமான தகவல்களுக்கு, தொழில்முறை மனித மொழிபெயர்ப்பு பரிந்துரைக்கப்படுகிறது. இந்த மொழிபெயர்ப்பைப் பயன்படுத்துவதால் ஏற்படும் எந்த தவறான புரிதல்களுக்கும் அல்லது தவறான விளக்கங்களுக்கும் நாங்கள் பொறுப்பல்ல.