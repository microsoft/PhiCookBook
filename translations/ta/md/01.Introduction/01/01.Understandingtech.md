<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-10-11T12:15:18+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "ta"
}
-->
# முக்கிய தொழில்நுட்பங்கள்

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12-ஐ அடிப்படையாகக் கொண்டு உருவாக்கப்பட்ட, ஹார்ட்வேரை வேகமாக செயல்படுத்தும் மெஷின் லெர்னிங் API.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia உருவாக்கிய ஒரு பார்‌லல் கம்ப்யூட்டிங் தளம் மற்றும் API மாடல், GPU-களில் பொதுவான செயல்பாடுகளை செயல்படுத்த உதவுகிறது.
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - மெஷின் லெர்னிங் மாடல்களை பிரதிநிதித்துவப்படுத்த உருவாக்கப்பட்ட ஒரு திறந்த வடிவமைப்பு, பல்வேறு ML கட்டமைப்புகளுக்கு இடையே இணக்கத்தன்மையை வழங்குகிறது.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - மெஷின் லெர்னிங் மாடல்களை பிரதிநிதித்துவப்படுத்தவும் புதுப்பிக்கவும் பயன்படுத்தப்படும் வடிவமைப்பு, குறிப்பாக CPUs-ல் 4-8bit குவாண்டைசேஷனுடன் சிறப்பாக இயங்கும் சிறிய மொழி மாடல்களுக்கு பயனுள்ளதாக உள்ளது.

## DirectML

DirectML என்பது ஹார்ட்வேரை வேகமாக செயல்படுத்தும் மெஷின் லெர்னிங் API ஆகும். இது DirectX 12-ஐ அடிப்படையாகக் கொண்டு GPU வேகத்தை பயன்படுத்துகிறது மற்றும் விற்பனையாளர் சார்ந்ததாக இல்லை, அதாவது பல GPU விற்பனையாளர்களுக்கு இடையே எந்த குறியீடு மாற்றங்களும் தேவையில்லை. இது முக்கியமாக GPU-களில் மாடல் பயிற்சி மற்றும் தீர்மான செயல்பாடுகளுக்கு பயன்படுத்தப்படுகிறது.

ஹார்ட்வேரின் ஆதரவு குறித்து, DirectML பல GPU-களுடன் வேலை செய்ய வடிவமைக்கப்பட்டுள்ளது, அதில் AMD ஒருங்கிணைந்த மற்றும் தனித்துவ GPU-கள், Intel ஒருங்கிணைந்த GPU-கள் மற்றும் NVIDIA தனித்துவ GPU-கள் அடங்கும். இது Windows AI தளத்தின் ஒரு பகுதியாகும் மற்றும் Windows 10 & 11-ல் ஆதரிக்கப்படுகிறது, இதனால் எந்த Windows சாதனத்திலும் மாடல் பயிற்சி மற்றும் தீர்மானங்களை இயக்க முடியும்.

DirectML-இல் 150 ONNX ஆபரேட்டர்களுக்கு ஆதரவு மற்றும் ONNX runtime மற்றும் WinML ஆகியவற்றால் பயன்படுத்தப்படுவது போன்ற புதுப்பிப்புகள் மற்றும் வாய்ப்புகள் உள்ளன. இது முக்கிய ஒருங்கிணைந்த ஹார்ட்வேர விற்பனையாளர்களால் ஆதரிக்கப்படுகிறது, ஒவ்வொன்றும் பல்வேறு மெட்டாகமாண்டுகளை செயல்படுத்துகிறது.

## CUDA

CUDA என்பது Compute Unified Device Architecture-ஐ குறிக்கிறது, இது Nvidia உருவாக்கிய ஒரு பார்‌லல் கம்ப்யூட்டிங் தளம் மற்றும் API மாடல் ஆகும். இது மென்பொருள் டெவலப்பர்களுக்கு CUDA-ஆதரவு கொண்ட GPU-ஐ பொதுவான செயல்பாடுகளுக்கு பயன்படுத்த அனுமதிக்கிறது – இது GPGPU (Graphics Processing Units-ல் பொதுவான கம்ப்யூட்டிங்) என அழைக்கப்படுகிறது. CUDA Nvidia-வின் GPU வேகத்தை இயக்குவதற்கான முக்கிய காரணியாகும் மற்றும் மெஷின் லெர்னிங், அறிவியல் கணக்கீடு மற்றும் வீடியோ செயலாக்கம் போன்ற பல துறைகளில் பரவலாக பயன்படுத்தப்படுகிறது.

CUDA-க்கு ஹார்ட்வேர ஆதரவு Nvidia-வின் GPU-களுக்கு மட்டுமே குறிப்பிட்டது, ஏனெனில் இது Nvidia உருவாக்கிய சொந்த தொழில்நுட்பமாகும். ஒவ்வொரு கட்டமைப்பும் CUDA டூல்கிட் பதிப்புகளை ஆதரிக்கிறது, இது டெவலப்பர்களுக்கு CUDA பயன்பாடுகளை உருவாக்கவும் இயக்கவும் தேவையான நூலகங்கள் மற்றும் கருவிகளை வழங்குகிறது.

## ONNX

ONNX (Open Neural Network Exchange) என்பது மெஷின் லெர்னிங் மாடல்களை பிரதிநிதித்துவப்படுத்த உருவாக்கப்பட்ட ஒரு திறந்த வடிவமைப்பு ஆகும். இது விரிவாக்கக்கூடிய கணக்கீட்டு கிராஃப் மாடலின் வரையறையை வழங்குகிறது, மேலும் உள்ளமைக்கப்பட்ட ஆபரேட்டர்கள் மற்றும் நிலையான தரவுத் வகைகளின் வரையறைகளை வழங்குகிறது. ONNX டெவலப்பர்களுக்கு மாடல்களை பல்வேறு ML கட்டமைப்புகளுக்கு இடையே நகர்த்த அனுமதிக்கிறது, இதனால் இணக்கத்தன்மை கிடைக்கிறது மற்றும் AI பயன்பாடுகளை உருவாக்கவும் வெளியிடவும் எளிதாகிறது.

Phi3 mini ONNX Runtime-ஐ CPU மற்றும் GPU-களில் பல சாதனங்களில் இயக்க முடியும், அதில் சர்வர் தளங்கள், Windows, Linux மற்றும் Mac டெஸ்க்டாப்கள் மற்றும் மொபைல் CPU-கள் அடங்கும்.
நாங்கள் சேர்த்துள்ள மேம்படுத்தப்பட்ட கட்டமைப்புகள்:

- int4 DML-க்கு ONNX மாடல்கள்: AWQ மூலம் int4-க்கு குவாண்டைசேஷன் செய்யப்பட்டது
- fp16 CUDA-க்கு ONNX மாடல்
- int4 CUDA-க்கு ONNX மாடல்: RTN மூலம் int4-க்கு குவாண்டைசேஷன் செய்யப்பட்டது
- int4 CPU மற்றும் மொபைலுக்கு ONNX மாடல்: RTN மூலம் int4-க்கு குவாண்டைசேஷன் செய்யப்பட்டது

## Llama.cpp

Llama.cpp என்பது C++-ல் எழுதப்பட்ட ஒரு திறந்த மூல மென்பொருள் நூலகமாகும். இது Llama உட்பட பல பெரிய மொழி மாடல்களில் தீர்மானங்களை செயல்படுத்துகிறது. ggml நூலகத்துடன் (ஒரு பொதுவான டென்சர் நூலகம்) இணைந்து உருவாக்கப்பட்டது, llama.cpp முதன்மை Python செயல்பாட்டுடன் ஒப்பிடும்போது வேகமான தீர்மானம் மற்றும் குறைந்த நினைவக பயன்பாட்டை வழங்குகிறது. இது ஹார்ட்வேர மேம்பாடு, குவாண்டைசேஷன் ஆகியவற்றை ஆதரிக்கிறது மற்றும் ஒரு எளிய API மற்றும் எடுத்துக்காட்டுகளை வழங்குகிறது. திறமையான LLM தீர்மானத்தில் ஆர்வமுள்ளவர்களுக்கு llama.cpp ஆராய்வதற்கு மதிப்புள்ளது, ஏனெனில் Phi3 Llama.cpp-ஐ இயக்க முடியும்.

## GGUF

GGUF (Generic Graph Update Format) என்பது மெஷின் லெர்னிங் மாடல்களை பிரதிநிதித்துவப்படுத்தவும் புதுப்பிக்கவும் பயன்படுத்தப்படும் ஒரு வடிவமைப்பு ஆகும். இது குறிப்பாக CPUs-ல் 4-8bit குவாண்டைசேஷனுடன் சிறப்பாக இயங்கும் சிறிய மொழி மாடல்களுக்கு பயனுள்ளதாக உள்ளது. GGUF விரைவான முன்னோட்டம் மற்றும் எட்ஜ் சாதனங்களில் அல்லது CI/CD பைப்‌லைன்கள் போன்ற தொகுதி வேலைகளில் மாடல்களை இயக்குவதற்கு பயனுள்ளதாக உள்ளது.

---

**குறிப்பு**:  
இந்த ஆவணம் [Co-op Translator](https://github.com/Azure/co-op-translator) என்ற AI மொழிபெயர்ப்பு சேவையைப் பயன்படுத்தி மொழிபெயர்க்கப்பட்டுள்ளது. நாங்கள் துல்லியத்திற்காக முயற்சிக்கின்றோம், ஆனால் தானியக்க மொழிபெயர்ப்புகளில் பிழைகள் அல்லது தவறான தகவல்கள் இருக்கக்கூடும் என்பதை தயவுசெய்து கவனத்தில் கொள்ளுங்கள். அதன் தாய்மொழியில் உள்ள மூல ஆவணம் அதிகாரப்பூர்வ ஆதாரமாக கருதப்பட வேண்டும். முக்கியமான தகவல்களுக்கு, தொழில்முறை மனித மொழிபெயர்ப்பு பரிந்துரைக்கப்படுகிறது. இந்த மொழிபெயர்ப்பைப் பயன்படுத்துவதால் ஏற்படும் எந்த தவறான புரிதல்கள் அல்லது தவறான விளக்கங்களுக்கு நாங்கள் பொறுப்பல்ல.