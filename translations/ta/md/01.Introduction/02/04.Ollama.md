<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0b38834693bb497f96bf53f0d941f9a1",
  "translation_date": "2025-10-11T12:28:54+00:00",
  "source_file": "md/01.Introduction/02/04.Ollama.md",
  "language_code": "ta"
}
-->
## Ollama-இல் Phi குடும்பம்

[Ollama](https://ollama.com) எளிய ஸ்கிரிப்ட்களைக் கொண்டு திறந்த மூல LLM அல்லது SLM-ஐ நேரடியாக பயன்படுத்த பலருக்கும் அனுமதிக்கிறது, மேலும் உள்ளூர் Copilot பயன்பாட்டு சூழல்களுக்கு API-களை உருவாக்க உதவுகிறது.

## **1. நிறுவல்**

Ollama Windows, macOS, மற்றும் Linux-இல் இயங்குவதற்கு ஆதரவு அளிக்கிறது. நீங்கள் Ollama-ஐ இந்த இணைப்பின் மூலம் நிறுவலாம் ([https://ollama.com/download](https://ollama.com/download)). நிறுவல் வெற்றிகரமாக முடிந்தவுடன், நீங்கள் Ollama ஸ்கிரிப்டைப் பயன்படுத்தி டெர்மினல் விண்டோவின் மூலம் Phi-3-ஐ அழைக்கலாம். Ollama-இல் உள்ள அனைத்து [கிடைக்கக்கூடிய நூலகங்களை](https://ollama.com/library) பார்க்கலாம். Codespace-இல் இந்த repository-ஐ திறந்தால், Ollama ஏற்கனவே நிறுவப்பட்டிருக்கும்.

```bash

ollama run phi4

```

> [!NOTE]
> நீங்கள் முதன்முதலில் இயக்கும்போது மாடல் முதலில் பதிவிறக்கம் செய்யப்படும். நீங்கள் நேரடியாக Phi-4 மாடலைப் பதிவிறக்கம் செய்து குறிப்பிடவும் முடியும். WSL-ஐ எடுத்துக்காட்டாகக் கொண்டு கட்டளையை இயக்குகிறோம். மாடல் வெற்றிகரமாக பதிவிறக்கம் செய்யப்பட்ட பிறகு, நீங்கள் டெர்மினலில் நேரடியாக தொடர்பு கொள்ளலாம்.

![run](../../../../../imgs/01/02/04/ollama_run.png)

## **2. Ollama-இல் இருந்து phi-4 API-ஐ அழைக்கவும்**

Ollama மூலம் உருவாக்கப்பட்ட Phi-4 API-ஐ அழைக்க விரும்பினால், Ollama சர்வரை தொடங்க டெர்மினலில் இந்த கட்டளையைப் பயன்படுத்தலாம்.

```bash

ollama serve

```

> [!NOTE]
> MacOS அல்லது Linux-இல் இயக்கும்போது, **"Error: listen tcp 127.0.0.1:11434: bind: address already in use"** என்ற பிழையை நீங்கள் சந்திக்கலாம். இந்த கட்டளையை இயக்கும்போது இந்த பிழை ஏற்படலாம். இந்த பிழையை நீங்கள் புறக்கணிக்கலாம், ஏனெனில் இது பொதுவாக சர்வர் ஏற்கனவே இயங்குகிறது என்பதை குறிக்கிறது, அல்லது Ollama-ஐ நிறுத்தி மீண்டும் தொடங்கலாம்:

**macOS**

```bash

brew services restart ollama

```

**Linux**

```bash

sudo systemctl stop ollama

```

Ollama இரண்டு API-க்களை ஆதரிக்கிறது: generate மற்றும் chat. Ollama வழங்கும் மாடல் API-ஐ உங்கள் தேவைகளுக்கு ஏற்ப அழைக்க, 11434 போர்டில் இயங்கும் உள்ளூர் சேவைக்கு கோரிக்கைகளை அனுப்பலாம்.

**Chat**

```bash

curl http://127.0.0.1:11434/api/chat -d '{
  "model": "phi3",
  "messages": [
    {
      "role": "system",
      "content": "Your are a python developer."
    },
    {
      "role": "user",
      "content": "Help me generate a bubble algorithm"
    }
  ],
  "stream": false
  
}'

This is the result in Postman

![Screenshot of JSON results for generate request](../../../../../imgs/01/02/04/ollama_gen.png)

## Additional Resources

Check the list of available models in Ollama in [their library](https://ollama.com/library).

Pull your model from the Ollama server using this command

```bash
ollama pull phi4
```

இந்த கட்டளையைப் பயன்படுத்தி மாடலை இயக்கவும்

```bash
ollama run phi4
```

***குறிப்பு:*** மேலும் அறிய இந்த இணைப்பை பார்வையிடவும் [https://github.com/ollama/ollama/blob/main/docs/api.md](https://github.com/ollama/ollama/blob/main/docs/api.md)

## Python-இல் இருந்து Ollama-ஐ அழைக்கவும்

மேலே பயன்படுத்திய உள்ளூர் சர்வர் எண்ட்பாயிண்ட்களுக்கு கோரிக்கைகளை செய்ய `requests` அல்லது `urllib3`-ஐ நீங்கள் பயன்படுத்தலாம். ஆனால், Python-இல் Ollama-ஐ பயன்படுத்த ஒரு பிரபலமான வழி [openai](https://pypi.org/project/openai/) SDK மூலம், ஏனெனில் Ollama OpenAI-க்கு இணக்கமான சர்வர் எண்ட்பாயிண்ட்களை வழங்குகிறது.

இது phi3-mini-க்கு ஒரு எடுத்துக்காட்டு:

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="nokeyneeded",
)

response = client.chat.completions.create(
    model="phi4",
    temperature=0.7,
    n=1,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a haiku about a hungry cat"},
    ],
)

print("Response:")
print(response.choices[0].message.content)
```

## JavaScript-இல் இருந்து Ollama-ஐ அழைக்கவும் 

```javascript
// Example of Summarize a file with Phi-4
script({
    model: "ollama:phi4",
    title: "Summarize with Phi-4",
    system: ["system"],
})

// Example of summarize
const file = def("FILE", env.files)
$`Summarize ${file} in a single paragraph.`
```

## C#-இல் இருந்து Ollama-ஐ அழைக்கவும்

ஒரு புதிய C# Console பயன்பாட்டை உருவாக்கி, கீழே உள்ள NuGet தொகுப்பைச் சேர்க்கவும்:

```bash
dotnet add package Microsoft.SemanticKernel --version 1.34.0
```

பிறகு `Program.cs` கோப்பில் இந்த குறியீட்டை மாற்றவும்

```csharp
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.ChatCompletion;

// add chat completion service using the local ollama server endpoint
#pragma warning disable SKEXP0001, SKEXP0003, SKEXP0010, SKEXP0011, SKEXP0050, SKEXP0052
builder.AddOpenAIChatCompletion(
    modelId: "phi4",
    endpoint: new Uri("http://localhost:11434/"),
    apiKey: "non required");

// invoke a simple prompt to the chat service
string prompt = "Write a joke about kittens";
var response = await kernel.InvokePromptAsync(prompt);
Console.WriteLine(response.GetValue<string>());
```

இந்த கட்டளையைப் பயன்படுத்தி பயன்பாட்டை இயக்கவும்:

```bash
dotnet run
```

---

**குறிப்பு**:  
இந்த ஆவணம் [Co-op Translator](https://github.com/Azure/co-op-translator) என்ற AI மொழிபெயர்ப்பு சேவையைப் பயன்படுத்தி மொழிபெயர்க்கப்பட்டுள்ளது. நாங்கள் துல்லியத்திற்காக முயற்சிக்கிறோம், ஆனால் தானியக்க மொழிபெயர்ப்புகளில் பிழைகள் அல்லது தவறான தகவல்கள் இருக்கக்கூடும் என்பதை கவனத்தில் கொள்ளவும். அதன் தாய்மொழியில் உள்ள மூல ஆவணம் அதிகாரப்பூர்வ ஆதாரமாக கருதப்பட வேண்டும். முக்கியமான தகவல்களுக்கு, தொழில்முறை மனித மொழிபெயர்ப்பு பரிந்துரைக்கப்படுகிறது. இந்த மொழிபெயர்ப்பைப் பயன்படுத்துவதால் ஏற்படும் எந்த தவறான புரிதல்கள் அல்லது தவறான விளக்கங்களுக்கு நாங்கள் பொறுப்பல்ல.