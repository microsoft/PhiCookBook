<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3139a6a82f357a9f90f1fe51c4caf65a",
  "translation_date": "2025-07-16T22:04:09+00:00",
  "source_file": "md/01.Introduction/04/UsingIntelOpenVINOQuantifyingPhi.md",
  "language_code": "ro"
}
-->
# **Cuantificarea Phi-3.5 folosind Intel OpenVINO**

Intel este cel mai tradiÈ›ional producÄƒtor de procesoare CPU, cu mulÈ›i utilizatori. OdatÄƒ cu creÈ™terea Ã®nvÄƒÈ›Äƒrii automate È™i a Ã®nvÄƒÈ›Äƒrii profunde, Intel a intrat È™i el Ã®n competiÈ›ia pentru accelerarea AI. Pentru inferenÈ›a modelelor, Intel foloseÈ™te nu doar GPU-uri È™i CPU-uri, ci È™i NPU-uri.

Ne dorim sÄƒ implementÄƒm familia Phi-3.x pe dispozitivele finale, sperÃ¢nd sÄƒ devinÄƒ cea mai importantÄƒ componentÄƒ a PC-urilor AI È™i Copilot PC. ÃncÄƒrcarea modelului pe dispozitivul final depinde de colaborarea dintre diferiÈ›i producÄƒtori de hardware. Acest capitol se concentreazÄƒ Ã®n principal pe scenariul de aplicare al Intel OpenVINO ca model cuantificat.

## **Ce este OpenVINO**

OpenVINO este un set de instrumente open-source pentru optimizarea È™i implementarea modelelor de Ã®nvÄƒÈ›are profundÄƒ de la cloud la edge. AccelereazÄƒ inferenÈ›a deep learning Ã®n diverse cazuri de utilizare, cum ar fi AI generativ, video, audio È™i limbaj, cu modele din framework-uri populare precum PyTorch, TensorFlow, ONNX È™i altele. ConverteÈ™te È™i optimizeazÄƒ modelele È™i le implementeazÄƒ pe o combinaÈ›ie de hardware È™i medii IntelÂ®, fie on-premises, pe dispozitiv, Ã®n browser sau Ã®n cloud.

Acum, cu OpenVINO, poÈ›i cuantifica rapid modelul GenAI pe hardware Intel È™i accelera referinÈ›a modelului.

Ãn prezent, OpenVINO suportÄƒ conversia prin cuantificare pentru Phi-3.5-Vision È™i Phi-3.5 Instruct.

### **Configurarea mediului**

AsigurÄƒ-te cÄƒ urmÄƒtoarele dependenÈ›e de mediu sunt instalate, acestea sunt Ã®n requirement.txt

```txt

--extra-index-url https://download.pytorch.org/whl/cpu
optimum-intel>=1.18.2
nncf>=2.11.0
openvino>=2024.3.0
transformers>=4.40
openvino-genai>=2024.3.0.0

```

### **Cuantificarea Phi-3.5-Instruct folosind OpenVINO**

Ãn Terminal, te rugÄƒm sÄƒ rulezi acest script

```bash


export llm_model_id = "microsoft/Phi-3.5-mini-instruct"

export llm_model_path = "your save quantizing Phi-3.5-instruct location"

optimum-cli export openvino --model {llm_model_id} --task text-generation-with-past --weight-format int4 --group-size 128 --ratio 0.6  --sym  --trust-remote-code {llm_model_path}


```

### **Cuantificarea Phi-3.5-Vision folosind OpenVINO**

Te rugÄƒm sÄƒ rulezi acest script Ã®n Python sau Jupyter lab

```python

import requests
from pathlib import Path
from ov_phi3_vision import convert_phi3_model
import nncf

if not Path("ov_phi3_vision.py").exists():
    r = requests.get(url="https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/phi-3-vision/ov_phi3_vision.py")
    open("ov_phi3_vision.py", "w").write(r.text)


if not Path("gradio_helper.py").exists():
    r = requests.get(url="https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/phi-3-vision/gradio_helper.py")
    open("gradio_helper.py", "w").write(r.text)

if not Path("notebook_utils.py").exists():
    r = requests.get(url="https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py")
    open("notebook_utils.py", "w").write(r.text)



model_id = "microsoft/Phi-3.5-vision-instruct"
out_dir = Path("../model/phi-3.5-vision-128k-instruct-ov")
compression_configuration = {
    "mode": nncf.CompressWeightsMode.INT4_SYM,
    "group_size": 64,
    "ratio": 0.6,
}
if not out_dir.exists():
    convert_phi3_model(model_id, out_dir, compression_configuration)

```

### **ğŸ¤– Exemple pentru Phi-3.5 cu Intel OpenVINO**

| Laboratoare    | Descriere | Acces |
| -------- | ------- |  ------- |
| ğŸš€ Lab-Introduce Phi-3.5 Instruct  | ÃnvaÈ›Äƒ cum sÄƒ foloseÈ™ti Phi-3.5 Instruct pe PC-ul tÄƒu AI    |  [Go](../../../../../code/09.UpdateSamples/Aug/intel-phi35-instruct-zh.ipynb)    |
| ğŸš€ Lab-Introduce Phi-3.5 Vision (imagine) | ÃnvaÈ›Äƒ cum sÄƒ foloseÈ™ti Phi-3.5 Vision pentru analiza imaginilor pe PC-ul tÄƒu AI      |  [Go](../../../../../code/09.UpdateSamples/Aug/intel-phi35-vision-img.ipynb)    |
| ğŸš€ Lab-Introduce Phi-3.5 Vision (video)   | ÃnvaÈ›Äƒ cum sÄƒ foloseÈ™ti Phi-3.5 Vision pentru analiza video pe PC-ul tÄƒu AI    |  [Go](../../../../../code/09.UpdateSamples/Aug/intel-phi35-vision-video.ipynb)    |

## **Resurse**

1. AflÄƒ mai multe despre Intel OpenVINO [https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html)

2. Intel OpenVINO GitHub Repo [https://github.com/openvinotoolkit/openvino.genai](https://github.com/openvinotoolkit/openvino.genai)

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). DeÈ™i ne strÄƒduim pentru acurateÈ›e, vÄƒ rugÄƒm sÄƒ reÈ›ineÈ›i cÄƒ traducerile automate pot conÈ›ine erori sau inexactitÄƒÈ›i. Documentul original Ã®n limba sa nativÄƒ trebuie considerat sursa autorizatÄƒ. Pentru informaÈ›ii critice, se recomandÄƒ traducerea profesionalÄƒ realizatÄƒ de un specialist uman. Nu ne asumÄƒm rÄƒspunderea pentru eventualele neÃ®nÈ›elegeri sau interpretÄƒri greÈ™ite rezultate din utilizarea acestei traduceri.