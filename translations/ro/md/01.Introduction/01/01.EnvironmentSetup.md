<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-09T07:18:19+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "ro"
}
-->
# Începeți cu Phi-3 local

Acest ghid vă va ajuta să configurați mediul local pentru a rula modelul Phi-3 folosind Ollama. Puteți rula modelul în mai multe moduri, inclusiv folosind GitHub Codespaces, VS Code Dev Containers sau mediul local.

## Configurarea mediului

### GitHub Codespaces

Puteți rula acest șablon virtual folosind GitHub Codespaces. Butonul va deschide o instanță VS Code bazată pe web în browserul dvs.:

1. Deschideți șablonul (aceasta poate dura câteva minute):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Deschideți o fereastră de terminal

### VS Code Dev Containers

⚠️ Această opțiune va funcționa doar dacă Docker Desktop are alocați cel puțin 16 GB de RAM. Dacă aveți mai puțin de 16 GB RAM, puteți încerca opțiunea [GitHub Codespaces](../../../../../md/01.Introduction/01) sau [configurați-l local](../../../../../md/01.Introduction/01).

O opțiune similară este VS Code Dev Containers, care va deschide proiectul în VS Code local folosind extensia [Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Porniți Docker Desktop (instalați-l dacă nu este deja instalat)
2. Deschideți proiectul:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. În fereastra VS Code care se deschide, după ce apar fișierele proiectului (aceasta poate dura câteva minute), deschideți o fereastră de terminal.
4. Continuați cu [pașii de implementare](../../../../../md/01.Introduction/01)

### Mediu local

1. Asigurați-vă că următoarele unelte sunt instalate:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Testați modelul

1. Cereți lui Ollama să descarce și să ruleze modelul phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Aceasta va dura câteva minute pentru descărcarea modelului.

2. După ce vedeți „success” în output, puteți trimite un mesaj către model din prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. După câteva secunde, ar trebui să vedeți un flux de răspunsuri din partea modelului.

4. Pentru a învăța despre diferite tehnici folosite cu modelele de limbaj, deschideți notebook-ul Python [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) și rulați fiecare celulă. Dacă ați folosit un model diferit de 'phi3:mini', schimbați `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` din partea de sus a fișierului după cum este necesar, și puteți modifica și mesajul sistem sau adăuga exemple few-shot dacă doriți.

**Declinare a responsabilității**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). Deși ne străduim pentru acuratețe, vă rugăm să rețineți că traducerile automate pot conține erori sau inexactități. Documentul original, în limba sa nativă, trebuie considerat sursa autoritară. Pentru informații critice, se recomandă traducerea profesională realizată de un specialist uman. Nu ne asumăm răspunderea pentru eventualele neînțelegeri sau interpretări greșite care pot apărea din utilizarea acestei traduceri.