<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:31:53+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "ro"
}
-->
# Tehnologii cheie menționate includ

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - un API la nivel scăzut pentru învățare automată accelerată hardware, construit peste DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - o platformă de calcul paralel și un model API dezvoltat de Nvidia, care permite procesarea generală pe unități de procesare grafică (GPU-uri).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - un format deschis creat pentru a reprezenta modelele de învățare automată, oferind interoperabilitate între diferite cadre ML.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - un format folosit pentru reprezentarea și actualizarea modelelor de învățare automată, util mai ales pentru modelele de limbaj mai mici care pot rula eficient pe CPU-uri cu cuantizare pe 4-8 biți.

## DirectML

DirectML este un API la nivel scăzut care permite învățarea automată accelerată hardware. Este construit peste DirectX 12 pentru a folosi accelerarea GPU și este independent de furnizor, ceea ce înseamnă că nu necesită modificări de cod pentru a funcționa pe GPU-uri de la diferiți producători. Este folosit în principal pentru antrenarea modelelor și sarcini de inferență pe GPU-uri.

În ceea ce privește suportul hardware, DirectML este proiectat să funcționeze cu o gamă largă de GPU-uri, inclusiv GPU-uri integrate și discrete AMD, GPU-uri integrate Intel și GPU-uri discrete NVIDIA. Face parte din Windows AI Platform și este suportat pe Windows 10 și 11, permițând antrenarea și inferența modelelor pe orice dispozitiv Windows.

Au existat actualizări și oportunități legate de DirectML, cum ar fi suportul pentru până la 150 de operatori ONNX și utilizarea atât de ONNX runtime, cât și de WinML. Este susținut de principalii Integrated Hardware Vendors (IHVs), fiecare implementând diverse metacomenzi.

## CUDA

CUDA, care înseamnă Compute Unified Device Architecture, este o platformă de calcul paralel și un model API creat de Nvidia. Permite dezvoltatorilor software să utilizeze un GPU compatibil CUDA pentru procesare generală – o abordare denumită GPGPU (General-Purpose computing on Graphics Processing Units). CUDA este un factor cheie pentru accelerarea GPU-urilor Nvidia și este folosit pe scară largă în diverse domenii, inclusiv în învățarea automată, calculul științific și procesarea video.

Suportul hardware pentru CUDA este specific GPU-urilor Nvidia, fiind o tehnologie proprietară dezvoltată de Nvidia. Fiecare arhitectură suportă versiuni specifice ale toolkit-ului CUDA, care oferă bibliotecile și uneltele necesare pentru dezvoltatori să construiască și să ruleze aplicații CUDA.

## ONNX

ONNX (Open Neural Network Exchange) este un format deschis creat pentru a reprezenta modelele de învățare automată. Oferă o definiție a unui model extensibil de graf de calcul, precum și definiții pentru operatori încorporați și tipuri standard de date. ONNX permite dezvoltatorilor să transfere modele între diferite cadre ML, facilitând interoperabilitatea și simplificând crearea și implementarea aplicațiilor AI.

Phi3 mini poate rula cu ONNX Runtime pe CPU și GPU pe diverse dispozitive, inclusiv platforme server, Windows, Linux și Mac desktop, precum și CPU-uri mobile.  
Configurațiile optimizate pe care le-am adăugat sunt:

- Modele ONNX pentru int4 DML: cuantizate la int4 prin AWQ  
- Model ONNX pentru fp16 CUDA  
- Model ONNX pentru int4 CUDA: cuantizat la int4 prin RTN  
- Model ONNX pentru int4 CPU și Mobile: cuantizat la int4 prin RTN

## Llama.cpp

Llama.cpp este o bibliotecă software open-source scrisă în C++. Realizează inferență pe diverse modele mari de limbaj (LLM), inclusiv Llama. Dezvoltată împreună cu biblioteca ggml (o bibliotecă generală pentru tensori), llama.cpp urmărește să ofere inferență mai rapidă și un consum mai redus de memorie comparativ cu implementarea originală în Python. Suportă optimizarea hardware, cuantizarea și oferă un API simplu și exemple3. Dacă ești interesat de inferența eficientă a LLM-urilor, llama.cpp merită explorat, deoarece Phi3 poate rula Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) este un format folosit pentru reprezentarea și actualizarea modelelor de învățare automată. Este deosebit de util pentru modelele de limbaj mai mici (SLM) care pot rula eficient pe CPU-uri cu cuantizare pe 4-8 biți. GGUF este benefic pentru prototipare rapidă și rularea modelelor pe dispozitive edge sau în joburi batch precum pipeline-uri CI/CD.

**Declinare a responsabilității**:  
Acest document a fost tradus folosind serviciul de traducere automată AI [Co-op Translator](https://github.com/Azure/co-op-translator). Deși ne străduim pentru acuratețe, vă rugăm să rețineți că traducerile automate pot conține erori sau inexactități. Documentul original, în limba sa nativă, trebuie considerat sursa autorizată. Pentru informații critice, se recomandă traducerea profesională realizată de un specialist uman. Nu ne asumăm răspunderea pentru eventualele neînțelegeri sau interpretări greșite care pot rezulta din utilizarea acestei traduceri.