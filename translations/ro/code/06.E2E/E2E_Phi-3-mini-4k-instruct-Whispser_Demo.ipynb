{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot interactiv Phi 3 Mini 4K Instruct cu Whisper\n",
    "\n",
    "### Introducere:\n",
    "Chatbotul interactiv Phi 3 Mini 4K Instruct este un instrument care permite utilizatorilor să interacționeze cu demonstrația Microsoft Phi 3 Mini 4K Instruct folosind intrări text sau audio. Chatbotul poate fi utilizat pentru o varietate de sarcini, cum ar fi traduceri, actualizări meteo și colectarea de informații generale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[Creează-ți Token-ul de Acces Huggingface](https://huggingface.co/settings/tokens)\n",
    "\n",
    "Creează un token nou  \n",
    "Furnizează un nume nou  \n",
    "Selectează permisiunile de scriere  \n",
    "Copiază token-ul și salvează-l într-un loc sigur  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Următorul cod Python îndeplinește două sarcini principale: importarea modulului `os` și setarea unei variabile de mediu.\n",
    "\n",
    "1. Importarea modulului `os`:\n",
    "   - Modulul `os` din Python oferă o modalitate de a interacționa cu sistemul de operare. Permite efectuarea diverselor sarcini legate de sistemul de operare, cum ar fi accesarea variabilelor de mediu, lucrul cu fișiere și directoare etc.\n",
    "   - În acest cod, modulul `os` este importat folosind instrucțiunea `import`. Această instrucțiune face ca funcționalitatea modulului `os` să fie disponibilă pentru utilizare în scriptul Python curent.\n",
    "\n",
    "2. Setarea unei variabile de mediu:\n",
    "   - O variabilă de mediu este o valoare care poate fi accesată de programele care rulează pe sistemul de operare. Este o modalitate de a stoca setări de configurare sau alte informații care pot fi utilizate de mai multe programe.\n",
    "   - În acest cod, o nouă variabilă de mediu este setată folosind dicționarul `os.environ`. Cheia dicționarului este `'HF_TOKEN'`, iar valoarea este atribuită din variabila `HUGGINGFACE_TOKEN`.\n",
    "   - Variabila `HUGGINGFACE_TOKEN` este definită chiar deasupra acestui fragment de cod și i se atribuie o valoare de tip șir de caractere `\"hf_**************\"` folosind sintaxa `#@param`. Această sintaxă este adesea utilizată în Jupyter notebooks pentru a permite introducerea de date de către utilizator și configurarea parametrilor direct în interfața notebook-ului.\n",
    "   - Prin setarea variabilei de mediu `'HF_TOKEN'`, aceasta poate fi accesată de alte părți ale programului sau de alte programe care rulează pe același sistem de operare.\n",
    "\n",
    "În concluzie, acest cod importă modulul `os` și setează o variabilă de mediu numită `'HF_TOKEN'` cu valoarea furnizată în variabila `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acest fragment de cod definește o funcție numită clear_output, care este utilizată pentru a șterge rezultatul afișat în celula curentă din Jupyter Notebook sau IPython. Haideți să analizăm codul și să înțelegem funcționalitatea sa:\n",
    "\n",
    "Funcția clear_output primește un parametru numit wait, care este o valoare booleană. În mod implicit, wait este setat la False. Acest parametru determină dacă funcția ar trebui să aștepte până când este disponibil un nou rezultat pentru a înlocui rezultatul existent înainte de a-l șterge.\n",
    "\n",
    "Funcția în sine este utilizată pentru a șterge rezultatul afișat în celula curentă. În Jupyter Notebook sau IPython, atunci când o celulă produce un rezultat, cum ar fi text imprimat sau grafice, acel rezultat este afișat sub celulă. Funcția clear_output permite ștergerea acelui rezultat.\n",
    "\n",
    "Implementarea funcției nu este furnizată în fragmentul de cod, așa cum este indicat de punctele de suspensie (...). Punctele de suspensie reprezintă un substitut pentru codul real care realizează ștergerea rezultatului. Implementarea funcției poate implica interacțiunea cu API-ul Jupyter Notebook sau IPython pentru a elimina rezultatul existent din celulă.\n",
    "\n",
    "În concluzie, această funcție oferă o modalitate convenabilă de a șterge rezultatul afișat în celula curentă din Jupyter Notebook sau IPython, facilitând gestionarea și actualizarea rezultatelor afișate în timpul sesiunilor interactive de programare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efectuați conversia text-în-vorbire (TTS) utilizând serviciul Edge TTS. Să analizăm implementările funcțiilor relevante una câte una:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Această funcție primește o valoare de intrare și calculează șirul de rată pentru vocea TTS. Valoarea de intrare reprezintă viteza dorită a vorbirii, unde o valoare de 1 reprezintă viteza normală. Funcția calculează șirul de rată scăzând 1 din valoarea de intrare, înmulțind rezultatul cu 100 și apoi determinând semnul în funcție de faptul dacă valoarea de intrare este mai mare sau egală cu 1. Funcția returnează șirul de rată în formatul \"{semn}{rată}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: Această funcție primește un text de intrare și o limbă ca parametri. Ea împarte textul de intrare în fragmente pe baza regulilor specifice limbii. În această implementare, dacă limba este \"English\", funcția împarte textul la fiecare punct (\".\") și elimină orice spațiu alb de la început sau sfârșit. Apoi adaugă un punct la fiecare fragment și returnează lista filtrată de fragmente.\n",
    "\n",
    "3. `tts_file_name(text)`: Această funcție generează un nume de fișier pentru fișierul audio TTS pe baza textului de intrare. Ea efectuează mai multe transformări asupra textului: elimină un punct final (dacă este prezent), convertește textul în litere mici, elimină spațiile albe de la început și sfârșit și înlocuiește spațiile cu liniuțe de subliniere. Apoi, textul este trunchiat la maximum 25 de caractere (dacă este mai lung) sau se folosește textul complet dacă este gol. În final, generează un șir aleatoriu utilizând modulul [`uuid`] și îl combină cu textul trunchiat pentru a crea numele fișierului în formatul \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Această funcție îmbină mai multe fișiere audio într-un singur fișier audio. Primește o listă de căi ale fișierelor audio și o cale de ieșire ca parametri. Funcția inițializează un obiect gol `AudioSegment` numit [`merged_audio`]. Apoi, parcurge fiecare cale a fișierului audio, încarcă fișierul audio utilizând metoda `AudioSegment.from_file()` din biblioteca `pydub` și adaugă fișierul audio curent la obiectul [`merged_audio`]. În final, exportă audio-ul îmbinat la calea de ieșire specificată în formatul MP3.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: Această funcție efectuează operațiunea TTS utilizând serviciul Edge TTS. Primește o listă de fragmente text, viteza vorbirii, numele vocii și calea de salvare ca parametri. Dacă numărul de fragmente este mai mare de 1, funcția creează un director pentru stocarea fișierelor audio individuale ale fragmentelor. Apoi, parcurge fiecare fragment, construiește o comandă Edge TTS utilizând funcția `calculate_rate_string()`, numele vocii și textul fragmentului, și execută comanda utilizând funcția `os.system()`. Dacă execuția comenzii are succes, adaugă calea fișierului audio generat într-o listă. După procesarea tuturor fragmentelor, îmbină fișierele audio individuale utilizând funcția `merge_audio_files()` și salvează audio-ul îmbinat la calea de salvare specificată. Dacă există un singur fragment, generează direct comanda Edge TTS și salvează audio-ul la calea de salvare. În final, returnează calea de salvare a fișierului audio generat.\n",
    "\n",
    "6. `random_audio_name_generate()`: Această funcție generează un nume aleatoriu pentru fișierul audio utilizând modulul [`uuid`]. Generează un UUID aleatoriu, îl convertește într-un șir, ia primele 8 caractere, adaugă extensia \".mp3\" și returnează numele aleatoriu al fișierului audio.\n",
    "\n",
    "7. `talk(input_text)`: Această funcție este punctul principal de intrare pentru efectuarea operațiunii TTS. Primește un text de intrare ca parametru. Mai întâi verifică lungimea textului de intrare pentru a determina dacă este o propoziție lungă (mai mare sau egală cu 600 de caractere). Pe baza lungimii și a valorii variabilei `translate_text_flag`, determină limba și generează lista de fragmente text utilizând funcția `make_chunks()`. Apoi generează o cale de salvare pentru fișierul audio utilizând funcția `random_audio_name_generate()`. În final, apelează funcția `edge_free_tts()` pentru a efectua operațiunea TTS și returnează calea de salvare a fișierului audio generat.\n",
    "\n",
    "În ansamblu, aceste funcții lucrează împreună pentru a împărți textul de intrare în fragmente, a genera un nume de fișier pentru fișierul audio, a efectua operațiunea TTS utilizând serviciul Edge TTS și a îmbina fișierele audio individuale într-un singur fișier audio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementarea a două funcții: convert_to_text și run_text_prompt, precum și declarația a două clase: str și Audio.\n",
    "\n",
    "Funcția convert_to_text primește ca input un audio_path și transcrie audio-ul în text folosind un model numit whisper_model. Funcția verifică mai întâi dacă flag-ul gpu este setat pe True. Dacă este, whisper_model este utilizat cu anumiți parametri, cum ar fi word_timestamps=True, fp16=True, language='English' și task='translate'. Dacă flag-ul gpu este False, whisper_model este utilizat cu fp16=False. Transcrierea rezultată este apoi salvată într-un fișier numit 'scan.txt' și returnată ca text.\n",
    "\n",
    "Funcția run_text_prompt primește un mesaj și un chat_history ca input. Aceasta folosește funcția phi_demo pentru a genera un răspuns de la un chatbot pe baza mesajului de intrare. Răspunsul generat este apoi transmis funcției talk, care convertește răspunsul într-un fișier audio și returnează calea fișierului. Clasa Audio este utilizată pentru a afișa și reda fișierul audio. Audio-ul este afișat folosind funcția display din modulul IPython.display, iar obiectul Audio este creat cu parametrul autoplay=True, astfel încât audio-ul începe să se redea automat. chat_history este actualizat cu mesajul de intrare și răspunsul generat, iar un șir gol și chat_history actualizat sunt returnate.\n",
    "\n",
    "Clasa str este o clasă încorporată în Python care reprezintă o secvență de caractere. Aceasta oferă diverse metode pentru manipularea și lucrul cu șiruri de caractere, cum ar fi capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill și altele. Aceste metode permit efectuarea de operații precum căutarea, înlocuirea, formatarea și manipularea șirurilor de caractere.\n",
    "\n",
    "Clasa Audio este o clasă personalizată care reprezintă un obiect audio. Este utilizată pentru a crea un player audio în mediul Jupyter Notebook. Clasa acceptă diferiți parametri, cum ar fi data, filename, url, embed, rate, autoplay și normalize. Parametrul data poate fi un array numpy, o listă de mostre, un șir care reprezintă un nume de fișier sau URL, sau date PCM brute. Parametrul filename este utilizat pentru a specifica un fișier local din care să se încarce datele audio, iar parametrul url este utilizat pentru a specifica un URL de unde să se descarce datele audio. Parametrul embed determină dacă datele audio ar trebui să fie încorporate folosind un URI de date sau referite din sursa originală. Parametrul rate specifică rata de eșantionare a datelor audio. Parametrul autoplay determină dacă audio-ul ar trebui să înceapă să se redea automat. Parametrul normalize specifică dacă datele audio ar trebui să fie normalizate (rescalate) la intervalul maxim posibil. Clasa Audio oferă, de asemenea, metode precum reload pentru a reîncărca datele audio din fișier sau URL, și atribute precum src_attr, autoplay_attr și element_id_attr pentru a obține atributele corespunzătoare pentru elementul audio în HTML.\n",
    "\n",
    "În general, aceste funcții și clase sunt utilizate pentru a transcrie audio în text, a genera răspunsuri audio de la un chatbot și a afișa și reda audio în mediul Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Declinare de responsabilitate**:  \nAcest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). Deși ne străduim să asigurăm acuratețea, vă rugăm să fiți conștienți că traducerile automate pot conține erori sau inexactități. Documentul original în limba sa natală ar trebui considerat sursa autoritară. Pentru informații critice, se recomandă traducerea profesională realizată de un specialist uman. Nu ne asumăm responsabilitatea pentru eventualele neînțelegeri sau interpretări greșite care pot apărea din utilizarea acestei traduceri.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-13T07:02:12+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "ro"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}