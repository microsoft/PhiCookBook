# इंटरैक्टिव फि 3 मिनी 4K इंस्ट्रक्ट चैटबोट विथ व्हिस्पर

## अवलोकन

इंटरैक्टिव फि 3 मिनी 4K इंस्ट्रक्ट चैटबोट एक उपकरण है जो उपयोगकर्ताओं को माइक्रोसॉफ्ट फि 3 मिनी 4K इंस्ट्रक्ट डेमो के साथ टेक्स्ट या ऑडियो इनपुट के माध्यम से बातचीत करने की अनुमति देता है। चैटबोट का उपयोग विभिन्न कार्यों के लिए किया जा सकता है, जैसे अनुवाद, मौसम अपडेट और सामान्य जानकारी प्राप्त करना।

### शुरूआत

इस चैटबोट का उपयोग करने के लिए, बस इन निर्देशों का पालन करें:

1. एक नया [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) खोलें
2. नोटबुक की मुख्य विंडो में, आपको टेक्स्ट इनपुट बॉक्स और एक "Send" बटन के साथ एक चैटबॉक्स इंटरफेस दिखाई देगा।
3. टेक्स्ट-आधारित चैटबोट का उपयोग करने के लिए, बस अपना संदेश टेक्स्ट इनपुट बॉक्स में टाइप करें और "Send" बटन पर क्लिक करें। चैटबोट एक ऑडियो फ़ाइल के साथ जवाब देगा जिसे सीधे नोटबुक के भीतर ही प्ले किया जा सकता है।

**नोट**: इस टूल के लिए GPU और माइक्रोसॉफ्ट फि-3 और OpenAI Whisper मॉडल्स तक पहुंच की आवश्यकता होती है, जो भाषण मान्यता और अनुवाद के लिए उपयोग किए जाते हैं।

### GPU आवश्यकताएँ

इस डेमो को चलाने के लिए आपको 12GB GPU मेमोरी चाहिए।

GPU पर **Microsoft-Phi-3-Mini-4K instruct** डेमो चलाने के लिए मेमोरी की आवश्यकताएँ कई कारकों पर निर्भर करेंगी, जैसे इनपुट डेटा का आकार (ऑडियो या टेक्स्ट), अनुवाद के लिए उपयोग की गई भाषा, मॉडल की गति, और GPU पर उपलब्ध मेमोरी।

आम तौर पर, Whisper मॉडल GPUs पर चलाने के लिए डिज़ाइन किया गया है। Whisper मॉडल चलाने के लिए सुझाया गया न्यूनतम GPU मेमोरी 8 GB है, लेकिन आवश्यकता पर यह बड़ी मेमोरी भी संभाल सकता है।

यह महत्वपूर्ण है कि बड़े डेटा या उच्च मात्रा में अनुरोधों को मॉडल पर चलाने से अधिक GPU मेमोरी की जरूरत हो सकती है और/या प्रदर्शन संबंधी समस्याएँ आ सकती हैं। अपनी विशिष्ट आवश्यकताओं के लिए विभिन्न कॉन्फ़िगरेशन के साथ परीक्षण करना और मेमोरी उपयोग की निगरानी करना सलाह दी जाती है।

## इंटरैक्टिव फि 3 मिनी 4K इंस्ट्रक्ट चैटबोट विथ व्हिस्पर के लिए E2E नमूना

[इंटरैक्टिव फि 3 मिनी 4K इंस्ट्रक्ट चैटबोट विथ व्हिस्पर](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) नामक जूपिटर नोटबुक दिखाता है कि माइक्रोसॉफ्ट फि 3 मिनी 4K इंस्ट्रक्ट डेमो का उपयोग ऑडियो या लिखित टेक्स्ट इनपुट से टेक्स्ट जनरेट करने के लिए कैसे किया जाता है। यह नोटबुक कई फ़ंक्शंस परिभाषित करता है:

1. `tts_file_name(text)`: यह फ़ंक्शन इनपुट टेक्स्ट के आधार पर उत्पन्न ऑडियो फ़ाइल को सहेजने के लिए एक फ़ाइल नाम बनाता है।
1. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: यह फ़ंक्शन इनपुट टेक्स्ट के टुकड़ों की सूची से ऑडियो फ़ाइल जनरेट करने के लिए Edge TTS API का उपयोग करता है। इनपुट पैरामीटर टुकड़ों की सूची, भाषण गति, वॉइस नाम, और उत्पन्न ऑडियो फ़ाइल सहेजने का पथ हैं।
1. `talk(input_text)`: यह फ़ंक्शन Edge TTS API का उपयोग करके एक ऑडियो फ़ाइल उत्पन्न करता है और इसे /content/audio निर्देशिका में एक रैंडम फ़ाइल नाम के साथ सहेजता है। इनपुट पैरामीटर वह टेक्स्ट है जिसे बोलने में बदला जाएगा।
1. `run_text_prompt(message, chat_history)`: यह फ़ंक्शन Microsoft Phi 3 Mini 4K इंस्ट्रक्ट डेमो का उपयोग करके एक संदेश इनपुट से ऑडियो फ़ाइल उत्पन्न करता है और चैट इतिहास में जोड़ता है।
1. `run_audio_prompt(audio, chat_history)`: यह फ़ंक्शन Whisper मॉडल API का उपयोग करके एक ऑडियो फ़ाइल को टेक्स्ट में बदलता है और उसे `run_text_prompt()` फ़ंक्शन को पास करता है।
1. कोड एक Gradio ऐप शुरू करता है जो उपयोगकर्ताओं को Phi 3 Mini 4K इंस्ट्रक्ट डेमो के साथ संदेश टाइप करके या ऑडियो फ़ाइलें अपलोड करके बातचीत करने की अनुमति देता है। आउटपुट ऐप के भीतर एक टेक्स्ट संदेश के रूप में प्रदर्शित होता है।

## समस्या निवारण

Cuda GPU ड्राइवर इंस्टॉल करना

1. सुनिश्चित करें कि आपका लिनक्स एप्लीकेशन अपडेटेड है

    ```bash
    sudo apt update
    ```

1. Cuda ड्राइवर इंस्टॉल करें

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. cuda ड्राइवर स्थान पंजीकृत करें

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. Nvidia GPU मेमोरी साइज़ जांचें (12GB GPU मेमोरी आवश्यक)

    ```bash
    nvidia-smi
    ```

1. कैश खाली करें: यदि आप PyTorch का उपयोग कर रहे हैं, तो torch.cuda.empty_cache() कॉल करके सभी अप्रयुक्त कैश मेमोरी को रिलीज़ कर सकते हैं ताकि वह अन्य GPU एप्लिकेशन द्वारा उपयोग हो सके

    ```python
    torch.cuda.empty_cache() 
    ```

1. Nvidia Cuda जांचें

    ```bash
    nvcc --version
    ```

1. Hugging Face टोकन बनाने के लिए निम्न कार्य करें।

    - [Hugging Face Token Settings पेज](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo) पर जाएं।
    - **New token** चुनें।
    - उपयोग करना चाहते प्रोजेक्ट **Name** दर्ज करें।
    - **Type** को **Write** चुनें।

> [!NOTE]
>
> यदि आपको निम्न त्रुटि मिलती है:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> इसे हल करने के लिए, अपने टर्मिनल के अंदर निम्न कमांड टाइप करें।
>
> ```bash
> sudo ldconfig
> ```

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**अस्वीकरण**:
इस दस्तावेज़ का अनुवाद AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान रखें कि स्वचालित अनुवाद में त्रुटियाँ या अशुद्धियाँ हो सकती हैं। मूल दस्तावेज़ अपनी मूल भाषा में प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सलाह दी जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।
<!-- CO-OP TRANSLATOR DISCLAIMER END -->