<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f737bf207e1691cdc654535c48dd2df4",
  "translation_date": "2025-04-04T18:20:51+00:00",
  "source_file": "md\\02.Application\\01.TextAndChat\\Phi3\\E2E_Phi-3-mini_with_whisper.md",
  "language_code": "hi"
}
-->
# इंटरएक्टिव Phi 3 Mini 4K इंस्ट्रक्ट चैटबॉट विथ Whisper

## परिचय

इंटरएक्टिव Phi 3 Mini 4K इंस्ट्रक्ट चैटबॉट एक उपकरण है जो उपयोगकर्ताओं को Microsoft Phi 3 Mini 4K इंस्ट्रक्ट डेमो के साथ टेक्स्ट या ऑडियो इनपुट का उपयोग करके बातचीत करने की अनुमति देता है। इस चैटबॉट का उपयोग विभिन्न कार्यों के लिए किया जा सकता है, जैसे अनुवाद, मौसम अपडेट, और सामान्य जानकारी प्राप्त करना।

### शुरुआत कैसे करें

इस चैटबॉट का उपयोग करने के लिए, नीचे दिए गए निर्देशों का पालन करें:

1. एक नया [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) खोलें।
2. नोटबुक की मुख्य विंडो में, आपको एक चैटबॉक्स इंटरफेस दिखाई देगा जिसमें एक टेक्स्ट इनपुट बॉक्स और "Send" बटन होगा।
3. टेक्स्ट-आधारित चैटबॉट का उपयोग करने के लिए, अपने संदेश को टेक्स्ट इनपुट बॉक्स में टाइप करें और "Send" बटन पर क्लिक करें। चैटबॉट एक ऑडियो फ़ाइल के साथ जवाब देगा जिसे नोटबुक के भीतर ही चलाया जा सकता है।

**नोट**: इस उपकरण को GPU और Microsoft Phi-3 और OpenAI Whisper मॉडल तक पहुंच की आवश्यकता होती है, जो स्पीच रिकग्निशन और अनुवाद के लिए उपयोग किया जाता है।

### GPU आवश्यकताएँ

इस डेमो को चलाने के लिए आपको 12GB GPU मेमोरी की आवश्यकता होगी।

**Microsoft-Phi-3-Mini-4K इंस्ट्रक्ट** डेमो को GPU पर चलाने के लिए मेमोरी आवश्यकताएँ कई कारकों पर निर्भर करेंगी, जैसे इनपुट डेटा का आकार (ऑडियो या टेक्स्ट), अनुवाद के लिए उपयोग की जाने वाली भाषा, मॉडल की गति, और GPU पर उपलब्ध मेमोरी।

सामान्य रूप से, Whisper मॉडल GPU पर चलाने के लिए डिज़ाइन किया गया है। Whisper मॉडल को चलाने के लिए अनुशंसित न्यूनतम GPU मेमोरी 8 GB है, लेकिन यह आवश्यक होने पर अधिक मेमोरी संभाल सकता है।

यह ध्यान रखना महत्वपूर्ण है कि मॉडल पर बड़ी मात्रा में डेटा या उच्च मात्रा में अनुरोध चलाने के लिए अधिक GPU मेमोरी की आवश्यकता हो सकती है और/या प्रदर्शन संबंधी समस्याएँ उत्पन्न हो सकती हैं। आपके विशेष उपयोग के लिए आदर्श सेटिंग्स निर्धारित करने के लिए विभिन्न कॉन्फ़िगरेशन के साथ परीक्षण करना और मेमोरी उपयोग की निगरानी करना अनुशंसित है।

## E2E सैंपल फॉर इंटरएक्टिव Phi 3 Mini 4K इंस्ट्रक्ट चैटबॉट विथ Whisper

जुपिटर नोटबुक जिसका शीर्षक [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) है, यह दिखाता है कि Microsoft Phi 3 Mini 4K इंस्ट्रक्ट डेमो का उपयोग ऑडियो या लिखित टेक्स्ट इनपुट से टेक्स्ट उत्पन्न करने के लिए कैसे किया जाता है। नोटबुक कई फंक्शन्स को परिभाषित करता है:

1. `tts_file_name(text)`: यह फ़ंक्शन इनपुट टेक्स्ट के आधार पर एक फ़ाइल नाम उत्पन्न करता है ताकि उत्पन्न ऑडियो फ़ाइल को सेव किया जा सके।
1. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: यह फ़ंक्शन Edge TTS API का उपयोग करके इनपुट टेक्स्ट के हिस्सों की सूची से ऑडियो फ़ाइल उत्पन्न करता है। इनपुट पैरामीटर्स में सूची के हिस्से, स्पीच रेट, वॉयस नाम, और आउटपुट पथ शामिल हैं।
1. `talk(input_text)`: यह फ़ंक्शन Edge TTS API का उपयोग करके ऑडियो फ़ाइल उत्पन्न करता है और इसे /content/audio डायरेक्टरी में एक रैंडम फ़ाइल नाम पर सेव करता है। इनपुट पैरामीटर टेक्स्ट है जिसे स्पीच में बदलना है।
1. `run_text_prompt(message, chat_history)`: यह फ़ंक्शन Microsoft Phi 3 Mini 4K इंस्ट्रक्ट डेमो का उपयोग करके संदेश इनपुट से ऑडियो फ़ाइल उत्पन्न करता है और इसे चैट इतिहास में जोड़ता है।
1. `run_audio_prompt(audio, chat_history)`: यह फ़ंक्शन Whisper मॉडल API का उपयोग करके ऑडियो फ़ाइल को टेक्स्ट में बदलता है और इसे `run_text_prompt()` फ़ंक्शन को पास करता है।
1. कोड Gradio ऐप लॉन्च करता है जो उपयोगकर्ताओं को Phi 3 Mini 4K इंस्ट्रक्ट डेमो के साथ बातचीत करने की अनुमति देता है, चाहे वे संदेश टाइप करें या ऑडियो फ़ाइल अपलोड करें। आउटपुट ऐप में टेक्स्ट संदेश के रूप में प्रदर्शित होता है।

## समस्या निवारण

Cuda GPU ड्राइवर्स इंस्टॉल करना

1. सुनिश्चित करें कि आपके Linux एप्लिकेशन अपडेटेड हैं

    ```bash
    sudo apt update
    ```

1. Cuda ड्राइवर्स इंस्टॉल करें

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. Cuda ड्राइवर लोकेशन को रजिस्टर करें

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. Nvidia GPU मेमोरी साइज चेक करें (12GB GPU मेमोरी आवश्यक)

    ```bash
    nvidia-smi
    ```

1. कैश खाली करें: यदि आप PyTorch का उपयोग कर रहे हैं, तो torch.cuda.empty_cache() कॉल करें ताकि सभी अप्रयुक्त कैश मेमोरी रिलीज़ हो जाए और अन्य GPU एप्लिकेशन द्वारा उपयोग की जा सके।

    ```python
    torch.cuda.empty_cache() 
    ```

1. Nvidia Cuda चेक करें

    ```bash
    nvcc --version
    ```

1. Hugging Face टोकन बनाने के लिए निम्नलिखित कार्य करें:

    - [Hugging Face Token Settings page](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo) पर जाएँ।
    - **New token** चुनें।
    - उस प्रोजेक्ट का **Name** दर्ज करें जिसे आप उपयोग करना चाहते हैं।
    - **Type** को **Write** पर सेट करें।

> **नोट**
>
> यदि आपको निम्नलिखित त्रुटि का सामना करना पड़ता है:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> इसे हल करने के लिए, अपने टर्मिनल में निम्नलिखित कमांड टाइप करें:
>
> ```bash
> sudo ldconfig
> ```

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता सुनिश्चित करने का प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या गलतियां हो सकती हैं। मूल दस्तावेज़, जो उसकी मूल भाषा में है, को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।