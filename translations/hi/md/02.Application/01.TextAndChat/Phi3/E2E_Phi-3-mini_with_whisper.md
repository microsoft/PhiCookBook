<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "006e8cf75211d3297f24e1b22e38955f",
  "translation_date": "2025-05-08T05:44:34+00:00",
  "source_file": "md/02.Application/01.TextAndChat/Phi3/E2E_Phi-3-mini_with_whisper.md",
  "language_code": "hi"
}
-->
# Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper

## अवलोकन

Interactive Phi 3 Mini 4K Instruct Chatbot एक ऐसा टूल है जो उपयोगकर्ताओं को Microsoft Phi 3 Mini 4K instruct डेमो के साथ टेक्स्ट या ऑडियो इनपुट के माध्यम से बातचीत करने की सुविधा देता है। इस चैटबोट का उपयोग अनुवाद, मौसम अपडेट, और सामान्य जानकारी इकट्ठा करने जैसे कई कार्यों के लिए किया जा सकता है।

### शुरूआत कैसे करें

इस चैटबोट का उपयोग करने के लिए बस निम्नलिखित निर्देशों का पालन करें:

1. एक नया [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) खोलें
2. नोटबुक की मुख्य विंडो में, आपको एक चैटबॉक्स इंटरफेस दिखाई देगा जिसमें टेक्स्ट इनपुट बॉक्स और "Send" बटन होगा।
3. टेक्स्ट-आधारित चैटबोट का उपयोग करने के लिए, बस अपना संदेश टेक्स्ट इनपुट बॉक्स में टाइप करें और "Send" बटन पर क्लिक करें। चैटबोट एक ऑडियो फाइल के साथ जवाब देगा जिसे नोटबुक के भीतर सीधे चलाया जा सकता है।

**Note**: इस टूल के लिए GPU और Microsoft Phi-3 तथा OpenAI Whisper मॉडल्स की पहुँच आवश्यक है, जो स्पीच रिकग्निशन और अनुवाद के लिए उपयोग होते हैं।

### GPU आवश्यकताएँ

इस डेमो को चलाने के लिए आपको 12GB GPU मेमोरी की आवश्यकता होगी।

**Microsoft-Phi-3-Mini-4K instruct** डेमो को GPU पर चलाने के लिए मेमोरी की जरूरत कई कारकों पर निर्भर करती है, जैसे कि इनपुट डेटा का आकार (ऑडियो या टेक्स्ट), अनुवाद के लिए उपयोग की जाने वाली भाषा, मॉडल की गति, और GPU पर उपलब्ध मेमोरी।

आमतौर पर, Whisper मॉडल GPUs पर चलाने के लिए डिज़ाइन किया गया है। Whisper मॉडल को चलाने के लिए न्यूनतम GPU मेमोरी 8 GB की सिफारिश की जाती है, लेकिन जरूरत पड़ने पर यह अधिक मेमोरी को भी संभाल सकता है।

यह ध्यान रखना महत्वपूर्ण है कि बड़े डेटा या उच्च मात्रा में अनुरोध मॉडल पर चलाने से अधिक GPU मेमोरी की जरूरत हो सकती है और प्रदर्शन में समस्या आ सकती है। अपने उपयोग के केस को अलग-अलग सेटिंग्स के साथ टेस्ट करना और मेमोरी उपयोग की निगरानी करना बेहतर होगा ताकि आपके विशेष जरूरतों के लिए उपयुक्त सेटिंग्स निर्धारित की जा सकें।

## Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper के लिए E2E सैंपल

जुपिटर नोटबुक जिसका शीर्षक है [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb), यह दिखाता है कि Microsoft Phi 3 Mini 4K instruct डेमो का उपयोग ऑडियो या लिखित टेक्स्ट इनपुट से टेक्स्ट जनरेट करने के लिए कैसे किया जाता है। नोटबुक में कई फंक्शन परिभाषित किए गए हैं:

1. `tts_file_name(text)`: यह फंक्शन जनरेट किए गए ऑडियो फाइल को सेव करने के लिए इनपुट टेक्स्ट के आधार पर फाइल नाम बनाता है।
1. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: यह फंक्शन Edge TTS API का उपयोग करके इनपुट टेक्स्ट के टुकड़ों की सूची से ऑडियो फाइल बनाता है। इनपुट पैरामीटर में टुकड़ों की सूची, स्पीच रेट, वॉइस नाम, और जनरेट की गई ऑडियो फाइल को सेव करने के लिए आउटपुट पथ शामिल हैं।
1. `talk(input_text)`: यह फंक्शन Edge TTS API का उपयोग करके ऑडियो फाइल जनरेट करता है और इसे /content/audio डायरेक्टरी में रैंडम फाइल नाम के साथ सेव करता है। इनपुट पैरामीटर वह टेक्स्ट है जिसे स्पीच में बदला जाना है।
1. `run_text_prompt(message, chat_history)`: यह फंक्शन Microsoft Phi 3 Mini 4K instruct डेमो का उपयोग करके मैसेज इनपुट से ऑडियो फाइल बनाता है और इसे चैट हिस्ट्री में जोड़ता है।
1. `run_audio_prompt(audio, chat_history)`: यह फंक्शन Whisper मॉडल API का उपयोग करके ऑडियो फाइल को टेक्स्ट में कन्वर्ट करता है और इसे `run_text_prompt()` फंक्शन को पास करता है।
1. कोड एक Gradio ऐप लॉन्च करता है जो उपयोगकर्ताओं को Phi 3 Mini 4K instruct डेमो के साथ मैसेज टाइप करके या ऑडियो फाइल अपलोड करके इंटरैक्ट करने की अनुमति देता है। आउटपुट ऐप के अंदर टेक्स्ट मैसेज के रूप में दिखाया जाता है।

## समस्या निवारण

Cuda GPU ड्राइवर्स इंस्टॉल करना

1. सुनिश्चित करें कि आपके Linux एप्लिकेशन अपडेटेड हैं

    ```bash
    sudo apt update
    ```

1. Cuda ड्राइवर्स इंस्टॉल करें

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. cuda ड्राइवर लोकेशन रजिस्टर करें

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. Nvidia GPU मेमोरी साइज जांचें (आवश्यक 12GB GPU मेमोरी)

    ```bash
    nvidia-smi
    ```

1. कैश खाली करें: यदि आप PyTorch का उपयोग कर रहे हैं, तो torch.cuda.empty_cache() कॉल करके सभी अनयूज्ड कैश्ड मेमोरी रिलीज़ कर सकते हैं ताकि इसे अन्य GPU एप्लिकेशन उपयोग कर सकें

    ```python
    torch.cuda.empty_cache() 
    ```

1. Nvidia Cuda जांचें

    ```bash
    nvcc --version
    ```

1. Hugging Face टोकन बनाने के लिए निम्न कार्य करें:

    - [Hugging Face Token Settings page](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo) पर जाएं।
    - **New token** चुनें।
    - उस प्रोजेक्ट का **Name** दर्ज करें जिसे आप उपयोग करना चाहते हैं।
    - **Type** को **Write** चुनें।

> **Note**
>
> यदि आपको निम्न त्रुटि मिलती है:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> इसे ठीक करने के लिए, अपने टर्मिनल में निम्न कमांड टाइप करें।
>
> ```bash
> sudo ldconfig
> ```

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियाँ या गलतियाँ हो सकती हैं। मूल दस्तावेज़ अपनी मूल भाषा में ही प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सलाह दी जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।