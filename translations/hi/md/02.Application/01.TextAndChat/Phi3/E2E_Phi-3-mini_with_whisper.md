<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "006e8cf75211d3297f24e1b22e38955f",
  "translation_date": "2025-07-17T02:15:49+00:00",
  "source_file": "md/02.Application/01.TextAndChat/Phi3/E2E_Phi-3-mini_with_whisper.md",
  "language_code": "hi"
}
-->
# Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper

## अवलोकन

Interactive Phi 3 Mini 4K Instruct Chatbot एक ऐसा टूल है जो उपयोगकर्ताओं को Microsoft Phi 3 Mini 4K instruct डेमो के साथ टेक्स्ट या ऑडियो इनपुट के माध्यम से बातचीत करने की अनुमति देता है। इस चैटबोट का उपयोग विभिन्न कार्यों के लिए किया जा सकता है, जैसे अनुवाद, मौसम अपडेट, और सामान्य जानकारी एकत्र करना।

### शुरूआत कैसे करें

इस चैटबोट का उपयोग करने के लिए, बस निम्नलिखित निर्देशों का पालन करें:

1. एक नया [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) खोलें
2. नोटबुक की मुख्य विंडो में, आपको एक चैटबॉक्स इंटरफ़ेस दिखाई देगा जिसमें टेक्स्ट इनपुट बॉक्स और "Send" बटन होगा।
3. टेक्स्ट-आधारित चैटबोट का उपयोग करने के लिए, बस अपना संदेश टेक्स्ट इनपुट बॉक्स में टाइप करें और "Send" बटन पर क्लिक करें। चैटबोट एक ऑडियो फ़ाइल के साथ जवाब देगा जिसे सीधे नोटबुक के अंदर से चलाया जा सकता है।

**Note**: इस टूल के लिए GPU और Microsoft Phi-3 तथा OpenAI Whisper मॉडल्स तक पहुंच आवश्यक है, जो भाषण पहचान और अनुवाद के लिए उपयोग किए जाते हैं।

### GPU आवश्यकताएँ

इस डेमो को चलाने के लिए आपको 12GB GPU मेमोरी की आवश्यकता है।

**Microsoft-Phi-3-Mini-4K instruct** डेमो को GPU पर चलाने के लिए मेमोरी की जरूरत कई कारकों पर निर्भर करती है, जैसे इनपुट डेटा (ऑडियो या टेक्स्ट) का आकार, अनुवाद के लिए उपयोग की गई भाषा, मॉडल की गति, और GPU पर उपलब्ध मेमोरी।

आमतौर पर, Whisper मॉडल GPUs पर चलाने के लिए डिज़ाइन किया गया है। Whisper मॉडल को चलाने के लिए न्यूनतम GPU मेमोरी 8GB की सिफारिश की जाती है, लेकिन जरूरत पड़ने पर यह अधिक मेमोरी भी संभाल सकता है।

यह ध्यान रखना महत्वपूर्ण है कि बड़े डेटा या उच्च मात्रा में अनुरोध मॉडल पर चलाने से अधिक GPU मेमोरी की आवश्यकता हो सकती है और/या प्रदर्शन संबंधी समस्याएं हो सकती हैं। अपनी आवश्यकताओं के अनुसार विभिन्न कॉन्फ़िगरेशन के साथ परीक्षण करना और मेमोरी उपयोग की निगरानी करना बेहतर होगा ताकि आपके उपयोग के लिए उपयुक्त सेटिंग्स निर्धारित की जा सकें।

## Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper के लिए E2E नमूना

जुपिटर नोटबुक जिसका शीर्षक है [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb), यह दिखाता है कि Microsoft Phi 3 Mini 4K instruct डेमो का उपयोग ऑडियो या लिखित टेक्स्ट इनपुट से टेक्स्ट उत्पन्न करने के लिए कैसे किया जाता है। नोटबुक में कई फ़ंक्शन परिभाषित किए गए हैं:

1. `tts_file_name(text)`: यह फ़ंक्शन इनपुट टेक्स्ट के आधार पर एक फ़ाइल नाम बनाता है ताकि उत्पन्न ऑडियो फ़ाइल को सेव किया जा सके।
1. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: यह फ़ंक्शन Edge TTS API का उपयोग करके इनपुट टेक्स्ट के टुकड़ों की सूची से एक ऑडियो फ़ाइल बनाता है। इनपुट पैरामीटर में टुकड़ों की सूची, भाषण की गति, आवाज़ का नाम, और उत्पन्न ऑडियो फ़ाइल को सेव करने का पथ शामिल है।
1. `talk(input_text)`: यह फ़ंक्शन Edge TTS API का उपयोग करके एक ऑडियो फ़ाइल बनाता है और इसे /content/audio डायरेक्टरी में एक यादृच्छिक फ़ाइल नाम से सेव करता है। इनपुट पैरामीटर वह टेक्स्ट है जिसे भाषण में बदला जाना है।
1. `run_text_prompt(message, chat_history)`: यह फ़ंक्शन Microsoft Phi 3 Mini 4K instruct डेमो का उपयोग करके एक संदेश इनपुट से ऑडियो फ़ाइल बनाता है और उसे चैट इतिहास में जोड़ता है।
1. `run_audio_prompt(audio, chat_history)`: यह फ़ंक्शन Whisper मॉडल API का उपयोग करके एक ऑडियो फ़ाइल को टेक्स्ट में बदलता है और उसे `run_text_prompt()` फ़ंक्शन को पास करता है।
1. कोड एक Gradio ऐप लॉन्च करता है जो उपयोगकर्ताओं को Phi 3 Mini 4K instruct डेमो के साथ बातचीत करने की अनुमति देता है, चाहे वे संदेश टाइप करें या ऑडियो फ़ाइलें अपलोड करें। आउटपुट ऐप के अंदर टेक्स्ट संदेश के रूप में प्रदर्शित होता है।

## समस्या निवारण

Cuda GPU ड्राइवर इंस्टॉल करना

1. सुनिश्चित करें कि आपका Linux एप्लिकेशन अपडेटेड है

    ```bash
    sudo apt update
    ```

1. Cuda ड्राइवर इंस्टॉल करें

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. cuda ड्राइवर स्थान को रजिस्टर करें

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. Nvidia GPU मेमोरी साइज जांचें (आवश्यक 12GB GPU मेमोरी)

    ```bash
    nvidia-smi
    ```

1. कैश खाली करें: यदि आप PyTorch का उपयोग कर रहे हैं, तो आप torch.cuda.empty_cache() कॉल कर सकते हैं ताकि सभी अप्रयुक्त कैश्ड मेमोरी रिलीज़ हो जाए और अन्य GPU एप्लिकेशन द्वारा उपयोग की जा सके

    ```python
    torch.cuda.empty_cache() 
    ```

1. Nvidia Cuda जांचें

    ```bash
    nvcc --version
    ```

1. Hugging Face टोकन बनाने के लिए निम्न कार्य करें:

    - [Hugging Face Token Settings पेज](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo) पर जाएं।
    - **New token** चुनें।
    - उस प्रोजेक्ट का **Name** दर्ज करें जिसे आप उपयोग करना चाहते हैं।
    - **Type** को **Write** पर सेट करें।

> **Note**
>
> यदि आपको निम्न त्रुटि मिलती है:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> इसे ठीक करने के लिए, अपने टर्मिनल में निम्न कमांड टाइप करें।
>
> ```bash
> sudo ldconfig
> ```

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियाँ या अशुद्धियाँ हो सकती हैं। मूल दस्तावेज़ अपनी मूल भाषा में ही अधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सलाह दी जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।