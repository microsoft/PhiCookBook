<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "743d7e9cb9c4e8ea642d77bee657a7fa",
  "translation_date": "2025-07-17T09:53:52+00:00",
  "source_file": "md/03.FineTuning/LetPhi3gotoIndustriy.md",
  "language_code": "hi"
}
-->
# **Phi-3 को एक उद्योग विशेषज्ञ बनाएं**

Phi-3 मॉडल को किसी उद्योग में लागू करने के लिए, आपको उद्योग के व्यावसायिक डेटा को Phi-3 मॉडल में जोड़ना होगा। हमारे पास दो अलग-अलग विकल्प हैं, पहला है RAG (Retrieval Augmented Generation) और दूसरा है Fine Tuning।

## **RAG बनाम Fine-Tuning**

### **Retrieval Augmented Generation**

RAG डेटा पुनः प्राप्ति + टेक्स्ट जनरेशन है। उद्यम के संरचित और असंरचित डेटा को वेक्टर डेटाबेस में संग्रहित किया जाता है। जब प्रासंगिक सामग्री खोजी जाती है, तो संबंधित सारांश और सामग्री को संदर्भ बनाने के लिए पाया जाता है, और LLM/SLM की टेक्स्ट पूर्णता क्षमता के साथ मिलाकर सामग्री उत्पन्न की जाती है।

### **Fine-tuning**

Fine-tuning किसी विशेष मॉडल के सुधार पर आधारित है। इसे मॉडल एल्गोरिदम से शुरू करने की आवश्यकता नहीं होती, लेकिन डेटा को लगातार इकट्ठा करना पड़ता है। यदि आप उद्योग अनुप्रयोगों में अधिक सटीक शब्दावली और भाषा अभिव्यक्ति चाहते हैं, तो Fine-tuning आपके लिए बेहतर विकल्प है। लेकिन यदि आपका डेटा बार-बार बदलता रहता है, तो Fine-tuning जटिल हो सकता है।

### **कैसे चुनें**

1. यदि हमारे उत्तर में बाहरी डेटा शामिल करना आवश्यक है, तो RAG सबसे अच्छा विकल्प है।

2. यदि आपको स्थिर और सटीक उद्योग ज्ञान आउटपुट करना है, तो Fine-tuning अच्छा विकल्प होगा। RAG प्रासंगिक सामग्री को प्राथमिकता देता है लेकिन हमेशा विशेषज्ञता की सूक्ष्मताओं को पकड़ नहीं पाता।

3. Fine-tuning के लिए उच्च गुणवत्ता वाला डेटा सेट आवश्यक है, और यदि डेटा की मात्रा कम है, तो इसका ज्यादा प्रभाव नहीं होगा। RAG अधिक लचीला है।

4. Fine-tuning एक ब्लैक बॉक्स है, एक रहस्य, और इसके आंतरिक तंत्र को समझना मुश्किल होता है। लेकिन RAG डेटा के स्रोत को खोजने में आसान बनाता है, जिससे हॉलुसिनेशन या सामग्री त्रुटियों को प्रभावी ढंग से समायोजित किया जा सकता है और बेहतर पारदर्शिता मिलती है।

### **परिदृश्य**

1. विशिष्ट पेशेवर शब्दावली और अभिव्यक्तियों की आवश्यकता वाले वर्टिकल उद्योगों के लिए, ***Fine-tuning*** सबसे अच्छा विकल्प होगा।

2. QA सिस्टम, जिसमें विभिन्न ज्ञान बिंदुओं का संयोजन शामिल है, के लिए ***RAG*** सबसे अच्छा विकल्प होगा।

3. स्वचालित व्यावसायिक प्रवाह के संयोजन के लिए ***RAG + Fine-tuning*** सबसे अच्छा विकल्प है।

## **RAG का उपयोग कैसे करें**

![rag](../../../../translated_images/hi/rag.2014adc59e6f6007.webp)

वेक्टर डेटाबेस डेटा का एक संग्रह है जो गणितीय रूप में संग्रहीत होता है। वेक्टर डेटाबेस मशीन लर्निंग मॉडल को पिछले इनपुट याद रखने में मदद करता है, जिससे खोज, सिफारिशें और टेक्स्ट जनरेशन जैसे उपयोग मामलों का समर्थन किया जा सकता है। डेटा की पहचान समानता मेट्रिक्स के आधार पर की जाती है, न कि सटीक मेल के आधार पर, जिससे कंप्यूटर मॉडल डेटा के संदर्भ को समझ पाते हैं।

वेक्टर डेटाबेस RAG को साकार करने की कुंजी है। हम टेक्स्ट-एम्बेडिंग-3, jina-ai-embedding जैसे वेक्टर मॉडलों के माध्यम से डेटा को वेक्टर स्टोरेज में परिवर्तित कर सकते हैं।

RAG एप्लिकेशन बनाने के बारे में अधिक जानें [https://github.com/microsoft/Phi-3CookBook](https://github.com/microsoft/Phi-3CookBook?WT.mc_id=aiml-138114-kinfeylo)

## **Fine-tuning का उपयोग कैसे करें**

Fine-tuning में आमतौर पर उपयोग किए जाने वाले एल्गोरिदम Lora और QLora हैं। कैसे चुनें?
- [इस सैंपल नोटबुक के साथ और जानें](../../../../code/04.Finetuning/Phi_3_Inference_Finetuning.ipynb)
- [Python FineTuning सैंपल का उदाहरण](../../../../code/04.Finetuning/FineTrainingScript.py)

### **Lora और QLora**

![lora](../../../../translated_images/hi/qlora.e6446c988ee04ca0.webp)

LoRA (Low-Rank Adaptation) और QLoRA (Quantized Low-Rank Adaptation) दोनों Parameter Efficient Fine Tuning (PEFT) का उपयोग करके बड़े भाषा मॉडलों (LLMs) को फाइन-ट्यून करने की तकनीकें हैं। PEFT तकनीकें पारंपरिक तरीकों की तुलना में मॉडल को अधिक कुशलता से प्रशिक्षित करने के लिए डिज़ाइन की गई हैं।  
LoRA एक स्वतंत्र फाइन-ट्यूनिंग तकनीक है जो वेट अपडेट मैट्रिक्स पर लो-रैंक अप्रोक्सिमेशन लागू करके मेमोरी उपयोग को कम करती है। यह तेज़ प्रशिक्षण समय प्रदान करती है और पारंपरिक फाइन-ट्यूनिंग विधियों के करीब प्रदर्शन बनाए रखती है।

QLoRA LoRA का एक विस्तारित संस्करण है जो मेमोरी उपयोग को और कम करने के लिए क्वांटाइजेशन तकनीकों को शामिल करता है। QLoRA प्री-ट्रेंड LLM में वेट पैरामीटर की प्रिसिजन को 4-बिट प्रिसिजन तक क्वांटाइज़ करता है, जो LoRA की तुलना में अधिक मेमोरी कुशल है। हालांकि, अतिरिक्त क्वांटाइजेशन और डी-क्वांटाइजेशन चरणों के कारण QLoRA प्रशिक्षण LoRA प्रशिक्षण की तुलना में लगभग 30% धीमा होता है।

QLoRA क्वांटाइजेशन त्रुटियों के दौरान उत्पन्न त्रुटियों को ठीक करने के लिए LoRA का उपयोग करता है। QLoRA अपेक्षाकृत छोटे, उच्च उपलब्ध GPUs पर अरबों पैरामीटर वाले विशाल मॉडलों को फाइन-ट्यून करने में सक्षम बनाता है। उदाहरण के लिए, QLoRA 70B पैरामीटर मॉडल को केवल 2 GPUs के साथ फाइन-ट्यून कर सकता है, जबकि इसके लिए सामान्यतः 36 GPUs की आवश्यकता होती है।

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियाँ या अशुद्धियाँ हो सकती हैं। मूल दस्तावेज़ अपनी मूल भाषा में ही अधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सलाह दी जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।