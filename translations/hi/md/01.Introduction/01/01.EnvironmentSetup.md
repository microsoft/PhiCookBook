<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-07-16T18:08:15+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "hi"
}
-->
# Phi-3 के साथ स्थानीय रूप से शुरुआत करें

यह गाइड आपको Ollama का उपयोग करके Phi-3 मॉडल को चलाने के लिए अपना स्थानीय वातावरण सेटअप करने में मदद करेगा। आप मॉडल को कई तरीकों से चला सकते हैं, जिनमें GitHub Codespaces, VS Code Dev Containers, या आपका स्थानीय वातावरण शामिल है।

## वातावरण सेटअप

### GitHub Codespaces

आप GitHub Codespaces का उपयोग करके इस टेम्पलेट को वर्चुअली चला सकते हैं। यह बटन आपके ब्राउज़र में वेब-आधारित VS Code इंस्टेंस खोलेगा:

1. टेम्पलेट खोलें (इसमें कुछ मिनट लग सकते हैं):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. एक टर्मिनल विंडो खोलें

### VS Code Dev Containers

⚠️ यह विकल्प तभी काम करेगा जब आपके Docker Desktop को कम से कम 16 GB RAM आवंटित की गई हो। यदि आपके पास 16 GB से कम RAM है, तो आप [GitHub Codespaces विकल्प](../../../../../md/01.Introduction/01) या [स्थानीय रूप से सेटअप](../../../../../md/01.Introduction/01) करने का प्रयास कर सकते हैं।

एक संबंधित विकल्प VS Code Dev Containers है, जो आपके स्थानीय VS Code में प्रोजेक्ट को [Dev Containers एक्सटेंशन](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) का उपयोग करके खोलेगा:

1. Docker Desktop शुरू करें (यदि पहले से इंस्टॉल नहीं है तो इंस्टॉल करें)
2. प्रोजेक्ट खोलें:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. खुलने वाली VS Code विंडो में, जब प्रोजेक्ट फाइलें दिखने लगें (इसमें कुछ मिनट लग सकते हैं), तो एक टर्मिनल विंडो खोलें।
4. [डिप्लॉयमेंट स्टेप्स](../../../../../md/01.Introduction/01) के साथ जारी रखें

### स्थानीय वातावरण

1. सुनिश्चित करें कि निम्नलिखित टूल इंस्टॉल हैं:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## मॉडल का परीक्षण करें

1. Ollama से phi3:mini मॉडल डाउनलोड और चलाने के लिए कहें:

    ```shell
    ollama run phi3:mini
    ```

    मॉडल डाउनलोड होने में कुछ मिनट लगेंगे।

2. जब आउटपुट में "success" दिखे, तब आप उस मॉडल को प्रॉम्प्ट से संदेश भेज सकते हैं।

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. कुछ सेकंड बाद, आपको मॉडल से प्रतिक्रिया स्ट्रीम दिखाई देनी चाहिए।

4. भाषा मॉडल के साथ उपयोग की जाने वाली विभिन्न तकनीकों के बारे में जानने के लिए, Python नोटबुक [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) खोलें और प्रत्येक सेल चलाएं। यदि आपने 'phi3:mini' के अलावा कोई अन्य मॉडल इस्तेमाल किया है, तो पहले सेल में `MODEL_NAME` बदलें।

5. Python से phi3:mini मॉडल के साथ बातचीत करने के लिए, Python फाइल [chat.py](../../../../../code/01.Introduce/chat.py) खोलें और इसे चलाएं। आप ज़रूरत के अनुसार फाइल के शीर्ष पर `MODEL_NAME` बदल सकते हैं, और सिस्टम संदेश को संशोधित कर सकते हैं या कुछ फ्यू-शॉट उदाहरण भी जोड़ सकते हैं।

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियाँ या अशुद्धियाँ हो सकती हैं। मूल दस्तावेज़ अपनी मूल भाषा में ही अधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सलाह दी जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।