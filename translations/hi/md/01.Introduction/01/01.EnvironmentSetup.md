<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-08T06:17:12+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "hi"
}
-->
# Phi-3 के साथ स्थानीय रूप से शुरुआत करें

यह गाइड आपको Ollama का उपयोग करके Phi-3 मॉडल को चलाने के लिए अपना स्थानीय वातावरण सेटअप करने में मदद करेगा। आप मॉडल को कई तरीकों से चला सकते हैं, जिनमें GitHub Codespaces, VS Code Dev Containers, या आपका स्थानीय वातावरण शामिल है।

## पर्यावरण सेटअप

### GitHub Codespaces

आप GitHub Codespaces का उपयोग करके इस टेम्पलेट को वर्चुअली चला सकते हैं। यह बटन आपके ब्राउज़र में वेब-आधारित VS Code इंस्टेंस खोलेगा:

1. टेम्पलेट खोलें (इसमें कुछ मिनट लग सकते हैं):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. एक टर्मिनल विंडो खोलें

### VS Code Dev Containers

⚠️ यह विकल्प तभी काम करेगा जब आपके Docker Desktop को कम से कम 16 GB RAM आवंटित हो। यदि आपके पास 16 GB से कम RAM है, तो आप [GitHub Codespaces विकल्प](../../../../../md/01.Introduction/01) आज़मा सकते हैं या [स्थानीय रूप से सेटअप](../../../../../md/01.Introduction/01) कर सकते हैं।

एक संबंधित विकल्प VS Code Dev Containers है, जो आपके स्थानीय VS Code में प्रोजेक्ट को [Dev Containers एक्सटेंशन](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) का उपयोग करके खोलेगा:

1. Docker Desktop शुरू करें (यदि पहले से इंस्टॉल नहीं है तो इंस्टॉल करें)
2. प्रोजेक्ट खोलें:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. खुलने वाली VS Code विंडो में, जब प्रोजेक्ट फाइलें दिखने लगें (इसमें कुछ मिनट लग सकते हैं), तो एक टर्मिनल विंडो खोलें।
4. [डिप्लॉयमेंट स्टेप्स](../../../../../md/01.Introduction/01) के साथ जारी रखें

### स्थानीय वातावरण

1. सुनिश्चित करें कि निम्न टूल्स इंस्टॉल हैं:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## मॉडल का परीक्षण करें

1. Ollama से phi3:mini मॉडल डाउनलोड और चलाने के लिए कहें:

    ```shell
    ollama run phi3:mini
    ```

    मॉडल डाउनलोड होने में कुछ मिनट लगेंगे।

2. जब आउटपुट में "success" दिखे, तब आप उस मॉडल को प्रॉम्प्ट से मैसेज भेज सकते हैं।

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. कुछ सेकंड बाद, आपको मॉडल से प्रतिक्रिया स्ट्रीम दिखाई देगी।

4. भाषा मॉडल के साथ उपयोग की जाने वाली विभिन्न तकनीकों को जानने के लिए, Python नोटबुक [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) खोलें और हर सेल चलाएं। यदि आपने 'phi3:mini' के अलावा कोई मॉडल इस्तेमाल किया है, तो फ़ाइल के शीर्ष पर `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` को आवश्यकतानुसार बदलें, और आप सिस्टम संदेश या कुछ उदाहरण भी बदल सकते हैं यदि चाहें।

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनूदित किया गया है। हम सटीकता के लिए प्रयासरत हैं, लेकिन कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियाँ या गलतियाँ हो सकती हैं। मूल दस्तावेज़ अपनी मूल भाषा में ही आधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।