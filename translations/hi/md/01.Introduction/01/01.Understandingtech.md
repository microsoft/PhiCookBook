<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:42:25+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "hi"
}
-->
# प्रमुख तकनीकें जिनका उल्लेख किया गया है

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - एक लो-लेवल API जो DirectX 12 के ऊपर हार्डवेयर-त्वरित मशीन लर्निंग के लिए बनाया गया है।  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia द्वारा विकसित एक पैरेलल कंप्यूटिंग प्लेटफॉर्म और एप्लिकेशन प्रोग्रामिंग इंटरफेस (API) मॉडल, जो ग्राफिक्स प्रोसेसिंग यूनिट्स (GPUs) पर सामान्य प्रयोजन प्रोसेसिंग सक्षम करता है।  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - एक खुला फॉर्मेट जो मशीन लर्निंग मॉडल्स को प्रदर्शित करने के लिए डिज़ाइन किया गया है और विभिन्न ML फ्रेमवर्क्स के बीच इंटरऑपरेबिलिटी प्रदान करता है।  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - एक फॉर्मेट जो मशीन लर्निंग मॉडल्स को प्रदर्शित करने और अपडेट करने के लिए उपयोग किया जाता है, खासकर छोटे भाषा मॉडल्स के लिए जो 4-8 बिट क्वांटाइजेशन के साथ CPUs पर प्रभावी रूप से चल सकते हैं।  

## DirectML

DirectML एक लो-लेवल API है जो हार्डवेयर-त्वरित मशीन लर्निंग को सक्षम बनाता है। यह DirectX 12 के ऊपर बनाया गया है ताकि GPU एक्सेलेरेशन का उपयोग किया जा सके और यह वेंडर-एग्नोस्टिक है, यानी इसे विभिन्न GPU वेंडर्स पर काम करने के लिए कोड में बदलाव करने की जरूरत नहीं होती। इसे मुख्य रूप से GPU पर मॉडल ट्रेनिंग और इन्फरेंसिंग वर्कलोड्स के लिए उपयोग किया जाता है।

हार्डवेयर सपोर्ट की बात करें तो, DirectML को AMD के इंटीग्रेटेड और डिस्क्रीट GPUs, Intel के इंटीग्रेटेड GPUs, और NVIDIA के डिस्क्रीट GPUs सहित कई प्रकार के GPUs के साथ काम करने के लिए डिज़ाइन किया गया है। यह Windows AI प्लेटफॉर्म का हिस्सा है और Windows 10 एवं 11 पर समर्थित है, जिससे किसी भी Windows डिवाइस पर मॉडल ट्रेनिंग और इन्फरेंसिंग संभव है।

DirectML से जुड़ी अपडेट्स और अवसरों में 150 तक ONNX ऑपरेटर्स का समर्थन शामिल है, और इसे ONNX रनटाइम और WinML दोनों द्वारा उपयोग किया जाता है। इसे प्रमुख Integrated Hardware Vendors (IHVs) द्वारा समर्थित किया जाता है, जो विभिन्न मेटाकमांड्स को लागू करते हैं।  

## CUDA

CUDA, जिसका पूरा नाम Compute Unified Device Architecture है, Nvidia द्वारा बनाया गया एक पैरेलल कंप्यूटिंग प्लेटफॉर्म और एप्लिकेशन प्रोग्रामिंग इंटरफेस (API) मॉडल है। यह सॉफ़्टवेयर डेवलपर्स को CUDA-सक्षम GPU का उपयोग सामान्य प्रयोजन प्रोसेसिंग के लिए करने की अनुमति देता है — जिसे GPGPU (General-Purpose computing on Graphics Processing Units) कहा जाता है। CUDA Nvidia के GPU एक्सेलेरेशन का एक मुख्य स्तंभ है और मशीन लर्निंग, वैज्ञानिक कंप्यूटिंग, और वीडियो प्रोसेसिंग जैसे विभिन्न क्षेत्रों में व्यापक रूप से उपयोग किया जाता है।

CUDA का हार्डवेयर सपोर्ट विशेष रूप से Nvidia के GPUs के लिए है, क्योंकि यह Nvidia की एक स्वामित्व वाली तकनीक है। प्रत्येक आर्किटेक्चर CUDA टूलकिट के विशिष्ट संस्करणों का समर्थन करता है, जो डेवलपर्स को CUDA एप्लिकेशन बनाने और चलाने के लिए आवश्यक लाइब्रेरी और टूल्स प्रदान करता है।  

## ONNX

ONNX (Open Neural Network Exchange) एक खुला फॉर्मेट है जो मशीन लर्निंग मॉडल्स को प्रदर्शित करने के लिए डिज़ाइन किया गया है। यह एक एक्स्टेंसिबल कंप्यूटेशन ग्राफ मॉडल की परिभाषा प्रदान करता है, साथ ही बिल्ट-इन ऑपरेटर्स और मानक डेटा टाइप्स की परिभाषाएं भी देता है। ONNX डेवलपर्स को विभिन्न ML फ्रेमवर्क्स के बीच मॉडल्स को स्थानांतरित करने की अनुमति देता है, जिससे इंटरऑपरेबिलिटी संभव होती है और AI एप्लिकेशन बनाना और तैनात करना आसान हो जाता है।

Phi3 mini ONNX Runtime के साथ CPU और GPU पर विभिन्न डिवाइसेज पर चल सकता है, जिनमें सर्वर प्लेटफॉर्म, Windows, Linux और Mac डेस्कटॉप्स, और मोबाइल CPUs शामिल हैं।  
हमने जो ऑप्टिमाइज़्ड कॉन्फ़िगरेशन जोड़े हैं वे हैं:

- int4 DML के लिए ONNX मॉडल: AWQ के माध्यम से int4 में क्वांटाइज्ड  
- fp16 CUDA के लिए ONNX मॉडल  
- int4 CUDA के लिए ONNX मॉडल: RTN के माध्यम से int4 में क्वांटाइज्ड  
- int4 CPU और मोबाइल के लिए ONNX मॉडल: RTN के माध्यम से int4 में क्वांटाइज्ड  

## Llama.cpp

Llama.cpp एक ओपन-सोर्स सॉफ़्टवेयर लाइब्रेरी है जो C++ में लिखी गई है। यह विभिन्न बड़े भाषा मॉडल्स (LLMs) पर इन्फरेंस करता है, जिनमें Llama भी शामिल है। ggml लाइब्रेरी (एक जनरल-पर्पस टेंसर लाइब्रेरी) के साथ विकसित, llama.cpp का उद्देश्य मूल Python इम्प्लीमेंटेशन की तुलना में तेज़ इन्फरेंस और कम मेमोरी उपयोग प्रदान करना है। यह हार्डवेयर ऑप्टिमाइजेशन, क्वांटाइजेशन का समर्थन करता है, और एक सरल API तथा उदाहरण प्रदान करता है। यदि आप कुशल LLM इन्फरेंस में रुचि रखते हैं, तो llama.cpp को देखना उपयोगी होगा क्योंकि Phi3 Llama.cpp चला सकता है।  

## GGUF

GGUF (Generic Graph Update Format) एक फॉर्मेट है जो मशीन लर्निंग मॉडल्स को प्रदर्शित करने और अपडेट करने के लिए उपयोग किया जाता है। यह विशेष रूप से छोटे भाषा मॉडल्स (SLMs) के लिए उपयोगी है जो 4-8 बिट क्वांटाइजेशन के साथ CPUs पर प्रभावी रूप से चल सकते हैं। GGUF तेजी से प्रोटोटाइपिंग और एज डिवाइसेज या CI/CD पाइपलाइंस जैसे बैच जॉब्स में मॉडल्स चलाने के लिए लाभकारी है।

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियाँ या अशुद्धियाँ हो सकती हैं। मूल दस्तावेज़ अपनी मूल भाषा में ही अधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सलाह दी जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।