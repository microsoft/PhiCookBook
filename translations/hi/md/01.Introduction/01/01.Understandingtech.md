<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "583e1ebd3884b47b43c883072eb8fa03",
  "translation_date": "2025-04-04T17:35:25+00:00",
  "source_file": "md\\01.Introduction\\01\\01.Understandingtech.md",
  "language_code": "hi"
}
-->
# प्रमुख तकनीकों में शामिल हैं

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12 पर आधारित हार्डवेयर-त्वरित मशीन लर्निंग के लिए एक लो-लेवल API।
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia द्वारा विकसित एक समानांतर कंप्यूटिंग प्लेटफॉर्म और API मॉडल, जो ग्राफिक्स प्रोसेसिंग यूनिट्स (GPUs) पर सामान्य-उद्देश्यीय प्रोसेसिंग को सक्षम बनाता है।
3. [ONNX](https://onnx.ai/) (ओपन न्यूरल नेटवर्क एक्सचेंज) - मशीन लर्निंग मॉडल को दर्शाने के लिए डिज़ाइन किया गया एक खुला प्रारूप, जो विभिन्न ML फ्रेमवर्क्स के बीच इंटरऑपरेबिलिटी प्रदान करता है।
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (जनरल ग्राफ अपडेट फॉर्मेट) - मशीन लर्निंग मॉडल को दर्शाने और अपडेट करने के लिए उपयोग किया जाने वाला एक प्रारूप, खासकर छोटे भाषा मॉडल्स के लिए जो CPUs पर 4-8 बिट क्वांटाइजेशन के साथ प्रभावी ढंग से चल सकते हैं।

## DirectML

DirectML एक लो-लेवल API है जो हार्डवेयर-त्वरित मशीन लर्निंग को सक्षम बनाता है। यह GPU एक्सेलेरेशन का उपयोग करने के लिए DirectX 12 पर आधारित है और विक्रेता-स्वतंत्र है, यानी यह विभिन्न GPU विक्रेताओं पर बिना कोड परिवर्तन के काम करता है। इसे मुख्य रूप से मॉडल प्रशिक्षण और GPU पर इन्फरेंसिंग वर्कलोड्स के लिए उपयोग किया जाता है।

हार्डवेयर समर्थन की बात करें तो, DirectML AMD इंटीग्रेटेड और डिस्क्रीट GPUs, Intel इंटीग्रेटेड GPUs, और NVIDIA डिस्क्रीट GPUs सहित विभिन्न GPUs के साथ काम करने के लिए डिज़ाइन किया गया है। यह Windows AI प्लेटफॉर्म का हिस्सा है और Windows 10 और 11 पर समर्थित है, जिससे किसी भी Windows डिवाइस पर मॉडल प्रशिक्षण और इन्फरेंसिंग संभव है।

DirectML से जुड़े अपडेट और अवसरों में 150 तक ONNX ऑपरेटर्स का समर्थन और ONNX रनटाइम और WinML द्वारा इसका उपयोग शामिल है। इसे प्रमुख हार्डवेयर विक्रेताओं (IHVs) द्वारा समर्थन प्राप्त है, जो विभिन्न मेटाकमांड्स को लागू करते हैं।

## CUDA

CUDA, जिसका मतलब Compute Unified Device Architecture है, Nvidia द्वारा विकसित एक समानांतर कंप्यूटिंग प्लेटफॉर्म और API मॉडल है। यह सॉफ़्टवेयर डेवलपर्स को CUDA-सक्षम ग्राफिक्स प्रोसेसिंग यूनिट (GPU) का उपयोग सामान्य-उद्देश्यीय प्रोसेसिंग के लिए करने की अनुमति देता है – जिसे GPGPU (ग्राफिक्स प्रोसेसिंग यूनिट्स पर सामान्य-उद्देश्यीय कंप्यूटिंग) कहा जाता है। CUDA Nvidia के GPU एक्सेलेरेशन का एक प्रमुख सक्षम है और इसे मशीन लर्निंग, वैज्ञानिक कंप्यूटिंग, और वीडियो प्रोसेसिंग सहित विभिन्न क्षेत्रों में व्यापक रूप से उपयोग किया जाता है।

CUDA का हार्डवेयर समर्थन Nvidia के GPUs के लिए विशिष्ट है, क्योंकि यह Nvidia द्वारा विकसित एक मालिकाना तकनीक है। प्रत्येक आर्किटेक्चर CUDA टूलकिट के विशिष्ट संस्करणों का समर्थन करता है, जो डेवलपर्स को CUDA एप्लिकेशन बनाने और चलाने के लिए आवश्यक लाइब्रेरी और टूल प्रदान करता है।

## ONNX

ONNX (ओपन न्यूरल नेटवर्क एक्सचेंज) एक खुला प्रारूप है जो मशीन लर्निंग मॉडल को दर्शाने के लिए डिज़ाइन किया गया है। यह एक एक्स्टेंसिबल कंप्यूटेशन ग्राफ मॉडल की परिभाषा, साथ ही बिल्ट-इन ऑपरेटर्स और मानक डेटा प्रकारों की परिभाषा प्रदान करता है। ONNX डेवलपर्स को विभिन्न ML फ्रेमवर्क्स के बीच मॉडल को स्थानांतरित करने की अनुमति देता है, जिससे इंटरऑपरेबिलिटी सक्षम होती है और AI एप्लिकेशन बनाना और तैनात करना आसान हो जाता है।

Phi3 mini ONNX रनटाइम के साथ CPU और GPU पर विभिन्न डिवाइसों पर चल सकता है, जिसमें सर्वर प्लेटफॉर्म, Windows, Linux और Mac डेस्कटॉप्स, और मोबाइल CPUs शामिल हैं।
हमने जो अनुकूलित कॉन्फ़िगरेशन जोड़े हैं, वे हैं:

- int4 DML के लिए ONNX मॉडल: AWQ के माध्यम से int4 में क्वांटाइज्ड
- fp16 CUDA के लिए ONNX मॉडल
- int4 CUDA के लिए ONNX मॉडल: RTN के माध्यम से int4 में क्वांटाइज्ड
- int4 CPU और मोबाइल के लिए ONNX मॉडल: RTN के माध्यम से int4 में क्वांटाइज्ड

## Llama.cpp

Llama.cpp एक ओपन-सोर्स सॉफ़्टवेयर लाइब्रेरी है जो C++ में लिखी गई है। यह विभिन्न बड़े भाषा मॉडल्स (LLMs), जिसमें Llama शामिल है, पर इन्फरेंस करती है। ggml लाइब्रेरी (एक सामान्य-उद्देश्यीय टेंसर लाइब्रेरी) के साथ विकसित, llama.cpp तेज इन्फरेंस और मूल Python इम्प्लीमेंटेशन की तुलना में कम मेमोरी उपयोग प्रदान करता है। यह हार्डवेयर ऑप्टिमाइजेशन, क्वांटाइजेशन का समर्थन करता है और एक सरल API और उदाहरण प्रदान करता है। यदि आप कुशल LLM इन्फरेंस में रुचि रखते हैं, तो llama.cpp को एक्सप्लोर करना उपयोगी हो सकता है क्योंकि Phi3 Llama.cpp चला सकता है।

## GGUF

GGUF (जनरल ग्राफ अपडेट फॉर्मेट) मशीन लर्निंग मॉडल को दर्शाने और अपडेट करने के लिए उपयोग किया जाने वाला एक प्रारूप है। यह विशेष रूप से छोटे भाषा मॉडल्स (SLMs) के लिए उपयोगी है, जो CPUs पर 4-8 बिट क्वांटाइजेशन के साथ प्रभावी ढंग से चल सकते हैं। GGUF तेज प्रोटोटाइपिंग और एज डिवाइसों या CI/CD पाइपलाइनों जैसे बैच जॉब्स पर मॉडल चलाने के लिए लाभकारी है।

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता सुनिश्चित करने का प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में उपलब्ध दस्तावेज़ को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।