<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-08T06:16:24+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "hi"
}
-->
# प्रमुख तकनीकों में शामिल हैं

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12 के ऊपर बना एक लो-लेवल API जो हार्डवेयर-एक्सेलेरेटेड मशीन लर्निंग के लिए है।
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia द्वारा विकसित एक पैरेलल कंप्यूटिंग प्लेटफॉर्म और एप्लिकेशन प्रोग्रामिंग इंटरफेस (API) मॉडल, जो ग्राफिक्स प्रोसेसिंग यूनिट्स (GPUs) पर सामान्य प्रयोजन प्रोसेसिंग की अनुमति देता है।
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - एक ओपन फॉर्मेट जो मशीन लर्निंग मॉडल्स को दर्शाने के लिए बनाया गया है और विभिन्न ML फ्रेमवर्क्स के बीच इंटरऑपरेबिलिटी प्रदान करता है।
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - एक फॉर्मेट जो मशीन लर्निंग मॉडल्स को रिप्रेजेंट और अपडेट करने के लिए उपयोग होता है, खासकर छोटे भाषा मॉडल्स के लिए जो 4-8bit क्वांटाइजेशन के साथ CPUs पर प्रभावी रूप से चल सकते हैं।

## DirectML

DirectML एक लो-लेवल API है जो हार्डवेयर-एक्सेलेरेटेड मशीन लर्निंग को सक्षम बनाता है। यह GPU एक्सेलेरेशन का उपयोग करने के लिए DirectX 12 के ऊपर बनाया गया है और यह वेंडर-एग्नॉस्टिक है, यानी यह अलग-अलग GPU वेंडर्स पर काम करने के लिए कोड में बदलाव की जरूरत नहीं होती। इसे मुख्य रूप से मॉडल ट्रेनिंग और इनफेरेंसिंग वर्कलोड्स के लिए GPUs पर इस्तेमाल किया जाता है।

हार्डवेयर सपोर्ट की बात करें तो, DirectML कई प्रकार के GPUs के साथ काम करने के लिए डिजाइन किया गया है, जिसमें AMD इंटीग्रेटेड और डिस्क्रीट GPUs, Intel इंटीग्रेटेड GPUs, और NVIDIA डिस्क्रीट GPUs शामिल हैं। यह Windows AI Platform का हिस्सा है और Windows 10 एवं 11 पर समर्थित है, जिससे किसी भी Windows डिवाइस पर मॉडल ट्रेनिंग और इनफेरेंसिंग संभव होता है।

DirectML से जुड़ी अपडेट्स और अवसरों में 150 ONNX ऑपरेटर्स तक सपोर्ट शामिल है और इसे ONNX रनटाइम और WinML दोनों उपयोग करते हैं। इसे प्रमुख Integrated Hardware Vendors (IHVs) द्वारा समर्थित किया जाता है, जो विभिन्न मेटाकमांड्स को इम्प्लीमेंट करते हैं।

## CUDA

CUDA, जिसका पूरा नाम Compute Unified Device Architecture है, Nvidia द्वारा बनाया गया एक पैरेलल कंप्यूटिंग प्लेटफॉर्म और API मॉडल है। यह सॉफ्टवेयर डेवलपर्स को CUDA-सक्षम ग्राफिक्स प्रोसेसिंग यूनिट (GPU) का उपयोग सामान्य प्रयोजन प्रोसेसिंग के लिए करने की अनुमति देता है — इसे GPGPU (General-Purpose computing on Graphics Processing Units) कहा जाता है। CUDA Nvidia के GPU एक्सेलेरेशन का एक मुख्य स्तंभ है और इसे मशीन लर्निंग, वैज्ञानिक कंप्यूटिंग, और वीडियो प्रोसेसिंग जैसे विभिन्न क्षेत्रों में व्यापक रूप से इस्तेमाल किया जाता है।

CUDA का हार्डवेयर सपोर्ट Nvidia के GPUs तक सीमित है, क्योंकि यह Nvidia की एक प्रोपाइटरी टेक्नोलॉजी है। प्रत्येक आर्किटेक्चर CUDA टूलकिट के विशिष्ट संस्करणों का समर्थन करता है, जो डेवलपर्स को CUDA एप्लिकेशन बनाने और चलाने के लिए आवश्यक लाइब्रेरी और टूल प्रदान करता है।

## ONNX

ONNX (Open Neural Network Exchange) एक ओपन फॉर्मेट है जो मशीन लर्निंग मॉडल्स को दर्शाने के लिए बनाया गया है। यह एक एक्स्टेंसिबल कंप्यूटेशन ग्राफ मॉडल की परिभाषा प्रदान करता है, साथ ही बिल्ट-इन ऑपरेटर्स और स्टैण्डर्ड डेटा टाइप्स की परिभाषाएं भी देता है। ONNX डेवलपर्स को विभिन्न ML फ्रेमवर्क्स के बीच मॉडल्स को ट्रांसफर करने की अनुमति देता है, जिससे इंटरऑपरेबिलिटी संभव होती है और AI एप्लिकेशन बनाना और डिप्लॉय करना आसान हो जाता है।

Phi3 मिनी ONNX रनटाइम के साथ CPU और GPU पर विभिन्न डिवाइसेज पर चल सकता है, जिसमें सर्वर प्लेटफॉर्म, Windows, Linux और Mac डेस्कटॉप्स, और मोबाइल CPUs शामिल हैं।  
हमने जो ऑप्टिमाइज़्ड कॉन्फ़िगरेशन जोड़े हैं वे हैं:

- int4 DML के लिए ONNX मॉडल: AWQ के जरिए int4 में क्वांटाइज्ड  
- fp16 CUDA के लिए ONNX मॉडल  
- int4 CUDA के लिए ONNX मॉडल: RTN के जरिए int4 में क्वांटाइज्ड  
- int4 CPU और मोबाइल के लिए ONNX मॉडल: RTN के जरिए int4 में क्वांटाइज्ड  

## Llama.cpp

Llama.cpp एक ओपन-सोर्स सॉफ्टवेयर लाइब्रेरी है जो C++ में लिखी गई है। यह विभिन्न बड़े भाषा मॉडल्स (LLMs) पर इनफेरेंस करता है, जिनमें Llama भी शामिल है। ggml लाइब्रेरी (एक जनरल-पर्पस टेंसर लाइब्रेरी) के साथ विकसित, llama.cpp का उद्देश्य ओरिजिनल Python इम्प्लीमेंटेशन की तुलना में तेज़ इनफेरेंस और कम मेमोरी उपयोग प्रदान करना है। यह हार्डवेयर ऑप्टिमाइजेशन, क्वांटाइजेशन को सपोर्ट करता है और सरल API तथा उदाहरण प्रदान करता है। यदि आप कुशल LLM इनफेरेंस में रुचि रखते हैं, तो llama.cpp एक अच्छा विकल्प है क्योंकि Phi3 इसे चला सकता है।

## GGUF

GGUF (Generic Graph Update Format) एक फॉर्मेट है जो मशीन लर्निंग मॉडल्स को रिप्रेजेंट और अपडेट करने के लिए उपयोग होता है। यह खासतौर पर छोटे भाषा मॉडल्स (SLMs) के लिए उपयोगी है जो 4-8bit क्वांटाइजेशन के साथ CPUs पर प्रभावी रूप से चल सकते हैं। GGUF तेज़ प्रोटोटाइपिंग और एज डिवाइसेज या CI/CD पाइपलाइन्स जैसे बैच जॉब्स में मॉडल्स को चलाने के लिए फायदेमंद है।

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियाँ या असंगतियाँ हो सकती हैं। मूल दस्तावेज़ अपनी मूल भाषा में ही अधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सलाह दी जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।