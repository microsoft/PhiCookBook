<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a7aaeb42235207ba74581473b305581",
  "translation_date": "2025-04-04T17:59:26+00:00",
  "source_file": "md\\01.Introduction\\04\\UsingLlamacppQuantifyingPhi.md",
  "language_code": "hi"
}
-->
# **llama.cpp का उपयोग करके Phi परिवार को क्वांटाइज़ करना**

## **llama.cpp क्या है**

llama.cpp एक ओपन-सोर्स सॉफ्टवेयर लाइब्रेरी है, जो मुख्य रूप से C++ में लिखी गई है। यह विभिन्न बड़े भाषा मॉडल (LLMs) जैसे Llama पर इनफेरेंस करती है। इसका मुख्य उद्देश्य न्यूनतम सेटअप के साथ विभिन्न हार्डवेयर पर अत्याधुनिक प्रदर्शन प्रदान करना है। इसके अलावा, इस लाइब्रेरी के लिए Python bindings उपलब्ध हैं, जो टेक्स्ट कम्प्लीशन और OpenAI संगत वेब सर्वर के लिए उच्च-स्तरीय API प्रदान करते हैं।

llama.cpp का मुख्य उद्देश्य न्यूनतम सेटअप के साथ और अत्याधुनिक प्रदर्शन के साथ विभिन्न प्रकार के हार्डवेयर पर LLM इनफेरेंस को सक्षम बनाना है - स्थानीय और क्लाउड में।

- Plain C/C++ इम्प्लीमेंटेशन बिना किसी डिपेंडेंसी के
- Apple silicon को प्राथमिकता दी गई है - ARM NEON, Accelerate और Metal फ्रेमवर्क के माध्यम से ऑप्टिमाइज़ किया गया
- x86 आर्किटेक्चर के लिए AVX, AVX2 और AVX512 सपोर्ट
- तेज़ इनफेरेंस और कम मेमोरी उपयोग के लिए 1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, और 8-bit इंटीजर क्वांटाइज़ेशन
- NVIDIA GPUs पर LLMs चलाने के लिए कस्टम CUDA कर्नल्स (AMD GPUs के लिए HIP सपोर्ट)
- Vulkan और SYCL बैकएंड सपोर्ट
- CPU+GPU हाइब्रिड इनफेरेंस, जो VRAM क्षमता से बड़े मॉडल्स को आंशिक रूप से तेज़ करता है

## **llama.cpp का उपयोग करके Phi-3.5 को क्वांटाइज़ करना**

Phi-3.5-Instruct मॉडल को llama.cpp का उपयोग करके क्वांटाइज़ किया जा सकता है, लेकिन Phi-3.5-Vision और Phi-3.5-MoE अभी तक समर्थित नहीं हैं। llama.cpp द्वारा कनवर्ट किया गया फॉर्मेट gguf है, जो सबसे अधिक उपयोग किया जाने वाला क्वांटाइज़ेशन फॉर्मेट भी है।

Hugging Face पर बड़ी संख्या में क्वांटाइज़ किए गए GGUF फॉर्मेट मॉडल उपलब्ध हैं। AI Foundry, Ollama, और LlamaEdge llama.cpp पर निर्भर हैं, इसलिए GGUF मॉडल भी अक्सर उपयोग किए जाते हैं।

### **GGUF क्या है**

GGUF एक बाइनरी फॉर्मेट है जो मॉडल्स को तेज़ी से लोड और सेव करने के लिए ऑप्टिमाइज़ किया गया है, जिससे यह इनफेरेंस उद्देश्यों के लिए अत्यधिक प्रभावी बनता है। GGUF को GGML और अन्य एक्सीक्यूटर्स के साथ उपयोग के लिए डिज़ाइन किया गया है। GGUF को @ggerganov द्वारा विकसित किया गया था, जो llama.cpp के डेवलपर भी हैं, एक लोकप्रिय C/C++ LLM इनफेरेंस फ्रेमवर्क। PyTorch जैसे फ्रेमवर्क में विकसित मॉडल्स को GGUF फॉर्मेट में कनवर्ट किया जा सकता है ताकि इन इंजनों के साथ उपयोग किया जा सके।

### **ONNX बनाम GGUF**

ONNX एक पारंपरिक मशीन लर्निंग/डीप लर्निंग फॉर्मेट है, जो विभिन्न AI फ्रेमवर्क में अच्छी तरह से समर्थित है और एज डिवाइस में उपयोग के अच्छे परिदृश्य प्रदान करता है। GGUF, llama.cpp पर आधारित है और इसे GenAI युग में निर्मित कहा जा सकता है। दोनों का उपयोग समान है। यदि आप एम्बेडेड हार्डवेयर और एप्लिकेशन लेयर में बेहतर प्रदर्शन चाहते हैं, तो ONNX आपकी पसंद हो सकती है। यदि आप llama.cpp के डेरिवेटिव फ्रेमवर्क और तकनीक का उपयोग करते हैं, तो GGUF बेहतर हो सकता है।

### **llama.cpp का उपयोग करके Phi-3.5-Instruct को क्वांटाइज़ करना**

**1. एनवायरनमेंट कॉन्फ़िगरेशन**

```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```

**2. क्वांटाइज़ेशन**

llama.cpp का उपयोग करके Phi-3.5-Instruct को FP16 GGUF में कनवर्ट करें

```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Phi-3.5 को INT4 में क्वांटाइज़ करना

```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```

**3. टेस्टिंग**

llama-cpp-python इंस्टॉल करें

```bash

pip install llama-cpp-python -U

```

***नोट*** 

यदि आप Apple Silicon का उपयोग करते हैं, तो कृपया llama-cpp-python इस तरह इंस्टॉल करें

```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

टेस्टिंग 

```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```

## **संसाधन**

1. llama.cpp के बारे में अधिक जानें [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. GGUF के बारे में अधिक जानें [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियाँ हो सकती हैं। मूल दस्तावेज़, जो इसकी मूल भाषा में है, को आधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।