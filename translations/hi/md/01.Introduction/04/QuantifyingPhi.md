<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-05T16:35:22+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "hi"
}
-->
# **Phi परिवार का क्वांटिफिकेशन**

मॉडल क्वांटाइजेशन से तात्पर्य एक न्यूरल नेटवर्क मॉडल में पैरामीटर (जैसे वज़न और सक्रियण मान) को एक बड़े मान रेंज (आमतौर पर एक सतत मान रेंज) से एक छोटे सीमित मान रेंज में मैप करने की प्रक्रिया है। यह तकनीक मॉडल के आकार और गणनात्मक जटिलता को कम कर सकती है और मोबाइल उपकरणों या एम्बेडेड सिस्टम जैसे संसाधन-सीमित वातावरण में मॉडल की संचालन क्षमता को बेहतर बना सकती है। मॉडल क्वांटाइजेशन पैरामीटर की सटीकता को कम करके संपीड़न प्राप्त करता है, लेकिन इसके कारण सटीकता में कुछ हानि भी होती है। इसलिए, क्वांटाइजेशन प्रक्रिया में मॉडल का आकार, गणनात्मक जटिलता और सटीकता के बीच संतुलन स्थापित करना आवश्यक होता है। सामान्य क्वांटाइजेशन विधियों में फिक्स्ड-पॉइंट क्वांटाइजेशन, फ्लोटिंग-पॉइंट क्वांटाइजेशन आदि शामिल हैं। आप विशिष्ट परिदृश्य और ज़रूरतों के अनुसार उपयुक्त क्वांटाइजेशन रणनीति का चयन कर सकते हैं।

हम GenAI मॉडल को एज डिवाइस पर डिप्लॉय करना चाहते हैं और अधिक डिवाइसों को GenAI परिदृश्यों में प्रवेश करने देना चाहते हैं, जैसे मोबाइल उपकरण, AI PC/Copilot+PC, और पारंपरिक IoT डिवाइस। क्वांटाइजेशन मॉडल के माध्यम से, हम इसे विभिन्न उपकरणों के आधार पर विभिन्न एज डिवाइसों पर डिप्लॉय कर सकते हैं। हार्डवेयर निर्माता द्वारा प्रदान किए गए मॉडल त्वरक फ्रेमवर्क और क्वांटाइजेशन मॉडल के साथ मिलकर, हम बेहतर SLM एप्लिकेशन परिदृश्य बना सकते हैं।

क्वांटाइजेशन परिदृश्य में, हमारे पास विभिन्न सटीकताएँ होती हैं (INT4, INT8, FP16, FP32)। निम्नलिखित सामान्य क्वांटाइजेशन सटीकताओं की व्याख्या है:

### **INT4**

INT4 क्वांटाइजेशन एक कठोर क्वांटाइजेशन विधि है जो मॉडल के वज़न और सक्रियण मानों को 4-बिट पूर्णांकों में क्वांटाइज़ करती है। INT4 क्वांटाइजेशन आमतौर पर छोटे प्रतिनिधित्व रेंज और कम सटीकता के कारण अधिक सटीकता हानि का परिणाम होता है। हालांकि, INT8 क्वांटाइजेशन की तुलना में, INT4 क्वांटाइजेशन मॉडल की संग्रहण आवश्यकताओं और गणनात्मक जटिलता को और अधिक कम कर सकता है। ध्यान देना चाहिए कि व्यावहारिक अनुप्रयोगों में INT4 क्वांटाइजेशन अपेक्षाकृत दुर्लभ है, क्योंकि बहुत कम सटीकता मॉडल प्रदर्शन में महत्वपूर्ण गिरावट ला सकती है। इसके अलावा, सभी हार्डवेयर INT4 ऑपरेशन का समर्थन नहीं करते हैं, इसलिए क्वांटाइजेशन विधि चुनते समय हार्डवेयर संगतता पर विचार करना आवश्यक है।

### **INT8**

INT8 क्वांटाइजेशन मॉडल के वज़न और सक्रियण को फ्लोटिंग पॉइंट संख्याओं से 8-बिट पूर्णांकों में परिवर्तित करने की प्रक्रिया है। यद्यपि INT8 पूर्णांक द्वारा दर्शाई गई संख्यात्मक सीमा छोटी और कम सटीक होती है, यह संग्रहण और गणना आवश्यकताओं को काफी हद तक कम कर सकता है। INT8 क्वांटाइजेशन में, मॉडल के वज़न और सक्रियण मान क्वांटाइजेशन प्रक्रिया से गुजरते हैं, जिसमें स्केलिंग और ऑफसेट शामिल होता है, ताकि मौलिक फ्लोटिंग पॉइंट सूचना को यथासंभव सुरक्षित रखा जा सके। इन्फरेंस के दौरान, इन क्वांटाइज़्ड मानों को गणना के लिए वापस फ्लोटिंग पॉइंट संख्याओं में डीक्वांटाइज़ किया जाता है, और फिर अगले चरण के लिए पुनः INT8 में क्वांटाइज़ किया जाता है। यह विधि अधिकांश अनुप्रयोगों में पर्याप्त सटीकता प्रदान कर सकती है जबकि उच्च गणनात्मक दक्षता बनाए रखती है।

### **FP16**

FP16 प्रारूप, यानी 16-बिट फ्लोटिंग पॉइंट संख्याएं (float16), 32-बिट फ्लोटिंग पॉइंट संख्याओं (float32) की तुलना में मेमोरी खपत को आधा कर देता है, जो बड़े पैमाने पर गहन शिक्षण अनुप्रयोगों में महत्वपूर्ण लाभ प्रदान करता है। FP16 प्रारूप एक ही GPU मेमोरी सीमाओं में बड़े मॉडल लोड करने या अधिक डेटा संसाधित करने की अनुमति देता है। आधुनिक GPU हार्डवेयर के FP16 ऑपरेशनों का समर्थन जारी रखने के कारण, FP16 प्रारूप का उपयोग करने से कंप्यूटिंग गति में भी सुधार हो सकता है। हालांकि, FP16 प्रारूप की अपनी अंतर्निहित कमियाँ हैं, जैसे कम सटीकता, जो कुछ मामलों में संख्यात्मक अस्थिरता या सटीकता हानि का कारण बन सकती है।

### **FP32**

FP32 प्रारूप उच्च सटीकता प्रदान करता है और बड़ी संख्या में मानों का सटीक प्रतिनिधित्व कर सकता है। ऐसी परिदृश्यों में जहां जटिल गणितीय संचालन किए जाते हैं या उच्च सटीक परिणामों की आवश्यकता होती है, वहाँ FP32 प्रारूप को प्राथमिकता दी जाती है। हालांकि, उच्च सटीकता अधिक मेमोरी उपयोग और लंबे गणना समय का अर्थ भी है। बड़े पैमाने पर गहराई सीखने वाले मॉडलों के लिए, विशेषकर जब मॉडल के बहुत सारे पैरामीटर और विशाल डेटा होते हैं, FP32 प्रारूप GPU मेमोरी की कमी या इन्फरेंस गति में कमी का कारण बन सकता है।

मोबाइल उपकरणों या IoT डिवाइसों पर, हम Phi-3.x मॉडलों को INT4 में परिवर्तित कर सकते हैं, जबकि AI PC / Copilot PC उच्च सटीकता जैसे INT8, FP16, FP32 का उपयोग कर सकता है।

वर्तमान में, विभिन्न हार्डवेयर निर्माता जनरेटिव मॉडल का समर्थन करने के लिए अलग-अलग फ्रेमवर्क प्रदान करते हैं, जैसे Intel का OpenVINO, Qualcomm का QNN, Apple का MLX, और Nvidia का CUDA आदि, जो मॉडल क्वांटाइजेशन के साथ मिलकर स्थानीय डिप्लॉयमेंट पूरा करते हैं।

तकनीकी दृष्टि से, क्वांटाइजेशन के बाद हमारे पास विभिन्न प्रारूप समर्थन होते हैं, जैसे PyTorch / TensorFlow प्रारूप, GGUF, और ONNX। मैंने GGUF और ONNX के बीच प्रारूप तुलना और अनुप्रयोग परिदृश्यों पर काम किया है। यहां मैं ONNX क्वांटाइजेशन प्रारूप की सिफारिश करता हूं, जिसका मॉडल फ्रेमवर्क से लेकर हार्डवेयर तक अच्छा समर्थन है। इस अध्याय में, हम GenAI के लिए ONNX Runtime, OpenVINO, और Apple MLX पर केंद्रित होकर मॉडल क्वांटाइजेशन करेंगे (यदि आपके पास कोई बेहतर तरीका है, तो आप PR जमा करके हमें भी दे सकते हैं)।

**यह अध्याय सम्मिलित है**

1. [llama.cpp का उपयोग करके Phi-3.5 / 4 क्वांटाइज करना](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntime के लिए Generative AI एक्सटेंशन्स का उपयोग करके Phi-3.5 / 4 क्वांटाइज करना](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO का उपयोग करके Phi-3.5 / 4 क्वांटाइज करना](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX फ्रेमवर्क का उपयोग करके Phi-3.5 / 4 क्वांटाइज करना](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**अस्वीकरण**:  
इस दस्तावेज़ का अनुवाद AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके किया गया है। जबकि हम समानता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियाँ या असामर्थ्य हो सकती हैं। मूल दस्तावेज़ अपनी मूल भाषा में ही अधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।
<!-- CO-OP TRANSLATOR DISCLAIMER END -->