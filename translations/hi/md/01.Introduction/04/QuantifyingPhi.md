<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-07-16T21:43:53+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "hi"
}
-->
# **Phi परिवार का क्वांटिफिकेशन**

मॉडल क्वांटाइजेशन का मतलब है एक न्यूरल नेटवर्क मॉडल के पैरामीटर (जैसे वेट्स और एक्टिवेशन वैल्यूज) को एक बड़े मान रेंज (आमतौर पर निरंतर मान रेंज) से एक छोटे सीमित मान रेंज में मैप करना। यह तकनीक मॉडल के आकार और गणनात्मक जटिलता को कम कर सकती है और मोबाइल डिवाइस या एम्बेडेड सिस्टम जैसे संसाधन-सीमित वातावरण में मॉडल की कार्यक्षमता बढ़ा सकती है। मॉडल क्वांटाइजेशन पैरामीटर की प्रिसिजन को कम करके कम्प्रेशन हासिल करता है, लेकिन इससे कुछ हद तक प्रिसिजन की हानि भी होती है। इसलिए, क्वांटाइजेशन प्रक्रिया में मॉडल के आकार, गणनात्मक जटिलता और प्रिसिजन के बीच संतुलन बनाना जरूरी होता है। सामान्य क्वांटाइजेशन विधियों में फिक्स्ड-पॉइंट क्वांटाइजेशन, फ्लोटिंग-पॉइंट क्वांटाइजेशन आदि शामिल हैं। आप अपनी विशिष्ट स्थिति और आवश्यकताओं के अनुसार उपयुक्त क्वांटाइजेशन रणनीति चुन सकते हैं।

हम GenAI मॉडल को एज डिवाइस पर तैनात करना चाहते हैं ताकि अधिक डिवाइस GenAI परिदृश्यों में आ सकें, जैसे मोबाइल डिवाइस, AI PC/Copilot+PC, और पारंपरिक IoT डिवाइस। क्वांटाइजेशन मॉडल के माध्यम से, हम इसे विभिन्न एज डिवाइसों पर उनके अनुसार तैनात कर सकते हैं। हार्डवेयर निर्माताओं द्वारा प्रदान किए गए मॉडल एक्सेलेरेशन फ्रेमवर्क और क्वांटाइजेशन मॉडल के साथ मिलकर, हम बेहतर SLM एप्लिकेशन परिदृश्य बना सकते हैं।

क्वांटाइजेशन परिदृश्य में, हमारे पास विभिन्न प्रिसिजन विकल्प होते हैं (INT4, INT8, FP16, FP32)। नीचे सामान्यतः उपयोग किए जाने वाले क्वांटाइजेशन प्रिसिजन का विवरण दिया गया है।

### **INT4**

INT4 क्वांटाइजेशन एक कठोर क्वांटाइजेशन विधि है जो मॉडल के वेट्स और एक्टिवेशन वैल्यूज को 4-बिट पूर्णांकों में क्वांटाइज़ करता है। INT4 क्वांटाइजेशन आमतौर पर कम प्रतिनिधित्व रेंज और कम प्रिसिजन के कारण अधिक प्रिसिजन हानि करता है। हालांकि, INT8 क्वांटाइजेशन की तुलना में, INT4 क्वांटाइजेशन मॉडल के स्टोरेज आवश्यकताओं और गणनात्मक जटिलता को और भी कम कर सकता है। ध्यान देने वाली बात है कि व्यावहारिक अनुप्रयोगों में INT4 क्वांटाइजेशन अपेक्षाकृत कम होता है, क्योंकि बहुत कम सटीकता मॉडल के प्रदर्शन में महत्वपूर्ण गिरावट ला सकती है। इसके अलावा, सभी हार्डवेयर INT4 ऑपरेशंस का समर्थन नहीं करते, इसलिए क्वांटाइजेशन विधि चुनते समय हार्डवेयर संगतता पर विचार करना आवश्यक है।

### **INT8**

INT8 क्वांटाइजेशन वह प्रक्रिया है जिसमें मॉडल के वेट्स और एक्टिवेशन को फ्लोटिंग पॉइंट नंबर से 8-बिट पूर्णांकों में बदला जाता है। हालांकि INT8 पूर्णांक द्वारा दर्शाई गई संख्यात्मक सीमा छोटी और कम सटीक होती है, यह स्टोरेज और गणना आवश्यकताओं को काफी हद तक कम कर सकता है। INT8 क्वांटाइजेशन में, मॉडल के वेट्स और एक्टिवेशन वैल्यूज को स्केलिंग और ऑफसेट सहित क्वांटाइजेशन प्रक्रिया से गुजारा जाता है ताकि मूल फ्लोटिंग पॉइंट जानकारी को यथासंभव संरक्षित किया जा सके। इन्फरेंस के दौरान, ये क्वांटाइज़्ड मान फिर से फ्लोटिंग पॉइंट नंबर में डीक्वांटाइज़ किए जाते हैं, गणना की जाती है, और फिर अगले चरण के लिए वापस INT8 में क्वांटाइज़ किए जाते हैं। यह विधि अधिकांश अनुप्रयोगों में पर्याप्त सटीकता प्रदान करती है और उच्च गणनात्मक दक्षता बनाए रखती है।

### **FP16**

FP16 फॉर्मेट, यानी 16-बिट फ्लोटिंग पॉइंट नंबर (float16), 32-बिट फ्लोटिंग पॉइंट नंबर (float32) की तुलना में मेमोरी उपयोग को आधा कर देता है, जो बड़े पैमाने पर डीप लर्निंग अनुप्रयोगों में महत्वपूर्ण लाभ प्रदान करता है। FP16 फॉर्मेट के कारण, एक ही GPU मेमोरी सीमा के भीतर बड़े मॉडल लोड करना या अधिक डेटा प्रोसेस करना संभव होता है। आधुनिक GPU हार्डवेयर FP16 ऑपरेशंस का समर्थन करते रहते हैं, इसलिए FP16 फॉर्मेट का उपयोग कंप्यूटिंग गति में सुधार भी ला सकता है। हालांकि, FP16 फॉर्मेट की अपनी सीमाएं भी हैं, जैसे कम प्रिसिजन, जो कुछ मामलों में संख्यात्मक अस्थिरता या प्रिसिजन हानि का कारण बन सकती है।

### **FP32**

FP32 फॉर्मेट उच्च प्रिसिजन प्रदान करता है और व्यापक मानों की सटीक अभिव्यक्ति कर सकता है। जटिल गणितीय ऑपरेशंस या उच्च सटीकता वाले परिणामों की आवश्यकता वाले परिदृश्यों में FP32 फॉर्मेट को प्राथमिकता दी जाती है। हालांकि, उच्च सटीकता का मतलब अधिक मेमोरी उपयोग और लंबा गणना समय भी होता है। बड़े पैमाने पर डीप लर्निंग मॉडल के लिए, खासकर जब मॉडल पैरामीटर और डेटा की मात्रा बहुत अधिक हो, FP32 फॉर्मेट GPU मेमोरी की कमी या इन्फरेंस गति में कमी का कारण बन सकता है।

मोबाइल डिवाइस या IoT डिवाइस पर, हम Phi-3.x मॉडल को INT4 में कन्वर्ट कर सकते हैं, जबकि AI PC / Copilot PC उच्च प्रिसिजन जैसे INT8, FP16, FP32 का उपयोग कर सकते हैं।

वर्तमान में, विभिन्न हार्डवेयर निर्माता जनरेटिव मॉडल का समर्थन करने के लिए अलग-अलग फ्रेमवर्क प्रदान करते हैं, जैसे Intel का OpenVINO, Qualcomm का QNN, Apple का MLX, और Nvidia का CUDA आदि, जिन्हें मॉडल क्वांटाइजेशन के साथ मिलाकर लोकल डिप्लॉयमेंट पूरा किया जा सकता है।

तकनीकी दृष्टि से, क्वांटाइजेशन के बाद हमारे पास विभिन्न फॉर्मेट सपोर्ट होते हैं, जैसे PyTorch / Tensorflow फॉर्मेट, GGUF, और ONNX। मैंने GGUF और ONNX के बीच फॉर्मेट तुलना और अनुप्रयोग परिदृश्यों का अध्ययन किया है। यहां मैं ONNX क्वांटाइजेशन फॉर्मेट की सलाह देता हूं, जिसे मॉडल फ्रेमवर्क से लेकर हार्डवेयर तक अच्छा समर्थन प्राप्त है। इस अध्याय में, हम GenAI के लिए ONNX Runtime, OpenVINO, और Apple MLX पर मॉडल क्वांटाइजेशन पर ध्यान केंद्रित करेंगे (यदि आपके पास बेहतर तरीका हो, तो आप PR सबमिट करके हमें दे सकते हैं)।

**इस अध्याय में शामिल हैं**

1. [llama.cpp का उपयोग करके Phi-3.5 / 4 का क्वांटाइजेशन](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntime के लिए Generative AI एक्सटेंशन्स का उपयोग करके Phi-3.5 / 4 का क्वांटाइजेशन](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO का उपयोग करके Phi-3.5 / 4 का क्वांटाइजेशन](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX फ्रेमवर्क का उपयोग करके Phi-3.5 / 4 का क्वांटाइजेशन](./UsingAppleMLXQuantifyingPhi.md)

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियाँ या अशुद्धियाँ हो सकती हैं। मूल दस्तावेज़ अपनी मूल भाषा में ही अधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सलाह दी जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।