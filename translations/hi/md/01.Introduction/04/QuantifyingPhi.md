<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-05T01:45:44+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "hi"
}
-->
# **Phi परिवार का क्वांटाइज़ेशन**

Model quantization उस प्रक्रिया को कहते हैं जिसमें एक न्यूरल नेटवर्क मॉडल के पैरामीटर (जैसे weights और activation values) को एक बड़े मान दायरे (आमतौर पर एक सतत मान दायरा) से एक छोटे सीमित मान दायरे में मैप किया जाता है। यह तकनीक मॉडल के आकार और कम्प्यूटेशनल जटिलता को कम कर सकती है और मोबाइल डिवाइस या एम्बेडेड सिस्टम जैसे संसाधन-सीमित वातावरण में मॉडल की संचालन क्षमता में सुधार कर सकती है। मॉडल क्वांटाइज़ेशन पैरामीटर की प्रिसिजन घटाकर कम्प्रेशन हासिल करता है, लेकिन यह कुछ हद तक प्रिसिजन की हानि भी लाता है। इसलिए, क्वांटाइज़ेशन प्रक्रिया में मॉडल आकार, कम्प्यूटेशनल जटिलता और प्रिसिजन के बीच संतुलन बनाना आवश्यक होता है। सामान्य क्वांटाइज़ेशन विधियों में fixed-point quantization, floating-point quantization आदि शामिल हैं। आप विशिष्ट परिदृश्य और आवश्यकताओं के अनुसार उपयुक्त क्वांटाइज़ेशन रणनीति चुन सकते हैं।

हम GenAI मॉडल को एज डिवाइसों पर तैनात करने और अधिक डिवाइसों को GenAI परिदृश्यों में लाने की उम्मीद करते हैं, जैसे मोबाइल डिवाइस, AI PC/Copilot+PC, और पारंपरिक IoT डिवाइस। क्वांटाइज़ किए गए मॉडल के माध्यम से, हम इसे विभिन्न एज डिवाइसों पर डिवाइस के अनुसार तैनात कर सकते हैं। हार्डवेयर निर्माताओं द्वारा प्रदान किए गए मॉडल त्वरक फ़्रेमवर्क और क्वांटाइज़ेशन मॉडल के संयोजन से, हम बेहतर SLM एप्लिकेशन परिदृश्यों का निर्माण कर सकते हैं।

क्वांटाइज़ेशन परिदृश्य में, हमारे पास विभिन्न प्रिसीजन विकल्प होते हैं (INT4, INT8, FP16, FP32)। नीचे सामान्यतः उपयोग की जाने वाली क्वांटाइज़ेशन प्रिसीजन का विवरण दिया गया है

### **INT4**

INT4 क्वांटाइज़ेशन एक चरम क्वांटाइज़ेशन विधि है जो मॉडल के weights और activation values को 4-बिट इंटीजर में क्वांटाइज़ करती है। छोटे प्रतिनिधित्व दायरे और कम प्रिसिजन के कारण INT4 क्वांटाइज़ेशन आम तौर पर प्रिसिजन की अधिक हानि करता है। हालांकि, INT8 क्वांटाइज़ेशन की तुलना में, INT4 क्वांटाइज़ेशन मॉडल की भंडारण आवश्यकताओं और कम्प्यूटेशनल जटिलता को और अधिक कम कर सकता है। ध्यान देने वाली बात यह है कि व्यावहारिक अनुप्रयोगों में INT4 क्वांटाइज़ेशन अपेक्षाकृत दुर्लभ है, क्योंकि बहुत कम सटीकता मॉडल के प्रदर्शन में महत्वपूर्ण गिरावट का कारण बन सकती है। इसके अलावा, सभी हार्डवेयर INT4 ऑपरेशन्स का समर्थन नहीं करते, इसलिए कोई क्वांटाइज़ेशन विधि चुनते समय हार्डवेयर अनुकूलता पर विचार करना आवश्यक है।

### **INT8**

INT8 क्वांटाइज़ेशन वह प्रक्रिया है जिसमें मॉडल के weights और activations को फ्लोटिंग पॉइंट संख्याओं से 8-बिट इंटीजर में बदला जाता है। यद्यपि INT8 इंटीजर द्वारा प्रतिनिधित्व किया जाने वाला संख्यात्मक दायरा छोटा और कम सटीक होता है, यह भंडारण और गणना आवश्यकताओं को काफी हद तक कम कर सकता है। INT8 क्वांटाइज़ेशन में, मॉडल के weights और activation मानों पर स्केलिंग और ऑफ़सेट सहित एक क्वांटाइज़ेशन प्रक्रिया लागू होती है ताकि मूल फ्लोटिंग पॉइंट जानकारी को यथासंभव संरक्षित रखा जा सके। इनफरेंस के दौरान, ये क्वांटाइज़्ड मान गणना के लिए वापस फ्लोटिंग पॉइंट में डी-क्वांटाइज़ किए जाते हैं, और फिर अगले चरण के लिए फिर से INT8 में क्वांटाइज़ किए जाते हैं। यह तरीका अधिकांश अनुप्रयोगों में पर्याप्त सटीकता प्रदान कर सकता है जबकि उच्च कम्प्यूटेशनल दक्षता भी बनाए रखता है।

### **FP16**

FP16 फॉर्मेट, यानी 16-बिट फ्लोटिंग पॉइंट संख्याएँ (float16), 32-बिट फ्लोटिंग पॉइंट संख्याओं (float32) की तुलना में मेमोरी उपयोग को आधा कर देती है, जो बड़े पैमाने पर डीप लर्निंग अनुप्रयोगों में महत्वपूर्ण लाभ प्रदान करती है। FP16 फॉर्मेट एक ही GPU मेमोरी सीमाओं के भीतर बड़े मॉडल लोड करने या अधिक डेटा प्रोसेस करने की अनुमति देता है। जैसे-जैसे आधुनिक GPU हार्डवेयर FP16 ऑपरेशन्स का समर्थन करता जा रहा है, FP16 फॉर्मेट का उपयोग करने से गणना गति में भी सुधार हो सकता है। हालांकि, FP16 फॉर्मेट की अपनी अंतर्निहित कमजोरियाँ भी हैं, यानी कम प्रिसिजन, जो कुछ मामलों में संख्यात्मक अस्थिरता या प्रिसिजन हानि का कारण बन सकती है।

### **FP32**

FP32 फॉर्मेट उच्च प्रिसिजन प्रदान करता है और विभिन्न मूल्यों के विस्तृत रेंज को सटीक रूप से दर्शा सकता है। उन परिदृश्यों में जहाँ जटिल गणितीय ऑपरेशन्स किए जाते हैं या उच्च-प्रिसिजन परिणामों की आवश्यकता होती है, FP32 फॉर्मेट को प्राथमिकता दी जाती है। हालांकि, उच्च सटीकता का मतलब अधिक मेमोरी उपयोग और लंबा गणना समय भी होता है। विशेष रूप से जब मॉडल पैरामीटर अधिक हों और डेटा की मात्रा विशाल हो, तो FP32 फॉर्मेट GPU मेमोरी की अपर्याप्तता या इनफरेंस गति में कमी का कारण बन सकता है।

मोबाइल डिवाइसों या IoT डिवाइसों पर, हम Phi-3.x मॉडल्स को INT4 में बदल सकते हैं, जबकि AI PC / Copilot PC जैसे प्लेटफ़ॉर्म उच्च प्रिसीजन जैसे INT8, FP16, FP32 का उपयोग कर सकते हैं।

वर्तमान में, विभिन्न हार्डवेयर निर्माताओं के जनरेटिव मॉडल समर्थन के लिए अलग-अलग फ़्रेमवर्क हैं, जैसे Intel का OpenVINO, Qualcomm का QNN, Apple का MLX, और Nvidia का CUDA आदि, जिन्हें मॉडल क्वांटाइज़ेशन के साथ मिलाकर लोकल तैनाती पूरी की जा सकती है।

तकनीकी पक्ष से, क्वांटाइज़ेशन के बाद हमारे पास विभिन्न फॉर्मैट समर्थन होते हैं, जैसे PyTorch / TensorFlow फॉर्मैट, GGUF, और ONNX। मैंने GGUF और ONNX के बीच फॉर्मैट तुलना और उपयोग परिदृश्यों का विश्लेषण किया है। यहाँ मैं ONNX क्वांटाइज़ेशन फॉर्मैट की सिफारिश करता हूँ, जिसे मॉडल फ्रेमवर्क से लेकर हार्डवेयर तक अच्छा समर्थन प्राप्त है। इस अध्याय में, हम ONNX Runtime for GenAI, OpenVINO, और Apple MLX पर मॉडल क्वांटाइज़ेशन पर केंद्रित होंगे (यदि आपके पास बेहतर तरीका है, तो आप हमें PR सबमिट करके दे सकते हैं)।

**This chapter includes**

1. [llama.cpp का उपयोग करके Phi-3.5 / 4 को क्वांटाइज़ करना](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntime के लिए Generative AI एक्सटेंशन्स का उपयोग करके Phi-3.5 / 4 को क्वांटाइज़ करना](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO का उपयोग करके Phi-3.5 / 4 को क्वांटाइज़ करना](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX Framework का उपयोग करके Phi-3.5 / 4 को क्वांटाइज़ करना](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
अस्वीकरण:
यह दस्तावेज़ AI अनुवाद सेवा Co‑op Translator (https://github.com/Azure/co-op-translator) का उपयोग करके अनूदित किया गया है। हम सटीकता के लिए प्रयास करते हैं, पर कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियाँ या असंगतियाँ हो सकती हैं। मूल दस्तावेज़ को उसकी मूल भाषा में अधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए पेशेवर मानव अनुवाद की सलाह दी जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या दूसरी व्याख्या के लिए हम उत्तरदायी नहीं हैं।
<!-- CO-OP TRANSLATOR DISCLAIMER END -->