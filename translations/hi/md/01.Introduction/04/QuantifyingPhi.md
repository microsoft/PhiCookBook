<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "418c693c63cc0e817dc560558f730a7a",
  "translation_date": "2025-04-04T17:56:25+00:00",
  "source_file": "md\\01.Introduction\\04\\QuantifyingPhi.md",
  "language_code": "hi"
}
-->
# **Phi परिवार का मात्रात्मककरण**

मॉडल क्वांटाइजेशन का मतलब है कि न्यूरल नेटवर्क मॉडल में पैरामीटर (जैसे वज़न और सक्रियण मान) को एक बड़े मूल्य सीमा (आमतौर पर एक सतत मूल्य सीमा) से एक छोटे सीमित मूल्य सीमा में मैप करना। यह तकनीक मॉडल का आकार और गणनात्मक जटिलता को कम कर सकती है और इसे मोबाइल डिवाइस या एंबेडेड सिस्टम जैसे संसाधन-सीमित वातावरण में अधिक कुशलता से संचालित कर सकती है। मॉडल क्वांटाइजेशन पैरामीटर की सटीकता को कम करके संपीड़न प्राप्त करता है, लेकिन यह एक निश्चित सटीकता हानि भी लाता है। इसलिए, क्वांटाइजेशन प्रक्रिया में, मॉडल का आकार, गणनात्मक जटिलता, और सटीकता के बीच संतुलन बनाए रखना आवश्यक है। सामान्य क्वांटाइजेशन विधियों में फिक्स्ड-पॉइंट क्वांटाइजेशन, फ्लोटिंग-पॉइंट क्वांटाइजेशन आदि शामिल हैं। आप विशिष्ट परिदृश्य और आवश्यकताओं के अनुसार उपयुक्त क्वांटाइजेशन रणनीति चुन सकते हैं।

हम चाहते हैं कि GenAI मॉडल को एज डिवाइस पर तैनात किया जाए और अधिक डिवाइस को GenAI परिदृश्यों में प्रवेश करने की अनुमति दी जाए, जैसे मोबाइल डिवाइस, AI PC/Copilot+PC, और पारंपरिक IoT डिवाइस। क्वांटाइजेशन मॉडल के माध्यम से, हम इसे विभिन्न एज डिवाइस पर तैनात कर सकते हैं, जो विभिन्न डिवाइस पर आधारित होते हैं। हार्डवेयर निर्माताओं द्वारा प्रदान किए गए मॉडल एक्सेलेरेशन फ्रेमवर्क और क्वांटाइजेशन मॉडल को मिलाकर, हम बेहतर SLM एप्लिकेशन परिदृश्य बना सकते हैं।

क्वांटाइजेशन परिदृश्य में, हमारे पास विभिन्न सटीकताएँ होती हैं (INT4, INT8, FP16, FP32)। नीचे सामान्यतः उपयोग की जाने वाली क्वांटाइजेशन सटीकताओं का विवरण दिया गया है:

### **INT4**

INT4 क्वांटाइजेशन एक कट्टरपंथी क्वांटाइजेशन विधि है जो मॉडल के वज़न और सक्रियण मानों को 4-बिट पूर्णांक में बदलती है। INT4 क्वांटाइजेशन आमतौर पर बड़े सटीकता हानि का कारण बनता है क्योंकि इसका प्रतिनिधित्व सीमा छोटा होता है और सटीकता कम होती है। हालांकि, INT8 क्वांटाइजेशन की तुलना में, INT4 क्वांटाइजेशन मॉडल की संग्रहण आवश्यकताओं और गणनात्मक जटिलता को और कम कर सकता है। यह ध्यान देने योग्य है कि व्यावहारिक अनुप्रयोगों में INT4 क्वांटाइजेशन अपेक्षाकृत दुर्लभ है, क्योंकि अत्यधिक कम सटीकता मॉडल प्रदर्शन में महत्वपूर्ण गिरावट का कारण बन सकती है। इसके अलावा, सभी हार्डवेयर INT4 संचालन का समर्थन नहीं करते हैं, इसलिए क्वांटाइजेशन विधि चुनते समय हार्डवेयर संगतता पर विचार करना आवश्यक है।

### **INT8**

INT8 क्वांटाइजेशन प्रक्रिया में मॉडल के वज़न और सक्रियण को फ्लोटिंग पॉइंट नंबरों से 8-बिट पूर्णांकों में बदलना शामिल है। हालांकि INT8 पूर्णांकों द्वारा प्रतिनिधित्व किए गए संख्यात्मक सीमा छोटी और कम सटीक होती है, यह संग्रहण और गणना आवश्यकताओं को काफी कम कर सकती है। INT8 क्वांटाइजेशन में, मॉडल के वज़न और सक्रियण मानों को एक क्वांटाइजेशन प्रक्रिया से गुज़रना पड़ता है, जिसमें स्केलिंग और ऑफ़सेट शामिल होता है, ताकि मूल फ्लोटिंग पॉइंट जानकारी को यथासंभव संरक्षित किया जा सके। अनुमान के दौरान, इन क्वांटाइज किए गए मानों को गणना के लिए फ्लोटिंग पॉइंट नंबरों में वापस डी-क्वांटाइज किया जाएगा, और फिर अगले चरण के लिए INT8 में पुनः क्वांटाइज किया जाएगा। यह विधि अधिकांश अनुप्रयोगों में पर्याप्त सटीकता प्रदान कर सकती है जबकि उच्च गणनात्मक दक्षता बनाए रखती है।

### **FP16**

FP16 प्रारूप, अर्थात 16-बिट फ्लोटिंग पॉइंट नंबर (float16), 32-बिट फ्लोटिंग पॉइंट नंबर (float32) की तुलना में मेमोरी उपयोग को आधा कर देता है, जो बड़े पैमाने पर डीप लर्निंग अनुप्रयोगों में महत्वपूर्ण लाभ प्रदान करता है। FP16 प्रारूप बड़े मॉडल लोड करने या समान GPU मेमोरी सीमाओं के भीतर अधिक डेटा संसाधित करने की अनुमति देता है। जैसे-जैसे आधुनिक GPU हार्डवेयर FP16 संचालन का समर्थन करता है, FP16 प्रारूप का उपयोग करने से गणनात्मक गति में सुधार भी हो सकता है। हालांकि, FP16 प्रारूप की अपनी अंतर्निहित सीमाएँ भी हैं, जैसे कम सटीकता, जो कुछ मामलों में संख्यात्मक अस्थिरता या सटीकता हानि का कारण बन सकती है।

### **FP32**

FP32 प्रारूप उच्च सटीकता प्रदान करता है और व्यापक मूल्य सीमा को सटीक रूप से दर्शा सकता है। उन परिदृश्यों में जहाँ जटिल गणितीय संचालन किए जाते हैं या उच्च-सटीकता परिणामों की आवश्यकता होती है, FP32 प्रारूप को प्राथमिकता दी जाती है। हालांकि, उच्च सटीकता का मतलब अधिक मेमोरी उपयोग और लंबा गणना समय भी होता है। बड़े पैमाने पर डीप लर्निंग मॉडल के लिए, विशेष रूप से जब मॉडल पैरामीटर और डेटा की मात्रा बहुत अधिक होती है, FP32 प्रारूप GPU मेमोरी की कमी या अनुमान गति में कमी का कारण बन सकता है।

मोबाइल डिवाइस या IoT डिवाइस पर, हम Phi-3.x मॉडल को INT4 में बदल सकते हैं, जबकि AI PC / Copilot PC उच्च सटीकता जैसे INT8, FP16, FP32 का उपयोग कर सकते हैं।

वर्तमान में, विभिन्न हार्डवेयर निर्माताओं के पास जनरेटिव मॉडल का समर्थन करने के लिए अलग-अलग फ्रेमवर्क हैं, जैसे Intel का OpenVINO, Qualcomm का QNN, Apple का MLX, और Nvidia का CUDA। इन्हें मॉडल क्वांटाइजेशन के साथ मिलाकर स्थानीय तैनाती को पूरा किया जा सकता है।

तकनीकी दृष्टिकोण से, हमारे पास क्वांटाइजेशन के बाद विभिन्न प्रारूप समर्थन होते हैं, जैसे PyTorch / Tensorflow प्रारूप, GGUF, और ONNX। मैंने GGUF और ONNX के बीच प्रारूप तुलना और अनुप्रयोग परिदृश्य किए हैं। यहाँ मैं ONNX क्वांटाइजेशन प्रारूप की सिफारिश करता हूँ, जो मॉडल फ्रेमवर्क से हार्डवेयर तक अच्छा समर्थन प्रदान करता है। इस अध्याय में, हम GenAI के लिए ONNX Runtime, OpenVINO, और Apple MLX का उपयोग करके मॉडल क्वांटाइजेशन पर ध्यान केंद्रित करेंगे (यदि आपके पास बेहतर तरीका है, तो आप इसे हमें PR सबमिट करके भी दे सकते हैं)।

**इस अध्याय में शामिल हैं**

1. [Phi-3.5 / 4 का क्वांटाइजेशन llama.cpp का उपयोग करके](./UsingLlamacppQuantifyingPhi.md)

2. [Phi-3.5 / 4 का क्वांटाइजेशन Generative AI एक्सटेंशन का उपयोग करके onnxruntime के लिए](./UsingORTGenAIQuantifyingPhi.md)

3. [Phi-3.5 / 4 का क्वांटाइजेशन Intel OpenVINO का उपयोग करके](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Phi-3.5 / 4 का क्वांटाइजेशन Apple MLX Framework का उपयोग करके](./UsingAppleMLXQuantifyingPhi.md)

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता सुनिश्चित करने का प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या गलतियां हो सकती हैं। मूल दस्तावेज़, जो इसकी मूल भाषा में है, को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद का उपयोग करने से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।