<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-05-08T06:10:02+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "hi"
}
-->
# **Phi परिवार का क्वांटिफिकेशन**

मॉडल क्वांटाइज़ेशन का मतलब है न्यूरल नेटवर्क मॉडल के पैरामीटर्स (जैसे वेट्स और एक्टिवेशन वैल्यूज़) को एक बड़े वैल्यू रेंज (आमतौर पर निरंतर वैल्यू रेंज) से छोटे सीमित वैल्यू रेंज में मैप करना। यह तकनीक मॉडल के आकार और गणनात्मक जटिलता को कम कर सकती है और मोबाइल डिवाइस या एम्बेडेड सिस्टम जैसे संसाधन-सीमित वातावरण में मॉडल की कार्यक्षमता बढ़ा सकती है। मॉडल क्वांटाइज़ेशन पैरामीटर्स की प्रिसिजन कम करके कम्प्रेशन करता है, लेकिन इससे प्रिसिजन में कुछ हानि भी होती है। इसलिए क्वांटाइज़ेशन प्रक्रिया में मॉडल के आकार, गणनात्मक जटिलता और प्रिसिजन के बीच संतुलन बनाना जरूरी होता है। आमतौर पर उपयोग किए जाने वाले क्वांटाइज़ेशन तरीके हैं फिक्स्ड-पॉइंट क्वांटाइज़ेशन, फ्लोटिंग-पॉइंट क्वांटाइज़ेशन आदि। आप अपनी विशिष्ट स्थिति और जरूरत के अनुसार उचित क्वांटाइज़ेशन रणनीति चुन सकते हैं।

हम GenAI मॉडल को एज डिवाइस पर तैनात करना चाहते हैं ताकि अधिक डिवाइस GenAI परिदृश्यों में आ सकें, जैसे मोबाइल डिवाइस, AI PC/Copilot+PC, और पारंपरिक IoT डिवाइस। क्वांटाइज़ेशन मॉडल के माध्यम से, हम इसे विभिन्न एज डिवाइस पर डिवाइस के अनुसार तैनात कर सकते हैं। हार्डवेयर निर्माताओं द्वारा प्रदान किए गए मॉडल एक्सेलेरेशन फ्रेमवर्क और क्वांटाइज़ेशन मॉडल के साथ मिलकर, हम बेहतर SLM एप्लिकेशन परिदृश्य बना सकते हैं।

क्वांटाइज़ेशन परिदृश्य में हमारे पास विभिन्न प्रिसिजन होते हैं (INT4, INT8, FP16, FP32)। नीचे आमतौर पर इस्तेमाल होने वाले क्वांटाइज़ेशन प्रिसिजन की व्याख्या है।

### **INT4**

INT4 क्वांटाइज़ेशन एक कठोर क्वांटाइज़ेशन विधि है जो मॉडल के वेट्स और एक्टिवेशन वैल्यूज़ को 4-बिट इंटीजर में क्वांटाइज़ करता है। INT4 क्वांटाइज़ेशन आमतौर पर कम प्रतिनिधित्व रेंज और कम प्रिसिजन के कारण प्रिसिजन में ज्यादा हानि करता है। लेकिन INT8 क्वांटाइज़ेशन की तुलना में, INT4 क्वांटाइज़ेशन मॉडल की स्टोरेज जरूरतों और गणनात्मक जटिलता को और भी कम कर सकता है। ध्यान देने वाली बात है कि व्यावहारिक अनुप्रयोगों में INT4 क्वांटाइज़ेशन कम ही इस्तेमाल होता है, क्योंकि बहुत कम प्रिसिजन से मॉडल के प्रदर्शन में काफी गिरावट आ सकती है। साथ ही, सभी हार्डवेयर INT4 ऑपरेशन्स का समर्थन नहीं करते, इसलिए क्वांटाइज़ेशन विधि चुनते समय हार्डवेयर संगतता पर ध्यान देना जरूरी है।

### **INT8**

INT8 क्वांटाइज़ेशन वह प्रक्रिया है जिसमें मॉडल के वेट्स और एक्टिवेशन फ्लोटिंग पॉइंट नंबर से 8-बिट इंटीजर में बदले जाते हैं। हालांकि INT8 इंटीजर द्वारा दर्शाया गया संख्यात्मक रेंज छोटा और कम सटीक होता है, यह स्टोरेज और कैलकुलेशन की जरूरतों को काफी कम कर देता है। INT8 क्वांटाइज़ेशन में मॉडल के वेट्स और एक्टिवेशन वैल्यूज़ को स्केलिंग और ऑफसेट सहित क्वांटाइज़ेशन प्रक्रिया से गुजारा जाता है ताकि मूल फ्लोटिंग पॉइंट जानकारी को यथासंभव संरक्षित किया जा सके। इन्फरेंस के दौरान, ये क्वांटाइज़्ड वैल्यूज़ फिर से फ्लोटिंग पॉइंट में डी-क्वांटाइज़ की जाती हैं, गणना होती है, और फिर अगले स्टेप के लिए INT8 में वापस क्वांटाइज़ की जाती हैं। यह तरीका अधिकांश अनुप्रयोगों में पर्याप्त सटीकता प्रदान करता है और उच्च गणनात्मक दक्षता बनाए रखता है।

### **FP16**

FP16 फॉर्मेट, यानी 16-बिट फ्लोटिंग पॉइंट नंबर (float16), 32-बिट फ्लोटिंग पॉइंट नंबर (float32) की तुलना में मेमोरी उपयोग को आधा कर देता है, जो बड़े पैमाने पर डीप लर्निंग अनुप्रयोगों में महत्वपूर्ण फायदे देता है। FP16 फॉर्मेट के उपयोग से एक ही GPU मेमोरी सीमाओं के भीतर बड़े मॉडल लोड करना या अधिक डेटा प्रोसेस करना संभव होता है। जैसे-जैसे आधुनिक GPU हार्डवेयर FP16 ऑपरेशन्स का समर्थन करता है, FP16 फॉर्मेट का उपयोग करने से कंप्यूटिंग गति में भी सुधार हो सकता है। हालांकि, FP16 फॉर्मेट की अपनी सीमाएँ हैं, जैसे कम प्रिसिजन, जिससे कुछ मामलों में संख्यात्मक अस्थिरता या प्रिसिजन की हानि हो सकती है।

### **FP32**

FP32 फॉर्मेट उच्च प्रिसिजन प्रदान करता है और व्यापक वैल्यू रेंज को सटीक रूप से दर्शा सकता है। उन परिदृश्यों में जहाँ जटिल गणितीय ऑपरेशन्स किए जाते हैं या उच्च सटीकता के परिणाम चाहिए होते हैं, FP32 फॉर्मेट को प्राथमिकता दी जाती है। हालांकि, उच्च सटीकता का मतलब अधिक मेमोरी उपयोग और लंबा कैलकुलेशन समय भी होता है। बड़े पैमाने पर डीप लर्निंग मॉडल में, खासकर जब बहुत सारे मॉडल पैरामीटर और भारी डेटा होता है, FP32 फॉर्मेट GPU मेमोरी की कमी या इन्फरेंस गति में कमी का कारण बन सकता है।

मोबाइल डिवाइस या IoT डिवाइस पर, हम Phi-3.x मॉडल को INT4 में कन्वर्ट कर सकते हैं, जबकि AI PC / Copilot PC पर उच्च प्रिसिजन जैसे INT8, FP16, FP32 का उपयोग किया जा सकता है।

वर्तमान में, विभिन्न हार्डवेयर निर्माताओं के पास जनरेटिव मॉडल के लिए अलग-अलग फ्रेमवर्क हैं, जैसे Intel का OpenVINO, Qualcomm का QNN, Apple का MLX, और Nvidia का CUDA आदि, जिन्हें मॉडल क्वांटाइज़ेशन के साथ मिलाकर स्थानीय तैनाती पूरी की जा सकती है।

तकनीकी दृष्टि से, क्वांटाइज़ेशन के बाद हमारे पास विभिन्न फॉर्मेट सपोर्ट होते हैं, जैसे PyTorch / Tensorflow फॉर्मेट, GGUF, और ONNX। मैंने GGUF और ONNX के बीच फॉर्मेट तुलना और उपयोग परिदृश्यों पर काम किया है। यहाँ मैं ONNX क्वांटाइज़ेशन फॉर्मेट की सिफारिश करता हूँ, जिसे मॉडल फ्रेमवर्क से लेकर हार्डवेयर तक अच्छा समर्थन प्राप्त है। इस अध्याय में, हम GenAI के लिए ONNX Runtime, OpenVINO, और Apple MLX पर मॉडल क्वांटाइज़ेशन पर ध्यान केंद्रित करेंगे (अगर आपके पास बेहतर तरीका है, तो आप PR सबमिट करके हमें दे सकते हैं)।

**इस अध्याय में शामिल हैं**

1. [llama.cpp का उपयोग करके Phi-3.5 / 4 का क्वांटाइज़ेशन](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntime के लिए Generative AI एक्सटेंशन्स का उपयोग करके Phi-3.5 / 4 का क्वांटाइज़ेशन](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO का उपयोग करके Phi-3.5 / 4 का क्वांटाइज़ेशन](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX Framework का उपयोग करके Phi-3.5 / 4 का क्वांटाइज़ेशन](./UsingAppleMLXQuantifyingPhi.md)

**अस्वीकरण**:  
इस दस्तावेज़ का अनुवाद AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियाँ या अशुद्धियाँ हो सकती हैं। मूल दस्तावेज़ अपनी मूल भाषा में ही प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए पेशेवर मानव अनुवाद की सलाह दी जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।