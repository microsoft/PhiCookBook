<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d570fac7029d6697ad8ab1c963b43811",
  "translation_date": "2025-04-04T17:52:11+00:00",
  "source_file": "md\\01.Introduction\\03\\overview.md",
  "language_code": "hi"
}
-->
Phi-3-mini के संदर्भ में, इंफरेंस उस प्रक्रिया को संदर्भित करता है जिसमें मॉडल का उपयोग इनपुट डेटा के आधार पर भविष्यवाणी करने या आउटपुट उत्पन्न करने के लिए किया जाता है। आइए आपको Phi-3-mini और इसकी इंफरेंस क्षमताओं के बारे में अधिक जानकारी देते हैं।

Phi-3-mini, Microsoft द्वारा जारी Phi-3 मॉडल श्रृंखला का हिस्सा है। इन मॉडलों को छोटे भाषा मॉडलों (SLMs) के साथ नई संभावनाओं को परिभाषित करने के लिए डिज़ाइन किया गया है।

यहाँ Phi-3-mini और इसकी इंफरेंस क्षमताओं के बारे में कुछ मुख्य बिंदु दिए गए हैं:

## **Phi-3-mini का परिचय:**
- Phi-3-mini में 3.8 बिलियन पैरामीटर हैं।
- यह न केवल पारंपरिक कंप्यूटिंग उपकरणों पर चलता है, बल्कि मोबाइल डिवाइस और IoT डिवाइस जैसे एज डिवाइस पर भी काम कर सकता है।
- Phi-3-mini की रिलीज़ से व्यक्तियों और उद्यमों को संसाधन-सीमित वातावरण में विभिन्न हार्डवेयर उपकरणों पर SLMs को तैनात करने की सुविधा मिलती है।
- यह विभिन्न मॉडल प्रारूपों को कवर करता है, जिनमें पारंपरिक PyTorch प्रारूप, gguf प्रारूप का क्वांटाइज़्ड संस्करण, और ONNX-आधारित क्वांटाइज़्ड संस्करण शामिल हैं।

## **Phi-3-mini तक पहुँच:**
Phi-3-mini तक पहुँचने के लिए, आप [Semantic Kernel](https://github.com/microsoft/SemanticKernelCookBook?WT.mc_id=aiml-138114-kinfeylo) का उपयोग Copilot एप्लिकेशन में कर सकते हैं। Semantic Kernel आम तौर पर Azure OpenAI Service, Hugging Face पर उपलब्ध ओपन-सोर्स मॉडल्स, और लोकल मॉडल्स के साथ संगत है।  
आप [Ollama](https://ollama.com) या [LlamaEdge](https://llamaedge.com) का उपयोग करके क्वांटाइज़्ड मॉडल्स को कॉल कर सकते हैं। Ollama व्यक्तिगत उपयोगकर्ताओं को विभिन्न क्वांटाइज़्ड मॉडल्स को कॉल करने की अनुमति देता है, जबकि LlamaEdge GGUF मॉडल्स के लिए क्रॉस-प्लेटफ़ॉर्म उपलब्धता प्रदान करता है।

## **क्वांटाइज़्ड मॉडल्स:**
कई उपयोगकर्ता लोकल इंफरेंस के लिए क्वांटाइज़्ड मॉडल्स का उपयोग करना पसंद करते हैं। उदाहरण के लिए, आप सीधे Ollama run Phi-3 चला सकते हैं या इसे ऑफलाइन Modelfile का उपयोग करके कॉन्फ़िगर कर सकते हैं। Modelfile GGUF फाइल पथ और प्रॉम्प्ट प्रारूप निर्दिष्ट करता है।

## **जनरेटिव AI की संभावनाएँ:**
Phi-3-mini जैसे SLMs को मिलाकर जनरेटिव AI के लिए नई संभावनाएँ खुलती हैं। इंफरेंस केवल पहला कदम है; इन मॉडलों का उपयोग संसाधन-सीमित, कम विलंबता, और लागत-संवेदनशील परिदृश्यों में विभिन्न कार्यों के लिए किया जा सकता है।

## **Phi-3-mini के साथ जनरेटिव AI को अनलॉक करना: इंफरेंस और परिनियोजन के लिए गाइड** 
Semantic Kernel, Ollama/LlamaEdge, और ONNX Runtime का उपयोग करके Phi-3-mini मॉडल्स तक कैसे पहुँचा जाए और इंफरेंस किया जाए, और विभिन्न एप्लिकेशन परिदृश्यों में जनरेटिव AI की संभावनाओं का पता लगाएं।

**विशेषताएँ**  
Phi-3-mini मॉडल का इंफरेंस इन प्लेटफ़ॉर्म्स पर करें:

- [Semantic Kernel](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/semantickernel?WT.mc_id=aiml-138114-kinfeylo)  
- [Ollama](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ollama?WT.mc_id=aiml-138114-kinfeylo)  
- [LlamaEdge WASM](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm?WT.mc_id=aiml-138114-kinfeylo)  
- [ONNX Runtime](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/onnx?WT.mc_id=aiml-138114-kinfeylo)  
- [iOS](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ios?WT.mc_id=aiml-138114-kinfeylo)  

संक्षेप में, Phi-3-mini डेवलपर्स को विभिन्न मॉडल प्रारूपों का पता लगाने और विभिन्न एप्लिकेशन परिदृश्यों में जनरेटिव AI का लाभ उठाने की अनुमति देता है।

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता सुनिश्चित करने का प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल दस्तावेज़, जो उसकी मूल भाषा में है, को आधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।