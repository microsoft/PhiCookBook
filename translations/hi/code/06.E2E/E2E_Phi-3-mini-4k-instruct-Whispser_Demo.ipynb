{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## इंटरएक्टिव Phi 3 Mini 4K इंस्ट्रक्ट चैटबॉट विद व्हिस्पर\n",
    "\n",
    "### परिचय:\n",
    "इंटरएक्टिव Phi 3 Mini 4K इंस्ट्रक्ट चैटबॉट एक उपकरण है जो उपयोगकर्ताओं को Microsoft Phi 3 Mini 4K इंस्ट्रक्ट डेमो के साथ टेक्स्ट या ऑडियो इनपुट के माध्यम से बातचीत करने की अनुमति देता है। इस चैटबॉट का उपयोग विभिन्न कार्यों के लिए किया जा सकता है, जैसे अनुवाद, मौसम अपडेट, और सामान्य जानकारी जुटाना।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[अपने Huggingface एक्सेस टोकन बनाएं](https://huggingface.co/settings/tokens)\n",
    "\n",
    "एक नया टोकन बनाएं  \n",
    "एक नया नाम प्रदान करें  \n",
    "लिखने की अनुमति चुनें  \n",
    "टोकन को कॉपी करें और इसे सुरक्षित स्थान पर सहेजें  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "निम्नलिखित Python कोड दो मुख्य कार्य करता है: `os` मॉड्यूल को आयात करना और एक पर्यावरण चर सेट करना।\n",
    "\n",
    "1. `os` मॉड्यूल को आयात करना:\n",
    "   - Python में `os` मॉड्यूल ऑपरेटिंग सिस्टम के साथ इंटरैक्ट करने का तरीका प्रदान करता है। यह आपको विभिन्न ऑपरेटिंग सिस्टम से संबंधित कार्य करने की अनुमति देता है, जैसे पर्यावरण चर तक पहुंचना, फाइलों और डायरेक्टरी के साथ काम करना आदि।\n",
    "   - इस कोड में, `os` मॉड्यूल को `import` स्टेटमेंट का उपयोग करके आयात किया गया है। यह स्टेटमेंट `os` मॉड्यूल की कार्यक्षमता को वर्तमान Python स्क्रिप्ट में उपयोग के लिए उपलब्ध कराता है।\n",
    "\n",
    "2. पर्यावरण चर सेट करना:\n",
    "   - पर्यावरण चर एक मान होता है जिसे ऑपरेटिंग सिस्टम पर चलने वाले प्रोग्राम एक्सेस कर सकते हैं। यह कॉन्फ़िगरेशन सेटिंग्स या अन्य जानकारी को स्टोर करने का एक तरीका है जिसे कई प्रोग्राम उपयोग कर सकते हैं।\n",
    "   - इस कोड में, एक नया पर्यावरण चर `os.environ` डिक्शनरी का उपयोग करके सेट किया जा रहा है। डिक्शनरी की कुंजी `'HF_TOKEN'` है, और इसका मान `HUGGINGFACE_TOKEN` वेरिएबल से असाइन किया गया है।\n",
    "   - `HUGGINGFACE_TOKEN` वेरिएबल इस कोड स्निपेट के ठीक ऊपर परिभाषित किया गया है, और इसे `#@param` सिंटैक्स का उपयोग करके `\"hf_**************\"` स्ट्रिंग मान असाइन किया गया है। यह सिंटैक्स अक्सर Jupyter नोटबुक में उपयोगकर्ता इनपुट और पैरामीटर कॉन्फ़िगरेशन को सीधे नोटबुक इंटरफ़ेस में अनुमति देने के लिए उपयोग किया जाता है।\n",
    "   - `'HF_TOKEN'` पर्यावरण चर सेट करके, इसे प्रोग्राम के अन्य भागों या उसी ऑपरेटिंग सिस्टम पर चलने वाले अन्य प्रोग्रामों द्वारा एक्सेस किया जा सकता है।\n",
    "\n",
    "कुल मिलाकर, यह कोड `os` मॉड्यूल को आयात करता है और `'HF_TOKEN'` नामक एक पर्यावरण चर सेट करता है, जिसका मान `HUGGINGFACE_TOKEN` वेरिएबल में प्रदान किया गया है।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "यह कोड स्निपेट एक फ़ंक्शन clear_output को परिभाषित करता है, जिसका उपयोग Jupyter Notebook या IPython में वर्तमान सेल के आउटपुट को साफ़ करने के लिए किया जाता है। आइए कोड को समझें और इसकी कार्यक्षमता को विस्तार से देखें:\n",
    "\n",
    "फ़ंक्शन clear_output एक पैरामीटर लेता है जिसे wait कहा जाता है, जो एक बूलियन मान है। डिफ़ॉल्ट रूप से, wait False पर सेट होता है। यह पैरामीटर निर्धारित करता है कि क्या फ़ंक्शन को मौजूदा आउटपुट को साफ़ करने से पहले नए आउटपुट के उपलब्ध होने का इंतजार करना चाहिए।\n",
    "\n",
    "यह फ़ंक्शन मुख्य रूप से वर्तमान सेल के आउटपुट को साफ़ करने के लिए उपयोग किया जाता है। Jupyter Notebook या IPython में, जब कोई सेल आउटपुट उत्पन्न करता है, जैसे कि प्रिंट किया गया टेक्स्ट या ग्राफिकल प्लॉट्स, तो वह आउटपुट सेल के नीचे प्रदर्शित होता है। clear_output फ़ंक्शन आपको उस आउटपुट को साफ़ करने की अनुमति देता है।\n",
    "\n",
    "कोड स्निपेट में फ़ंक्शन का वास्तविक कार्यान्वयन प्रदान नहीं किया गया है, जैसा कि एलिप्सिस (...) द्वारा संकेतित है। एलिप्सिस उस वास्तविक कोड के लिए एक प्लेसहोल्डर है जो आउटपुट को साफ़ करने का कार्य करता है। फ़ंक्शन का कार्यान्वयन Jupyter Notebook या IPython API के साथ इंटरैक्ट करके सेल से मौजूदा आउटपुट को हटाने में शामिल हो सकता है।\n",
    "\n",
    "कुल मिलाकर, यह फ़ंक्शन Jupyter Notebook या IPython में वर्तमान सेल के आउटपुट को साफ़ करने का एक सुविधाजनक तरीका प्रदान करता है, जिससे इंटरैक्टिव कोडिंग सत्रों के दौरान प्रदर्शित आउटपुट को प्रबंधित और अपडेट करना आसान हो जाता है।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "एज टीटीएस सेवा का उपयोग करके टेक्स्ट-टू-स्पीच (TTS) करने के लिए निम्नलिखित कार्यान्वयन एक-एक करके समझते हैं:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: यह फ़ंक्शन एक इनपुट मान लेता है और TTS आवाज़ के लिए गति स्ट्रिंग की गणना करता है। इनपुट मान वांछित भाषण गति को दर्शाता है, जहां 1 सामान्य गति का प्रतिनिधित्व करता है। यह फ़ंक्शन इनपुट मान से 1 घटाता है, इसे 100 से गुणा करता है, और फिर यह निर्धारित करता है कि इनपुट मान 1 से अधिक या बराबर है या नहीं। यह फ़ंक्शन गति स्ट्रिंग को \"{sign}{rate}\" प्रारूप में लौटाता है।\n",
    "\n",
    "2. `make_chunks(input_text, language)`: यह फ़ंक्शन इनपुट टेक्स्ट और भाषा को पैरामीटर के रूप में लेता है। यह भाषा-विशिष्ट नियमों के आधार पर टेक्स्ट को टुकड़ों में विभाजित करता है। इस कार्यान्वयन में, यदि भाषा \"English\" है, तो फ़ंक्शन टेक्स्ट को प्रत्येक पूर्णविराम (\".\") पर विभाजित करता है और किसी भी अग्रणी या पिछली खाली जगह को हटा देता है। फिर यह प्रत्येक टुकड़े में पूर्णविराम जोड़ता है और फ़िल्टर किए गए टुकड़ों की सूची लौटाता है।\n",
    "\n",
    "3. `tts_file_name(text)`: यह फ़ंक्शन इनपुट टेक्स्ट के आधार पर TTS ऑडियो फ़ाइल के लिए एक फ़ाइल नाम उत्पन्न करता है। यह टेक्स्ट पर कई परिवर्तन करता है: यदि मौजूद हो तो अंतिम पूर्णविराम को हटाना, टेक्स्ट को लोअरकेस में बदलना, अग्रणी और पिछली खाली जगह को हटाना, और रिक्त स्थान को अंडरस्कोर में बदलना। फिर यह टेक्स्ट को अधिकतम 25 वर्णों तक छोटा करता है (यदि लंबा हो) या यदि टेक्स्ट खाली है तो पूर्ण टेक्स्ट का उपयोग करता है। अंत में, यह [`uuid`] मॉड्यूल का उपयोग करके एक रैंडम स्ट्रिंग उत्पन्न करता है और इसे छोटा किए गए टेक्स्ट के साथ जोड़कर फ़ाइल नाम \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\" प्रारूप में बनाता है।\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: यह फ़ंक्शन कई ऑडियो फ़ाइलों को एक ऑडियो फ़ाइल में मर्ज करता है। यह ऑडियो फ़ाइल पथों की सूची और आउटपुट पथ को पैरामीटर के रूप में लेता है। फ़ंक्शन एक खाली `AudioSegment` ऑब्जेक्ट [`merged_audio`] को प्रारंभ करता है। फिर यह प्रत्येक ऑडियो फ़ाइल पथ के माध्यम से पुनरावृत्ति करता है, `pydub` लाइब्रेरी के `AudioSegment.from_file()` विधि का उपयोग करके ऑडियो फ़ाइल को लोड करता है, और वर्तमान ऑडियो फ़ाइल को [`merged_audio`] ऑब्जेक्ट में जोड़ता है। अंत में, यह मर्ज किए गए ऑडियो को निर्दिष्ट आउटपुट पथ पर MP3 प्रारूप में निर्यात करता है।\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: यह फ़ंक्शन एज टीटीएस सेवा का उपयोग करके TTS ऑपरेशन करता है। यह टेक्स्ट टुकड़ों की सूची, भाषण की गति, आवाज़ का नाम, और सेव पथ को पैरामीटर के रूप में लेता है। यदि टुकड़ों की संख्या 1 से अधिक है, तो फ़ंक्शन व्यक्तिगत टुकड़ा ऑडियो फ़ाइलों को संग्रहीत करने के लिए एक निर्देशिका बनाता है। फिर यह प्रत्येक टुकड़े के माध्यम से पुनरावृत्ति करता है, `calculate_rate_string()` फ़ंक्शन, आवाज़ का नाम, और टुकड़ा टेक्स्ट का उपयोग करके एज टीटीएस कमांड बनाता है, और `os.system()` फ़ंक्शन का उपयोग करके कमांड को निष्पादित करता है। यदि कमांड निष्पादन सफल होता है, तो यह उत्पन्न ऑडियो फ़ाइल के पथ को एक सूची में जोड़ता है। सभी टुकड़ों को संसाधित करने के बाद, यह व्यक्तिगत ऑडियो फ़ाइलों को `merge_audio_files()` फ़ंक्शन का उपयोग करके मर्ज करता है और मर्ज किए गए ऑडियो को निर्दिष्ट सेव पथ पर सहेजता है। यदि केवल एक टुकड़ा है, तो यह सीधे एज टीटीएस कमांड उत्पन्न करता है और ऑडियो को सेव पथ पर सहेजता है। अंत में, यह उत्पन्न ऑडियो फ़ाइल के सेव पथ को लौटाता है।\n",
    "\n",
    "6. `random_audio_name_generate()`: यह फ़ंक्शन [`uuid`] मॉड्यूल का उपयोग करके एक रैंडम ऑडियो फ़ाइल नाम उत्पन्न करता है। यह एक रैंडम UUID उत्पन्न करता है, इसे स्ट्रिंग में बदलता है, पहले 8 वर्ण लेता है, \".mp3\" एक्सटेंशन जोड़ता है, और रैंडम ऑडियो फ़ाइल नाम लौटाता है।\n",
    "\n",
    "7. `talk(input_text)`: यह फ़ंक्शन TTS ऑपरेशन करने के लिए मुख्य प्रवेश बिंदु है। यह इनपुट टेक्स्ट को पैरामीटर के रूप में लेता है। यह पहले इनपुट टेक्स्ट की लंबाई की जांच करता है ताकि यह निर्धारित किया जा सके कि यह एक लंबा वाक्य है (600 वर्णों से अधिक या बराबर)। लंबाई और `translate_text_flag` वेरिएबल के मान के आधार पर, यह भाषा निर्धारित करता है और `make_chunks()` फ़ंक्शन का उपयोग करके टेक्स्ट टुकड़ों की सूची उत्पन्न करता है। फिर यह `random_audio_name_generate()` फ़ंक्शन का उपयोग करके ऑडियो फ़ाइल के लिए सेव पथ उत्पन्न करता है। अंत में, यह TTS ऑपरेशन करने के लिए `edge_free_tts()` फ़ंक्शन को कॉल करता है और उत्पन्न ऑडियो फ़ाइल के सेव पथ को लौटाता है।\n",
    "\n",
    "कुल मिलाकर, ये फ़ंक्शन इनपुट टेक्स्ट को टुकड़ों में विभाजित करने, ऑडियो फ़ाइल के लिए फ़ाइल नाम उत्पन्न करने, एज टीटीएस सेवा का उपयोग करके TTS ऑपरेशन करने, और व्यक्तिगत ऑडियो फ़ाइलों को एक ऑडियो फ़ाइल में मर्ज करने के लिए मिलकर काम करते हैं।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "दो फंक्शन्स: convert_to_text और run_text_prompt का कार्यान्वयन, और दो क्लासेस: str और Audio की घोषणा।\n",
    "\n",
    "convert_to_text फंक्शन एक audio_path को इनपुट के रूप में लेता है और whisper_model नामक मॉडल का उपयोग करके ऑडियो को टेक्स्ट में ट्रांसक्राइब करता है। यह फंक्शन पहले जांचता है कि gpu फ्लैग True सेट है या नहीं। अगर यह True है, तो whisper_model को कुछ पैरामीटर्स जैसे word_timestamps=True, fp16=True, language='English', और task='translate' के साथ उपयोग किया जाता है। अगर gpu फ्लैग False है, तो whisper_model को fp16=False के साथ उपयोग किया जाता है। प्राप्त ट्रांसक्रिप्शन को 'scan.txt' नामक फाइल में सेव किया जाता है और टेक्स्ट के रूप में रिटर्न किया जाता है।\n",
    "\n",
    "run_text_prompt फंक्शन एक message और chat_history को इनपुट के रूप में लेता है। यह phi_demo फंक्शन का उपयोग करके इनपुट message के आधार पर एक चैटबॉट से प्रतिक्रिया उत्पन्न करता है। उत्पन्न प्रतिक्रिया को talk फंक्शन में पास किया जाता है, जो प्रतिक्रिया को ऑडियो फाइल में बदलता है और फाइल पथ रिटर्न करता है। Audio क्लास का उपयोग ऑडियो फाइल को दिखाने और चलाने के लिए किया जाता है। ऑडियो को IPython.display मॉड्यूल के display फंक्शन का उपयोग करके दिखाया जाता है, और Audio ऑब्जेक्ट autoplay=True पैरामीटर के साथ बनाया जाता है ताकि ऑडियो स्वतः चलना शुरू हो जाए। chat_history को इनपुट message और उत्पन्न प्रतिक्रिया के साथ अपडेट किया जाता है, और एक खाली स्ट्रिंग और अपडेटेड chat_history रिटर्न किया जाता है।\n",
    "\n",
    "str क्लास Python में एक बिल्ट-इन क्लास है जो कैरेक्टर्स की एक सीक्वेंस को रिप्रेजेंट करता है। यह स्ट्रिंग्स को मैनिपुलेट और वर्क करने के लिए विभिन्न मेथड्स प्रदान करता है, जैसे capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill, और अन्य। ये मेथड्स आपको सर्चिंग, रिप्लेसिंग, फॉर्मेटिंग, और स्ट्रिंग्स को मैनिपुलेट करने जैसे ऑपरेशन्स करने की अनुमति देते हैं।\n",
    "\n",
    "Audio क्लास एक कस्टम क्लास है जो एक ऑडियो ऑब्जेक्ट को रिप्रेजेंट करता है। इसका उपयोग Jupyter Notebook एनवायरनमेंट में ऑडियो प्लेयर बनाने के लिए किया जाता है। यह क्लास विभिन्न पैरामीटर्स को स्वीकार करती है जैसे data, filename, url, embed, rate, autoplay, और normalize। data पैरामीटर एक numpy array, सैंपल्स की लिस्ट, एक filename या URL को रिप्रेजेंट करने वाला स्ट्रिंग, या raw PCM डेटा हो सकता है। filename पैरामीटर का उपयोग लोकल फाइल से ऑडियो डेटा लोड करने के लिए किया जाता है, और url पैरामीटर का उपयोग ऑडियो डेटा को डाउनलोड करने के लिए किया जाता है। embed पैरामीटर यह निर्धारित करता है कि ऑडियो डेटा को data URI का उपयोग करके एम्बेड किया जाए या ओरिजिनल सोर्स से रेफरेंस किया जाए। rate पैरामीटर ऑडियो डेटा की सैंपलिंग दर को निर्दिष्ट करता है। autoplay पैरामीटर यह निर्धारित करता है कि ऑडियो स्वतः चलना शुरू हो जाए। normalize पैरामीटर यह निर्दिष्ट करता है कि ऑडियो डेटा को अधिकतम संभव रेंज में नॉर्मलाइज़ (रीस्केल) किया जाए या नहीं। Audio क्लास reload जैसे मेथड्स प्रदान करती है ताकि फाइल या URL से ऑडियो डेटा को फिर से लोड किया जा सके, और src_attr, autoplay_attr, और element_id_attr जैसे एट्रिब्यूट्स प्रदान करती है ताकि HTML में ऑडियो एलिमेंट के लिए संबंधित एट्रिब्यूट्स को प्राप्त किया जा सके।\n",
    "\n",
    "कुल मिलाकर, ये फंक्शन्स और क्लासेस ऑडियो को टेक्स्ट में ट्रांसक्राइब करने, चैटबॉट से ऑडियो प्रतिक्रियाएं उत्पन्न करने, और Jupyter Notebook एनवायरनमेंट में ऑडियो को दिखाने और चलाने के लिए उपयोग किए जाते हैं।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**अस्वीकरण**:  \nयह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता सुनिश्चित करने का प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में उपलब्ध मूल दस्तावेज़ को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T16:40:31+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "hi"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}