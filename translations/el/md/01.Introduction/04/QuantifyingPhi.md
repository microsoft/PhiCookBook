<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-05T12:12:48+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "el"
}
-->
# **Ποσοτικοποίηση της Οικογένειας Phi**

Η ποσοτικοποίηση μοντέλου αναφέρεται στη διαδικασία χαρτογράφησης των παραμέτρων (όπως βάρη και τιμές ενεργοποίησης) σε ένα μοντέλο νευρωνικού δικτύου από ένα μεγάλο εύρος τιμών (συνήθως ένα συνεχές εύρος τιμών) σε ένα μικρότερο πεπερασμένο εύρος τιμών. Αυτή η τεχνολογία μπορεί να μειώσει το μέγεθος και την υπολογιστική πολυπλοκότητα του μοντέλου και να βελτιώσει την αποδοτικότητα λειτουργίας του μοντέλου σε περιβάλλοντα με περιορισμένους πόρους, όπως κινητές συσκευές ή ενσωματωμένα συστήματα. Η ποσοτικοποίηση μοντέλου επιτυγχάνει συμπίεση μειώνοντας την ακρίβεια των παραμέτρων, αλλά εισάγει και μια ορισμένη απώλεια ακρίβειας. Επομένως, στη διαδικασία ποσοτικοποίησης, είναι απαραίτητο να εξισορροπηθούν το μέγεθος του μοντέλου, η υπολογιστική πολυπλοκότητα και η ακρίβεια. Κοινές μέθοδοι ποσοτικοποίησης περιλαμβάνουν ποσοτικοποίηση σταθερής υποδιαστολής, ποσοτικοποίηση κινητής υποδιαστολής κ.ά. Μπορείτε να επιλέξετε την κατάλληλη στρατηγική ποσοτικοποίησης σύμφωνα με το συγκεκριμένο σενάριο και ανάγκες.

Ελπίζουμε να αναπτύξουμε μοντέλα GenAI σε συσκευές άκρου και να επιτρέψουμε σε περισσότερες συσκευές να συμμετέχουν σε σενάρια GenAI, όπως κινητές συσκευές, AI PC/Copilot+PC και παραδοσιακές συσκευές IoT. Μέσω του ποσοτικοποιημένου μοντέλου, μπορούμε να το αναπτύξουμε σε διαφορετικές συσκευές άκρου αναλόγως των συσκευών. Συνδυαστικά με το πλαίσιο επιτάχυνσης μοντέλου και το ποσοτικοποιημένο μοντέλο που παρέχουν οι κατασκευαστές υλικού, μπορούμε να δημιουργήσουμε καλύτερα σενάρια εφαρμογών SLM.

Σε σενάρια ποσοτικοποίησης, έχουμε διαφορετικές ακριβείας (INT4, INT8, FP16, FP32). Ακολουθεί μια εξήγηση των πιο συχνά χρησιμοποιούμενων ακριβείας ποσοτικοποίησης.

### **INT4**

Η ποσοτικοποίηση INT4 είναι μια ριζική μέθοδος ποσοτικοποίησης που μετατρέπει τα βάρη και τις τιμές ενεργοποίησης του μοντέλου σε ακέραιους 4-bit. Η ποσοτικοποίηση INT4 συνήθως οδηγεί σε μεγαλύτερη απώλεια ακρίβειας λόγω του μικρότερου εύρους αναπαράστασης και χαμηλότερης ακρίβειας. Ωστόσο, σε σύγκριση με την ποσοτικοποίηση INT8, η ποσοτικοποίηση INT4 μπορεί να μειώσει περαιτέρω τις απαιτήσεις αποθήκευσης και την υπολογιστική πολυπλοκότητα του μοντέλου. Αξίζει να σημειωθεί ότι η ποσοτικοποίηση INT4 είναι σχετικά σπάνια σε πρακτικές εφαρμογές, καθώς πολύ χαμηλή ακρίβεια μπορεί να προκαλέσει σημαντική υποβάθμιση της απόδοσης του μοντέλου. Επιπλέον, δεν υποστηρίζουν όλα τα υλικά την εκτέλεση εντολών INT4, επομένως η συμβατότητα υλικού πρέπει να ληφθεί υπόψη κατά την επιλογή μεθόδου ποσοτικοποίησης.

### **INT8**

Η ποσοτικοποίηση INT8 είναι η διαδικασία μετατροπής των βαρών και των ενεργοποιήσεων ενός μοντέλου από αριθμούς κινητής υποδιαστολής σε ακέραιους 8-bit. Παρόλο που το αριθμητικό εύρος που αναπαρίστανται από ακέραιους INT8 είναι μικρότερο και λιγότερο ακριβές, μπορεί να μειώσει σημαντικά τις απαιτήσεις αποθήκευσης και υπολογισμού. Στην ποσοτικοποίηση INT8, τα βάρη και οι τιμές ενεργοποίησης του μοντέλου περνούν από μια διαδικασία ποσοτικοποίησης, που περιλαμβάνει κλιμάκωση και απόκλιση, για να διατηρηθούν όσο το δυνατόν περισσότερες πληροφορίες κινητής υποδιαστολής. Κατά τη διάρκεια της εκτέλεσης, αυτές οι ποσοτικοποιημένες τιμές θα αποποσοτικοποιηθούν ξανά σε αριθμούς κινητής υποδιαστολής για υπολογισμό και στη συνέχεια θα ποσοτικοποιηθούν ξανά σε INT8 για το επόμενο βήμα. Αυτή η μέθοδος μπορεί να προσφέρει επαρκή ακρίβεια στις περισσότερες εφαρμογές, διατηρώντας ταυτόχρονα υψηλή υπολογιστική αποδοτικότητα.

### **FP16**

Η μορφή FP16, δηλαδή αριθμοί κινητής υποδιαστολής 16-bit (float16), μειώνει το αποτύπωμα μνήμης κατά το ήμισυ σε σύγκριση με αριθμούς κινητής υποδιαστολής 32-bit (float32), γεγονός που έχει σημαντικά πλεονεκτήματα σε εφαρμογές μεγάλης κλίμακας βαθιάς μάθησης. Η μορφή FP16 επιτρέπει τη φόρτωση μεγαλύτερων μοντέλων ή την επεξεργασία περισσότερων δεδομένων εντός των ίδιων ορίων μνήμης GPU. Καθώς το σύγχρονο υλικό GPU συνεχίζει να υποστηρίζει λειτουργίες FP16, η χρήση της μορφής FP16 μπορεί επίσης να βελτιώσει την ταχύτητα υπολογισμού. Ωστόσο, η μορφή FP16 έχει και εγγενείς μειονεκτήματα, δηλαδή χαμηλότερη ακρίβεια, που μπορεί να οδηγήσει σε αριθμητική αστάθεια ή απώλεια ακρίβειας σε ορισμένες περιπτώσεις.

### **FP32**

Η μορφή FP32 παρέχει υψηλότερη ακρίβεια και μπορεί να αναπαραστήσει με ακρίβεια ένα ευρύ φάσμα τιμών. Σε σενάρια όπου εκτελούνται σύνθετες μαθηματικές λειτουργίες ή απαιτούνται αποτελέσματα υψηλής ακρίβειας, η μορφή FP32 προτιμάται. Ωστόσο, η υψηλή ακρίβεια σημαίνει επίσης μεγαλύτερη κατανάλωση μνήμης και μεγαλύτερο χρόνο υπολογισμού. Για μοντέλα μεγάλης κλίμακας βαθιάς μάθησης, ειδικά όταν υπάρχουν πολλοί παράμετροι μοντέλου και τεράστια ποσότητα δεδομένων, η μορφή FP32 μπορεί να προκαλέσει ανεπαρκή μνήμη GPU ή μείωση της ταχύτητας εκτέλεσης.

Σε κινητές συσκευές ή συσκευές IoT, μπορούμε να μετατρέψουμε μοντέλα Phi-3.x σε INT4, ενώ τα AI PC / Copilot PC μπορούν να χρησιμοποιήσουν μεγαλύτερη ακρίβεια όπως INT8, FP16, FP32.

Προς το παρόν, διαφορετικοί κατασκευαστές υλικού διαθέτουν διαφορετικά πλαίσια υποστήριξης γεννητικών μοντέλων, όπως το OpenVINO της Intel, το QNN της Qualcomm, το MLX της Apple, και το CUDA της Nvidia κ.ά., σε συνδυασμό με ποσοτικοποίηση μοντέλου για ολοκλήρωση τοπικής ανάπτυξης.

Σε τεχνολογικό επίπεδο, έχουμε υποστήριξη διαφορετικών μορφών μετά την ποσοτικοποίηση, όπως μορφή PyTorch / TensorFlow, GGUF και ONNX. Έχω διενεργήσει σύγκριση μορφών και σεναρίων εφαρμογής ανάμεσα σε GGUF και ONNX. Εδώ προτείνω τη μορφή ποσοτικοποίησης ONNX, η οποία διαθέτει καλή υποστήριξη από το πλαίσιο μοντέλων μέχρι το υλικό. Σε αυτό το κεφάλαιο, θα εστιάσουμε στο ONNX Runtime για GenAI, OpenVINO και Apple MLX για την εκτέλεση ποσοτικοποίησης μοντέλων (εάν έχετε καλύτερο τρόπο, μπορείτε να μας τον προτείνετε μέσω υποβολής PR).

**Αυτό το κεφάλαιο περιλαμβάνει**

1. [Ποσοτικοποίηση Phi-3.5 / 4 με χρήση llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Ποσοτικοποίηση Phi-3.5 / 4 με χρήση Επεκτάσεων Generative AI για onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Ποσοτικοποίηση Phi-3.5 / 4 με χρήση Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Ποσοτικοποίηση Phi-3.5 / 4 με χρήση Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Αποποίηση ευθυνών**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία μετάφρασης με τεχνητή νοημοσύνη [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθεια για ακρίβεια, παρακαλούμε να λάβετε υπόψη ότι οι αυτοματοποιημένες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη γλώσσα του θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες συνιστάται η επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή λανθασμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->