<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-05-09T13:26:48+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "el"
}
-->
# **Ποσοτικοποίηση της Οικογένειας Phi**

Η ποσοτικοποίηση μοντέλου αναφέρεται στη διαδικασία μετατροπής των παραμέτρων (όπως βάρη και τιμές ενεργοποίησης) σε ένα νευρωνικό δίκτυο από ένα μεγάλο εύρος τιμών (συνήθως συνεχές) σε ένα μικρότερο πεπερασμένο εύρος τιμών. Αυτή η τεχνολογία μπορεί να μειώσει το μέγεθος και την υπολογιστική πολυπλοκότητα του μοντέλου και να βελτιώσει την αποδοτικότητα λειτουργίας σε περιβάλλοντα με περιορισμένους πόρους, όπως κινητές συσκευές ή ενσωματωμένα συστήματα. Η ποσοτικοποίηση επιτυγχάνει συμπίεση μειώνοντας την ακρίβεια των παραμέτρων, αλλά εισάγει και κάποια απώλεια ακρίβειας. Επομένως, κατά τη διαδικασία ποσοτικοποίησης χρειάζεται να ισορροπηθούν το μέγεθος του μοντέλου, η υπολογιστική πολυπλοκότητα και η ακρίβεια. Συνηθισμένες μέθοδοι ποσοτικοποίησης περιλαμβάνουν την ποσοτικοποίηση με σταθερό σημείο, την ποσοτικοποίηση κινητής υποδιαστολής κ.ά. Μπορείτε να επιλέξετε την κατάλληλη στρατηγική ανάλογα με το σενάριο και τις ανάγκες.

Επιδιώκουμε να αναπτύξουμε το μοντέλο GenAI σε edge συσκευές και να επιτρέψουμε σε περισσότερες συσκευές να εισέλθουν σε σενάρια GenAI, όπως κινητές συσκευές, AI PC/Copilot+PC και παραδοσιακές IoT συσκευές. Μέσω του ποσοτικοποιημένου μοντέλου, μπορούμε να το αναπτύξουμε σε διαφορετικές edge συσκευές ανάλογα με τη συσκευή. Σε συνδυασμό με το πλαίσιο επιτάχυνσης μοντέλου και το ποσοτικοποιημένο μοντέλο που παρέχουν οι κατασκευαστές υλικού, μπορούμε να δημιουργήσουμε καλύτερα σενάρια εφαρμογών SLM.

Στο σενάριο ποσοτικοποίησης, έχουμε διαφορετικές ακρίβειες (INT4, INT8, FP16, FP32). Ακολουθεί μια εξήγηση των συνηθισμένων ακρίβειών ποσοτικοποίησης.

### **INT4**

Η ποσοτικοποίηση INT4 είναι μια ριζική μέθοδος που ποσοτικοποιεί τα βάρη και τις τιμές ενεργοποίησης σε 4-bit ακέραιους. Η ποσοτικοποίηση INT4 συνήθως οδηγεί σε μεγαλύτερη απώλεια ακρίβειας λόγω του μικρότερου εύρους αναπαράστασης και της χαμηλότερης ακρίβειας. Ωστόσο, σε σύγκριση με την ποσοτικοποίηση INT8, η INT4 μειώνει περαιτέρω τις απαιτήσεις αποθήκευσης και υπολογιστικής πολυπλοκότητας. Σημειώνεται ότι η ποσοτικοποίηση INT4 είναι σχετικά σπάνια στην πράξη, καθώς η πολύ χαμηλή ακρίβεια μπορεί να προκαλέσει σημαντική υποβάθμιση της απόδοσης του μοντέλου. Επιπλέον, δεν υποστηρίζουν όλα τα υλικά τις λειτουργίες INT4, οπότε χρειάζεται να λαμβάνεται υπόψη η συμβατότητα υλικού κατά την επιλογή της μεθόδου ποσοτικοποίησης.

### **INT8**

Η ποσοτικοποίηση INT8 είναι η διαδικασία μετατροπής των βαρών και των ενεργοποιήσεων του μοντέλου από αριθμούς κινητής υποδιαστολής σε 8-bit ακέραιους. Αν και το αριθμητικό εύρος που αναπαρίστανται οι INT8 ακέραιοι είναι μικρότερο και λιγότερο ακριβές, μπορεί να μειώσει σημαντικά τις απαιτήσεις αποθήκευσης και υπολογισμών. Στην ποσοτικοποίηση INT8, τα βάρη και οι τιμές ενεργοποίησης περνούν από μια διαδικασία ποσοτικοποίησης που περιλαμβάνει κλιμάκωση και μετατόπιση, ώστε να διατηρείται όσο το δυνατόν περισσότερο η αρχική πληροφορία κινητής υποδιαστολής. Κατά την εκτέλεση, αυτές οι ποσοτικοποιημένες τιμές αποποσοτικοποιούνται ξανά σε αριθμούς κινητής υποδιαστολής για υπολογισμούς και στη συνέχεια ποσοτικοποιούνται ξανά σε INT8 για το επόμενο βήμα. Αυτή η μέθοδος προσφέρει επαρκή ακρίβεια στις περισσότερες εφαρμογές, διατηρώντας υψηλή υπολογιστική απόδοση.

### **FP16**

Η μορφή FP16, δηλαδή 16-bit αριθμοί κινητής υποδιαστολής (float16), μειώνει το αποτύπωμα μνήμης στο μισό σε σύγκριση με τους 32-bit αριθμούς κινητής υποδιαστολής (float32), προσφέροντας σημαντικά πλεονεκτήματα σε εφαρμογές μεγάλης κλίμακας βαθιάς μάθησης. Η μορφή FP16 επιτρέπει τη φόρτωση μεγαλύτερων μοντέλων ή την επεξεργασία περισσότερων δεδομένων εντός των ίδιων ορίων μνήμης GPU. Καθώς το σύγχρονο υλικό GPU συνεχίζει να υποστηρίζει λειτουργίες FP16, η χρήση της μορφής FP16 μπορεί επίσης να βελτιώσει την ταχύτητα υπολογισμών. Ωστόσο, η μορφή FP16 έχει και μειονεκτήματα, όπως η χαμηλότερη ακρίβεια, που μπορεί να οδηγήσει σε αριθμητική αστάθεια ή απώλεια ακρίβειας σε ορισμένες περιπτώσεις.

### **FP32**

Η μορφή FP32 παρέχει υψηλότερη ακρίβεια και μπορεί να αναπαραστήσει με ακρίβεια ένα ευρύ φάσμα τιμών. Σε σενάρια όπου εκτελούνται σύνθετοι μαθηματικοί υπολογισμοί ή απαιτούνται αποτελέσματα υψηλής ακρίβειας, προτιμάται η μορφή FP32. Ωστόσο, η υψηλή ακρίβεια συνεπάγεται μεγαλύτερη χρήση μνήμης και μεγαλύτερο χρόνο υπολογισμού. Για μεγάλα μοντέλα βαθιάς μάθησης, ιδιαίτερα όταν υπάρχουν πολλοί παράμετροι και τεράστιος όγκος δεδομένων, η μορφή FP32 μπορεί να προκαλέσει έλλειψη μνήμης GPU ή μείωση της ταχύτητας εκτέλεσης.

Σε κινητές συσκευές ή IoT συσκευές, μπορούμε να μετατρέψουμε τα μοντέλα Phi-3.x σε INT4, ενώ τα AI PC / Copilot PC μπορούν να χρησιμοποιήσουν υψηλότερη ακρίβεια όπως INT8, FP16, FP32.

Προς το παρόν, διαφορετικοί κατασκευαστές υλικού διαθέτουν διαφορετικά πλαίσια για την υποστήριξη γενετικών μοντέλων, όπως το OpenVINO της Intel, το QNN της Qualcomm, το MLX της Apple και το CUDA της Nvidia, σε συνδυασμό με την ποσοτικοποίηση μοντέλου για τοπική ανάπτυξη.

Σε τεχνολογικό επίπεδο, υποστηρίζουμε διαφορετικές μορφές μετά την ποσοτικοποίηση, όπως PyTorch / Tensorflow, GGUF και ONNX. Έχω κάνει σύγκριση μορφών και σενάρια εφαρμογής μεταξύ GGUF και ONNX. Εδώ προτείνω τη μορφή ποσοτικοποίησης ONNX, που έχει καλή υποστήριξη από το πλαίσιο μοντέλου έως το υλικό. Σε αυτό το κεφάλαιο, θα εστιάσουμε στο ONNX Runtime για GenAI, OpenVINO και Apple MLX για την εκτέλεση ποσοτικοποίησης μοντέλου (αν έχετε καλύτερο τρόπο, μπορείτε επίσης να μας τον προτείνετε υποβάλλοντας PR).

**Αυτό το κεφάλαιο περιλαμβάνει**

1. [Quantizing Phi-3.5 / 4 using llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Quantizing Phi-3.5 / 4 using Generative AI extensions for onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Quantizing Phi-3.5 / 4 using Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Quantizing Phi-3.5 / 4 using Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

**Αποποίηση ευθυνών**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που προσπαθούμε για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτόματες μεταφράσεις μπορεί να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη γλώσσα του πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή λανθασμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.