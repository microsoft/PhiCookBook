<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-07-16T21:46:40+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "el"
}
-->
# **Ποσοτικοποίηση της Οικογένειας Phi**

Η ποσοτικοποίηση μοντέλου αναφέρεται στη διαδικασία μετατροπής των παραμέτρων (όπως βάρη και τιμές ενεργοποίησης) σε ένα νευρωνικό δίκτυο από ένα μεγάλο εύρος τιμών (συνήθως συνεχές) σε ένα μικρότερο πεπερασμένο εύρος τιμών. Αυτή η τεχνολογία μπορεί να μειώσει το μέγεθος και την υπολογιστική πολυπλοκότητα του μοντέλου και να βελτιώσει την αποδοτικότητα λειτουργίας του σε περιβάλλοντα με περιορισμένους πόρους, όπως κινητές συσκευές ή ενσωματωμένα συστήματα. Η ποσοτικοποίηση επιτυγχάνει συμπίεση μειώνοντας την ακρίβεια των παραμέτρων, αλλά εισάγει και κάποια απώλεια ακρίβειας. Επομένως, στη διαδικασία ποσοτικοποίησης είναι απαραίτητο να βρεθεί η ισορροπία μεταξύ μεγέθους μοντέλου, υπολογιστικής πολυπλοκότητας και ακρίβειας. Συνηθισμένες μέθοδοι ποσοτικοποίησης περιλαμβάνουν την ποσοτικοποίηση με σταθερή υποδιαστολή, την ποσοτικοποίηση κινητής υποδιαστολής κ.ά. Μπορείτε να επιλέξετε την κατάλληλη στρατηγική ποσοτικοποίησης ανάλογα με το συγκεκριμένο σενάριο και τις ανάγκες.

Ελπίζουμε να αναπτύξουμε το μοντέλο GenAI σε συσκευές άκρου και να επιτρέψουμε σε περισσότερες συσκευές να ενταχθούν σε σενάρια GenAI, όπως κινητές συσκευές, AI PC/Copilot+PC και παραδοσιακές συσκευές IoT. Μέσω του ποσοτικοποιημένου μοντέλου, μπορούμε να το αναπτύξουμε σε διαφορετικές συσκευές άκρου ανάλογα με τη συσκευή. Σε συνδυασμό με το πλαίσιο επιτάχυνσης μοντέλου και το ποσοτικοποιημένο μοντέλο που παρέχουν οι κατασκευαστές υλικού, μπορούμε να δημιουργήσουμε καλύτερα σενάρια εφαρμογών SLM.

Στο σενάριο ποσοτικοποίησης, έχουμε διαφορετικές ακρίβειες (INT4, INT8, FP16, FP32). Ακολουθεί μια εξήγηση των συνηθισμένων ακρίβειων ποσοτικοποίησης.

### **INT4**

Η ποσοτικοποίηση INT4 είναι μια ριζοσπαστική μέθοδος που ποσοτικοποιεί τα βάρη και τις τιμές ενεργοποίησης του μοντέλου σε ακέραιους 4-bit. Η ποσοτικοποίηση INT4 συνήθως οδηγεί σε μεγαλύτερη απώλεια ακρίβειας λόγω του μικρότερου εύρους αναπαράστασης και της χαμηλότερης ακρίβειας. Ωστόσο, σε σύγκριση με την ποσοτικοποίηση INT8, η INT4 μπορεί να μειώσει περαιτέρω τις απαιτήσεις αποθήκευσης και την υπολογιστική πολυπλοκότητα του μοντέλου. Αξίζει να σημειωθεί ότι η ποσοτικοποίηση INT4 είναι σχετικά σπάνια σε πρακτικές εφαρμογές, καθώς η πολύ χαμηλή ακρίβεια μπορεί να προκαλέσει σημαντική υποβάθμιση της απόδοσης του μοντέλου. Επιπλέον, δεν υποστηρίζουν όλα τα υλικά λειτουργίες INT4, οπότε πρέπει να ληφθεί υπόψη η συμβατότητα υλικού κατά την επιλογή μεθόδου ποσοτικοποίησης.

### **INT8**

Η ποσοτικοποίηση INT8 είναι η διαδικασία μετατροπής των βαρών και των ενεργοποιήσεων ενός μοντέλου από αριθμούς κινητής υποδιαστολής σε ακέραιους 8-bit. Αν και το αριθμητικό εύρος που αναπαριστούν οι ακέραιοι INT8 είναι μικρότερο και λιγότερο ακριβές, μπορεί να μειώσει σημαντικά τις απαιτήσεις αποθήκευσης και υπολογισμών. Στην ποσοτικοποίηση INT8, τα βάρη και οι τιμές ενεργοποίησης περνούν από μια διαδικασία ποσοτικοποίησης, που περιλαμβάνει κλιμάκωση και μετατόπιση, ώστε να διατηρηθούν όσο το δυνατόν περισσότερες πληροφορίες από την αρχική κινητή υποδιαστολή. Κατά την εκτέλεση, αυτές οι ποσοτικοποιημένες τιμές αποποσοτικοποιούνται ξανά σε αριθμούς κινητής υποδιαστολής για τους υπολογισμούς και στη συνέχεια ποσοτικοποιούνται ξανά σε INT8 για το επόμενο βήμα. Αυτή η μέθοδος παρέχει επαρκή ακρίβεια στις περισσότερες εφαρμογές, διατηρώντας παράλληλα υψηλή υπολογιστική αποδοτικότητα.

### **FP16**

Η μορφή FP16, δηλαδή οι αριθμοί κινητής υποδιαστολής 16-bit (float16), μειώνει το αποτύπωμα μνήμης στο μισό σε σύγκριση με τους 32-bit αριθμούς κινητής υποδιαστολής (float32), κάτι που έχει σημαντικά πλεονεκτήματα σε εφαρμογές μεγάλης κλίμακας βαθιάς μάθησης. Η μορφή FP16 επιτρέπει τη φόρτωση μεγαλύτερων μοντέλων ή την επεξεργασία περισσότερων δεδομένων εντός των ίδιων ορίων μνήμης GPU. Καθώς το σύγχρονο υλικό GPU συνεχίζει να υποστηρίζει λειτουργίες FP16, η χρήση της μορφής FP16 μπορεί επίσης να βελτιώσει την ταχύτητα υπολογισμών. Ωστόσο, η μορφή FP16 έχει και τα εγγενή μειονεκτήματά της, δηλαδή τη χαμηλότερη ακρίβεια, που μπορεί να οδηγήσει σε αριθμητική αστάθεια ή απώλεια ακρίβειας σε ορισμένες περιπτώσεις.

### **FP32**

Η μορφή FP32 παρέχει υψηλότερη ακρίβεια και μπορεί να αναπαραστήσει με ακρίβεια ένα ευρύ φάσμα τιμών. Σε σενάρια όπου εκτελούνται πολύπλοκες μαθηματικές πράξεις ή απαιτούνται αποτελέσματα υψηλής ακρίβειας, προτιμάται η μορφή FP32. Ωστόσο, η υψηλή ακρίβεια σημαίνει και μεγαλύτερη χρήση μνήμης και μεγαλύτερο χρόνο υπολογισμού. Για μεγάλα μοντέλα βαθιάς μάθησης, ειδικά όταν υπάρχουν πολλοί παράμετροι μοντέλου και τεράστιος όγκος δεδομένων, η μορφή FP32 μπορεί να προκαλέσει ανεπαρκή μνήμη GPU ή μείωση της ταχύτητας εκτέλεσης.

Σε κινητές συσκευές ή συσκευές IoT, μπορούμε να μετατρέψουμε τα μοντέλα Phi-3.x σε INT4, ενώ τα AI PC / Copilot PC μπορούν να χρησιμοποιήσουν υψηλότερη ακρίβεια όπως INT8, FP16, FP32.

Προς το παρόν, διαφορετικοί κατασκευαστές υλικού διαθέτουν διαφορετικά πλαίσια για την υποστήριξη γεννητικών μοντέλων, όπως το OpenVINO της Intel, το QNN της Qualcomm, το MLX της Apple και το CUDA της Nvidia, κ.ά., σε συνδυασμό με την ποσοτικοποίηση μοντέλου για την ολοκλήρωση της τοπικής ανάπτυξης.

Σε τεχνολογικό επίπεδο, έχουμε διαφορετική υποστήριξη μορφών μετά την ποσοτικοποίηση, όπως μορφές PyTorch / Tensorflow, GGUF και ONNX. Έχω κάνει μια σύγκριση μορφών και σεναρίων εφαρμογής μεταξύ GGUF και ONNX. Εδώ προτείνω τη μορφή ποσοτικοποίησης ONNX, η οποία έχει καλή υποστήριξη από το πλαίσιο μοντέλου μέχρι το υλικό. Σε αυτό το κεφάλαιο, θα εστιάσουμε στο ONNX Runtime για GenAI, OpenVINO και Apple MLX για την εκτέλεση ποσοτικοποίησης μοντέλου (αν έχετε καλύτερο τρόπο, μπορείτε επίσης να μας τον προτείνετε υποβάλλοντας PR).

**Αυτό το κεφάλαιο περιλαμβάνει**

1. [Ποσοτικοποίηση Phi-3.5 / 4 με χρήση llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Ποσοτικοποίηση Phi-3.5 / 4 με χρήση επεκτάσεων Generative AI για onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Ποσοτικοποίηση Phi-3.5 / 4 με χρήση Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Ποσοτικοποίηση Phi-3.5 / 4 με χρήση Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

**Αποποίηση ευθυνών**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που επιδιώκουμε την ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη γλώσσα του θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή λανθασμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.