<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "462bddc47427d8785f3c9fd817b346fe",
  "translation_date": "2025-05-09T14:10:09+00:00",
  "source_file": "md/01.Introduction/04/UsingLlamacppQuantifyingPhi.md",
  "language_code": "el"
}
-->
# **Ποσοτικοποίηση της οικογένειας Phi με χρήση του llama.cpp**

## **Τι είναι το llama.cpp**

Το llama.cpp είναι μια βιβλιοθήκη ανοιχτού κώδικα, γραμμένη κυρίως σε C++, που εκτελεί συμπερασματολογία σε διάφορα Μεγάλα Μοντέλα Γλώσσας (LLMs), όπως το Llama. Ο κύριος στόχος του είναι να προσφέρει κορυφαία απόδοση στην εκτέλεση LLMs σε ευρύ φάσμα υλικού με ελάχιστη ρύθμιση. Επιπλέον, υπάρχουν διαθέσιμες Python bindings για αυτή τη βιβλιοθήκη, που παρέχουν υψηλού επιπέδου API για ολοκλήρωση κειμένου και έναν OpenAI συμβατό web server.

Ο βασικός σκοπός του llama.cpp είναι να επιτρέψει την εκτέλεση LLM με ελάχιστη ρύθμιση και κορυφαία απόδοση σε ποικιλία υλικού — τοπικά και στο cloud.

- Καθαρή υλοποίηση σε C/C++ χωρίς εξαρτήσεις
- Η Apple silicon υποστηρίζεται πλήρως — βελτιστοποιημένη μέσω ARM NEON, Accelerate και Metal frameworks
- Υποστήριξη AVX, AVX2 και AVX512 για αρχιτεκτονικές x86
- Ποσοτικοποίηση σε 1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit και 8-bit ακέραιους για ταχύτερη εκτέλεση και μειωμένη χρήση μνήμης
- Προσαρμοσμένοι πυρήνες CUDA για εκτέλεση LLM σε NVIDIA GPUs (υποστήριξη AMD GPUs μέσω HIP)
- Υποστήριξη Vulkan και SYCL backend
- Υβριδική εκτέλεση CPU+GPU για μερική επιτάχυνση μοντέλων που είναι μεγαλύτερα από τη συνολική χωρητικότητα VRAM

## **Ποσοτικοποίηση Phi-3.5 με llama.cpp**

Το μοντέλο Phi-3.5-Instruct μπορεί να ποσοτικοποιηθεί με χρήση του llama.cpp, ενώ τα Phi-3.5-Vision και Phi-3.5-MoE δεν υποστηρίζονται ακόμα. Η μορφή που μετατρέπει το llama.cpp είναι η gguf, η οποία είναι και η πιο διαδεδομένη μορφή ποσοτικοποίησης.

Υπάρχουν πολλά μοντέλα σε ποσοτικοποιημένη μορφή GGUF στο Hugging Face. Οι AI Foundry, Ollama και LlamaEdge βασίζονται στο llama.cpp, γι’ αυτό και τα μοντέλα GGUF χρησιμοποιούνται συχνά.

### **Τι είναι το GGUF**

Το GGUF είναι μια δυαδική μορφή, βελτιστοποιημένη για γρήγορη φόρτωση και αποθήκευση μοντέλων, καθιστώντας το ιδιαίτερα αποδοτικό για εκτέλεση συμπερασμάτων. Το GGUF έχει σχεδιαστεί για χρήση με GGML και άλλους εκτελεστές. Το GGUF αναπτύχθηκε από τον @ggerganov, ο οποίος είναι επίσης ο δημιουργός του llama.cpp, ενός δημοφιλούς C/C++ πλαισίου εκτέλεσης LLM. Μοντέλα που αρχικά αναπτύχθηκαν σε πλαίσια όπως το PyTorch μπορούν να μετατραπούν σε μορφή GGUF για χρήση με αυτούς τους εκτελεστές.

### **ONNX vs GGUF**

Το ONNX είναι μια παραδοσιακή μορφή μηχανικής μάθησης/βαθιάς μάθησης, που υποστηρίζεται καλά σε διάφορα AI frameworks και έχει καλές εφαρμογές σε συσκευές άκρης. Από την άλλη, το GGUF βασίζεται στο llama.cpp και μπορεί να θεωρηθεί προϊόν της εποχής της Γενετικής Τεχνητής Νοημοσύνης (GenAI). Οι δύο μορφές έχουν παρόμοιες χρήσεις. Αν θέλετε καλύτερη απόδοση σε ενσωματωμένο υλικό και επίπεδα εφαρμογών, το ONNX μπορεί να είναι η επιλογή σας. Αν χρησιμοποιείτε το παράγωγο πλαίσιο και τεχνολογία του llama.cpp, τότε το GGUF ίσως είναι καλύτερο.

### **Ποσοτικοποίηση Phi-3.5-Instruct με χρήση του llama.cpp**

**1. Ρύθμιση Περιβάλλοντος**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. Ποσοτικοποίηση**

Μετατροπή του Phi-3.5-Instruct σε FP16 GGUF με χρήση του llama.cpp


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Ποσοτικοποίηση του Phi-3.5 σε INT4


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. Δοκιμή**

Εγκατάσταση του llama-cpp-python


```bash

pip install llama-cpp-python -U

```

***Note***

Αν χρησιμοποιείτε Apple Silicon, παρακαλώ εγκαταστήστε το llama-cpp-python ως εξής


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

Δοκιμή


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **Πόροι**

1. Μάθετε περισσότερα για το llama.cpp [https://github.com/ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)  
2. Μάθετε περισσότερα για το onnxruntime [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)  
3. Μάθετε περισσότερα για το GGUF [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**Αποποίηση ευθυνών**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης με τεχνητή νοημοσύνη [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που προσπαθούμε για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτόματες μεταφράσεις μπορεί να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα πρέπει να θεωρείται η επίσημη πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή λανθασμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.