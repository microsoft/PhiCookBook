# Ξεκινώντας με το Phi-3 τοπικά

Αυτός ο οδηγός θα σας βοηθήσει να ρυθμίσετε το τοπικό σας περιβάλλον για να τρέξετε το μοντέλο Phi-3 χρησιμοποιώντας το Ollama. Μπορείτε να εκτελέσετε το μοντέλο με διάφορους τρόπους, όπως μέσω GitHub Codespaces, VS Code Dev Containers ή στο τοπικό σας περιβάλλον.

## Ρύθμιση περιβάλλοντος

### GitHub Codespaces

Μπορείτε να τρέξετε αυτό το πρότυπο εικονικά χρησιμοποιώντας το GitHub Codespaces. Το κουμπί θα ανοίξει μια web-based έκδοση του VS Code στον περιηγητή σας:

1. Ανοίξτε το πρότυπο (αυτό μπορεί να πάρει μερικά λεπτά):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Ανοίξτε ένα παράθυρο τερματικού

### VS Code Dev Containers

⚠️ Αυτή η επιλογή θα λειτουργήσει μόνο αν το Docker Desktop σας έχει τουλάχιστον 16 GB RAM διαθέσιμα. Αν έχετε λιγότερα από 16 GB RAM, μπορείτε να δοκιμάσετε την [επιλογή GitHub Codespaces](../../../../../md/01.Introduction/01) ή να [το ρυθμίσετε τοπικά](../../../../../md/01.Introduction/01).

Μια σχετική επιλογή είναι τα VS Code Dev Containers, που θα ανοίξουν το έργο στο τοπικό σας VS Code χρησιμοποιώντας την [επέκταση Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Εκκινήστε το Docker Desktop (εγκαταστήστε το αν δεν το έχετε ήδη)
2. Ανοίξτε το έργο:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Στο παράθυρο του VS Code που θα ανοίξει, μόλις εμφανιστούν τα αρχεία του έργου (αυτό μπορεί να πάρει μερικά λεπτά), ανοίξτε ένα παράθυρο τερματικού.
4. Συνεχίστε με τα [βήματα ανάπτυξης](../../../../../md/01.Introduction/01)

### Τοπικό Περιβάλλον

1. Βεβαιωθείτε ότι έχετε εγκαταστήσει τα παρακάτω εργαλεία:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Δοκιμή του μοντέλου

1. Ζητήστε από το Ollama να κατεβάσει και να τρέξει το μοντέλο phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Η λήψη του μοντέλου θα πάρει μερικά λεπτά.

2. Μόλις δείτε το μήνυμα "success" στην έξοδο, μπορείτε να στείλετε μήνυμα σε αυτό το μοντέλο από το prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Μετά από μερικά δευτερόλεπτα, θα πρέπει να δείτε να ρέει η απάντηση από το μοντέλο.

4. Για να μάθετε για τις διάφορες τεχνικές που χρησιμοποιούνται με γλωσσικά μοντέλα, ανοίξτε το Python notebook [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) και εκτελέστε κάθε κελί. Αν χρησιμοποιήσατε μοντέλο διαφορετικό από το 'phi3:mini', αλλάξτε το `MODEL_NAME` στο πρώτο κελί.

5. Για να συνομιλήσετε με το μοντέλο phi3:mini από Python, ανοίξτε το αρχείο Python [chat.py](../../../../../code/01.Introduce/chat.py) και τρέξτε το. Μπορείτε να αλλάξετε το `MODEL_NAME` στην κορυφή του αρχείου όπως χρειάζεται, και επίσης να τροποποιήσετε το μήνυμα συστήματος ή να προσθέσετε παραδείγματα few-shot αν το επιθυμείτε.

**Αποποίηση ευθυνών**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που επιδιώκουμε την ακρίβεια, παρακαλούμε να γνωρίζετε ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη γλώσσα του θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή λανθασμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.