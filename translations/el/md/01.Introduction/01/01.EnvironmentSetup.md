<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-09T07:08:34+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "el"
}
-->
# Ξεκινώντας με το Phi-3 τοπικά

Αυτός ο οδηγός θα σας βοηθήσει να ρυθμίσετε το τοπικό σας περιβάλλον για να τρέξετε το μοντέλο Phi-3 χρησιμοποιώντας το Ollama. Μπορείτε να τρέξετε το μοντέλο με διάφορους τρόπους, συμπεριλαμβανομένης της χρήσης GitHub Codespaces, VS Code Dev Containers ή του τοπικού σας περιβάλλοντος.

## Ρύθμιση περιβάλλοντος

### GitHub Codespaces

Μπορείτε να τρέξετε αυτό το πρότυπο εικονικά χρησιμοποιώντας το GitHub Codespaces. Το κουμπί θα ανοίξει ένα web-based παράθυρο VS Code στον browser σας:

1. Ανοίξτε το πρότυπο (αυτό μπορεί να πάρει μερικά λεπτά):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Ανοίξτε ένα παράθυρο τερματικού

### VS Code Dev Containers

⚠️ Αυτή η επιλογή θα λειτουργήσει μόνο αν το Docker Desktop έχει τουλάχιστον 16 GB RAM. Αν έχετε λιγότερα από 16 GB RAM, μπορείτε να δοκιμάσετε την [επιλογή GitHub Codespaces](../../../../../md/01.Introduction/01) ή να [το ρυθμίσετε τοπικά](../../../../../md/01.Introduction/01).

Μια σχετική επιλογή είναι τα VS Code Dev Containers, που θα ανοίξουν το project στον τοπικό σας VS Code χρησιμοποιώντας την [επέκταση Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Ξεκινήστε το Docker Desktop (εγκαταστήστε το αν δεν είναι ήδη εγκατεστημένο)
2. Ανοίξτε το project:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Στο παράθυρο του VS Code που ανοίγει, μόλις εμφανιστούν τα αρχεία του project (μπορεί να πάρει μερικά λεπτά), ανοίξτε ένα παράθυρο τερματικού.
4. Συνεχίστε με τα [βήματα ανάπτυξης](../../../../../md/01.Introduction/01)

### Τοπικό Περιβάλλον

1. Βεβαιωθείτε ότι τα παρακάτω εργαλεία είναι εγκατεστημένα:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Δοκιμή του μοντέλου

1. Ζητήστε από το Ollama να κατεβάσει και να τρέξει το μοντέλο phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Η λήψη του μοντέλου μπορεί να πάρει μερικά λεπτά.

2. Μόλις δείτε "success" στην έξοδο, μπορείτε να στείλετε μήνυμα σε αυτό το μοντέλο από το prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Μετά από λίγα δευτερόλεπτα, θα πρέπει να δείτε να ρέει η απάντηση από το μοντέλο.

4. Για να μάθετε για διάφορες τεχνικές που χρησιμοποιούνται με γλωσσικά μοντέλα, ανοίξτε το Python notebook [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) και εκτελέστε κάθε κελί. Αν χρησιμοποιήσατε μοντέλο διαφορετικό από το 'phi3:mini', αλλάξτε το `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` στην κορυφή του αρχείου όπως χρειάζεται, και μπορείτε επίσης να τροποποιήσετε το μήνυμα συστήματος ή να προσθέσετε παραδείγματα few-shot αν το επιθυμείτε.

**Αποποίηση ευθυνών**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που προσπαθούμε για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτοματοποιημένες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα πρέπει να θεωρείται η επίσημη πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική μετάφραση από άνθρωπο. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή λανθασμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.