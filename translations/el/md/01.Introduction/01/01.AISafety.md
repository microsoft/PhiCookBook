<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c8273672cc57df2be675407a1383aaf0",
  "translation_date": "2025-07-16T17:48:10+00:00",
  "source_file": "md/01.Introduction/01/01.AISafety.md",
  "language_code": "el"
}
-->
# Ασφάλεια AI για τα μοντέλα Phi  
Η οικογένεια μοντέλων Phi αναπτύχθηκε σύμφωνα με το [Microsoft Responsible AI Standard](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFl), ένα εταιρικό σύνολο απαιτήσεων βασισμένο σε έξι αρχές: λογοδοσία, διαφάνεια, δικαιοσύνη, αξιοπιστία και ασφάλεια, ιδιωτικότητα και ασφάλεια, καθώς και συμπερίληψη, που αποτελούν τα [Responsible AI principles της Microsoft](https://www.microsoft.com/ai/responsible-ai).

Όπως και τα προηγούμενα μοντέλα Phi, υιοθετήθηκε μια πολυδιάστατη προσέγγιση αξιολόγησης και εκπαίδευσης ασφάλειας μετά την εκπαίδευση, με επιπλέον μέτρα για να ληφθούν υπόψη οι πολυγλωσσικές δυνατότητες αυτής της έκδοσης. Η προσέγγισή μας στην εκπαίδευση και τις αξιολογήσεις ασφάλειας, που περιλαμβάνει δοκιμές σε πολλές γλώσσες και κατηγορίες κινδύνου, περιγράφεται στο [Phi Safety Post-Training Paper](https://arxiv.org/abs/2407.13833). Ενώ τα μοντέλα Phi επωφελούνται από αυτή την προσέγγιση, οι προγραμματιστές θα πρέπει να εφαρμόζουν τις βέλτιστες πρακτικές υπεύθυνης AI, συμπεριλαμβανομένης της χαρτογράφησης, μέτρησης και μείωσης των κινδύνων που σχετίζονται με τη συγκεκριμένη χρήση και το πολιτισμικό και γλωσσικό πλαίσιο.

## Βέλτιστες Πρακτικές

Όπως και άλλα μοντέλα, η οικογένεια μοντέλων Phi μπορεί ενδεχομένως να συμπεριφέρεται με τρόπους που είναι άδικοι, αναξιόπιστοι ή προσβλητικοί.

Μερικές από τις περιοριστικές συμπεριφορές των SLM και LLM που πρέπει να γνωρίζετε περιλαμβάνουν:

- **Ποιότητα Υπηρεσίας:** Τα μοντέλα Phi εκπαιδεύονται κυρίως σε αγγλικό κείμενο. Γλώσσες εκτός των αγγλικών θα παρουσιάσουν χειρότερη απόδοση. Παραλλαγές της αγγλικής γλώσσας με λιγότερη εκπροσώπηση στα δεδομένα εκπαίδευσης μπορεί να έχουν χειρότερη απόδοση σε σχέση με το τυπικό αμερικανικό αγγλικά.  
- **Αναπαράσταση Βλαβών & Διατήρηση Στερεοτύπων:** Αυτά τα μοντέλα μπορεί να υπερεκπροσωπούν ή να υποεκπροσωπούν ομάδες ανθρώπων, να διαγράφουν την εκπροσώπηση ορισμένων ομάδων ή να ενισχύουν υποτιμητικά ή αρνητικά στερεότυπα. Παρά την εκπαίδευση ασφάλειας μετά την αρχική εκπαίδευση, αυτοί οι περιορισμοί μπορεί να παραμένουν λόγω διαφορετικών επιπέδων εκπροσώπησης των ομάδων ή της ύπαρξης παραδειγμάτων αρνητικών στερεοτύπων στα δεδομένα εκπαίδευσης που αντικατοπτρίζουν πραγματικά κοινωνικά πρότυπα και προκαταλήψεις.  
- **Ακατάλληλο ή Προσβλητικό Περιεχόμενο:** Τα μοντέλα αυτά μπορεί να παράγουν άλλους τύπους ακατάλληλου ή προσβλητικού περιεχομένου, γεγονός που μπορεί να τα καθιστά ακατάλληλα για χρήση σε ευαίσθητα περιβάλλοντα χωρίς επιπλέον μέτρα μετριασμού που να είναι ειδικά για την περίπτωση χρήσης.  
- **Αξιοπιστία Πληροφορίας:** Τα γλωσσικά μοντέλα μπορεί να παράγουν ανοησίες ή να επινοούν περιεχόμενο που ακούγεται λογικό αλλά είναι ανακριβές ή ξεπερασμένο.  
- **Περιορισμένο Εύρος για Κώδικα:** Η πλειονότητα των δεδομένων εκπαίδευσης του Phi-3 βασίζεται σε Python και χρησιμοποιεί κοινά πακέτα όπως "typing, math, random, collections, datetime, itertools". Αν το μοντέλο παράγει Python scripts που χρησιμοποιούν άλλα πακέτα ή scripts σε άλλες γλώσσες, συνιστούμε έντονα στους χρήστες να επαληθεύουν χειροκίνητα όλες τις χρήσεις API.

Οι προγραμματιστές θα πρέπει να εφαρμόζουν τις βέλτιστες πρακτικές υπεύθυνης AI και είναι υπεύθυνοι να διασφαλίζουν ότι μια συγκεκριμένη χρήση συμμορφώνεται με τους σχετικούς νόμους και κανονισμούς (π.χ. ιδιωτικότητα, εμπόριο κ.ά.).

## Σκέψεις για την Υπεύθυνη AI

Όπως και άλλα γλωσσικά μοντέλα, τα μοντέλα της σειράς Phi μπορεί να συμπεριφέρονται με τρόπους που είναι άδικοι, αναξιόπιστοι ή προσβλητικοί. Μερικές από τις περιοριστικές συμπεριφορές που πρέπει να γνωρίζετε περιλαμβάνουν:

**Ποιότητα Υπηρεσίας:** Τα μοντέλα Phi εκπαιδεύονται κυρίως σε αγγλικό κείμενο. Γλώσσες εκτός των αγγλικών θα παρουσιάσουν χειρότερη απόδοση. Παραλλαγές της αγγλικής γλώσσας με λιγότερη εκπροσώπηση στα δεδομένα εκπαίδευσης μπορεί να έχουν χειρότερη απόδοση σε σχέση με το τυπικό αμερικανικό αγγλικά.

**Αναπαράσταση Βλαβών & Διατήρηση Στερεοτύπων:** Αυτά τα μοντέλα μπορεί να υπερεκπροσωπούν ή να υποεκπροσωπούν ομάδες ανθρώπων, να διαγράφουν την εκπροσώπηση ορισμένων ομάδων ή να ενισχύουν υποτιμητικά ή αρνητικά στερεότυπα. Παρά την εκπαίδευση ασφάλειας μετά την αρχική εκπαίδευση, αυτοί οι περιορισμοί μπορεί να παραμένουν λόγω διαφορετικών επιπέδων εκπροσώπησης των ομάδων ή της ύπαρξης παραδειγμάτων αρνητικών στερεοτύπων στα δεδομένα εκπαίδευσης που αντικατοπτρίζουν πραγματικά κοινωνικά πρότυπα και προκαταλήψεις.

**Ακατάλληλο ή Προσβλητικό Περιεχόμενο:** Τα μοντέλα αυτά μπορεί να παράγουν άλλους τύπους ακατάλληλου ή προσβλητικού περιεχομένου, γεγονός που μπορεί να τα καθιστά ακατάλληλα για χρήση σε ευαίσθητα περιβάλλοντα χωρίς επιπλέον μέτρα μετριασμού που να είναι ειδικά για την περίπτωση χρήσης.  
Αξιοπιστία Πληροφορίας: Τα γλωσσικά μοντέλα μπορεί να παράγουν ανοησίες ή να επινοούν περιεχόμενο που ακούγεται λογικό αλλά είναι ανακριβές ή ξεπερασμένο.

**Περιορισμένο Εύρος για Κώδικα:** Η πλειονότητα των δεδομένων εκπαίδευσης του Phi-3 βασίζεται σε Python και χρησιμοποιεί κοινά πακέτα όπως "typing, math, random, collections, datetime, itertools". Αν το μοντέλο παράγει Python scripts που χρησιμοποιούν άλλα πακέτα ή scripts σε άλλες γλώσσες, συνιστούμε έντονα στους χρήστες να επαληθεύουν χειροκίνητα όλες τις χρήσεις API.

Οι προγραμματιστές θα πρέπει να εφαρμόζουν τις βέλτιστες πρακτικές υπεύθυνης AI και είναι υπεύθυνοι να διασφαλίζουν ότι μια συγκεκριμένη χρήση συμμορφώνεται με τους σχετικούς νόμους και κανονισμούς (π.χ. ιδιωτικότητα, εμπόριο κ.ά.). Σημαντικοί τομείς προς εξέταση περιλαμβάνουν:

**Κατανομή:** Τα μοντέλα μπορεί να μην είναι κατάλληλα για σενάρια που θα μπορούσαν να έχουν σημαντική επίπτωση σε νομική κατάσταση ή στην κατανομή πόρων ή ευκαιριών ζωής (π.χ. στέγαση, απασχόληση, πίστωση κ.ά.) χωρίς περαιτέρω αξιολογήσεις και επιπλέον τεχνικές απομάκρυνσης προκαταλήψεων.

**Σενάρια Υψηλού Κινδύνου:** Οι προγραμματιστές θα πρέπει να αξιολογούν την καταλληλότητα χρήσης των μοντέλων σε σενάρια υψηλού κινδύνου όπου άδικες, αναξιόπιστες ή προσβλητικές απαντήσεις μπορεί να έχουν πολύ μεγάλο κόστος ή να προκαλέσουν βλάβη. Αυτό περιλαμβάνει την παροχή συμβουλών σε ευαίσθητους ή εξειδικευμένους τομείς όπου η ακρίβεια και η αξιοπιστία είναι κρίσιμες (π.χ. νομικές ή ιατρικές συμβουλές). Πρέπει να εφαρμόζονται επιπλέον μέτρα ασφαλείας σε επίπεδο εφαρμογής ανάλογα με το πλαίσιο ανάπτυξης.

**Παραπληροφόρηση:** Τα μοντέλα μπορεί να παράγουν ανακριβείς πληροφορίες. Οι προγραμματιστές θα πρέπει να ακολουθούν βέλτιστες πρακτικές διαφάνειας και να ενημερώνουν τους τελικούς χρήστες ότι αλληλεπιδρούν με ένα σύστημα AI. Σε επίπεδο εφαρμογής, οι προγραμματιστές μπορούν να δημιουργήσουν μηχανισμούς ανατροφοδότησης και ροές εργασίας για να βασίζουν τις απαντήσεις σε πληροφορίες συγκεκριμένες για την περίπτωση χρήσης και το πλαίσιο, μια τεχνική γνωστή ως Retrieval Augmented Generation (RAG).

**Παραγωγή Βλαβερών Περιεχομένων:** Οι προγραμματιστές θα πρέπει να αξιολογούν τις απαντήσεις με βάση το πλαίσιο και να χρησιμοποιούν διαθέσιμους ταξινομητές ασφάλειας ή προσαρμοσμένες λύσεις κατάλληλες για την περίπτωση χρήσης τους.

**Κακόβουλη Χρήση:** Άλλες μορφές κακόβουλης χρήσης, όπως απάτη, ανεπιθύμητα μηνύματα ή παραγωγή κακόβουλου λογισμικού, μπορεί να είναι πιθανές, και οι προγραμματιστές πρέπει να διασφαλίζουν ότι οι εφαρμογές τους δεν παραβιάζουν τους ισχύοντες νόμους και κανονισμούς.

### Fine-tuning και Ασφάλεια Περιεχομένου AI

Μετά το fine-tuning ενός μοντέλου, συνιστούμε έντονα τη χρήση των μέτρων [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) για την παρακολούθηση του περιεχομένου που παράγουν τα μοντέλα, την αναγνώριση και τον αποκλεισμό πιθανών κινδύνων, απειλών και θεμάτων ποιότητας.

![Phi3AISafety](../../../../../translated_images/01.phi3aisafety.c0d7fc42f5a5c40507c5e8be556615b8377a63b8764865d057d4faac3757a478.el.png)

Το [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) υποστηρίζει τόσο κείμενο όσο και εικόνες. Μπορεί να αναπτυχθεί στο cloud, σε απομονωμένα containers και σε edge/ενσωματωμένες συσκευές.

## Επισκόπηση του Azure AI Content Safety

Το Azure AI Content Safety δεν είναι μια λύση «ένα μέγεθος για όλους»· μπορεί να προσαρμοστεί ώστε να ευθυγραμμίζεται με τις συγκεκριμένες πολιτικές των επιχειρήσεων. Επιπλέον, τα πολυγλωσσικά μοντέλα του του επιτρέπουν να κατανοεί πολλές γλώσσες ταυτόχρονα.

![AIContentSafety](../../../../../translated_images/01.AIcontentsafety.a288819b8ce8da1a56cf708aff010a541799d002ae7ae84bb819b19ab8950591.el.png)

- **Azure AI Content Safety**  
- **Microsoft Developer**  
- **5 βίντεο**

Η υπηρεσία Azure AI Content Safety ανιχνεύει βλαβερό περιεχόμενο που δημιουργείται από χρήστες και AI σε εφαρμογές και υπηρεσίες. Περιλαμβάνει APIs για κείμενο και εικόνες που επιτρέπουν την ανίχνευση βλαβερών ή ακατάλληλων υλικών.

[AI Content Safety Playlist](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

**Αποποίηση ευθυνών**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που επιδιώκουμε την ακρίβεια, παρακαλούμε να γνωρίζετε ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή λανθασμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.