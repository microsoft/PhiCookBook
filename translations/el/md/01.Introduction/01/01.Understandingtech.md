<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:22:28+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "el"
}
-->
# Κύριες τεχνολογίες που αναφέρονται περιλαμβάνουν

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - ένα χαμηλού επιπέδου API για επιτάχυνση μηχανικής μάθησης με υλικό, βασισμένο στο DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - μια πλατφόρμα παράλληλου υπολογισμού και μοντέλο API που αναπτύχθηκε από τη Nvidia, επιτρέποντας γενικού σκοπού επεξεργασία σε GPUs.  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - μια ανοιχτή μορφή σχεδιασμένη να αναπαριστά μοντέλα μηχανικής μάθησης, προσφέροντας διαλειτουργικότητα μεταξύ διαφορετικών πλαισίων ML.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - μια μορφή που χρησιμοποιείται για την αναπαράσταση και ενημέρωση μοντέλων μηχανικής μάθησης, ιδιαίτερα χρήσιμη για μικρότερα γλωσσικά μοντέλα που μπορούν να τρέξουν αποτελεσματικά σε CPUs με κβαντισμό 4-8bit.

## DirectML

Το DirectML είναι ένα χαμηλού επιπέδου API που επιτρέπει επιτάχυνση μηχανικής μάθησης με υλικό. Βασίζεται στο DirectX 12 για αξιοποίηση της επιτάχυνσης GPU και είναι ανεξάρτητο από τον προμηθευτή, δηλαδή δεν απαιτεί αλλαγές στον κώδικα για να λειτουργήσει σε διαφορετικούς κατασκευαστές GPU. Χρησιμοποιείται κυρίως για εκπαίδευση μοντέλων και εργασίες inferencing σε GPUs.

Όσον αφορά την υποστήριξη υλικού, το DirectML έχει σχεδιαστεί να λειτουργεί με ευρύ φάσμα GPUs, συμπεριλαμβανομένων των ενσωματωμένων και διακριτών AMD GPUs, ενσωματωμένων Intel GPUs και διακριτών NVIDIA GPUs. Αποτελεί μέρος της πλατφόρμας Windows AI και υποστηρίζεται στα Windows 10 & 11, επιτρέποντας εκπαίδευση και inferencing μοντέλων σε οποιαδήποτε συσκευή Windows.

Υπήρξαν ενημερώσεις και ευκαιρίες σχετικά με το DirectML, όπως η υποστήριξη έως 150 ONNX operators και η χρήση του τόσο από το ONNX runtime όσο και το WinML. Υποστηρίζεται από σημαντικούς Integrated Hardware Vendors (IHVs), ο καθένας υλοποιώντας διάφορες μεταεντολές.

## CUDA

Η CUDA, που σημαίνει Compute Unified Device Architecture, είναι μια πλατφόρμα παράλληλου υπολογισμού και μοντέλο API που δημιουργήθηκε από τη Nvidia. Επιτρέπει στους προγραμματιστές να χρησιμοποιούν μια GPU με υποστήριξη CUDA για γενικού σκοπού επεξεργασία – μια προσέγγιση γνωστή ως GPGPU (General-Purpose computing on Graphics Processing Units). Η CUDA είναι βασικός παράγοντας επιτάχυνσης των GPUs της Nvidia και χρησιμοποιείται ευρέως σε διάφορους τομείς, όπως μηχανική μάθηση, επιστημονικούς υπολογισμούς και επεξεργασία βίντεο.

Η υποστήριξη υλικού για την CUDA αφορά αποκλειστικά τις GPUs της Nvidia, καθώς είναι ιδιόκτητη τεχνολογία που ανέπτυξε η Nvidia. Κάθε αρχιτεκτονική υποστηρίζει συγκεκριμένες εκδόσεις του CUDA toolkit, το οποίο παρέχει τις απαραίτητες βιβλιοθήκες και εργαλεία για τους προγραμματιστές να δημιουργούν και να τρέχουν εφαρμογές CUDA.

## ONNX

Το ONNX (Open Neural Network Exchange) είναι μια ανοιχτή μορφή σχεδιασμένη να αναπαριστά μοντέλα μηχανικής μάθησης. Παρέχει ορισμό ενός επεκτάσιμου μοντέλου υπολογιστικού γραφήματος, καθώς και ορισμούς ενσωματωμένων τελεστών και τυπικών τύπων δεδομένων. Το ONNX επιτρέπει στους προγραμματιστές να μεταφέρουν μοντέλα μεταξύ διαφορετικών πλαισίων ML, διευκολύνοντας τη διαλειτουργικότητα και καθιστώντας πιο εύκολη τη δημιουργία και ανάπτυξη εφαρμογών AI.

Το Phi3 mini μπορεί να τρέξει με ONNX Runtime σε CPU και GPU σε διάφορες συσκευές, συμπεριλαμβανομένων πλατφορμών server, Windows, Linux και Mac desktops, καθώς και σε mobile CPUs.  
Οι βελτιστοποιημένες ρυθμίσεις που έχουμε προσθέσει είναι

- ONNX μοντέλα για int4 DML: Κβαντισμένα σε int4 μέσω AWQ  
- ONNX μοντέλο για fp16 CUDA  
- ONNX μοντέλο για int4 CUDA: Κβαντισμένα σε int4 μέσω RTN  
- ONNX μοντέλο για int4 CPU και Mobile: Κβαντισμένα σε int4 μέσω RTN

## Llama.cpp

Το Llama.cpp είναι μια ανοιχτού κώδικα βιβλιοθήκη λογισμικού γραμμένη σε C++. Εκτελεί inferencing σε διάφορα Μεγάλα Γλωσσικά Μοντέλα (LLMs), συμπεριλαμβανομένου του Llama. Αναπτύχθηκε παράλληλα με τη βιβλιοθήκη ggml (μια γενικού σκοπού βιβλιοθήκη τανυστών), με στόχο να προσφέρει ταχύτερο inferencing και χαμηλότερη χρήση μνήμης σε σύγκριση με την αρχική υλοποίηση σε Python. Υποστηρίζει βελτιστοποίηση υλικού, κβαντισμό και προσφέρει απλό API και παραδείγματα. Αν ενδιαφέρεστε για αποδοτικό inferencing LLM, το llama.cpp αξίζει να το εξερευνήσετε, καθώς το Phi3 μπορεί να τρέξει Llama.cpp.

## GGUF

Το GGUF (Generic Graph Update Format) είναι μια μορφή που χρησιμοποιείται για την αναπαράσταση και ενημέρωση μοντέλων μηχανικής μάθησης. Είναι ιδιαίτερα χρήσιμο για μικρότερα γλωσσικά μοντέλα (SLMs) που μπορούν να τρέξουν αποτελεσματικά σε CPUs με κβαντισμό 4-8bit. Το GGUF είναι ωφέλιμο για γρήγορη πρωτοτυποποίηση και εκτέλεση μοντέλων σε edge συσκευές ή σε batch εργασίες όπως CI/CD pipelines.

**Αποποίηση Ευθυνών**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης με τεχνητή νοημοσύνη [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθεια για ακρίβεια, παρακαλούμε να λάβετε υπόψη ότι οι αυτόματες μεταφράσεις μπορεί να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα πρέπει να θεωρείται η επίσημη πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή λανθασμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.