<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:44:33+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "el"
}
-->
# Κύριες τεχνολογίες που αναφέρονται περιλαμβάνουν

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - ένα χαμηλού επιπέδου API για επιταχυνόμενη από υλικό μηχανική μάθηση, βασισμένο στο DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - μια πλατφόρμα παράλληλου υπολογισμού και μοντέλο API που αναπτύχθηκε από την Nvidia, επιτρέποντας γενικού σκοπού επεξεργασία σε μονάδες επεξεργασίας γραφικών (GPUs).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - ένα ανοιχτό φορμά σχεδιασμένο να αναπαριστά μοντέλα μηχανικής μάθησης, προσφέροντας διαλειτουργικότητα μεταξύ διαφορετικών πλαισίων ML.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - ένα φορμά που χρησιμοποιείται για την αναπαράσταση και ενημέρωση μοντέλων μηχανικής μάθησης, ιδιαίτερα χρήσιμο για μικρότερα γλωσσικά μοντέλα που μπορούν να τρέξουν αποτελεσματικά σε CPUs με κβαντοποίηση 4-8bit.

## DirectML

Το DirectML είναι ένα χαμηλού επιπέδου API που επιτρέπει επιτάχυνση μηχανικής μάθησης μέσω υλικού. Βασίζεται στο DirectX 12 για να αξιοποιήσει την επιτάχυνση GPU και είναι ανεξάρτητο από τον κατασκευαστή, που σημαίνει ότι δεν απαιτούνται αλλαγές στον κώδικα για να λειτουργήσει σε διαφορετικούς κατασκευαστές GPU. Χρησιμοποιείται κυρίως για εκπαίδευση μοντέλων και εργασίες inferencing σε GPUs.

Όσον αφορά την υποστήριξη υλικού, το DirectML έχει σχεδιαστεί να λειτουργεί με ευρύ φάσμα GPUs, συμπεριλαμβανομένων των ενσωματωμένων και διακριτών GPUs της AMD, των ενσωματωμένων GPUs της Intel και των διακριτών GPUs της NVIDIA. Αποτελεί μέρος της πλατφόρμας Windows AI και υποστηρίζεται στα Windows 10 & 11, επιτρέποντας εκπαίδευση και inferencing μοντέλων σε οποιαδήποτε συσκευή Windows.

Υπήρξαν ενημερώσεις και ευκαιρίες σχετικά με το DirectML, όπως η υποστήριξη έως και 150 ONNX operators και η χρήση του τόσο από το ONNX runtime όσο και από το WinML. Υποστηρίζεται από σημαντικούς Integrated Hardware Vendors (IHVs), καθένας από τους οποίους υλοποιεί διάφορες metacommands.

## CUDA

Η CUDA, που σημαίνει Compute Unified Device Architecture, είναι μια πλατφόρμα παράλληλου υπολογισμού και μοντέλο API που δημιουργήθηκε από την Nvidia. Επιτρέπει στους προγραμματιστές να χρησιμοποιούν μια GPU με υποστήριξη CUDA για γενικού σκοπού επεξεργασία – μια προσέγγιση γνωστή ως GPGPU (General-Purpose computing on Graphics Processing Units). Η CUDA είναι βασικός παράγοντας επιτάχυνσης των GPUs της Nvidia και χρησιμοποιείται ευρέως σε διάφορους τομείς, όπως μηχανική μάθηση, επιστημονικούς υπολογισμούς και επεξεργασία βίντεο.

Η υποστήριξη υλικού για την CUDA είναι συγκεκριμένη για τις GPUs της Nvidia, καθώς πρόκειται για ιδιόκτητη τεχνολογία που αναπτύχθηκε από την Nvidia. Κάθε αρχιτεκτονική υποστηρίζει συγκεκριμένες εκδόσεις του CUDA toolkit, το οποίο παρέχει τις απαραίτητες βιβλιοθήκες και εργαλεία για τους προγραμματιστές ώστε να δημιουργούν και να τρέχουν εφαρμογές CUDA.

## ONNX

Το ONNX (Open Neural Network Exchange) είναι ένα ανοιχτό φορμά σχεδιασμένο να αναπαριστά μοντέλα μηχανικής μάθησης. Παρέχει έναν ορισμό ενός επεκτάσιμου μοντέλου υπολογιστικού γραφήματος, καθώς και ορισμούς ενσωματωμένων operators και τυποποιημένων τύπων δεδομένων. Το ONNX επιτρέπει στους προγραμματιστές να μεταφέρουν μοντέλα μεταξύ διαφορετικών πλαισίων ML, διευκολύνοντας τη διαλειτουργικότητα και καθιστώντας πιο εύκολη τη δημιουργία και ανάπτυξη εφαρμογών AI.

Το Phi3 mini μπορεί να τρέξει με ONNX Runtime σε CPU και GPU σε διάφορες συσκευές, συμπεριλαμβανομένων πλατφορμών server, Windows, Linux και Mac desktops, καθώς και σε mobile CPUs.  
Οι βελτιστοποιημένες ρυθμίσεις που έχουμε προσθέσει είναι:

- ONNX μοντέλα για int4 DML: Κβαντοποιημένα σε int4 μέσω AWQ  
- ONNX μοντέλο για fp16 CUDA  
- ONNX μοντέλο για int4 CUDA: Κβαντοποιημένα σε int4 μέσω RTN  
- ONNX μοντέλο για int4 CPU και Mobile: Κβαντοποιημένα σε int4 μέσω RTN

## Llama.cpp

Το Llama.cpp είναι μια ανοιχτού κώδικα βιβλιοθήκη γραμμένη σε C++. Εκτελεί inferencing σε διάφορα Μεγάλα Γλωσσικά Μοντέλα (LLMs), συμπεριλαμβανομένου του Llama. Αναπτύχθηκε παράλληλα με τη βιβλιοθήκη ggml (μια γενικού σκοπού βιβλιοθήκη tensor) και στοχεύει να προσφέρει ταχύτερο inferencing και χαμηλότερη κατανάλωση μνήμης σε σύγκριση με την αρχική υλοποίηση σε Python. Υποστηρίζει βελτιστοποίηση υλικού, κβαντοποίηση και προσφέρει απλό API και παραδείγματα. Αν ενδιαφέρεστε για αποδοτικό inferencing LLM, το llama.cpp αξίζει να το εξερευνήσετε, καθώς το Phi3 μπορεί να τρέξει Llama.cpp.

## GGUF

Το GGUF (Generic Graph Update Format) είναι ένα φορμά που χρησιμοποιείται για την αναπαράσταση και ενημέρωση μοντέλων μηχανικής μάθησης. Είναι ιδιαίτερα χρήσιμο για μικρότερα γλωσσικά μοντέλα (SLMs) που μπορούν να τρέξουν αποτελεσματικά σε CPUs με κβαντοποίηση 4-8bit. Το GGUF είναι ωφέλιμο για γρήγορο πρωτοτυποποίηση και εκτέλεση μοντέλων σε edge συσκευές ή σε batch εργασίες όπως CI/CD pipelines.

**Αποποίηση ευθυνών**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που επιδιώκουμε την ακρίβεια, παρακαλούμε να γνωρίζετε ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή λανθασμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.