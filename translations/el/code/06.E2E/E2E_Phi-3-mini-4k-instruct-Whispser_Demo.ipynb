{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Διαδραστικό Phi 3 Mini 4K Instruct Chatbot με Whisper\n",
    "\n",
    "### Εισαγωγή:\n",
    "Το Διαδραστικό Phi 3 Mini 4K Instruct Chatbot είναι ένα εργαλείο που επιτρέπει στους χρήστες να αλληλεπιδρούν με το Microsoft Phi 3 Mini 4K instruct demo χρησιμοποιώντας κείμενο ή ηχητική είσοδο. Το chatbot μπορεί να χρησιμοποιηθεί για διάφορες εργασίες, όπως μετάφραση, ενημερώσεις καιρού και γενική συλλογή πληροφοριών.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "Δημιουργήστε το Huggingface Access Token σας\n",
    "\n",
    "Δημιουργήστε ένα νέο token  \n",
    "Δώστε ένα νέο όνομα  \n",
    "Επιλέξτε δικαιώματα εγγραφής  \n",
    "Αντιγράψτε το token και αποθηκεύστε το σε ασφαλές μέρος\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Το ακόλουθο Python script εκτελεί δύο κύριες λειτουργίες: την εισαγωγή της ενότητας `os` και τη ρύθμιση μιας μεταβλητής περιβάλλοντος.\n",
    "\n",
    "1. Εισαγωγή της ενότητας `os`:\n",
    "   - Η ενότητα `os` στη Python παρέχει έναν τρόπο αλληλεπίδρασης με το λειτουργικό σύστημα. Επιτρέπει την εκτέλεση διάφορων εργασιών που σχετίζονται με το λειτουργικό σύστημα, όπως η πρόσβαση σε μεταβλητές περιβάλλοντος, η εργασία με αρχεία και καταλόγους κ.λπ.\n",
    "   - Σε αυτόν τον κώδικα, η ενότητα `os` εισάγεται χρησιμοποιώντας τη δήλωση `import`. Αυτή η δήλωση καθιστά διαθέσιμη τη λειτουργικότητα της ενότητας `os` για χρήση στο τρέχον Python script.\n",
    "\n",
    "2. Ρύθμιση μιας μεταβλητής περιβάλλοντος:\n",
    "   - Μια μεταβλητή περιβάλλοντος είναι μια τιμή που μπορεί να προσπελαστεί από προγράμματα που εκτελούνται στο λειτουργικό σύστημα. Είναι ένας τρόπος αποθήκευσης ρυθμίσεων παραμετροποίησης ή άλλων πληροφοριών που μπορούν να χρησιμοποιηθούν από πολλαπλά προγράμματα.\n",
    "   - Σε αυτόν τον κώδικα, μια νέα μεταβλητή περιβάλλοντος ορίζεται χρησιμοποιώντας το λεξικό `os.environ`. Το κλειδί του λεξικού είναι `'HF_TOKEN'`, και η τιμή του ανατίθεται από τη μεταβλητή `HUGGINGFACE_TOKEN`.\n",
    "   - Η μεταβλητή `HUGGINGFACE_TOKEN` ορίζεται ακριβώς πάνω από αυτό το κομμάτι κώδικα και της ανατίθεται μια συμβολοσειρά `\"hf_**************\"` χρησιμοποιώντας τη σύνταξη `#@param`. Αυτή η σύνταξη χρησιμοποιείται συχνά σε Jupyter notebooks για να επιτρέψει την εισαγωγή δεδομένων από τον χρήστη και τη ρύθμιση παραμέτρων απευθείας από το περιβάλλον του notebook.\n",
    "   - Με τη ρύθμιση της μεταβλητής περιβάλλοντος `'HF_TOKEN'`, αυτή μπορεί να προσπελαστεί από άλλα μέρη του προγράμματος ή από άλλα προγράμματα που εκτελούνται στο ίδιο λειτουργικό σύστημα.\n",
    "\n",
    "Συνολικά, αυτός ο κώδικας εισάγει την ενότητα `os` και ορίζει μια μεταβλητή περιβάλλοντος με όνομα `'HF_TOKEN'` και τιμή που παρέχεται από τη μεταβλητή `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αυτό το απόσπασμα κώδικα ορίζει μια συνάρτηση που ονομάζεται clear_output, η οποία χρησιμοποιείται για την εκκαθάριση της εξόδου του τρέχοντος κελιού σε Jupyter Notebook ή IPython. Ας αναλύσουμε τον κώδικα και κατανοήσουμε τη λειτουργικότητά του:\n",
    "\n",
    "Η συνάρτηση clear_output δέχεται μία παράμετρο που ονομάζεται wait, η οποία είναι μια λογική τιμή (boolean). Από προεπιλογή, το wait έχει την τιμή False. Αυτή η παράμετρος καθορίζει εάν η συνάρτηση πρέπει να περιμένει μέχρι να είναι διαθέσιμη νέα έξοδος για να αντικαταστήσει την υπάρχουσα έξοδο πριν την εκκαθάριση.\n",
    "\n",
    "Η ίδια η συνάρτηση χρησιμοποιείται για την εκκαθάριση της εξόδου του τρέχοντος κελιού. Σε Jupyter Notebook ή IPython, όταν ένα κελί παράγει έξοδο, όπως εκτυπωμένο κείμενο ή γραφικές παραστάσεις, αυτή η έξοδος εμφανίζεται κάτω από το κελί. Η συνάρτηση clear_output επιτρέπει την εκκαθάριση αυτής της εξόδου.\n",
    "\n",
    "Η υλοποίηση της συνάρτησης δεν παρέχεται στο απόσπασμα κώδικα, όπως υποδεικνύεται από τις αποσιωπητικές (...). Οι αποσιωπητικές αντιπροσωπεύουν ένα placeholder για τον πραγματικό κώδικα που εκτελεί την εκκαθάριση της εξόδου. Η υλοποίηση της συνάρτησης μπορεί να περιλαμβάνει αλληλεπίδραση με το API του Jupyter Notebook ή του IPython για την αφαίρεση της υπάρχουσας εξόδου από το κελί.\n",
    "\n",
    "Συνολικά, αυτή η συνάρτηση παρέχει έναν εύχρηστο τρόπο για την εκκαθάριση της εξόδου του τρέχοντος κελιού σε Jupyter Notebook ή IPython, διευκολύνοντας τη διαχείριση και την ενημέρωση της εμφανιζόμενης εξόδου κατά τη διάρκεια διαδραστικών συνεδριών προγραμματισμού.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εκτέλεση μετατροπής κειμένου σε ομιλία (TTS) χρησιμοποιώντας την υπηρεσία Edge TTS. Ας εξετάσουμε τις σχετικές υλοποιήσεις συναρτήσεων μία προς μία:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Αυτή η συνάρτηση λαμβάνει μια τιμή εισόδου και υπολογίζει τη συμβολοσειρά ταχύτητας για τη φωνή TTS. Η τιμή εισόδου αντιπροσωπεύει την επιθυμητή ταχύτητα της ομιλίας, όπου η τιμή 1 αντιστοιχεί στην κανονική ταχύτητα. Η συνάρτηση υπολογίζει τη συμβολοσειρά ταχύτητας αφαιρώντας 1 από την τιμή εισόδου, πολλαπλασιάζοντάς την με το 100 και στη συνέχεια καθορίζοντας το πρόσημο με βάση το αν η τιμή εισόδου είναι μεγαλύτερη ή ίση με 1. Η συνάρτηση επιστρέφει τη συμβολοσειρά ταχύτητας στη μορφή \"{sign}{rate}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: Αυτή η συνάρτηση λαμβάνει ένα κείμενο εισόδου και μια γλώσσα ως παραμέτρους. Χωρίζει το κείμενο εισόδου σε τμήματα με βάση τους κανόνες που ισχύουν για τη συγκεκριμένη γλώσσα. Σε αυτήν την υλοποίηση, αν η γλώσσα είναι \"Αγγλικά\", η συνάρτηση χωρίζει το κείμενο σε κάθε τελεία (\".\") και αφαιρεί τυχόν κενά στην αρχή ή στο τέλος. Στη συνέχεια, προσθέτει μια τελεία σε κάθε τμήμα και επιστρέφει τη λίστα των τμημάτων.\n",
    "\n",
    "3. `tts_file_name(text)`: Αυτή η συνάρτηση δημιουργεί ένα όνομα αρχείου για το ηχητικό αρχείο TTS με βάση το κείμενο εισόδου. Εκτελεί διάφορους μετασχηματισμούς στο κείμενο: αφαιρεί μια τελεία στο τέλος (αν υπάρχει), μετατρέπει το κείμενο σε πεζά, αφαιρεί κενά στην αρχή και στο τέλος και αντικαθιστά τα κενά με κάτω παύλες. Στη συνέχεια, περικόπτει το κείμενο σε μέγιστο μήκος 25 χαρακτήρων (αν είναι μεγαλύτερο) ή χρησιμοποιεί το πλήρες κείμενο αν είναι κενό. Τέλος, δημιουργεί μια τυχαία συμβολοσειρά χρησιμοποιώντας τη μονάδα [`uuid`] και τη συνδυάζει με το περικομμένο κείμενο για να δημιουργήσει το όνομα αρχείου στη μορφή \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Αυτή η συνάρτηση συγχωνεύει πολλαπλά ηχητικά αρχεία σε ένα ενιαίο ηχητικό αρχείο. Λαμβάνει μια λίστα με διαδρομές ηχητικών αρχείων και μια διαδρομή εξόδου ως παραμέτρους. Η συνάρτηση αρχικοποιεί ένα κενό αντικείμενο `AudioSegment` που ονομάζεται [`merged_audio`]. Στη συνέχεια, επαναλαμβάνει κάθε διαδρομή ηχητικού αρχείου, φορτώνει το ηχητικό αρχείο χρησιμοποιώντας τη μέθοδο `AudioSegment.from_file()` από τη βιβλιοθήκη `pydub` και προσθέτει το τρέχον ηχητικό αρχείο στο αντικείμενο [`merged_audio`]. Τέλος, εξάγει το συγχωνευμένο ηχητικό αρχείο στη συγκεκριμένη διαδρομή εξόδου σε μορφή MP3.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: Αυτή η συνάρτηση εκτελεί τη λειτουργία TTS χρησιμοποιώντας την υπηρεσία Edge TTS. Λαμβάνει μια λίστα με τμήματα κειμένου, την ταχύτητα της ομιλίας, το όνομα της φωνής και τη διαδρομή αποθήκευσης ως παραμέτρους. Αν ο αριθμός των τμημάτων είναι μεγαλύτερος από 1, η συνάρτηση δημιουργεί έναν κατάλογο για την αποθήκευση των ηχητικών αρχείων των επιμέρους τμημάτων. Στη συνέχεια, επαναλαμβάνει κάθε τμήμα, κατασκευάζει μια εντολή Edge TTS χρησιμοποιώντας τη συνάρτηση `calculate_rate_string()`, το όνομα της φωνής και το κείμενο του τμήματος, και εκτελεί την εντολή χρησιμοποιώντας τη συνάρτηση `os.system()`. Αν η εκτέλεση της εντολής είναι επιτυχής, προσθέτει τη διαδρομή του παραγόμενου ηχητικού αρχείου σε μια λίστα. Μετά την επεξεργασία όλων των τμημάτων, συγχωνεύει τα επιμέρους ηχητικά αρχεία χρησιμοποιώντας τη συνάρτηση `merge_audio_files()` και αποθηκεύει το συγχωνευμένο ηχητικό αρχείο στη συγκεκριμένη διαδρομή αποθήκευσης. Αν υπάρχει μόνο ένα τμήμα, δημιουργεί απευθείας την εντολή Edge TTS και αποθηκεύει το ηχητικό αρχείο στη διαδρομή αποθήκευσης. Τέλος, επιστρέφει τη διαδρομή αποθήκευσης του παραγόμενου ηχητικού αρχείου.\n",
    "\n",
    "6. `random_audio_name_generate()`: Αυτή η συνάρτηση δημιουργεί ένα τυχαίο όνομα ηχητικού αρχείου χρησιμοποιώντας τη μονάδα [`uuid`]. Δημιουργεί ένα τυχαίο UUID, το μετατρέπει σε συμβολοσειρά, παίρνει τους πρώτους 8 χαρακτήρες, προσθέτει την επέκταση \".mp3\" και επιστρέφει το τυχαίο όνομα ηχητικού αρχείου.\n",
    "\n",
    "7. `talk(input_text)`: Αυτή η συνάρτηση είναι το κύριο σημείο εισόδου για την εκτέλεση της λειτουργίας TTS. Λαμβάνει ένα κείμενο εισόδου ως παράμετρο. Πρώτα ελέγχει το μήκος του κειμένου εισόδου για να καθορίσει αν είναι μια μεγάλη πρόταση (μεγαλύτερη ή ίση με 600 χαρακτήρες). Με βάση το μήκος και την τιμή της μεταβλητής `translate_text_flag`, καθορίζει τη γλώσσα και δημιουργεί τη λίστα των τμημάτων κειμένου χρησιμοποιώντας τη συνάρτηση `make_chunks()`. Στη συνέχεια, δημιουργεί μια διαδρομή αποθήκευσης για το ηχητικό αρχείο χρησιμοποιώντας τη συνάρτηση `random_audio_name_generate()`. Τέλος, καλεί τη συνάρτηση `edge_free_tts()` για να εκτελέσει τη λειτουργία TTS και επιστρέφει τη διαδρομή αποθήκευσης του παραγόμενου ηχητικού αρχείου.\n",
    "\n",
    "Συνολικά, αυτές οι συναρτήσεις συνεργάζονται για να χωρίσουν το κείμενο εισόδου σε τμήματα, να δημιουργήσουν ένα όνομα αρχείου για το ηχητικό αρχείο, να εκτελέσουν τη λειτουργία TTS χρησιμοποιώντας την υπηρεσία Edge TTS και να συγχωνεύσουν τα επιμέρους ηχητικά αρχεία σε ένα ενιαίο ηχητικό αρχείο.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η υλοποίηση δύο συναρτήσεων: convert_to_text και run_text_prompt, καθώς και η δήλωση δύο κλάσεων: str και Audio.\n",
    "\n",
    "Η συνάρτηση convert_to_text λαμβάνει ως είσοδο ένα audio_path και μεταγράφει τον ήχο σε κείμενο χρησιμοποιώντας ένα μοντέλο που ονομάζεται whisper_model. Η συνάρτηση πρώτα ελέγχει αν η σημαία gpu έχει οριστεί σε True. Εάν ναι, το whisper_model χρησιμοποιείται με συγκεκριμένες παραμέτρους όπως word_timestamps=True, fp16=True, language='English', και task='translate'. Εάν η σημαία gpu είναι False, το whisper_model χρησιμοποιείται με fp16=False. Η προκύπτουσα μεταγραφή αποθηκεύεται σε ένα αρχείο με όνομα 'scan.txt' και επιστρέφεται ως κείμενο.\n",
    "\n",
    "Η συνάρτηση run_text_prompt λαμβάνει ένα μήνυμα και ένα chat_history ως είσοδο. Χρησιμοποιεί τη συνάρτηση phi_demo για να δημιουργήσει μια απάντηση από ένα chatbot βάσει του εισερχόμενου μηνύματος. Η παραγόμενη απάντηση στη συνέχεια περνά στη συνάρτηση talk, η οποία μετατρέπει την απάντηση σε αρχείο ήχου και επιστρέφει τη διαδρομή του αρχείου. Η κλάση Audio χρησιμοποιείται για την εμφάνιση και την αναπαραγωγή του αρχείου ήχου. Ο ήχος εμφανίζεται χρησιμοποιώντας τη συνάρτηση display από τη μονάδα IPython.display, και το αντικείμενο Audio δημιουργείται με την παράμετρο autoplay=True, ώστε ο ήχος να αρχίζει να παίζει αυτόματα. Το chat_history ενημερώνεται με το εισερχόμενο μήνυμα και την παραγόμενη απάντηση, και επιστρέφονται ένα κενό string και το ενημερωμένο chat_history.\n",
    "\n",
    "Η κλάση str είναι μια ενσωματωμένη κλάση στην Python που αντιπροσωπεύει μια ακολουθία χαρακτήρων. Παρέχει διάφορες μεθόδους για τη διαχείριση και την επεξεργασία συμβολοσειρών, όπως capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill, και άλλα. Αυτές οι μέθοδοι σας επιτρέπουν να εκτελείτε λειτουργίες όπως αναζήτηση, αντικατάσταση, μορφοποίηση και επεξεργασία συμβολοσειρών.\n",
    "\n",
    "Η κλάση Audio είναι μια προσαρμοσμένη κλάση που αντιπροσωπεύει ένα αντικείμενο ήχου. Χρησιμοποιείται για τη δημιουργία ενός audio player στο περιβάλλον Jupyter Notebook. Η κλάση δέχεται διάφορες παραμέτρους όπως data, filename, url, embed, rate, autoplay, και normalize. Η παράμετρος data μπορεί να είναι ένας πίνακας numpy, μια λίστα δειγμάτων, μια συμβολοσειρά που αντιπροσωπεύει ένα όνομα αρχείου ή URL, ή δεδομένα raw PCM. Η παράμετρος filename χρησιμοποιείται για να καθορίσει ένα τοπικό αρχείο από το οποίο θα φορτωθούν τα δεδομένα ήχου, και η παράμετρος url χρησιμοποιείται για να καθορίσει ένα URL από το οποίο θα ληφθούν τα δεδομένα ήχου. Η παράμετρος embed καθορίζει αν τα δεδομένα ήχου πρέπει να ενσωματωθούν χρησιμοποιώντας ένα data URI ή να αναφερθούν από την αρχική πηγή. Η παράμετρος rate καθορίζει τον ρυθμό δειγματοληψίας των δεδομένων ήχου. Η παράμετρος autoplay καθορίζει αν ο ήχος πρέπει να αρχίσει να παίζει αυτόματα. Η παράμετρος normalize καθορίζει αν τα δεδομένα ήχου πρέπει να κανονικοποιηθούν (αναπροσαρμοστούν) στο μέγιστο δυνατό εύρος. Η κλάση Audio παρέχει επίσης μεθόδους όπως reload για την επαναφόρτωση των δεδομένων ήχου από αρχείο ή URL, και ιδιότητες όπως src_attr, autoplay_attr, και element_id_attr για την ανάκτηση των αντίστοιχων χαρακτηριστικών για το στοιχείο ήχου σε HTML.\n",
    "\n",
    "Συνολικά, αυτές οι συναρτήσεις και κλάσεις χρησιμοποιούνται για τη μεταγραφή ήχου σε κείμενο, τη δημιουργία ηχητικών απαντήσεων από ένα chatbot, και την εμφάνιση και αναπαραγωγή ήχου στο περιβάλλον Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Αποποίηση ευθύνης**:  \nΑυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτοματοποιημένες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T23:13:37+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "el"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}