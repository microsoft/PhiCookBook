{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama + OpenAI + Python\n",
    "\n",
    "## 1. Καθορίστε το όνομα του μοντέλου\n",
    "\n",
    "Αν έχετε εισαγάγει ένα διαφορετικό μοντέλο από το \"phi3:mini\", αλλάξτε την τιμή στο κελί παρακάτω. Αυτή η μεταβλητή θα χρησιμοποιηθεί στον κώδικα σε όλο το σημειωματάριο.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"phi3:mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ρύθμιση του Open AI client\n",
    "\n",
    "Συνήθως, ο OpenAI client χρησιμοποιείται με το OpenAI.com ή το Azure OpenAI για την αλληλεπίδραση με μεγάλα γλωσσικά μοντέλα.  \n",
    "Ωστόσο, μπορεί επίσης να χρησιμοποιηθεί με το Ollama, καθώς το Ollama παρέχει ένα endpoint συμβατό με OpenAI στη διεύθυνση \"http://localhost:11434/v1\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"nokeyneeded\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Δημιουργία συνομιλίας\n",
    "\n",
    "Τώρα μπορούμε να χρησιμοποιήσουμε το OpenAI SDK για να δημιουργήσουμε μια απάντηση σε μια συνομιλία. Αυτό το αίτημα θα πρέπει να δημιουργήσει ένα χαϊκού για τις γάτες:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about a hungry cat\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Μηχανική προτροπών\n",
    "\n",
    "Το πρώτο μήνυμα που αποστέλλεται στο γλωσσικό μοντέλο ονομάζεται \"μήνυμα συστήματος\" ή \"προτροπή συστήματος\" και καθορίζει τις γενικές οδηγίες για το μοντέλο.  \n",
    "Μπορείτε να παρέχετε τη δική σας προσαρμοσμένη προτροπή συστήματος για να καθοδηγήσετε το γλωσσικό μοντέλο να παράγει αποτελέσματα με διαφορετικό τρόπο.  \n",
    "Τροποποιήστε το `SYSTEM_MESSAGE` παρακάτω ώστε να απαντά όπως ο αγαπημένος σας διάσημος χαρακτήρας από ταινία ή τηλεοπτική σειρά, ή πάρτε έμπνευση για άλλες προτροπές συστήματος από το [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts?tab=readme-ov-file#prompts).\n",
    "\n",
    "Αφού προσαρμόσετε το μήνυμα συστήματος, δώστε την πρώτη ερώτηση χρήστη στο `USER_MESSAGE`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "I want you to act like Elmo from Sesame Street.\n",
    "I want you to respond and answer like Elmo using the tone, manner and vocabulary that Elmo would use.\n",
    "Do not write any explanations. Only answer like Elmo.\n",
    "You must know all of the knowledge of Elmo, and nothing more.\n",
    "\"\"\"\n",
    "\n",
    "USER_MESSAGE = \"\"\"\n",
    "Hi Elmo, how are you doing today?\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Παραδείγματα λίγων βημάτων\n",
    "\n",
    "Ένας άλλος τρόπος καθοδήγησης ενός γλωσσικού μοντέλου είναι να παρέχουμε \"λίγα βήματα\", μια ακολουθία παραδειγμάτων ερωτήσεων/απαντήσεων που δείχνουν πώς πρέπει να απαντά.\n",
    "\n",
    "Το παρακάτω παράδειγμα προσπαθεί να κάνει ένα γλωσσικό μοντέλο να λειτουργήσει σαν βοηθός διδασκαλίας, παρέχοντας μερικά παραδείγματα ερωτήσεων και απαντήσεων που θα μπορούσε να δώσει ένας βοηθός διδασκαλίας, και στη συνέχεια δίνει στο μοντέλο μια ερώτηση που θα μπορούσε να κάνει ένας φοιτητής.\n",
    "\n",
    "Δοκιμάστε το πρώτα και μετά τροποποιήστε το `SYSTEM_MESSAGE`, `EXAMPLES` και `USER_MESSAGE` για ένα νέο σενάριο.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are a helpful assistant that helps students with their homework.\n",
    "Instead of providing the full answer, you respond with a hint or a clue.\n",
    "\"\"\"\n",
    "\n",
    "EXAMPLES = [\n",
    "    (\n",
    "        \"What is the capital of France?\",\n",
    "        \"Can you remember the name of the city that is known for the Eiffel Tower?\"\n",
    "    ),\n",
    "    (\n",
    "        \"What is the square root of 144?\",\n",
    "        \"What number multiplied by itself equals 144?\"\n",
    "    ),\n",
    "    (   \"What is the atomic number of oxygen?\",\n",
    "        \"How many protons does an oxygen atom have?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "USER_MESSAGE = \"What is the largest planet in our solar system?\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": EXAMPLES[0][0]},\n",
    "        {\"role\": \"assistant\", \"content\": EXAMPLES[0][1]},\n",
    "        {\"role\": \"user\", \"content\": EXAMPLES[1][0]},\n",
    "        {\"role\": \"assistant\", \"content\": EXAMPLES[1][1]},\n",
    "        {\"role\": \"user\", \"content\": EXAMPLES[2][0]},\n",
    "        {\"role\": \"assistant\", \"content\": EXAMPLES[2][1]},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE},\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ανάκτηση Ενισχυμένης Γενεσιμότητας\n",
    "\n",
    "Η RAG (Retrieval Augmented Generation) είναι μια τεχνική που επιτρέπει σε ένα γλωσσικό μοντέλο να απαντά με ακρίβεια σε ερωτήσεις για έναν συγκεκριμένο τομέα, πρώτα ανακτώντας σχετικές πληροφορίες από μια πηγή γνώσης και στη συνέχεια δημιουργώντας μια απάντηση βασισμένη σε αυτές τις πληροφορίες.\n",
    "\n",
    "Έχουμε παρέχει ένα τοπικό αρχείο CSV με δεδομένα σχετικά με υβριδικά αυτοκίνητα. Ο παρακάτω κώδικας διαβάζει το αρχείο CSV, αναζητά αντιστοιχίες με την ερώτηση του χρήστη και στη συνέχεια δημιουργεί μια απάντηση βασισμένη στις πληροφορίες που βρέθηκαν. Σημειώστε ότι αυτό θα πάρει περισσότερο χρόνο από οποιοδήποτε από τα προηγούμενα παραδείγματα, καθώς στέλνει περισσότερα δεδομένα στο μοντέλο. Εάν παρατηρήσετε ότι η απάντηση εξακολουθεί να μην βασίζεται στα δεδομένα, μπορείτε να δοκιμάσετε την προσαρμογή του συστήματος ή να δοκιμάσετε άλλα μοντέλα. Γενικά, η RAG είναι πιο αποτελεσματική είτε με μεγαλύτερα μοντέλα είτε με εκδόσεις SLM που έχουν υποστεί λεπτομερή προσαρμογή.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are a helpful assistant that answers questions about cars based off a hybrid car data set.\n",
    "You must use the data set to answer the questions, you should not provide any information that is not in the provided sources.\n",
    "\"\"\"\n",
    "\n",
    "USER_MESSAGE = \"how fast is a prius?\"\n",
    "\n",
    "# Open the CSV and store in a list\n",
    "with open(\"hybrid.csv\", \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    rows = list(reader)\n",
    "\n",
    "# Normalize the user question to replace punctuation and make lowercase\n",
    "normalized_message = USER_MESSAGE.lower().replace(\"?\", \"\").replace(\"(\", \" \").replace(\")\", \" \")\n",
    "\n",
    "# Search the CSV for user question using very naive search\n",
    "words = normalized_message.split()\n",
    "matches = []\n",
    "for row in rows[1:]:\n",
    "    # if the word matches any word in row, add the row to the matches\n",
    "    if any(word in row[0].lower().split() for word in words) or any(word in row[5].lower().split() for word in words):\n",
    "        matches.append(row)\n",
    "\n",
    "# Format as a markdown table, since language models understand markdown\n",
    "matches_table = \" | \".join(rows[0]) + \"\\n\" + \" | \".join(\" --- \" for _ in range(len(rows[0]))) + \"\\n\"\n",
    "matches_table += \"\\n\".join(\" | \".join(row) for row in matches)\n",
    "print(f\"Found {len(matches)} matches:\")\n",
    "print(matches_table)\n",
    "\n",
    "# Now we can use the matches to generate a response\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    n=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": USER_MESSAGE + \"\\nSources: \" + matches_table},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Αποποίηση ευθύνης**:  \nΑυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτοματοποιημένες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "coopTranslator": {
   "original_hash": "6f9e40a7dbbd892aae50aff77da4b4be",
   "translation_date": "2025-09-12T23:31:50+00:00",
   "source_file": "code/01.Introduce/ollama.ipynb",
   "language_code": "el"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}