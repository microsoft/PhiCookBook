{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phi-4 RAGとAzure AI Search\n",
    "\n",
    "このノートブックでは、Phi-4-miniおよびPhi-4-multimodalを使用してAzure AI Searchでの検索強化生成（RAG）を実現する方法を示します。テキストのみの単一モダリティと、テキストと画像を組み合わせたマルチモダリティの両方のシナリオをカバーしています。\n",
    "\n",
    "**前提条件:**\n",
    "*   Azure AI Searchベクターインデックス（[こちらの手順](https://learn.microsoft.com/azure/search/search-get-started-portal-import-vectors?tabs=sample-data-storage%2Cmodel-aoai%2Cconnect-data-storage)に従って作成してください）\n",
    "*   Azure AI FoundryにデプロイされたPhi-4-miniまたはPhi-4-multimodalエンドポイント\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-ai-inference azure-search-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストのみのRAGとPhi-4-mini\n",
    "\n",
    "このセクションでは、Phi-4-miniをRAGのチャット補完モデルとして使用し、テキストのみを入力として利用する方法を示します。Azure AI Foundry InferenceとAzure AI Searchに接続し、関連するドキュメントを取得し、取得したコンテキストを使用して回答を生成します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Connect to Azure AI Foundry Inference & Azure AI Search\n",
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "\n",
    "chat_client = ChatCompletionsClient(\n",
    "    endpoint=os.environ[\"AZURE_INFERENCE_ENDPOINT\"], # Phi-4-mini endpoint\n",
    "    credential=AzureKeyCredential(os.environ[\"AZURE_INFERENCE_CREDENTIAL\"]),\n",
    ")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=os.environ[\"AZURE_SEARCH_ENDPOINT\"],\n",
    "    index_name=os.environ[\"AZURE_SEARCH_INDEX\"],\n",
    "    credential=AzureKeyCredential(os.environ[\"AZURE_SEARCH_KEY\"])\n",
    ")\n",
    "\n",
    "# Step 2: Retrieve relevant documents from Azure AI Search\n",
    "def retrieve_documents(query: str, top: int = 10):\n",
    "    vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=top, fields=\"text_vector\")\n",
    "    results = search_client.search(search_text=query,vector_queries=[vector_query],select=[\"content\"],top=top)\n",
    "    return [doc[\"content\"] for doc in results]\n",
    "\n",
    "# Step 3: Generate answer using RAG!\n",
    "def generate_rag_response(query: str):\n",
    "    docs = retrieve_documents(query)\n",
    "    context = \"\\n---\\n\".join(docs)\n",
    "    prompt = f\"\"\"You are a helpful assistant. Use only the following context to answer the question. If the answer isn't in the context, say 'I don't know'.\n",
    "    Context: {context} Question: {query} Answer:\"\"\"\n",
    "    response = chat_client.complete(messages=[UserMessage(content=prompt)])\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage\n",
    "user_query = \"What is the capital of France?\"\n",
    "answer = generate_rag_response(user_query)\n",
    "print(f\"Q: {user_query}\\nA: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### マルチモーダルRAGとPhi-4-multimodal\n",
    "\n",
    "このセクションでは、Phi-4-multimodalをRAGのチャット補完モデルとして使用し、テキストと画像入力の両方を組み合わせる方法を説明します。Azure AI InferenceとAzure AI Searchへの接続、関連するドキュメントの取得、そしてマルチモーダル応答の生成について取り上げます。\n",
    "\n",
    "**注:** Azure AI Searchインデックスに`text_vector`と`image_vector`フィールドの両方がある場合、マルチベクトルクエリを実行することも可能です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Connect to Azure AI Inference & Azure AI Search\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import (\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    "    TextContentItem,\n",
    "    ImageContentItem,\n",
    "    ImageUrl,\n",
    "    ImageDetailLevel,\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "\n",
    "\n",
    "chat_client = ChatCompletionsClient(\n",
    "    endpoint=os.environ[\"AZURE_INFERENCE_ENDPOINT\"], #Phi-4-multimodal serverless endpoint\n",
    "    credential=AzureKeyCredential(os.environ[\"AZURE_INFERENCE_CREDENTIAL\"]),\n",
    ")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=os.environ[\"AZURE_SEARCH_ENDPOINT\"],\n",
    "    index_name=os.environ[\"AZURE_SEARCH_INDEX\"],\n",
    "    credential=AzureKeyCredential(os.environ[\"AZURE_SEARCH_KEY\"])\n",
    ")\n",
    "\n",
    "# Step 2: Retrieve relevant documents from Azure AI Search\n",
    "def retrieve_documents(query: str, top: int = 3):\n",
    "    vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=top, fields=\"text_vector\")\n",
    "    results = search_client.search(search_text=query, vector_queries=[vector_query], select=[\"content\"], top=top)    \n",
    "    return [doc[\"content\"] for doc in results]\n",
    "\n",
    "## Example for Muli-modal search if you have a text_vector AND image_vector field in your vector_index\n",
    "## NOTE, image vectorization is in preview at the time of writing this code, please use azure-search-documents pypi version >11.6.0b6 \n",
    "# def retrieve_documents_multimodal(query: str, image_url: str, top: int = 3):\n",
    "#     text_vector_query = VectorizableTextQuery(\n",
    "#         text=query,\n",
    "#         k_nearest_neighbors=top,\n",
    "#         fields=\"text_vector\",\n",
    "#         weight=0.5  # Adjust weight as needed\n",
    "#     )\n",
    "#     image_vector_query = VectorizableImageUrlQuery(\n",
    "#         url=image_url,\n",
    "#         k_nearest_neighbors=top,\n",
    "#         fields=\"image_vector\",\n",
    "#         weight=0.5  # Adjust weight as needed\n",
    "#     )\n",
    "\n",
    "#     results = search_client.search(\n",
    "#         search_text=query,  \n",
    "#         vector_queries=[text_vector_query, image_vector_query],\n",
    "#         select=[\"content\"],\n",
    "#         top=top\n",
    "#     )\n",
    "#     return [doc[\"content\"] for doc in results]\n",
    "\n",
    "\n",
    "# Step 3: Generate a multimodal RAG-based answer using retrieved text and an image input\n",
    "def generate_multimodal_rag_response(query: str, image_url: str):\n",
    "    # Retrieve text context from search\n",
    "    docs = retrieve_documents(query)\n",
    "    context = \"\\n---\\n\".join(docs)\n",
    "\n",
    "    # Build a prompt that combines the retrieved context with the user query\n",
    "    prompt = f\"\"\"You are a helpful assistant. Use only the following context to answer the question. If the answer isn't in the context, say 'I don't know'.\n",
    "    Context: {context} Question: {query} Answer:\"\"\"\n",
    "    # Create a chat request that includes both text and image input\n",
    "    response = chat_client.complete(\n",
    "        messages=[\n",
    "            SystemMessage(content=\"You are a helpful assistant that can process both text and images.\"),\n",
    "            UserMessage(\n",
    "                content=[\n",
    "                    TextContentItem(text=prompt),\n",
    "                    ImageContentItem(image_url=ImageUrl(url=image_url, detail=ImageDetailLevel.HIGH)),\n",
    "                ]\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage\n",
    "user_query = \"Can you search for similar items to this shoe?\"\n",
    "sample_image_url = \"https://images.unsplash.com/photo-1542291026-7eec264c27ff?q=80&w=1770&auto=format&fit=crop&ixlib=rb-4.0.3\"\n",
    "answer = generate_multimodal_rag_response(user_query, sample_image_url)\n",
    "print(f\"Q: {user_query}\\nA: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**免責事項**:  \nこの文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があります。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解釈について、当社は責任を負いません。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "coopTranslator": {
   "original_hash": "011d815d092b18b9de890ccb92fbff5c",
   "translation_date": "2025-09-12T16:22:33+00:00",
   "source_file": "code/06.E2E/E2E_Phi-4-RAG-Azure-AI-Search.ipynb",
   "language_code": "ja"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}