{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## インタラクティブ Phi 3 Mini 4K インストラクトチャットボットとWhisper\n",
    "\n",
    "### はじめに:\n",
    "インタラクティブ Phi 3 Mini 4K インストラクトチャットボットは、Microsoft Phi 3 Mini 4K インストラクトデモとテキストまたは音声入力を使ってやり取りできるツールです。このチャットボットは、翻訳、天気情報の更新、一般的な情報収集など、さまざまなタスクに利用できます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[Huggingfaceアクセストークンを作成する](https://huggingface.co/settings/tokens)\n",
    "\n",
    "新しいトークンを作成  \n",
    "新しい名前を入力  \n",
    "書き込み権限を選択  \n",
    "トークンをコピーして安全な場所に保存\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下はPythonコードで、主に2つのタスクを実行します: `os`モジュールのインポートと環境変数の設定。\n",
    "\n",
    "1. `os`モジュールのインポート:\n",
    "   - Pythonの`os`モジュールは、オペレーティングシステムとやり取りするための方法を提供します。環境変数へのアクセス、ファイルやディレクトリの操作など、さまざまなOS関連のタスクを実行することができます。\n",
    "   - このコードでは、`import`文を使用して`os`モジュールをインポートしています。この文により、現在のPythonスクリプト内で`os`モジュールの機能が利用可能になります。\n",
    "\n",
    "2. 環境変数の設定:\n",
    "   - 環境変数は、オペレーティングシステム上で動作するプログラムがアクセスできる値です。これは、複数のプログラムで使用される設定情報やその他のデータを保存する方法です。\n",
    "   - このコードでは、`os.environ`辞書を使用して新しい環境変数を設定しています。辞書のキーは`'HF_TOKEN'`で、値は`HUGGINGFACE_TOKEN`変数から割り当てられます。\n",
    "   - `HUGGINGFACE_TOKEN`変数は、このコードスニペットの直前で定義されており、`#@param`構文を使用して文字列値`\"hf_**************\"`が割り当てられています。この構文は、Jupyterノートブックでユーザー入力やパラメータ設定を直接ノートブックインターフェースで行うためによく使用されます。\n",
    "   - `'HF_TOKEN'`環境変数を設定することで、プログラムの他の部分や同じオペレーティングシステム上で動作する他のプログラムからアクセス可能になります。\n",
    "\n",
    "全体として、このコードは`os`モジュールをインポートし、`HUGGINGFACE_TOKEN`変数で提供された値を使用して`'HF_TOKEN'`という名前の環境変数を設定しています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードスニペットは、Jupyter NotebookやIPythonで現在のセルの出力をクリアするための関数、clear_outputを定義しています。コードを分解してその機能を理解してみましょう。\n",
    "\n",
    "関数clear_outputは、waitという名前の1つのパラメータを取ります。このパラメータはブール値で、デフォルトではFalseに設定されています。waitは、新しい出力が既存の出力を置き換えるまで待つかどうかを決定します。\n",
    "\n",
    "この関数自体は、現在のセルの出力をクリアするために使用されます。Jupyter NotebookやIPythonでは、セルが出力を生成すると（例えば、テキストの印刷やグラフィカルなプロットなど）、その出力はセルの下に表示されます。clear_output関数を使用すると、その出力をクリアすることができます。\n",
    "\n",
    "コードスニペットでは、関数の実装は省略記号（...）で示されており、実際のコードは提供されていません。この省略記号は、出力をクリアする処理を行う実際のコードのプレースホルダーを表しています。関数の実装は、Jupyter NotebookやIPythonのAPIと連携してセルの既存の出力を削除することを含む可能性があります。\n",
    "\n",
    "全体として、この関数はJupyter NotebookやIPythonで現在のセルの出力をクリアする便利な方法を提供します。これにより、インタラクティブなコーディングセッション中に表示される出力を管理および更新しやすくなります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge TTSサービスを使用してテキスト読み上げ（TTS）を実行します。以下の関数実装を順番に説明します：\n",
    "\n",
    "1. `calculate_rate_string(input_value)`：この関数は入力値を受け取り、TTS音声の速度文字列を計算します。入力値は読み上げ速度を表し、1が通常速度を意味します。この関数は、入力値から1を引き、100を掛けて速度を計算し、入力値が1以上かどうかで符号を決定します。関数は速度文字列を\"{sign}{rate}\"の形式で返します。\n",
    "\n",
    "2. `make_chunks(input_text, language)`：この関数は入力テキストと言語を受け取り、言語固有のルールに基づいてテキストをチャンクに分割します。この実装では、言語が「英語」の場合、テキストをピリオド（\".\"）で分割し、前後の空白を削除します。その後、各チャンクにピリオドを追加し、フィルタリングされたチャンクのリストを返します。\n",
    "\n",
    "3. `tts_file_name(text)`：この関数は入力テキストに基づいてTTS音声ファイルの名前を生成します。テキストの末尾のピリオドを削除し（存在する場合）、小文字に変換し、前後の空白を削除し、スペースをアンダースコアに置き換えます。その後、テキストが25文字を超える場合は切り詰め、空の場合はそのまま使用します。最後に、[`uuid`]モジュールを使用してランダムな文字列を生成し、切り詰めたテキストと組み合わせて\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"形式のファイル名を作成します。\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`：この関数は複数の音声ファイルを1つの音声ファイルに結合します。音声ファイルのパスのリストと出力パスを受け取ります。関数は空の[`AudioSegment`]オブジェクトを初期化し、各音声ファイルのパスを反復処理して、`pydub`ライブラリの`AudioSegment.from_file()`メソッドを使用して音声ファイルを読み込み、[`merged_audio`]オブジェクトに追加します。最後に、指定された出力パスにMP3形式で結合された音声をエクスポートします。\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`：この関数はEdge TTSサービスを使用してTTS操作を実行します。テキストチャンクのリスト、読み上げ速度、音声名、保存パスを受け取ります。チャンク数が1より多い場合、個々のチャンク音声ファイルを保存するディレクトリを作成します。その後、各チャンクを反復処理し、`calculate_rate_string()`関数、音声名、チャンクテキストを使用してEdge TTSコマンドを構築し、`os.system()`関数を使用してコマンドを実行します。コマンドが正常に実行されると、生成された音声ファイルのパスをリストに追加します。すべてのチャンクを処理した後、`merge_audio_files()`関数を使用して個々の音声ファイルを結合し、指定された保存パスに結合された音声を保存します。チャンクが1つだけの場合は、直接Edge TTSコマンドを生成して音声を保存パスに保存します。最後に、生成された音声ファイルの保存パスを返します。\n",
    "\n",
    "6. `random_audio_name_generate()`：この関数は[`uuid`]モジュールを使用してランダムな音声ファイル名を生成します。ランダムなUUIDを生成し、文字列に変換して最初の8文字を取得し、\".mp3\"拡張子を追加してランダムな音声ファイル名を返します。\n",
    "\n",
    "7. `talk(input_text)`：この関数はTTS操作を実行するメインエントリポイントです。入力テキストを受け取ります。まず、入力テキストの長さを確認し、長文（600文字以上）かどうかを判断します。長さと`translate_text_flag`変数の値に基づいて言語を決定し、`make_chunks()`関数を使用してテキストチャンクのリストを生成します。その後、`random_audio_name_generate()`関数を使用して音声ファイルの保存パスを生成します。最後に、`edge_free_tts()`関数を呼び出してTTS操作を実行し、生成された音声ファイルの保存パスを返します。\n",
    "\n",
    "これらの関数は連携して、入力テキストをチャンクに分割し、音声ファイルの名前を生成し、Edge TTSサービスを使用してTTS操作を実行し、個々の音声ファイルを1つの音声ファイルに結合します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2つの関数、convert_to_textとrun_text_promptの実装、および2つのクラス、strとAudioの宣言について。\n",
    "\n",
    "convert_to_text関数はaudio_pathを入力として受け取り、whisper_modelというモデルを使用して音声をテキストに変換します。この関数はまずgpuフラグがTrueかどうかを確認します。Trueの場合、whisper_modelはword_timestamps=True、fp16=True、language='English'、task='translate'といった特定のパラメータで使用されます。gpuフラグがFalseの場合、whisper_modelはfp16=Falseで使用されます。変換されたテキストは'scan.txt'という名前のファイルに保存され、テキストとして返されます。\n",
    "\n",
    "run_text_prompt関数はmessageとchat_historyを入力として受け取ります。この関数はphi_demo関数を使用して、入力されたメッセージに基づいてチャットボットからの応答を生成します。生成された応答はtalk関数に渡され、応答を音声ファイルに変換してファイルパスを返します。Audioクラスは音声ファイルを表示し再生するために使用されます。音声はIPython.displayモジュールのdisplay関数を使用して表示され、Audioオブジェクトはautoplay=Trueパラメータで作成されるため、音声は自動的に再生されます。chat_historyは入力メッセージと生成された応答で更新され、空の文字列と更新されたchat_historyが返されます。\n",
    "\n",
    "strクラスはPythonの組み込みクラスで、文字列のシーケンスを表します。このクラスは、文字列を操作したり処理するためのさまざまなメソッドを提供します。例えば、capitalize、casefold、center、count、encode、endswith、expandtabs、find、format、index、isalnum、isalpha、isascii、isdecimal、isdigit、isidentifier、islower、isnumeric、isprintable、isspace、istitle、isupper、join、ljust、lower、lstrip、partition、replace、removeprefix、removesuffix、rfind、rindex、rjust、rpartition、rsplit、rstrip、split、splitlines、startswith、strip、swapcase、title、translate、upper、zfillなどがあります。これらのメソッドを使用すると、検索、置換、フォーマット、文字列の操作などが可能です。\n",
    "\n",
    "Audioクラスはカスタムクラスで、音声オブジェクトを表します。このクラスはJupyter Notebook環境で音声プレーヤーを作成するために使用されます。クラスはdata、filename、url、embed、rate、autoplay、normalizeといったさまざまなパラメータを受け取ります。dataパラメータはnumpy配列、サンプルのリスト、ファイル名やURLを表す文字列、または生のPCMデータを指定できます。filenameパラメータは音声データを読み込むローカルファイルを指定し、urlパラメータは音声データをダウンロードするURLを指定します。embedパラメータは音声データをデータURIとして埋め込むか、元のソースから参照するかを決定します。rateパラメータは音声データのサンプリングレートを指定します。autoplayパラメータは音声を自動的に再生するかどうかを決定します。normalizeパラメータは音声データを最大可能範囲に正規化するかどうかを指定します。Audioクラスはreloadメソッドを提供してファイルやURLから音声データを再読み込みすることができ、src_attr、autoplay_attr、element_id_attrといった属性を使用してHTML内の音声要素の対応する属性を取得できます。\n",
    "\n",
    "これらの関数とクラスは、音声をテキストに変換したり、チャットボットからの音声応答を生成したり、Jupyter Notebook環境で音声を表示・再生するために使用されます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**免責事項**:  \nこの文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があります。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解釈について、当方は一切の責任を負いません。\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T16:35:39+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "ja"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}