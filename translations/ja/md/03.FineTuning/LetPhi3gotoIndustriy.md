<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "743d7e9cb9c4e8ea642d77bee657a7fa",
  "translation_date": "2025-07-17T09:53:25+00:00",
  "source_file": "md/03.FineTuning/LetPhi3gotoIndustriy.md",
  "language_code": "ja"
}
-->
# **Phi-3を業界の専門家に育てる**

Phi-3モデルを業界に適用するには、業界のビジネスデータをPhi-3モデルに追加する必要があります。方法は2つあり、1つはRAG（Retrieval Augmented Generation）、もう1つはファインチューニングです。

## **RAGとファインチューニングの比較**

### **Retrieval Augmented Generation**

RAGはデータ検索＋テキスト生成の組み合わせです。企業の構造化データと非構造化データをベクトルデータベースに保存します。関連する内容を検索すると、関連する要約やコンテンツが見つかり、それをコンテキストとして形成し、LLM/SLMのテキスト補完機能と組み合わせてコンテンツを生成します。

### **ファインチューニング**

ファインチューニングは特定のモデルの改善に基づいています。モデルのアルゴリズムから始める必要はなく、データを継続的に蓄積していく必要があります。業界アプリケーションでより正確な専門用語や表現を求める場合、ファインチューニングが適しています。ただし、データが頻繁に変わる場合は、ファインチューニングが複雑になることがあります。

### **選び方**

1. 回答に外部データの導入が必要な場合は、RAGが最適です。

2. 安定して正確な業界知識を出力したい場合は、ファインチューニングが良い選択です。RAGは関連コンテンツを引き出すことを優先しますが、専門的なニュアンスを必ずしも完璧に捉えられるわけではありません。

3. ファインチューニングには高品質なデータセットが必要で、データ範囲が狭い場合はあまり効果が出ません。RAGの方が柔軟性があります。

4. ファインチューニングはブラックボックス的で内部の仕組みが理解しづらいですが、RAGはデータの出所を特定しやすく、幻覚や誤情報の調整がしやすいため透明性が高いです。

### **利用シナリオ**

1. 特定の専門用語や表現が必要な垂直産業では、***ファインチューニング***が最適です。

2. 複数の知識ポイントを統合するQAシステムでは、***RAG***が最適です。

3. 自動化された業務フローの組み合わせでは、***RAG + ファインチューニング***が最適です。

## **RAGの使い方**

![rag](../../../../translated_images/rag.2014adc59e6f6007bafac13e800a6cbc3e297fbb9903efe20a93129bd13987e9.ja.png)

ベクトルデータベースは、データを数学的な形式で保存したものです。ベクトルデータベースは機械学習モデルが過去の入力を記憶しやすくし、検索、推薦、テキスト生成などのユースケースをサポートします。データは完全一致ではなく類似度指標に基づいて識別されるため、コンピュータモデルがデータの文脈を理解できます。

ベクトルデータベースはRAG実現の鍵です。text-embedding-3やjina-ai-embeddingなどのベクトルモデルを使ってデータをベクトル形式に変換できます。

RAGアプリケーションの作成について詳しくは[https://github.com/microsoft/Phi-3CookBook](https://github.com/microsoft/Phi-3CookBook?WT.mc_id=aiml-138114-kinfeylo)をご覧ください。

## **ファインチューニングの使い方**

ファインチューニングでよく使われるアルゴリズムはLoraとQLoraです。どちらを選ぶべきか？
- [このサンプルノートブックで詳しく学ぶ](../../../../code/04.Finetuning/Phi_3_Inference_Finetuning.ipynb)
- [Pythonファインチューニングサンプルの例](../../../../code/04.Finetuning/FineTrainingScript.py)

### **LoraとQLora**

![lora](../../../../translated_images/qlora.e6446c988ee04ca08807488bb7d9e2c0ea7ef4af9d000fc6d13032b4ac2de18d.ja.png)

LoRA（Low-Rank Adaptation）とQLoRA（Quantized Low-Rank Adaptation）は、パラメータ効率の良いファインチューニング（PEFT）を用いて大規模言語モデル（LLM）を微調整する技術です。PEFTは従来の方法より効率的にモデルを訓練することを目的としています。

LoRAは重み更新行列に低ランク近似を適用することでメモリ使用量を削減する単独のファインチューニング技術です。高速な訓練時間を実現し、従来のファインチューニングに近い性能を維持します。

QLoRAはLoRAの拡張版で、量子化技術を取り入れてさらにメモリ使用量を削減します。QLoRAは事前学習済みLLMの重みパラメータを4ビット精度に量子化し、LoRAよりもメモリ効率が高いです。ただし、量子化と逆量子化の処理が追加されるため、QLoRAの訓練はLoRAより約30%遅くなります。

QLoRAは量子化による誤差を補正するためにLoRAを補助的に使用します。QLoRAは数十億パラメータの巨大モデルを比較的小規模で入手しやすいGPU上でファインチューニング可能にします。例えば、QLoRAは36台のGPUが必要な70Bパラメータモデルをわずか2台のGPUでファインチューニングできます。

**免責事項**：  
本書類はAI翻訳サービス「[Co-op Translator](https://github.com/Azure/co-op-translator)」を使用して翻訳されました。正確性の向上に努めておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があります。原文の言語によるオリジナル文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。本翻訳の利用により生じたいかなる誤解や誤訳についても、当方は一切の責任を負いかねます。