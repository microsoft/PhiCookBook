<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-07-16T21:43:24+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ja"
}
-->
# **Phiファミリーの量子化**

モデル量子化とは、ニューラルネットワークモデルのパラメータ（重みや活性化値など）を、大きな値の範囲（通常は連続値の範囲）から、より小さな有限の値の範囲にマッピングするプロセスを指します。この技術により、モデルのサイズや計算の複雑さを削減し、モバイルデバイスや組み込みシステムなどリソースが限られた環境でのモデルの動作効率を向上させることができます。モデル量子化はパラメータの精度を下げることで圧縮を実現しますが、その分精度の損失も伴います。したがって、量子化の過程ではモデルサイズ、計算コスト、精度のバランスを取る必要があります。一般的な量子化手法には固定小数点量子化や浮動小数点量子化などがあります。具体的なシナリオやニーズに応じて適切な量子化戦略を選択してください。

私たちはGenAIモデルをエッジデバイスに展開し、モバイルデバイス、AI PC/Copilot+PC、従来のIoTデバイスなど、より多くのデバイスをGenAIシナリオに参加させたいと考えています。量子化モデルを通じて、異なるデバイスに応じて様々なエッジデバイスに展開可能です。ハードウェアメーカーが提供するモデル加速フレームワークや量子化モデルと組み合わせることで、より良いSLMアプリケーションシナリオを構築できます。

量子化のシナリオでは、INT4、INT8、FP16、FP32といった異なる精度があります。以下に一般的に使われる量子化精度の説明を示します。

### **INT4**

INT4量子化は、モデルの重みや活性化値を4ビット整数に量子化する非常に大胆な手法です。INT4量子化は表現範囲が狭く精度も低いため、通常はより大きな精度損失が生じます。しかし、INT8量子化と比べて、モデルのストレージ要件や計算コストをさらに削減できます。ただし、INT4量子化は実際の応用ではあまり一般的ではありません。精度が低すぎるとモデル性能が大幅に低下する可能性があるためです。また、すべてのハードウェアがINT4演算をサポートしているわけではないため、量子化手法を選ぶ際にはハードウェアの互換性も考慮する必要があります。

### **INT8**

INT8量子化は、モデルの重みや活性化を浮動小数点数から8ビット整数に変換するプロセスです。INT8整数が表現できる数値範囲は狭く精度も低いですが、ストレージや計算の負荷を大幅に削減できます。INT8量子化では、モデルの重みや活性化値に対してスケーリングやオフセットを含む量子化処理を行い、元の浮動小数点情報をできるだけ保持します。推論時にはこれらの量子化値を一旦浮動小数点に復元して計算し、その後再びINT8に量子化して次の処理に渡します。この方法は多くのアプリケーションで十分な精度を保ちつつ、高い計算効率を実現します。

### **FP16**

FP16フォーマット、すなわち16ビット浮動小数点数（float16）は、32ビット浮動小数点数（float32）と比べてメモリ使用量を半分に削減できるため、大規模な深層学習アプリケーションで大きな利点があります。FP16フォーマットを使うことで、同じGPUメモリ制限内でより大きなモデルを読み込んだり、より多くのデータを処理したりできます。近年のGPUハードウェアはFP16演算をサポートしているため、FP16フォーマットの使用は計算速度の向上にもつながる可能性があります。ただし、FP16は精度が低いため、数値の不安定性や精度の損失が発生する場合もあります。

### **FP32**

FP32フォーマットは高い精度を提供し、広範囲の値を正確に表現できます。複雑な数学的演算を行う場合や高精度な結果が求められる場合にはFP32が好まれます。しかし、高精度である分、メモリ使用量が多く計算時間も長くなります。特にパラメータ数が多くデータ量が膨大な大規模深層学習モデルでは、FP32フォーマットはGPUメモリ不足や推論速度の低下を招くことがあります。

モバイルデバイスやIoTデバイスではPhi-3.xモデルをINT4に変換できますが、AI PCやCopilot PCではINT8、FP16、FP32などより高精度のフォーマットを使用可能です。

現在、IntelのOpenVINO、QualcommのQNN、AppleのMLX、NvidiaのCUDAなど、各ハードウェアメーカーが生成モデルをサポートするための異なるフレームワークを提供しており、これらとモデル量子化を組み合わせてローカル展開を実現しています。

技術面では、量子化後にPyTorch/Tensorflowフォーマット、GGUF、ONNXなど異なるフォーマットのサポートがあります。私はGGUFとONNXのフォーマット比較と適用シナリオを行いましたが、モデルフレームワークからハードウェアまで幅広くサポートされているONNX量子化フォーマットを推奨します。本章では、GenAI向けのONNX Runtime、OpenVINO、Apple MLXを使ったモデル量子化に焦点を当てます（より良い方法があればPRで教えてください）。

**本章の内容**

1. [llama.cppを使ったPhi-3.5 / 4の量子化](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntimeのGenerative AI拡張を使ったPhi-3.5 / 4の量子化](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINOを使ったPhi-3.5 / 4の量子化](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLXフレームワークを使ったPhi-3.5 / 4の量子化](./UsingAppleMLXQuantifyingPhi.md)

**免責事項**：  
本書類はAI翻訳サービス「[Co-op Translator](https://github.com/Azure/co-op-translator)」を使用して翻訳されました。正確性の向上に努めておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があります。原文の言語によるオリジナル文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。本翻訳の利用により生じたいかなる誤解や誤訳についても、当方は一切の責任を負いかねます。