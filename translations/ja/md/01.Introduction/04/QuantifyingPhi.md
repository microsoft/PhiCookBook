<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-05T01:20:25+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ja"
}
-->
# **Phiファミリーの量子化**

モデル量子化とは、ニューラルネットワークモデルのパラメータ（重みや活性化値など）を大きな値の範囲（通常は連続値の範囲）からより小さい有限の値の範囲へマッピングするプロセスを指します。この技術はモデルのサイズや計算の複雑さを低減し、モバイルデバイスや組み込みシステムといったリソース制約のある環境でのモデルの動作効率を向上させることができます。モデル量子化はパラメータの精度を下げることで圧縮を実現しますが、同時に一定の精度損失も生じます。そのため、量子化プロセスではモデルサイズ、計算の複雑さ、精度のバランスを取る必要があります。一般的な量子化手法には固定小数点量子化や浮動小数点量子化などがあります。具体的なシナリオやニーズに応じて適切な量子化戦略を選択できます。

我々は GenAI モデルをエッジデバイスにデプロイし、モバイルデバイス、AI PC/Copilot+PC、従来の IoT デバイスなど、より多くのデバイスを GenAI シナリオに参加させたいと考えています。量子化モデルを通じて、異なるデバイスに応じて各種エッジデバイスへデプロイできます。ハードウェアメーカーが提供するモデルアクセラレーションフレームワークや量子化モデルと組み合わせることで、より良い SLM アプリケーションシナリオを構築できます。

量子化のシナリオでは、INT4、INT8、FP16、FP32 といった異なる精度があります。以下は一般的に使用される量子化精度の説明です。

### **INT4**

INT4 量子化は、モデルの重みや活性化値を 4 ビット整数に量子化する思い切った手法です。表現範囲が小さく精度が低いため、INT4 量子化は通常より大きな精度損失を招きます。しかし、INT8 量子化と比べて、INT4 量子化はさらにモデルのストレージ要件や計算の複雑さを削減できます。実用上、精度が低すぎるとモデル性能が大幅に低下する可能性があるため、INT4 量子化は比較的稀です。さらに、すべてのハードウェアが INT4 演算をサポートしているわけではないため、量子化方式を選択する際にはハードウェアの互換性を考慮する必要があります。

### **INT8**

INT8 量子化は、モデルの重みと活性化を浮動小数点数から 8 ビット整数に変換するプロセスです。INT8 整数が表現する数値範囲は小さく精度も低いですが、ストレージと計算の要件を大幅に削減できます。INT8 量子化では、モデルの重みと活性化値はスケーリングやオフセットを含む量子化プロセスを経て、元の浮動小数点情報をできるだけ保持するようにします。推論時には、これらの量子化された値は計算のために再び浮動小数点にデクォンタイ化され、次のステップのために再度 INT8 に量子化されます。この方法は、多くのアプリケーションで十分な精度を提供しつつ高い計算効率を維持できます。

### **FP16**

FP16 形式、すなわち 16 ビット浮動小数点数（float16）は、32 ビット浮動小数点数（float32）と比べてメモリフットプリントを半分に削減でき、大規模なディープラーニングアプリケーションで大きな利点があります。FP16 形式を用いることで、同じ GPU メモリ制限内でより大きなモデルを読み込んだり、より多くのデータを処理したりできます。近年の GPU ハードウェアが FP16 演算を継続的にサポートしているため、FP16 形式を使用することで計算速度の向上が見込まれることもあります。ただし、FP16 形式は精度が低いという固有の欠点もあり、場合によっては数値不安定性や精度損失を招く可能性があります。

### **FP32**

FP32 形式はより高い精度を提供し、広範な値を正確に表現できます。複雑な数学的演算を行う場合や高精度の結果が必要な場合には FP32 形式が好まれます。しかし、高精度であることはメモリ使用量が増加し計算時間が長くなることも意味します。特にモデルパラメータが多くデータ量が膨大な大規模ディープラーニングモデルでは、FP32 形式は GPU メモリ不足や推論速度の低下を引き起こす可能性があります。

モバイルデバイスや IoT デバイスでは Phi-3.x モデルを INT4 に変換できる一方、AI PC / Copilot PC は INT8、FP16、FP 32 のようなより高い精度を使用できます。

現時点では、異なるハードウェアメーカーが生成モデルをサポートするために OpenVINO（Intel）、QNN（Qualcomm）、MLX（Apple）、CUDA（Nvidia）などの異なるフレームワークを提供しており、これらとモデル量子化を組み合わせてローカルデプロイを実現できます。

技術面では、量子化後にサポートされるフォーマットが PyTorch / TensorFlow フォーマット、GGUF、ONNX などと異なります。私は GGUF と ONNX のフォーマット比較と適用シナリオを行いました。ここでは、モデルフレームワークからハードウェアまで幅広くサポートのある ONNX 量子化フォーマットを推奨します。本章では、ONNX Runtime for GenAI、OpenVINO、および Apple MLX を中心にモデル量子化を行います（より良い方法があれば、PR を提出して共有してください）。

**この章には以下が含まれます**

1. [llama.cpp を使用した Phi-3.5 / 4 の量子化](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntime の Generative AI 拡張を使用した Phi-3.5 / 4 の量子化](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO を使用した Phi-3.5 / 4 の量子化](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX フレームワークを使用した Phi-3.5 / 4 の量子化](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
免責事項:
この文書は AI 翻訳サービス「Co-op Translator」（https://github.com/Azure/co-op-translator）を用いて翻訳されました。正確性には努めておりますが、自動翻訳には誤りや不正確さが含まれる可能性があることをご承知おきください。原文（原言語の文書）を正式な信頼できる情報源として扱ってください。重要な情報については、専門の人間による翻訳を推奨します。本翻訳の利用により生じたいかなる誤解や誤訳についても、当社は責任を負いません。
<!-- CO-OP TRANSLATOR DISCLAIMER END -->