<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-05-08T06:09:25+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ja"
}
-->
# **Phiファミリーの量子化**

モデル量子化とは、ニューラルネットワークモデルのパラメータ（重みや活性化値など）を、大きな値の範囲（通常は連続値の範囲）から、より小さな有限の値の範囲にマッピングするプロセスを指します。この技術により、モデルのサイズや計算の複雑さを削減し、モバイル機器や組み込みシステムなどリソース制約のある環境でのモデルの動作効率を向上させることができます。モデル量子化はパラメータの精度を下げることで圧縮を実現しますが、その分精度の損失も伴います。したがって、量子化の過程ではモデルサイズ、計算の複雑さ、精度のバランスを取る必要があります。一般的な量子化手法には固定小数点量子化や浮動小数点量子化などがあり、具体的なシナリオやニーズに応じて適切な量子化戦略を選択できます。

私たちはGenAIモデルをエッジデバイスに展開し、モバイル機器、AI PC/Copilot+PC、従来のIoTデバイスなど、より多くのデバイスをGenAIシナリオに参加させたいと考えています。量子化モデルを通じて、異なるデバイスに基づいた異なるエッジデバイスへ展開が可能になります。ハードウェアメーカーが提供するモデル加速フレームワークと量子化モデルを組み合わせることで、より良いSLMアプリケーションシナリオを構築できます。

量子化のシナリオでは、異なる精度（INT4、INT8、FP16、FP32）があります。以下に一般的に使われる量子化精度について説明します。

### **INT4**

INT4量子化はモデルの重みや活性化値を4ビット整数に量子化する大胆な手法です。表現範囲が狭く精度が低いため、通常はより大きな精度の損失が生じます。しかし、INT8量子化と比べて、モデルのストレージ要件や計算の複雑さをさらに削減できます。ただし、INT4量子化は実際のアプリケーションでは比較的稀で、精度が低すぎるとモデル性能が著しく低下する可能性があります。また、すべてのハードウェアがINT4演算をサポートしているわけではないため、量子化手法を選ぶ際にはハードウェアの互換性も考慮する必要があります。

### **INT8**

INT8量子化は、モデルの重みと活性化を浮動小数点数から8ビット整数に変換するプロセスです。INT8整数が表現できる数値範囲は狭く精度も低いですが、ストレージと計算の要求を大幅に削減できます。INT8量子化では、モデルの重みや活性化値に対してスケーリングやオフセットを含む量子化処理を行い、元の浮動小数点情報をできるだけ保持します。推論時にはこれらの量子化された値を浮動小数点数に戻して計算し、その後次のステップのために再びINT8に量子化します。この方法は多くのアプリケーションで十分な精度を保ちつつ、高い計算効率を実現します。

### **FP16**

FP16フォーマット、つまり16ビット浮動小数点数（float16）は、32ビット浮動小数点数（float32）と比べてメモリ使用量を半分に削減でき、大規模なディープラーニングアプリケーションで大きな利点があります。FP16フォーマットを使うことで、同じGPUメモリ制限内でより大きなモデルを読み込んだり、より多くのデータを処理したりできます。近年のGPUハードウェアはFP16演算をサポートし続けているため、FP16フォーマットの使用は計算速度の向上にもつながる可能性があります。ただし、FP16フォーマットには精度が低いという固有の欠点があり、場合によっては数値の不安定性や精度の損失を引き起こすことがあります。

### **FP32**

FP32フォーマットはより高い精度を提供し、広範囲の値を正確に表現できます。複雑な数学的演算を行う場合や高精度の結果が求められる場合にFP32フォーマットが好まれます。ただし、高精度である分、メモリ使用量が多く計算時間も長くなります。大規模なディープラーニングモデル、特にパラメータ数が多くデータ量が膨大な場合、FP32フォーマットはGPUメモリ不足や推論速度の低下を招くことがあります。

モバイル機器やIoTデバイスではPhi-3.xモデルをINT4に変換できますが、AI PC / Copilot PCではINT8、FP16、FP32などより高精度のものを使用可能です。

現在、IntelのOpenVINO、QualcommのQNN、AppleのMLX、NvidiaのCUDAなど、各ハードウェアメーカーが生成モデルをサポートする異なるフレームワークを提供しており、モデル量子化と組み合わせてローカル展開を実現しています。

技術面では、量子化後にPyTorch / Tensorflow形式、GGUF、ONNXなど異なるフォーマットのサポートがあります。私はGGUFとONNXのフォーマット比較と適用シナリオを行いました。ここでは、モデルフレームワークからハードウェアまで幅広くサポートされているONNX量子化フォーマットを推奨します。本章ではONNX Runtime for GenAI、OpenVINO、Apple MLXを使ったモデル量子化に焦点を当てます（もしより良い方法があれば、PRを提出して教えてください）。

**本章の内容**

1. [llama.cppを使ったPhi-3.5 / 4の量子化](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntimeのGenerative AI拡張を使ったPhi-3.5 / 4の量子化](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINOを使ったPhi-3.5 / 4の量子化](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLXフレームワークを使ったPhi-3.5 / 4の量子化](./UsingAppleMLXQuantifyingPhi.md)

**免責事項**:  
本書類はAI翻訳サービス[Co-op Translator](https://github.com/Azure/co-op-translator)を使用して翻訳されています。正確性を期しておりますが、自動翻訳には誤りや不正確な箇所が含まれる可能性があることをご承知おきください。原文の言語によるオリジナル文書が正式な情報源とみなされます。重要な情報については、専門の人間による翻訳を推奨します。本翻訳の利用により生じた誤解や誤訳について、一切の責任を負いかねます。