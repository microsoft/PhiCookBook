<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "418c693c63cc0e817dc560558f730a7a",
  "translation_date": "2025-04-04T12:12:04+00:00",
  "source_file": "md\\01.Introduction\\04\\QuantifyingPhi.md",
  "language_code": "ja"
}
-->
# **Phiファミリーの量子化**

モデル量子化とは、ニューラルネットワークモデルのパラメータ（例えば重みや活性化値）を、大きな値の範囲（通常は連続的な値の範囲）から、より小さな有限の値の範囲にマッピングするプロセスを指します。この技術により、モデルのサイズや計算の複雑さを削減し、モバイルデバイスや組み込みシステムなどのリソースが制約された環境でモデルの動作効率を向上させることができます。モデル量子化はパラメータの精度を下げることで圧縮を実現しますが、それに伴い一定の精度損失も発生します。そのため、量子化プロセスではモデルサイズ、計算の複雑さ、精度のバランスを取る必要があります。一般的な量子化手法には固定小数点量子化、浮動小数点量子化などがあります。具体的なシナリオやニーズに応じて適切な量子化戦略を選択することが可能です。

私たちはGenAIモデルをエッジデバイスにデプロイし、モバイルデバイス、AI PC/Copilot+PC、従来のIoTデバイスなど、より多くのデバイスをGenAIシナリオに参加させることを目指しています。量子化モデルを通じて、異なるデバイスに基づいて異なるエッジデバイスにデプロイできます。ハードウェアメーカーが提供するモデル加速フレームワークや量子化モデルを組み合わせることで、より良いSLMアプリケーションシナリオを構築できます。

量子化のシナリオでは、異なる精度（INT4、INT8、FP16、FP32）があります。以下は、一般的に使用される量子化精度の説明です。

### **INT4**

INT4量子化は、モデルの重みや活性化値を4ビット整数に量子化する大胆な方法です。INT4量子化は、表現範囲が狭く精度が低いため、通常は精度損失が大きくなります。しかし、INT8量子化と比較して、INT4量子化はモデルのストレージ要件や計算の複雑さをさらに削減できます。ただし、INT4量子化は実際のアプリケーションでは比較的稀であり、精度が低すぎるとモデル性能が大幅に低下する可能性があります。また、すべてのハードウェアがINT4操作をサポートしているわけではないため、量子化手法を選択する際にはハードウェアの互換性を考慮する必要があります。

### **INT8**

INT8量子化は、モデルの重みや活性化値を浮動小数点数から8ビット整数に変換するプロセスです。INT8整数で表現される数値範囲は小さく精度も低いですが、ストレージや計算要件を大幅に削減することができます。INT8量子化では、モデルの重みや活性化値がスケーリングやオフセットを含む量子化プロセスを経て、元の浮動小数点情報をできるだけ保持します。推論時には、これらの量子化された値を浮動小数点数にデクアンタイズして計算し、その後次のステップのために再びINT8に量子化します。この方法は、ほとんどのアプリケーションで十分な精度を提供しつつ、高い計算効率を維持できます。

### **FP16**

FP16形式、つまり16ビット浮動小数点数（float16）は、32ビット浮動小数点数（float32）と比較してメモリ使用量を半減させ、大規模な深層学習アプリケーションで大きな利点があります。FP16形式を使用することで、同じGPUメモリ制約内でより大きなモデルをロードしたり、より多くのデータを処理したりすることが可能です。現代のGPUハードウェアがFP16操作をサポートし続けているため、FP16形式の使用は計算速度の向上をもたらす可能性もあります。ただし、FP16形式には固有の欠点もあり、精度が低いため、一部のケースでは数値の不安定性や精度の損失を引き起こす可能性があります。

### **FP32**

FP32形式はより高い精度を提供し、広範な値を正確に表現できます。複雑な数学的操作が必要な場合や高精度な結果が求められるシナリオでは、FP32形式が好まれます。しかし、高い精度はより多くのメモリ使用量と長い計算時間を意味します。特にモデルパラメータが多く、データ量が膨大な大規模な深層学習モデルでは、FP32形式がGPUメモリ不足や推論速度の低下を引き起こす可能性があります。

モバイルデバイスやIoTデバイスでは、Phi-3.xモデルをINT4に変換することができます。一方で、AI PCやCopilot PCでは、INT8、FP16、FP32といったより高い精度を使用できます。

現在、異なるハードウェアメーカーは、IntelのOpenVINO、QualcommのQNN、AppleのMLX、NvidiaのCUDAなど、生成モデルをサポートする異なるフレームワークを提供しています。これらをモデル量子化と組み合わせることで、ローカルデプロイを完了させることができます。

技術面では、量子化後にPyTorch/Tensorflow形式、GGUF、ONNXといった異なる形式をサポートしています。GGUFとONNXの形式比較と適用シナリオを行いましたが、ここではONNX量子化形式を推奨します。ONNXはモデルフレームワークからハードウェアまでのサポートが優れています。本章では、GenAI用ONNX Runtime、OpenVINO、Apple MLXを使用してモデル量子化を実行します（もしより良い方法があれば、PRを提出してお知らせください）。

**本章に含まれる内容**

1. [Phi-3.5 / 4をllama.cppを使用して量子化する](./UsingLlamacppQuantifyingPhi.md)

2. [Phi-3.5 / 4をonnxruntimeのGenerative AI拡張機能を使用して量子化する](./UsingORTGenAIQuantifyingPhi.md)

3. [Phi-3.5 / 4をIntel OpenVINOを使用して量子化する](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Phi-3.5 / 4をApple MLX Frameworkを使用して量子化する](./UsingAppleMLXQuantifyingPhi.md)

**免責事項**:  
この文書は、AI翻訳サービス[Co-op Translator](https://github.com/Azure/co-op-translator)を使用して翻訳されています。正確性を追求していますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご承知おきください。元の言語で記載された文書が正式な情報源としてみなされるべきです。重要な情報については、専門的な人間による翻訳を推奨します。この翻訳の利用によって生じる誤解や誤解釈について、当方は責任を負いかねます。