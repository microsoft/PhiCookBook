# **Phiファミリーの量子化**

モデル量子化とは、ニューラルネットワークモデル内のパラメータ（重みや活性化値など）を、大きな値の範囲（通常は連続値の範囲）からより小さな有限の値の範囲にマッピングするプロセスを指します。この技術は、モデルのサイズや計算量を削減し、モバイルデバイスや組み込みシステムなどのリソース制約環境でのモデルの動作効率を向上させることができます。モデル量子化はパラメータの精度を下げることで圧縮を実現しますが、同時に一定の精度損失も生じます。したがって、量子化プロセスではモデルのサイズ、計算量、精度のバランスを取る必要があります。一般的な量子化手法には固定小数点量子化、浮動小数点量子化などがあります。具体的なシナリオやニーズに応じて適切な量子化戦略を選択できます。

私たちはGenAIモデルをエッジデバイスに展開し、モバイルデバイス、AI PC/Copilot+PC、従来のIoTデバイスなど多くのデバイスをGenAIシナリオに参加させたいと考えています。量子化モデルを通じて、異なるデバイスに基づいてそれぞれ異なるエッジデバイスに展開できます。ハードウェアメーカーが提供するモデルアクセラレーションフレームワークと量子化モデルと組み合わせて、より良いSLMアプリケーションシナリオを構築できます。

量子化のシナリオでは、INT4、INT8、FP16、FP32などの異なる精度があります。以下は一般的な量子化精度の説明です。

### **INT4**

INT4量子化はモデルの重みと活性化値を4ビット整数に量子化する大胆な量子化手法です。INT4量子化は表現範囲が狭く精度が低いため、通常より大きな精度損失が生じます。しかし、INT8量子化と比べてさらにモデルのストレージ要件と計算量を削減できます。注意すべき点は、実際の応用ではINT4量子化は比較的稀であり、あまりに低い精度はモデル性能の大幅な劣化を招く可能性があることです。また、すべてのハードウェアがINT4操作をサポートしているわけではないため、量子化手法を選ぶ際にハードウェアの互換性も考慮する必要があります。

### **INT8**

INT8量子化は、モデルの重みと活性化を浮動小数点数から8ビット整数に変換するプロセスです。INT8整数の表現範囲は小さく精度も低いものの、ストレージと計算の要件を大幅に削減できます。INT8量子化では、重みと活性化値にスケーリングやオフセットを含む量子化処理を施し、元の浮動小数点情報をできるだけ保持します。推論時にはこれらの量子化値が再び浮動小数点に復号されて計算され、次のステップのために再度INT8に量子化されます。この方法はほとんどのアプリケーションで十分な精度を保ちつつ高い計算効率を実現します。

### **FP16**

FP16フォーマット、つまり16ビット浮動小数点数（float16）は、32ビット浮動小数点数（float32）に比べてメモリ使用量を半分に削減できるため、大規模な深層学習アプリケーションにおいて大きな利点があります。FP16形式を使うことで、同じGPUメモリ制限内でより大きなモデルを読み込むか、より多くのデータを処理できます。近年のGPUハードウェアはFP16演算をサポートし続けており、FP16を使うことで計算速度の向上も期待できます。ただし、FP16フォーマットには低精度ゆえの不足もあり、場合によっては数値的な不安定さや精度損失を引き起こす可能性があります。

### **FP32**

FP32フォーマットは高精度で幅広い値を正確に表現できます。複雑な数学演算を行う場合や高精度結果が必要な場合にFP32フォーマットが好まれます。しかし高精度はメモリ使用量増加や計算時間の増大も意味します。特に多くのモデルパラメータや大量のデータを扱う大規模深層学習モデルでは、FP32フォーマットはGPUメモリ不足や推論速度の低下を引き起こすことがあります。

モバイルやIoTデバイスではPhi-3.xモデルをINT4に変換できますが、AI PCやCopilot PCではINT8、FP16、FP32のようなより高精度を使うことができます。

現在、IntelのOpenVINO、QualcommのQNN、AppleのMLX、NvidiaのCUDAなど、異なるハードウェアメーカーが生成モデルをサポートする異なるフレームワークを持ち、モデル量子化と組み合わせてローカル展開を完了できます。

技術面では、量子化後にPyTorch / TensorFlowフォーマット、GGUF、ONNXなどの異なるフォーマットサポートがあります。私はGGUFとONNXのフォーマット比較と適用シナリオを行いました。ここでは、モデルフレームワークからハードウェアまで幅広いサポートがあるONNX量子化フォーマットを推奨します。本章ではONNX Runtime for GenAI、OpenVINO、Apple MLXを用いたモデル量子化に重点を置きます（もしより良い方法があればPRを通じて教えてください）。

**本章の内容**

1. [llama.cppを用いたPhi-3.5 / 4の量子化](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntimeの生成AI拡張を用いたPhi-3.5 / 4の量子化](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINOを用いたPhi-3.5 / 4の量子化](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLXフレームワークを用いたPhi-3.5 / 4の量子化](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**免責事項**：  
本書類はAI翻訳サービス[Co-op Translator](https://github.com/Azure/co-op-translator)を使用して翻訳されました。正確性に努めておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があります。原文の内容が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨いたします。本翻訳の利用により生じた誤解や解釈の違いについて、当方は一切責任を負いかねます。
<!-- CO-OP TRANSLATOR DISCLAIMER END -->