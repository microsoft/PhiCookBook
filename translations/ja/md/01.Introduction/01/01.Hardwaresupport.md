<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c4afa6ffd13f29eb34e5f204b94310ff",
  "translation_date": "2025-04-04T11:48:45+00:00",
  "source_file": "md\\01.Introduction\\01\\01.Hardwaresupport.md",
  "language_code": "ja"
}
-->
# Phi ハードウェア対応

Microsoft Phi は ONNX Runtime に最適化されており、Windows DirectML をサポートしています。GPU、CPU、さらにはモバイルデバイスを含むさまざまなハードウェアタイプで良好に動作します。

## デバイスハードウェア 
具体的に対応しているハードウェアは以下の通りです：

- GPU SKU: RTX 4090 (DirectML)
- GPU SKU: 1 A100 80GB (CUDA)
- CPU SKU: Standard F64s v2 (64 vCPU、128 GiB メモリ)

## モバイル SKU

- Android - Samsung Galaxy S21
- Apple iPhone 14 またはそれ以上の A16/A17 プロセッサ

## Phi ハードウェア仕様

- 必要な最小構成
- Windows: DirectX 12 対応 GPU および最低 4GB の統合 RAM

CUDA: Compute Capability >= 7.02 を持つ NVIDIA GPU

![HardwareSupport](../../../../../translated_images/01.phihardware.925db5699da7752cf486314e6db087580583cfbcd548970f8a257e31a8aa862c.ja.png)

## 複数 GPU での onnxruntime 実行

現在利用可能な Phi ONNX モデルは 1 GPU 用のみです。Phi モデルでのマルチ GPU 対応は可能ですが、2 GPU を使用した ORT が 2 つの ORT インスタンスと比較してスループットが向上する保証はありません。最新情報については [ONNX Runtime](https://onnxruntime.ai/) をご確認ください。

[Build 2024 の GenAI ONNX チーム](https://youtu.be/WLW4SE8M9i8?si=EtG04UwDvcjunyfC) は、Phi モデルにおいてマルチ GPU の代わりにマルチインスタンスを有効にしたことを発表しました。

現在では、CUDA_VISIBLE_DEVICES 環境変数を使用して、以下のように 1 つの onnxruntime または onnxruntime-genai インスタンスを実行することが可能です。

```Python
CUDA_VISIBLE_DEVICES=0 python infer.py
CUDA_VISIBLE_DEVICES=1 python infer.py
```

さらに詳しくは [Azure AI Foundry](https://ai.azure.com) で Phi を探索してください。

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求していますが、自動翻訳には誤りや不正確さが含まれる可能性があることをご承知ください。元の言語で記載された原文が正式な情報源として考慮されるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の利用に起因する誤解や誤読について、当方は責任を負いません。