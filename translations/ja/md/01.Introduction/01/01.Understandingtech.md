<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:42:03+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "ja"
}
-->
# 主要な技術

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12上に構築された、ハードウェアアクセラレーション対応の低レベル機械学習API。
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidiaが開発した並列コンピューティングプラットフォームおよびAPIモデルで、GPU上での汎用処理を可能にする。
3. [ONNX](https://onnx.ai/)（Open Neural Network Exchange）- 機械学習モデルの相互運用性を提供するために設計されたオープンフォーマット。
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)（Generic Graph Update Format）- 機械学習モデルの表現と更新に使われるフォーマットで、特に4-8bit量子化によりCPU上で効率的に動作する小型言語モデルに適している。

## DirectML

DirectMLはハードウェアアクセラレーション対応の機械学習を可能にする低レベルAPIです。DirectX 12の上に構築されており、GPUアクセラレーションを活用します。ベンダーに依存しない設計で、異なるGPUベンダー間でコードの変更なしに動作します。主にGPU上でのモデルのトレーニングや推論処理に使われます。

ハードウェアサポートとしては、AMDの統合・ディスクリートGPU、Intelの統合GPU、NVIDIAのディスクリートGPUなど幅広いGPUに対応しています。Windows AIプラットフォームの一部であり、Windows 10および11でサポートされているため、あらゆるWindowsデバイスでモデルのトレーニングや推論が可能です。

DirectMLには、150以上のONNXオペレーターのサポートやONNXランタイムおよびWinMLでの利用などのアップデートや機会があります。主要な統合ハードウェアベンダー（IHV）がそれぞれ様々なメタコマンドを実装して支えています。

## CUDA

CUDA（Compute Unified Device Architecture）はNvidiaが開発した並列コンピューティングプラットフォームおよびAPIモデルです。CUDA対応GPUを使って汎用処理を行うことができるため、GPGPU（General-Purpose computing on Graphics Processing Units）と呼ばれています。CUDAはNvidiaのGPUアクセラレーションの中核技術であり、機械学習、科学計算、映像処理など多くの分野で広く利用されています。

CUDAのハードウェアサポートはNvidiaのGPUに限定されており、Nvidia独自の技術です。各GPUアーキテクチャはCUDAツールキットの特定バージョンをサポートしており、開発者はこれによりCUDAアプリケーションの構築と実行に必要なライブラリやツールを利用できます。

## ONNX

ONNX（Open Neural Network Exchange）は機械学習モデルを表現するためのオープンフォーマットです。拡張可能な計算グラフモデルの定義に加え、組み込みオペレーターや標準データ型の定義も提供します。ONNXにより、異なるMLフレームワーク間でモデルを移動できるため、相互運用性が向上し、AIアプリケーションの作成と展開が容易になります。

Phi3 miniはONNX Runtimeを使い、サーバープラットフォーム、Windows、Linux、Macデスクトップ、モバイルCPUなど様々なデバイスのCPUおよびGPU上で動作します。
追加した最適化構成は以下の通りです。

- int4 DML用ONNXモデル：AWQによるint4量子化
- fp16 CUDA用ONNXモデル
- int4 CUDA用ONNXモデル：RTNによるint4量子化
- int4 CPUおよびモバイル用ONNXモデル：RTNによるint4量子化

## Llama.cpp

Llama.cppはC++で書かれたオープンソースのソフトウェアライブラリです。Llamaを含む様々な大規模言語モデル（LLM）の推論を行います。汎用テンソルライブラリであるggmlと共に開発され、元のPython実装に比べて高速な推論と低メモリ使用を目指しています。ハードウェア最適化や量子化をサポートし、シンプルなAPIとサンプルも提供しています。効率的なLLM推論に興味があるなら、Phi3がLlama.cppを動かせることもあり、ぜひ検討する価値があります。

## GGUF

GGUF（Generic Graph Update Format）は機械学習モデルの表現と更新に使われるフォーマットです。特に4-8bit量子化によりCPU上で効率的に動作する小型言語モデル（SLM）に適しています。GGUFは迅速なプロトタイピングやエッジデバイス、CI/CDパイプラインのようなバッチ処理でのモデル実行に有用です。

**免責事項**：  
本書類はAI翻訳サービス「[Co-op Translator](https://github.com/Azure/co-op-translator)」を使用して翻訳されました。正確性を期しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があります。原文の言語による文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。本翻訳の利用により生じた誤解や誤訳について、当方は一切の責任を負いかねます。