<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "583e1ebd3884b47b43c883072eb8fa03",
  "translation_date": "2025-04-04T11:50:47+00:00",
  "source_file": "md\\01.Introduction\\01\\01.Understandingtech.md",
  "language_code": "ja"
}
-->
# 記載されている主要技術

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12上に構築された、ハードウェアアクセラレーションを活用した機械学習向けの低レベルAPI。
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidiaが開発した並列コンピューティングプラットフォームおよびAPIモデルで、GPUを用いた汎用的な処理を可能にします。
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - 機械学習モデルを表現するためのオープンフォーマットで、異なるMLフレームワーク間の相互運用性を提供します。
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - 機械学習モデルを表現・更新するためのフォーマットで、特に4-8ビット量子化を用いてCPU上で効果的に動作する小型の言語モデルに有用です。

## DirectML

DirectMLは、ハードウェアアクセラレーションを活用した機械学習を可能にする低レベルAPIです。DirectX 12上に構築されており、GPUアクセラレーションを活用しつつ、ベンダーに依存しない設計となっています。そのため、異なるGPUベンダー間でコード変更を必要とせずに動作します。主にGPU上でのモデルトレーニングや推論ワークロードに使用されます。

ハードウェアサポートに関しては、DirectMLは幅広いGPUに対応しており、AMDの統合型および独立型GPU、Intelの統合型GPU、NVIDIAの独立型GPUを含みます。Windows AI Platformの一部として、Windows 10および11でサポートされており、すべてのWindowsデバイスでモデルのトレーニングと推論を可能にします。

DirectMLには、150以上のONNXオペレーターのサポートや、ONNX RuntimeおよびWinMLによる利用などのアップデートや機会があります。主要な統合ハードウェアベンダー（IHV）によって支えられており、それぞれがさまざまなメタコマンドを実装しています。

## CUDA

CUDA（Compute Unified Device Architecture）は、Nvidiaによって開発された並列コンピューティングプラットフォームおよびAPIモデルです。CUDA対応のGPUを使用して汎用的な処理を行うことができ、このアプローチはGPGPU（Graphics Processing Unitsでの汎用計算）と呼ばれます。CUDAはNvidiaのGPUアクセラレーションの主要技術であり、機械学習、科学計算、ビデオ処理など、さまざまな分野で広く使用されています。

CUDAのハードウェアサポートはNvidiaのGPUに特化しており、Nvidiaが開発した独自技術です。各アーキテクチャは特定のバージョンのCUDAツールキットをサポートしており、これには開発者がCUDAアプリケーションを構築・実行するためのライブラリやツールが含まれています。

## ONNX

ONNX（Open Neural Network Exchange）は、機械学習モデルを表現するためのオープンフォーマットです。拡張可能な計算グラフモデルの定義や、組み込みオペレーターの定義、標準データ型の定義を提供します。ONNXは、異なるMLフレームワーク間でモデルを移動できるようにし、相互運用性を実現してAIアプリケーションの作成と展開を容易にします。

Phi3 miniは、ONNX Runtimeを使用してCPUおよびGPU上で動作し、サーバープラットフォーム、Windows、Linux、Macデスクトップ、モバイルCPUを含むさまざまなデバイスで利用可能です。
追加された最適化構成は以下の通りです：

- int4 DML用ONNXモデル：AWQを使用してint4に量子化
- fp16 CUDA用ONNXモデル
- int4 CUDA用ONNXモデル：RTNを使用してint4に量子化
- int4 CPUおよびモバイル用ONNXモデル：RTNを使用してint4に量子化

## Llama.cpp

Llama.cppは、C++で記述されたオープンソースのソフトウェアライブラリです。Llamaを含むさまざまな大規模言語モデル（LLM）の推論を実行します。汎用テンソルライブラリであるggmlライブラリと共に開発され、元のPython実装と比較して高速な推論と低メモリ使用を提供することを目的としています。ハードウェア最適化、量子化をサポートし、シンプルなAPIと例を提供します。効率的なLLM推論に興味がある場合は、Phi3がLlama.cppを実行できるため、探求する価値があります。

## GGUF

GGUF（Generic Graph Update Format）は、機械学習モデルを表現・更新するためのフォーマットです。特に4-8ビット量子化を使用してCPU上で効果的に動作する小型の言語モデル（SLM）に有用です。GGUFは迅速なプロトタイピングや、エッジデバイス上でのモデルの実行、CI/CDパイプラインのようなバッチジョブにおいて有益です。

**免責事項**:  
この文書は、AI翻訳サービス[Co-op Translator](https://github.com/Azure/co-op-translator)を使用して翻訳されています。正確性を追求していますが、自動翻訳には誤りや不正確な箇所が含まれる可能性があります。元の言語で記載された原文が正式な情報源としてみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解釈について、当方は一切の責任を負いません。