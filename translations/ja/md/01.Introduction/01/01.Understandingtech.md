<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-08T06:15:53+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "ja"
}
-->
# 主要な技術

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12上に構築されたハードウェアアクセラレーション機能を持つ低レベルの機械学習API。
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidiaが開発した並列コンピューティングプラットフォームおよびAPIモデルで、GPU上での汎用処理を可能にする。
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - 機械学習モデルを表現するためのオープンフォーマットで、異なるMLフレームワーク間の相互運用性を提供。
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - 機械学習モデルの表現と更新に使われるフォーマットで、特に4-8bit量子化によりCPU上で効率的に動作する小型言語モデルに適している。

## DirectML

DirectMLはハードウェアアクセラレーションを活用した機械学習を可能にする低レベルAPIです。DirectX 12の上に構築されており、GPUアクセラレーションを利用しつつ、ベンダーに依存しないため、異なるGPUベンダー間でコードの変更なしに動作します。主にGPU上でのモデルのトレーニングや推論処理に使われています。

ハードウェアサポートとしては、AMDの統合・ディスクリートGPU、Intelの統合GPU、NVIDIAのディスクリートGPUなど幅広いGPUに対応しています。Windows AIプラットフォームの一部であり、Windows 10および11でサポートされているため、あらゆるWindowsデバイスでモデルのトレーニングと推論が可能です。

DirectMLに関するアップデートや新たな機能としては、最大150のONNXオペレーターのサポートや、ONNXランタイムやWinMLでの利用があります。主要な統合ハードウェアベンダー（IHV）によって支えられており、それぞれが様々なメタコマンドを実装しています。

## CUDA

CUDA（Compute Unified Device Architecture）はNvidiaが開発した並列コンピューティングプラットフォームおよびAPIモデルです。CUDA対応GPUを使って一般的な処理を行うことができるため、GPGPU（Graphics Processing Units上での汎用計算）と呼ばれています。CUDAはNvidiaのGPUアクセラレーションの中核技術であり、機械学習、科学計算、映像処理など多岐にわたる分野で広く利用されています。

ハードウェアサポートはNvidiaのGPUに限定されており、CUDAツールキットの各バージョンはそれぞれのGPUアーキテクチャに対応しています。ツールキットには開発者がCUDAアプリケーションを構築・実行するためのライブラリやツールが含まれています。

## ONNX

ONNX（Open Neural Network Exchange）は機械学習モデルを表現するためのオープンフォーマットです。拡張可能な計算グラフモデルの定義、組み込みオペレーターや標準データ型の定義を提供します。ONNXにより、異なるMLフレームワーク間でモデルを移行でき、AIアプリケーションの作成と展開が容易になります。

Phi3 miniはONNX Runtimeを使ってCPUやGPU上で動作し、サーバープラットフォーム、Windows、Linux、Macのデスクトップ、モバイルCPUなど様々なデバイスに対応しています。追加した最適化構成は以下の通りです。

- int4 DML向けONNXモデル：AWQによるint4量子化
- fp16 CUDA向けONNXモデル
- int4 CUDA向けONNXモデル：RTNによるint4量子化
- int4 CPUおよびモバイル向けONNXモデル：RTNによるint4量子化

## Llama.cpp

Llama.cppはC++で書かれたオープンソースのソフトウェアライブラリで、Llamaを含む様々な大規模言語モデル（LLM）の推論を行います。汎用テンソルライブラリであるggmlと共に開発され、元のPython実装と比べて高速な推論と低いメモリ使用量を実現しています。ハードウェア最適化や量子化をサポートし、シンプルなAPIとサンプルも提供しています。効率的なLLM推論に興味があるなら、Phi3がLlama.cppを動かせることもあり、ぜひ検討する価値があります。

## GGUF

GGUF（Generic Graph Update Format）は機械学習モデルの表現と更新に使われるフォーマットです。特に4-8bit量子化によりCPU上で効率的に動作する小型言語モデル（SLM）に適しています。GGUFは迅速なプロトタイピングやエッジデバイス、CI/CDパイプラインのようなバッチ処理でのモデル実行に有用です。

**免責事項**：  
本書類はAI翻訳サービス「[Co-op Translator](https://github.com/Azure/co-op-translator)」を使用して翻訳されました。正確性の向上に努めておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご了承ください。原文の母国語版が正式な情報源とみなされます。重要な情報については、専門の人間による翻訳を推奨します。本翻訳の使用により生じた誤解や誤訳について、当方は一切の責任を負いかねます。