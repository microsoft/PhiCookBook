<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7e58d5075509bcb4a65bc8370bd21a8b",
  "translation_date": "2025-04-04T11:46:44+00:00",
  "source_file": "md\\01.Introduction\\01\\01.EnvironmentSetup.md",
  "language_code": "ja"
}
-->
# Phi-3をローカル環境で始める

このガイドでは、Ollamaを使用してPhi-3モデルをローカル環境で実行するためのセットアップ方法を説明します。GitHub Codespaces、VS Code Dev Containers、またはローカル環境を使用してモデルを実行することができます。

## 環境のセットアップ

### GitHub Codespaces

GitHub Codespacesを使用すると、このテンプレートを仮想的に実行することができます。ボタンをクリックすると、ブラウザ内でWebベースのVS Codeインスタンスが開きます：

1. テンプレートを開きます（数分かかる場合があります）：

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. ターミナルウィンドウを開きます

### VS Code Dev Containers

⚠️ このオプションは、Docker Desktopに最低16 GBのRAMが割り当てられている場合にのみ動作します。16 GB未満の場合は、[GitHub Codespacesオプション](../../../../../md/01.Introduction/01)を試すか、[ローカル環境をセットアップ](../../../../../md/01.Introduction/01)してください。

関連するオプションとして、VS Code Dev Containersを使用すると、[Dev Containers拡張機能](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)を使ってプロジェクトをローカルのVS Codeで開くことができます：

1. Docker Desktopを起動します（インストールされていない場合はインストールしてください）
2. プロジェクトを開きます：

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. VS Codeウィンドウが開き、プロジェクトファイルが表示されたら（数分かかる場合があります）、ターミナルウィンドウを開きます。
4. [デプロイ手順](../../../../../md/01.Introduction/01)に進みます。

### ローカル環境

1. 以下のツールがインストールされていることを確認してください：

    * [Ollama](https://ollama.com/)
    * [Python 3.10以上](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## モデルのテスト

1. Ollamaにphi3:miniモデルをダウンロードして実行するよう依頼します：

    ```shell
    ollama run phi3:mini
    ```

    モデルのダウンロードには数分かかります。

2. 出力に「success」と表示されたら、プロンプトからそのモデルにメッセージを送信できます。

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. 数秒後、モデルからの応答がストリーム形式で表示されるはずです。

4. 言語モデルで使用されるさまざまな技術について学ぶには、Pythonノートブック[ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb)を開き、各セルを実行してください。'phi3:mini'以外のモデルを使用した場合は、ファイルの冒頭で`MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME`を必要に応じて変更し、システムメッセージや数ショット例を追加することもできます。

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があります。元の言語で記載された原文が正式な情報源として考慮されるべきです。重要な情報については、専門の人間による翻訳を推奨します。本翻訳の利用によって生じた誤解や誤解釈について、当方は一切責任を負いません。