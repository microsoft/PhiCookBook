<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "52973a5680a65a810aa80b7036afd31f",
  "translation_date": "2025-06-27T13:35:16+00:00",
  "source_file": "md/01.Introduction/02/07.FoundryLocal.md",
  "language_code": "ja"
}
-->
## Foundry LocalでのPhiファミリーモデルの始め方

### Foundry Localの紹介

Foundry Localは、企業レベルのAI機能をローカルハードウェア上で直接利用できる強力なオンデバイスAI推論ソリューションです。このチュートリアルでは、Foundry LocalでPhiファミリーモデルを設定・使用する方法を案内し、プライバシーを守りつつコストを抑えながらAIワークロードを完全にコントロールする方法を紹介します。

Foundry Localは、デバイス上でAIモデルを実行することでパフォーマンス、プライバシー、カスタマイズ性、コスト面での利点を提供します。直感的なCLI、SDK、REST APIを通じて既存のワークフローやアプリケーションにシームレスに統合できます。


![arch](../../../../../translated_images/foundry-local-arch.8823e321dd8258d7d68815ddb0153503587142ff32e6997041c7cf0c9df24b49.ja.png)

### Foundry Localを選ぶ理由

Foundry Localのメリットを理解することで、AI導入戦略の判断に役立ちます：

- **オンデバイス推論:** 自分のハードウェア上でモデルを実行し、コストを削減しつつデータをデバイス内に保持します。

- **モデルのカスタマイズ:** 事前設定済みモデルから選択するか、自分のモデルを使用して特定の要件やユースケースに対応可能です。

- **コスト効率:** 既存のハードウェアを活用してクラウドサービスの継続的な費用を削減し、AIの利用をより身近にします。

- **シームレスな統合:** SDK、APIエンドポイント、CLIを通じてアプリケーションと接続でき、ニーズに応じてAzure AI Foundryへのスケールアップも容易です。

> **はじめに:** このチュートリアルでは、CLIとSDKの両方を使ったFoundry Localの操作方法に焦点を当てています。用途に応じて最適な方法を選べるように学びましょう。

## パート1: Foundry Local CLIのセットアップ

### ステップ1: インストール

Foundry Local CLIは、ローカルでAIモデルを管理・実行するための入り口です。まずはシステムにインストールしましょう。

**対応プラットフォーム:** WindowsとmacOS

詳しいインストール手順は、[公式Foundry Localドキュメント](https://github.com/microsoft/Foundry-Local/blob/main/README.md)をご参照ください。

### ステップ2: 利用可能なモデルの確認

Foundry Local CLIをインストールしたら、利用可能なモデルを確認しましょう。以下のコマンドでサポートされているモデルが一覧表示されます：


```bash
foundry model list
```

### ステップ3: Phiファミリーモデルの理解

Phiファミリーは、さまざまなユースケースやハードウェア構成に最適化されたモデル群です。Foundry Localで利用できるPhiモデルは以下の通りです：

**利用可能なPhiモデル：** 

- **phi-3.5-mini** - 基本タスク向けのコンパクトモデル
- **phi-3-mini-128k** - 長い会話に対応した拡張コンテキストモデル
- **phi-3-mini-4k** - 一般用途向けの標準コンテキストモデル
- **phi-4** - 高度な機能を持つモデル
- **phi-4-mini** - Phi-4の軽量版
- **phi-4-mini-reasoning** - 複雑な推論タスクに特化したモデル

> **ハードウェア対応:** 各モデルはシステムの能力に応じてCPUやGPUなど異なるハードウェアアクセラレーションで設定可能です。

### ステップ4: 最初のPhiモデルの実行

実践例として、複雑な問題を段階的に解くのに優れた`phi-4-mini-reasoning`モデルを実行してみましょう。


**モデル実行コマンド：**

```bash
foundry model run Phi-4-mini-reasoning-generic-cpu
```

> **初回実行時の注意:** モデルを初めて実行する際、Foundry Localが自動的にモデルをローカルデバイスにダウンロードします。ネットワーク速度によってダウンロード時間が異なるため、初期セットアップ時はしばらくお待ちください。

### ステップ5: 実際の問題でモデルをテスト

次に、古典的な論理問題を使ってモデルの段階的推論能力を試してみましょう：

**例題：**

```txt
Please calculate the following step by step: Now there are pheasants and rabbits in the same cage, there are thirty-five heads on top and ninety-four legs on the bottom, how many pheasants and rabbits are there?
```

**期待される動作:** モデルはこの問題を論理的なステップに分解し、キジが2本足、ウサギが4本足であるという事実を使って連立方程式を解くはずです。

**結果：**

![cli](../../../../../translated_images/cli.862ec6b55c2b5d916093866d4df99190150d4198fd33ab79e586f9d6f5403089.ja.png)

## パート2: Foundry Local SDKを使ったアプリケーション開発

### なぜSDKを使うのか？

CLIはテストや簡単な操作に最適ですが、SDKを使うことでFoundry Localをプログラム的にアプリケーションに組み込めます。これにより以下が可能になります：

- カスタムAIアプリケーションの構築
- 自動化ワークフローの作成
- 既存システムへのAI機能統合
- チャットボットやインタラクティブツールの開発

### 対応プログラミング言語

Foundry Localは複数のプログラミング言語向けSDKを提供しており、開発環境に合わせて選べます：

**📦 利用可能なSDK：**

- **C# (.NET):** [SDKドキュメント＆サンプル](https://github.com/microsoft/Foundry-Local/tree/main/sdk/cs)
- **Python:** [SDKドキュメント＆サンプル](https://github.com/microsoft/Foundry-Local/tree/main/sdk/python)
- **JavaScript:** [SDKドキュメント＆サンプル](https://github.com/microsoft/Foundry-Local/tree/main/sdk/js)
- **Rust:** [SDKドキュメント＆サンプル](https://github.com/microsoft/Foundry-Local/tree/main/sdk/rust)

### 次のステップ

1. **開発環境に合ったSDKを選択**
2. **SDKごとの詳細な実装ガイドに従う**
3. **まずはシンプルな例から始める**
4. **各SDKリポジトリにあるサンプルコードを活用する**

## まとめ

これまでに以下のことを学びました：
- ✅ Foundry Local CLIのインストールとセットアップ
- ✅ Phiファミリーモデルの探索と実行
- ✅ 実際の問題を使ったモデルのテスト
- ✅ アプリケーション開発向けSDKの選択肢の理解

Foundry Localは、AI機能をローカル環境に直接もたらし、パフォーマンス、プライバシー、コストをコントロールしながら、必要に応じてクラウドソリューションへのスケールも可能にする強力な基盤を提供します。

**免責事項**：  
本書類はAI翻訳サービス「[Co-op Translator](https://github.com/Azure/co-op-translator)」を使用して翻訳されました。正確性には努めておりますが、自動翻訳には誤りや不正確な箇所が含まれる可能性があります。原文（原言語版）が正式な情報源とみなされます。重要な情報については、専門の人間による翻訳を推奨します。本翻訳の利用により生じたいかなる誤解や誤訳についても、当方は責任を負いかねます。