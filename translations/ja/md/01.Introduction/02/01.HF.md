# **Hugging FaceでPhiファミリーを使う**

[Hugging Face](https://huggingface.co/)は、豊富なデータとオープンソースモデルのリソースを持つ非常に人気のあるAIコミュニティです。Microsoft、Meta、Mistral、Apple、Googleなど、さまざまなメーカーがHugging Faceを通じてオープンソースのLLMやSLMを公開しています。

MicrosoftのPhiファミリーはHugging Faceで公開されています。開発者はシナリオやビジネスに応じて対応するPhiファミリーのモデルをダウンロードできます。PhiのPytorchモデルをHugging Faceで展開するだけでなく、量子化モデルもリリースしており、GGUFやONNX形式を使ってエンドユーザーに選択肢を提供しています。

## **Hugging Faceでモデルをダウンロードする**

以下のリンクからPhiファミリーモデルをダウンロードできます。

[Microsoft Models on Hugging Face](https://huggingface.co/microsoft)

-  **Phi-1 / 1.5** https://huggingface.co/collections/microsoft/phi-1-6626e29134744e94e222d572

-  **Phi-3 / 3.5** https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3

-  **Phi-4** https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4

- **Phi-4-reasoning** https://huggingface.co/microsoft/Phi-4-reasoning

- **Phi-4-reasoning Plus** https://huggingface.co/microsoft/Phi-4-reasoning-plus 

- **Phi-4-mini-reasoning** https://huggingface.co/microsoft/Phi-4-mini-reasoning

モデルはさまざまな方法でダウンロード可能です。たとえば、***Hugging Face CLI SDK***をインストールするか、***git clone***を使う方法があります。

### **Hugging Face CLIを使ってPhiファミリーモデルをダウンロードする**

- Hugging Face CLIをインストールする

```bash

pip install -U "huggingface_hub[cli]"

```

- huggingface-cliでログインする

[設定ページ](https://huggingface.co/settings/tokens)から取得した[ユーザーアクセストークン](https://huggingface.co/docs/hub/security-tokens)を使ってHugging Faceにログインします。

```bash

huggingface-cli login --token $HF_TOKEN --add-to-git-credential

```

- ダウンロードする

モデルをダウンロードしてキャッシュに保存できます。

```bash

huggingface-cli download microsoft/phi-4

```

特定の場所に保存することも可能です。

```bash

huggingface-cli download microsoft/phi-4 --local-dir $YOUR_PATH

```

### **git cloneを使ってPhiファミリーモデルをダウンロードする**

***git clone***を使ってモデルをダウンロードすることもできます。

```bash

git lfs install

git clone https://huggingface.co/microsoft/phi-4

```

## **サンプル - Microsoft Phi-4の推論**

- **transformersライブラリのインストール**

```bash

pip install transformers -U

```

- **VSCodeでこのコードを実行する**

```python

import transformers

pipeline = transformers.pipeline(
    "text-generation",
    model="microsoft/phi-4",
    model_kwargs={"torch_dtype": "auto"},
    device_map="auto",
)

messages = [
    {"role": "user", "content": "I have $20,000 in my savings account, where I receive a 4% profit per year and payments twice a year. Can you please tell me how long it will take for me to become a millionaire? Also, can you please explain the math step by step as if you were explaining it to an uneducated person?"},
]

outputs = pipeline(messages, max_new_tokens=2048)
print(outputs[0]["generated_text"][-1])

```

**免責事項**：  
本書類はAI翻訳サービス「[Co-op Translator](https://github.com/Azure/co-op-translator)」を使用して翻訳されました。正確性を期しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があります。原文の言語によるオリジナル文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。本翻訳の利用により生じた誤解や誤訳について、当方は一切の責任を負いかねます。