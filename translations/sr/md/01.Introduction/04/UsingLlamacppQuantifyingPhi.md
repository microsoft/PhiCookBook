<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "462bddc47427d8785f3c9fd817b346fe",
  "translation_date": "2025-07-16T22:12:28+00:00",
  "source_file": "md/01.Introduction/04/UsingLlamacppQuantifyingPhi.md",
  "language_code": "sr"
}
-->
# **Квантизација Phi породице користећи llama.cpp**

## **Шта је llama.cpp**

llama.cpp је софтверска библиотека отвореног кода, углавном написана у C++, која изводи инференцу на различитим великим језичким моделима (LLM), као што је Llama. Њен главни циљ је да обезбеди врхунске перформансе за инференцу LLM модела на широком спектру хардвера уз минималну конфигурацију. Поред тога, постоје Python биндинзи за ову библиотеку који нуде високонапонски API за допуњавање текста и OpenAI компатибилан веб сервер.

Главни циљ llama.cpp је да омогући инференцу LLM модела са минималном конфигурацијом и врхунским перформансама на разноврсном хардверу - локално и у облаку.

- Чиста C/C++ имплементација без икаквих зависности  
- Apple silicon је првокласни корисник - оптимизован преко ARM NEON, Accelerate и Metal фрејмворка  
- Подршка за AVX, AVX2 и AVX512 на x86 архитектурама  
- Квантизација у 1.5-битним, 2-битним, 3-битним, 4-битним, 5-битним, 6-битним и 8-битним целобројним форматима за бржу инференцу и мању потрошњу меморије  
- Прилагођени CUDA кернели за покретање LLM модела на NVIDIA GPU-овима (подршка за AMD GPU преко HIP-а)  
- Подршка за Vulkan и SYCL бекенд  
- Хибридна инференца CPU+GPU за делимично убрзање модела већих од укупног капацитета VRAM-а  

## **Квантизација Phi-3.5 помоћу llama.cpp**

Phi-3.5-Instruct модел може бити квантизован коришћењем llama.cpp, али Phi-3.5-Vision и Phi-3.5-MoE још нису подржани. Формат који конвертује llama.cpp је gguf, који је такође најчешће коришћени формат за квантизацију.

Постоји велики број квантизованих модела у GGUF формату на Hugging Face-у. AI Foundry, Ollama и LlamaEdge се ослањају на llama.cpp, па се GGUF модели често користе.

### **Шта је GGUF**

GGUF је бинарни формат оптимизован за брзо учитавање и чување модела, што га чини веома ефикасним за инференцу. GGUF је дизајниран за коришћење са GGML и другим извршиоцима. GGUF је развио @ggerganov, који је такође и аутор llama.cpp, популарног C/C++ фрејмворка за инференцу LLM модела. Модели који су првобитно развијени у фрејмворцима као што је PyTorch могу бити конвертовани у GGUF формат за употребу са тим моторима.

### **ONNX у односу на GGUF**

ONNX је традиционални формат за машинско учење/дубоко учење, који је добро подржан у различитим AI фрејмворцима и има добре примене на уређајима на ивици мреже. Што се тиче GGUF-а, он је базиран на llama.cpp и може се рећи да је настао у ери GenAI. Оба имају сличне намене. Ако желите боље перформансе на уграђеном хардверу и у апликационим слојевима, ONNX може бити ваш избор. Ако користите деривативни фрејмворк и технологију llama.cpp, онда GGUF може бити бољи.

### **Квантизација Phi-3.5-Instruct помоћу llama.cpp**

**1. Конфигурација окружења**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. Квантизација**

Коришћењем llama.cpp конвертујте Phi-3.5-Instruct у FP16 GGUF


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Квантизација Phi-3.5 у INT4


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. Тестирање**

Инсталирајте llama-cpp-python


```bash

pip install llama-cpp-python -U

```

***Напомена*** 

Ако користите Apple Silicon, молимо инсталирајте llama-cpp-python на овај начин


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

Тестирање 


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **Ресурси**

1. Сазнајте више о llama.cpp [https://github.com/ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)  
2. Сазнајте више о onnxruntime [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)  
3. Сазнајте више о GGUF [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем AI услуге за превођење [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да превод буде тачан, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати ауторитетним извором. За критичне информације препоручује се професионални људски превод. Нисмо одговорни за било каква неспоразума или погрешна тумачења која произилазе из коришћења овог превода.