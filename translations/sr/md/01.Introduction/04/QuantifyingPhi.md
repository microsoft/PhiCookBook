<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-05T15:26:57+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "sr"
}
-->
# **Квантификовање Phi фамилије**

Квантизација модела односи се на процес мапирања параметара (као што су тежине и вредности активирања) у моделу неуронске мреже из великог опсега вредности (обично континуални опсег вредности) у мањи коначан опсег вредности. Ова технологија може смањити величину и рачунску сложеност модела и побољшати оперативну ефикасност модела у окружењима са ограниченим ресурсима као што су мобилни уређаји или уграђени системи. Квантизација модела постиже компресију смањењем прецизности параметара, али такође уводи одређени губитак прецизности. Стога је у процесу квантизације неопходно постићи равнотежу између величине модела, рачунске сложености и прецизности. Уобичајене методе квантизације укључују квантизацију фиксне тачке, квантизацију покретне тачке итд. Можете изабрати одговарајућу стратегију квантизације у зависности од специфичног сценарија и потреба.

Надамо се да ћемо ГенАИ модел деплојовати на ивичне уређаје и омогућити више уређаја улазак у ГенАИ сценарије, као што су мобилни уређаји, AI PC/Copilot+PC и традиционални IoT уређаји. Преко квантизованог модела можемо га распоредити на различите ивичне уређаје у зависности од различитих уређаја. У комбинацији са оквиром за убрзавање модела и квантизованим моделом које обезбеђују произвођачи хардвера, можемо изградити боље SLM апликационе сценарије.

У сценарију квантизације, имамо различите прецизности (INT4, INT8, FP16, FP32). Следи објашњење уобичајених прецизности квантизације.

### **INT4**

INT4 квантизација је радикална метода квантизације која квантфикује тежине и вредности активирања модела у 4-битне целе бројеве. INT4 квантизација обично доводи до већег губитка прецизности због мањег опсега репрезентације и нижег нивоа прецизности. Међутим, у поређењу са INT8 квантизацијом, INT4 квантизација може даље смањити захтеве за складиштењем и рачунску сложеност модела. Треба имати у виду да је INT4 квантизација релативно ретка у практичним применама, јер превише ниска прецизност може проузроковати значајно погоршање перформанси модела. Поред тога, није сваки хардвер у стању да подржи INT4 операције, па треба узети у обзир компатибилност хардвера приликом избора методе квантизације.

### **INT8**

INT8 квантизација је процес претварања тежина и активирања модела са покретних тачака у 8-битне целе бројеве. Иако је нумерички опсег који представљају INT8 цели бројеви мањи и мање прецизан, значајно може смањити захтеве за складиштењем и прорачуном. При INT8 квантизацији, тежине и вредности активирања модела пролазе кроз процес квантизације, укључујући скалирање и офсет, како би се што више сачувале оригиналне информације у покретној тачки. Током информисања, ове квантоване вредности ће бити де-квантоване назад у вредности покретне тачке за прорачун, а затим поново квантоване у INT8 за следећи корак. Ова метода може пружити довољну прецизност у већини апликација уз одржавање високе рачунске ефикасности.

### **FP16**

FP16 формат, то јест 16-битни бројеви покретне тачке (float16), смањује употребу меморије на пола у односу на 32-битне бројеве покретне тачке (float32), што има значајне предности у великим дубоким учењима. FP16 формат омогућава учитавање већих модела или обраду више података у оквиру истих ограничења GPU меморије. Како модерни GPU хардвер наставља да подржава FP16 операције, коришћење FP16 формата такође може донети побољшање брзине рачунања. Међутим, FP16 формат има и своје унутрашње недостатке, наиме нижу прецизност, што може довести до нумеричке нестабилности или губитка прецизности у неким случајевима.

### **FP32**

FP32 формат пружа већу прецизност и може тачно представити широк опсег вредности. У сценаријима где се изводе сложене математичке операције или су потребни резултати високе прецизности, преферира се FP32 формат. Међутим, висока прецизност такође значи већу употребу меморије и дужи прорачунски временски период. За велике дубоке моделе учења, посебно када има много параметара модела и велико количество података, FP32 формат може довести до недовољне GPU меморије или смањења брзине информисања.

На мобилним уређајима или IoT уређајима, можемо конвертовати Phi-3.x моделе у INT4, док AI PC / Copilot PC могу користити већу прецизност као што су INT8, FP16, FP32.

Тренутно, различити произвођачи хардвера имају различите оквире за подршку генеративним моделима, као што су Intel OpenVINO, Qualcomm QNN, Apple MLX и Nvidia CUDA и други, у комбинацији са квантизацијом модела за локални распоред.

Што се тиче технологије, имамо различиту подршку формата након квантизације, као што су PyTorch / TensorFlow формат, GGUF и ONNX. Урадио сам поређење формата и сценарије примене између GGUF и ONNX. Овде препоручујем ONNX квантизациони формат, који има добру подршку од оквира модела до хардвера. У овом поглављу ћемо се фокусирати на ONNX Runtime за GenAI, OpenVINO и Apple MLX за извођење квантизације модела (уколико имате бољи начин, можете нам га такође послати путем PR).

**Ово поглавље укључује**

1. [Квантизацију Phi-3.5 / 4 користећи llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Квантизацију Phi-3.5 / 4 користећи Generative AI екстензије за onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Квантизацију Phi-3.5 / 4 користећи Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Квантизацију Phi-3.5 / 4 користећи Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Одрицање од одговорности**:  
Овај документ је преведен коришћењем AI преводилачке услуге [Co-op Translator](https://github.com/Azure/co-op-translator). Иако тежимо прецизности, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати ауторитативним извором. За критичне информације препоручује се професионални превод људским ресурсима. Не сносимо одговорност за било каква неспоразума или погрешна тумачења настала употребом овог превода.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->