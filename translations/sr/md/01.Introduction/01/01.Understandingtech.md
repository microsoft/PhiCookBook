<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:32:57+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "sr"
}
-->
# Ključne tehnologije koje se pominju uključuju

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - niskonivoi API za hardverski ubrzano mašinsko učenje izgrađen na DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - paralelna platforma za računanje i API model razvijen od strane Nvidije, koji omogućava opštu obradu na grafičkim procesorima (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - otvoreni format dizajniran za predstavljanje modela mašinskog učenja koji omogućava interoperabilnost između različitih ML okvira.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - format koji se koristi za predstavljanje i ažuriranje modela mašinskog učenja, posebno koristan za manje jezičke modele koji mogu efikasno da rade na CPU sa 4-8bit kvantizacijom.

## DirectML

DirectML je niskonivoi API koji omogućava hardverski ubrzano mašinsko učenje. Izgrađen je na DirectX 12 kako bi iskoristio GPU akceleraciju i nezavisan je od proizvođača, što znači da ne zahteva izmene koda da bi radio na različitim GPU proizvođačima. Prvenstveno se koristi za treniranje modela i inferencu na GPU.

Što se tiče hardverske podrške, DirectML je dizajniran da radi sa širokim spektrom GPU-ova, uključujući AMD integrisane i diskretne GPU, Intel integrisane GPU i NVIDIA diskretne GPU. Deo je Windows AI platforme i podržan je na Windows 10 i 11, omogućavajući treniranje modela i inferencu na bilo kom Windows uređaju.

Bilo je ažuriranja i mogućnosti vezanih za DirectML, kao što je podrška za do 150 ONNX operatora i korišćenje u ONNX runtime-u i WinML-u. Podržan je od strane glavnih Integrated Hardware Vendors (IHVs), od kojih svaki implementira različite metakomande.

## CUDA

CUDA, što znači Compute Unified Device Architecture, je paralelna platforma za računanje i API model koji je razvila Nvidia. Omogućava programerima softvera da koriste CUDA-om omogućeni grafički procesor (GPU) za opštu obradu – pristup poznat kao GPGPU (General-Purpose computing on Graphics Processing Units). CUDA je ključni pokretač Nvidia GPU akceleracije i široko se koristi u različitim oblastima, uključujući mašinsko učenje, naučna računanja i obradu video zapisa.

Hardverska podrška za CUDA je specifična za Nvidia GPU, jer je to vlasnička tehnologija koju je razvila Nvidia. Svaka arhitektura podržava određene verzije CUDA alata koji pružaju neophodne biblioteke i alate za razvoj i pokretanje CUDA aplikacija.

## ONNX

ONNX (Open Neural Network Exchange) je otvoreni format dizajniran za predstavljanje modela mašinskog učenja. Pruža definiciju proširivog modela računarskog grafa, kao i definicije ugrađenih operatora i standardnih tipova podataka. ONNX omogućava programerima da prenose modele između različitih ML okvira, čime se omogućava interoperabilnost i olakšava kreiranje i implementacija AI aplikacija.

Phi3 mini može da radi sa ONNX Runtime na CPU i GPU preko različitih uređaja, uključujući serverske platforme, Windows, Linux i Mac desktop računare, kao i mobilne CPU.
Optimizovane konfiguracije koje smo dodali su

- ONNX modeli za int4 DML: kvantizovani na int4 putem AWQ
- ONNX model za fp16 CUDA
- ONNX model za int4 CUDA: kvantizovan na int4 putem RTN
- ONNX model za int4 CPU i Mobile: kvantizovan na int4 putem RTN

## Llama.cpp

Llama.cpp je open-source softverska biblioteka napisana u C++. Izvršava inferencu na različitim velikim jezičkim modelima (LLM), uključujući Llama. Razvijen je zajedno sa ggml bibliotekom (opšta tensor biblioteka), a cilj llama.cpp je da obezbedi bržu inferencu i manju potrošnju memorije u odnosu na originalnu Python implementaciju. Podržava hardversku optimizaciju, kvantizaciju i nudi jednostavan API i primere. Ako vas zanima efikasna inferenca LLM modela, llama.cpp vredi istražiti jer Phi3 može da pokreće Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) je format koji se koristi za predstavljanje i ažuriranje modela mašinskog učenja. Posebno je koristan za manje jezičke modele (SLM) koji mogu efikasno da rade na CPU sa 4-8bit kvantizacijom. GGUF je koristan za brzo prototipisanje i pokretanje modela na edge uređajima ili u batch poslovima poput CI/CD pipeline-a.

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем AI преводилачке услуге [Co-op Translator](https://github.com/Azure/co-op-translator). Иако тежимо прецизности, имајте у виду да аутоматизовани преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати ауторитетним извором. За критичне информације препоручује се професионални људски превод. Нисмо одговорни за било каква неспоразума или погрешна тумачења која могу произаћи из коришћења овог превода.