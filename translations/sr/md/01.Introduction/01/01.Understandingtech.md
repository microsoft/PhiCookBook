<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:48:18+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "sr"
}
-->
# Кључне технологије које се помињу укључују

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - ниско-нивoски API за хардверски убрзано машинско учење изграђен на врху DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - платформа за паралелно рачунање и API модел који је развио Nvidia, омогућавајући општу обраду на графичким процесорима (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - отворени формат дизајниран за представљање модела машинског учења који пружа интероперабилност између различитих ML оквира.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - формат који се користи за представљање и ажурирање модела машинског учења, посебно користан за мање језичке моделе који могу ефикасно да раде на CPU-овима са 4-8 битном квантизацијом.

## DirectML

DirectML је ниско-нивoски API који омогућава хардверски убрзано машинско учење. Изграђен је на врху DirectX 12 како би искористио GPU убрзање и није везан за одређеног произвођача, што значи да не захтева промене у коду да би радио на различитим GPU-овима. Примарно се користи за тренинг модела и извођење инференце на GPU-овима.

Што се тиче подршке за хардвер, DirectML је дизајниран да ради са широким спектром GPU-ова, укључујући AMD интегрисане и дискретне GPU-ове, Intel интегрисане GPU-ове и NVIDIA дискретне GPU-ове. Део је Windows AI платформе и подржан је на Windows 10 и 11, омогућавајући тренинг и инференцу модела на било ком Windows уређају.

Било је ажурирања и нових могућности у вези са DirectML, као што је подршка за до 150 ONNX оператора и коришћење од стране ONNX runtime-а и WinML-а. Подржавају га главни произвођачи хардвера (IHVs), од којих сваки имплементира различите метакоманде.

## CUDA

CUDA, што значи Compute Unified Device Architecture, је платформа за паралелно рачунање и API модел који је развио Nvidia. Омогућава програмерима да користе CUDA-ом омогућени графички процесор (GPU) за општу обраду – приступ познат као GPGPU (општа обрада на графичким процесорима). CUDA је кључни покретач Nvidia GPU убрзања и широко се користи у разним областима, укључујући машинско учење, научно рачунање и обраду видео записа.

Подршка за хардвер код CUDA је специфична за Nvidia GPU-ове, јер је то власничка технологија коју је развила Nvidia. Свака архитектура подржава одређене верзије CUDA алатног сета, који пружа неопходне библиотеке и алате за развој и покретање CUDA апликација.

## ONNX

ONNX (Open Neural Network Exchange) је отворени формат дизајниран за представљање модела машинског учења. Пружа дефиницију проширивог модела графа израчунавања, као и дефиниције уграђених оператора и стандардних типова података. ONNX омогућава програмерима да преносе моделе између различитих ML оквира, омогућавајући интероперабилност и олакшавајући креирање и имплементацију AI апликација.

Phi3 mini може да ради са ONNX Runtime-ом на CPU-у и GPU-у на различитим уређајима, укључујући серверске платформе, Windows, Linux и Mac десктопове, као и мобилне CPU-ове.  
Оптимизоване конфигурације које смо додали су

- ONNX модели за int4 DML: квантизовани у int4 преко AWQ
- ONNX модел за fp16 CUDA
- ONNX модел за int4 CUDA: квантизовани у int4 преко RTN
- ONNX модел за int4 CPU и Mobile: квантизовани у int4 преко RTN

## Llama.cpp

Llama.cpp је софтверска библиотека отвореног кода написана у C++. Изводи инференцу на различитим великим језичким моделима (LLM), укључујући Llama. Развијена заједно са ggml библиотеком (општа библиотека за тензоре), llama.cpp има за циљ да пружи бржу инференцу и мању потрошњу меморије у односу на оригиналну Python имплементацију. Подржава хардверску оптимизацију, квантизацију и нуди једноставан API и примере. Ако вас занима ефикасна инференца LLM модела, llama.cpp вреди испробати јер Phi3 може да покреће Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) је формат који се користи за представљање и ажурирање модела машинског учења. Посебно је користан за мање језичке моделе (SLM) који могу ефикасно да раде на CPU-овима са 4-8 битном квантизацијом. GGUF је погодан за брзо прототиписање и покретање модела на уређајима на ивици мреже или у серијским задацима као што су CI/CD процеси.

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем AI услуге за превођење [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да превод буде тачан, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати ауторитетним извором. За критичне информације препоручује се професионални људски превод. Нисмо одговорни за било каква неспоразума или погрешна тумачења која произилазе из коришћења овог превода.