{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Интерактивный чат-бот Phi 3 Mini 4K Instruct с Whisper\n",
    "\n",
    "### Введение:\n",
    "Интерактивный чат-бот Phi 3 Mini 4K Instruct — это инструмент, который позволяет пользователям взаимодействовать с демонстрацией Microsoft Phi 3 Mini 4K Instruct с помощью текстового или голосового ввода. Чат-бот может использоваться для выполнения различных задач, таких как перевод, обновления погоды и сбор общей информации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[Создайте ваш токен доступа Huggingface](https://huggingface.co/settings/tokens)\n",
    "\n",
    "Создайте новый токен  \n",
    "Укажите новое имя  \n",
    "Выберите права на запись  \n",
    "Скопируйте токен и сохраните его в надежном месте\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий код на Python выполняет две основные задачи: импорт модуля `os` и установку переменной окружения.\n",
    "\n",
    "1. Импорт модуля `os`:\n",
    "   - Модуль `os` в Python предоставляет возможность взаимодействовать с операционной системой. Он позволяет выполнять различные задачи, связанные с операционной системой, такие как доступ к переменным окружения, работа с файлами и директориями и т.д.\n",
    "   - В этом коде модуль `os` импортируется с помощью оператора `import`. Этот оператор делает функциональность модуля `os` доступной для использования в текущем Python-скрипте.\n",
    "\n",
    "2. Установка переменной окружения:\n",
    "   - Переменная окружения — это значение, к которому могут обращаться программы, работающие в операционной системе. Это способ хранения настроек конфигурации или другой информации, которая может использоваться несколькими программами.\n",
    "   - В этом коде новая переменная окружения устанавливается с использованием словаря `os.environ`. Ключ словаря — `'HF_TOKEN'`, а значение присваивается из переменной `HUGGINGFACE_TOKEN`.\n",
    "   - Переменная `HUGGINGFACE_TOKEN` определяется прямо над этим фрагментом кода и ей присваивается строковое значение `\"hf_**************\"` с использованием синтаксиса `#@param`. Этот синтаксис часто используется в Jupyter-ноутбуках для ввода данных пользователем и настройки параметров непосредственно в интерфейсе ноутбука.\n",
    "   - Установив переменную окружения `'HF_TOKEN'`, она становится доступной для других частей программы или других программ, работающих в той же операционной системе.\n",
    "\n",
    "В целом, этот код импортирует модуль `os` и устанавливает переменную окружения с именем `'HF_TOKEN'` и значением, указанным в переменной `HUGGINGFACE_TOKEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот фрагмент кода определяет функцию под названием clear_output, которая используется для очистки вывода текущей ячейки в Jupyter Notebook или IPython. Давайте разберем код и поймем его функциональность:\n",
    "\n",
    "Функция clear_output принимает один параметр под названием wait, который является булевым значением. По умолчанию wait установлен в значение False. Этот параметр определяет, должна ли функция ждать появления нового вывода, чтобы заменить существующий, прежде чем очистить его.\n",
    "\n",
    "Сама функция используется для очистки вывода текущей ячейки. В Jupyter Notebook или IPython, когда ячейка генерирует вывод, например, текст или графические изображения, этот вывод отображается под ячейкой. Функция clear_output позволяет очистить этот вывод.\n",
    "\n",
    "Реализация функции не предоставлена в данном фрагменте кода, что обозначено многоточием (...). Многоточие представляет собой заполнитель для фактического кода, который выполняет очистку вывода. Реализация функции может включать взаимодействие с API Jupyter Notebook или IPython для удаления существующего вывода из ячейки.\n",
    "\n",
    "В целом, эта функция предоставляет удобный способ очистки вывода текущей ячейки в Jupyter Notebook или IPython, что упрощает управление и обновление отображаемого вывода во время интерактивных сеансов кодирования.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполните преобразование текста в речь (TTS) с использованием сервиса Edge TTS. Давайте рассмотрим реализацию соответствующих функций одну за другой:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: Эта функция принимает входное значение и вычисляет строку скорости для голоса TTS. Входное значение представляет желаемую скорость речи, где значение 1 соответствует нормальной скорости. Функция вычисляет строку скорости, вычитая 1 из входного значения, умножая результат на 100 и определяя знак в зависимости от того, больше или равно ли входное значение 1. Функция возвращает строку скорости в формате \"{sign}{rate}\".\n",
    "\n",
    "2. `make_chunks(input_text, language)`: Эта функция принимает текст и язык в качестве параметров. Она разбивает текст на части, основываясь на языковых правилах. В данной реализации, если язык — \"English\", функция делит текст на части по точкам (\".\") и удаляет любые начальные или конечные пробелы. Затем она добавляет точку к каждой части и возвращает отфильтрованный список частей.\n",
    "\n",
    "3. `tts_file_name(text)`: Эта функция генерирует имя файла для аудиофайла TTS на основе входного текста. Она выполняет несколько преобразований текста: удаляет завершающую точку (если она есть), преобразует текст в нижний регистр, удаляет начальные и конечные пробелы и заменяет пробелы на подчеркивания. Затем она сокращает текст до максимальной длины в 25 символов (если он длиннее) или использует полный текст, если он пустой. Наконец, она генерирует случайную строку с использованием модуля [`uuid`] и объединяет её с сокращённым текстом, чтобы создать имя файла в формате \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\".\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: Эта функция объединяет несколько аудиофайлов в один. Она принимает список путей к аудиофайлам и путь для сохранения результата в качестве параметров. Функция инициализирует пустой объект `AudioSegment` под названием [`merged_audio`]. Затем она проходит по каждому пути к аудиофайлу, загружает аудиофайл с помощью метода `AudioSegment.from_file()` из библиотеки `pydub` и добавляет текущий аудиофайл к объекту [`merged_audio`]. В конце она экспортирует объединённый аудиофайл в указанный путь в формате MP3.\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: Эта функция выполняет операцию TTS с использованием сервиса Edge TTS. Она принимает список текстовых частей, скорость речи, имя голоса и путь для сохранения результата в качестве параметров. Если количество частей больше одной, функция создаёт директорию для хранения отдельных аудиофайлов частей. Затем она проходит по каждой части, формирует команду Edge TTS с использованием функции `calculate_rate_string()`, имени голоса и текста части, и выполняет команду с помощью функции `os.system()`. Если выполнение команды прошло успешно, она добавляет путь к созданному аудиофайлу в список. После обработки всех частей она объединяет отдельные аудиофайлы с помощью функции `merge_audio_files()` и сохраняет объединённый аудиофайл в указанный путь. Если есть только одна часть, она напрямую генерирует команду Edge TTS и сохраняет аудиофайл в указанный путь. В конце она возвращает путь к созданному аудиофайлу.\n",
    "\n",
    "6. `random_audio_name_generate()`: Эта функция генерирует случайное имя аудиофайла с использованием модуля [`uuid`]. Она генерирует случайный UUID, преобразует его в строку, берёт первые 8 символов, добавляет расширение \".mp3\" и возвращает случайное имя аудиофайла.\n",
    "\n",
    "7. `talk(input_text)`: Эта функция является основной точкой входа для выполнения операции TTS. Она принимает входной текст в качестве параметра. Сначала она проверяет длину входного текста, чтобы определить, является ли он длинным предложением (600 символов или больше). В зависимости от длины и значения переменной `translate_text_flag` она определяет язык и генерирует список текстовых частей с помощью функции `make_chunks()`. Затем она генерирует путь для сохранения аудиофайла с помощью функции `random_audio_name_generate()`. В конце она вызывает функцию `edge_free_tts()` для выполнения операции TTS и возвращает путь к созданному аудиофайлу.\n",
    "\n",
    "В целом, эти функции работают вместе, чтобы разбить входной текст на части, сгенерировать имя файла для аудиофайла, выполнить операцию TTS с использованием сервиса Edge TTS и объединить отдельные аудиофайлы в один.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация двух функций: convert_to_text и run_text_prompt, а также объявление двух классов: str и Audio.\n",
    "\n",
    "Функция convert_to_text принимает audio_path в качестве входного параметра и преобразует аудио в текст с использованием модели под названием whisper_model. Сначала функция проверяет, установлен ли флаг gpu в значение True. Если да, то whisper_model используется с определенными параметрами, такими как word_timestamps=True, fp16=True, language='English' и task='translate'. Если флаг gpu установлен в значение False, whisper_model используется с параметром fp16=False. Полученная транскрипция сохраняется в файл с именем 'scan.txt' и возвращается в виде текста.\n",
    "\n",
    "Функция run_text_prompt принимает сообщение и историю чата в качестве входных данных. Она использует функцию phi_demo для генерации ответа от чат-бота на основе входного сообщения. Сгенерированный ответ затем передается функции talk, которая преобразует ответ в аудиофайл и возвращает путь к файлу. Класс Audio используется для отображения и воспроизведения аудиофайла. Аудио отображается с помощью функции display из модуля IPython.display, а объект Audio создается с параметром autoplay=True, чтобы аудио начинало воспроизводиться автоматически. История чата обновляется входным сообщением и сгенерированным ответом, после чего возвращаются пустая строка и обновленная история чата.\n",
    "\n",
    "Класс str — это встроенный класс в Python, который представляет последовательность символов. Он предоставляет различные методы для работы со строками, такие как capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill и другие. Эти методы позволяют выполнять операции, такие как поиск, замена, форматирование и манипуляции со строками.\n",
    "\n",
    "Класс Audio — это пользовательский класс, который представляет аудиообъект. Он используется для создания аудиоплеера в среде Jupyter Notebook. Класс принимает различные параметры, такие как data, filename, url, embed, rate, autoplay и normalize. Параметр data может быть numpy-массивом, списком семплов, строкой, представляющей имя файла или URL, или необработанными данными PCM. Параметр filename используется для указания локального файла, из которого загружаются аудиоданные, а параметр url — для указания URL, с которого загружаются аудиоданные. Параметр embed определяет, должны ли аудиоданные быть встроены с использованием data URI или ссылаться на исходный источник. Параметр rate задает частоту дискретизации аудиоданных. Параметр autoplay определяет, должно ли аудио начинать воспроизводиться автоматически. Параметр normalize указывает, должны ли аудиоданные быть нормализованы (масштабированы) до максимально возможного диапазона. Класс Audio также предоставляет методы, такие как reload для перезагрузки аудиоданных из файла или URL, и атрибуты, такие как src_attr, autoplay_attr и element_id_attr для получения соответствующих атрибутов аудиоэлемента в HTML.\n",
    "\n",
    "В целом, эти функции и классы используются для преобразования аудио в текст, генерации аудиоответов от чат-бота, а также отображения и воспроизведения аудио в среде Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**Отказ от ответственности**:  \nЭтот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Хотя мы стремимся к точности, пожалуйста, учитывайте, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникшие в результате использования данного перевода.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T19:51:06+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "ru"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}