<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "743d7e9cb9c4e8ea642d77bee657a7fa",
  "translation_date": "2025-07-17T09:51:31+00:00",
  "source_file": "md/03.FineTuning/LetPhi3gotoIndustriy.md",
  "language_code": "ru"
}
-->
# **Пусть Phi-3 станет экспертом в отрасли**

Чтобы внедрить модель Phi-3 в конкретную отрасль, необходимо добавить в неё бизнес-данные этой отрасли. У нас есть два варианта: первый — RAG (Retrieval Augmented Generation), второй — Fine Tuning (тонкая настройка).

## **RAG против Fine-Tuning**

### **Retrieval Augmented Generation**

RAG — это поиск данных + генерация текста. Структурированные и неструктурированные данные предприятия хранятся в векторной базе данных. При поиске релевантного контента находится соответствующее резюме и материалы, которые формируют контекст, а затем используется способность LLM/SLM к дополнению текста для генерации ответа.

### **Fine-tuning**

Fine-tuning — это улучшение конкретной модели. Не нужно начинать с алгоритма модели, но требуется постоянное накопление данных. Если вам нужна более точная терминология и выражения в отраслевых приложениях, тонкая настройка — лучший выбор. Однако при частых изменениях данных fine-tuning может стать сложным.

### **Как выбрать**

1. Если для ответа требуется использование внешних данных, лучше выбрать RAG.

2. Если нужно выдавать стабильные и точные отраслевые знания, fine-tuning будет хорошим вариантом. RAG ориентирован на поиск релевантного контента, но не всегда учитывает специализированные нюансы.

3. Fine-tuning требует качественного набора данных, и если данных мало, эффект будет незначительным. RAG более гибок.

4. Fine-tuning — это «чёрный ящик», метафизика, сложно понять внутренний механизм. RAG же позволяет легче отследить источник данных, что помогает корректировать ошибки и галлюцинации, обеспечивая лучшую прозрачность.

### **Сценарии**

1. Вертикальные отрасли, требующие специфической профессиональной лексики и выражений — ***Fine-tuning*** будет лучшим выбором.

2. Системы вопросов и ответов, где нужно объединять разные знания — ***RAG*** будет оптимальным вариантом.

3. Для автоматизации бизнес-процессов лучше всего подходит комбинация ***RAG + Fine-tuning***.

## **Как использовать RAG**

![rag](../../../../translated_images/rag.2014adc59e6f6007.ru.png)

Векторная база данных — это коллекция данных, хранящихся в математической форме. Векторные базы упрощают запоминание предыдущих данных для моделей машинного обучения, что позволяет использовать их для поиска, рекомендаций и генерации текста. Данные идентифицируются на основе метрик сходства, а не точного совпадения, что помогает моделям лучше понимать контекст.

Векторная база — ключ к реализации RAG. Мы можем преобразовывать данные в векторное представление с помощью моделей, таких как text-embedding-3, jina-ai-embedding и других.

Подробнее о создании RAG-приложений можно узнать по ссылке [https://github.com/microsoft/Phi-3CookBook](https://github.com/microsoft/Phi-3CookBook?WT.mc_id=aiml-138114-kinfeylo)

## **Как использовать Fine-tuning**

Чаще всего для Fine-tuning применяются алгоритмы Lora и QLora. Как выбрать?
- [Подробнее в этом примере ноутбука](../../../../code/04.Finetuning/Phi_3_Inference_Finetuning.ipynb)
- [Пример Python-скрипта для FineTuning](../../../../code/04.Finetuning/FineTrainingScript.py)

### **Lora и QLora**

![lora](../../../../translated_images/qlora.e6446c988ee04ca0.ru.png)

LoRA (Low-Rank Adaptation) и QLoRA (Quantized Low-Rank Adaptation) — это методы тонкой настройки больших языковых моделей (LLM) с использованием Parameter Efficient Fine Tuning (PEFT). PEFT позволяет обучать модели эффективнее, чем традиционные методы.

LoRA — самостоятельный метод тонкой настройки, который снижает потребление памяти за счёт низкоранговой аппроксимации матрицы обновления весов. Обеспечивает быстрое обучение и сохраняет производительность, близкую к традиционному fine-tuning.

QLoRA — расширенная версия LoRA с применением квантования для ещё большего снижения использования памяти. QLoRA квантует параметры весов предобученной модели до 4-битной точности, что экономит память по сравнению с LoRA. Однако обучение QLoRA примерно на 30% медленнее из-за дополнительных шагов квантования и деквантования.

QLoRA использует LoRA для корректировки ошибок, возникающих при квантовании. QLoRA позволяет тонко настраивать огромные модели с миллиардами параметров на относительно небольших и доступных GPU. Например, QLoRA может настроить модель с 70 миллиардами параметров, которая обычно требует 36 GPU, используя всего 2.

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия по обеспечению точности, просим учитывать, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется обращаться к профессиональному переводу, выполненному человеком. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования данного перевода.