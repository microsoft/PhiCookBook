<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-03-27T06:05:44+00:00",
  "source_file": "md\\01.Introduction\\01\\01.Understandingtech.md",
  "language_code": "ru"
}
-->
# Основные упомянутые технологии

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) — низкоуровневый API для аппаратно-ускоренного машинного обучения, построенный на базе DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) — платформа параллельных вычислений и модель программного интерфейса приложений (API), разработанная Nvidia, позволяющая использовать графические процессоры (GPU) для общих вычислительных задач.
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) — открытый формат для представления моделей машинного обучения, обеспечивающий совместимость между различными ML-фреймворками.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) — формат, используемый для представления и обновления моделей машинного обучения, особенно полезный для небольших языковых моделей, которые могут эффективно работать на процессорах с 4-8-битной квантизацией.

## DirectML

DirectML — это низкоуровневый API, который обеспечивает аппаратно-ускоренное машинное обучение. Он построен на базе DirectX 12 для использования ускорения на GPU и не зависит от конкретного производителя, то есть не требует изменений кода для работы с разными GPU. DirectML в основном используется для обучения моделей и выполнения выводов на GPU.

Что касается аппаратной поддержки, DirectML разработан для работы с широким спектром GPU, включая интегрированные и дискретные GPU AMD, интегрированные GPU Intel и дискретные GPU NVIDIA. Он является частью платформы Windows AI и поддерживается в Windows 10 и 11, что позволяет обучать модели и выполнять выводы на любом устройстве с Windows.

Существуют обновления и возможности, связанные с DirectML, такие как поддержка до 150 операторов ONNX, а также использование в ONNX Runtime и WinML. Технология поддерживается основными производителями аппаратного обеспечения (IHVs), каждый из которых реализует различные метакоманды.

## CUDA

CUDA (Compute Unified Device Architecture) — это платформа параллельных вычислений и модель программного интерфейса приложений (API), созданная Nvidia. Она позволяет разработчикам использовать графические процессоры (GPU), поддерживающие CUDA, для общих вычислительных задач — подход, называемый GPGPU (General-Purpose computing on Graphics Processing Units). CUDA является ключевой технологией для ускорения GPU от Nvidia и широко используется в различных областях, включая машинное обучение, научные вычисления и обработку видео.

Аппаратная поддержка CUDA ограничивается GPU от Nvidia, так как это проприетарная технология, разработанная компанией. Каждая архитектура GPU поддерживает определённые версии инструментария CUDA, который предоставляет необходимые библиотеки и инструменты для разработки и выполнения приложений CUDA.

## ONNX

ONNX (Open Neural Network Exchange) — это открытый формат, разработанный для представления моделей машинного обучения. Он предоставляет описание расширяемой модели вычислительного графа, а также определения встроенных операторов и стандартных типов данных. ONNX позволяет разработчикам переносить модели между различными ML-фреймворками, обеспечивая совместимость и упрощая создание и развертывание AI-приложений.

Phi3 mini может работать с ONNX Runtime на процессорах и GPU на различных устройствах, включая серверные платформы, настольные ПК с Windows, Linux и Mac, а также мобильные процессоры. Оптимизации, которые мы добавили:

- ONNX-модели для int4 DML: Квантованы до int4 с помощью AWQ
- ONNX-модель для fp16 CUDA
- ONNX-модель для int4 CUDA: Квантованы до int4 с помощью RTN
- ONNX-модель для int4 CPU и мобильных устройств: Квантованы до int4 с помощью RTN

## Llama.cpp

Llama.cpp — это библиотека с открытым исходным кодом, написанная на C++. Она выполняет вывод на различных крупных языковых моделях (LLMs), включая Llama. Разработанная вместе с библиотекой ggml (универсальной тензорной библиотекой), llama.cpp стремится обеспечить более быстрый вывод и меньшее использование памяти по сравнению с оригинальной реализацией на Python. Она поддерживает аппаратную оптимизацию, квантизацию и предлагает простой API и примеры. Если вас интересует эффективный вывод LLM, llama.cpp заслуживает внимания, так как Phi3 может работать с Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) — это формат, используемый для представления и обновления моделей машинного обучения. Он особенно полезен для небольших языковых моделей (SLMs), которые могут эффективно работать на процессорах с 4-8-битной квантизацией. GGUF полезен для быстрого прототипирования и запуска моделей на устройствах с ограниченными ресурсами или в пакетных заданиях, таких как CI/CD-пайплайны.

**Отказ от ответственности**:  
Этот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия по обеспечению точности, автоматический перевод может содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникшие в результате использования данного перевода.