<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-07T14:58:51+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "ru"
}
-->
# Основные упомянутые технологии включают

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) — низкоуровневый API для аппаратно-ускоренного машинного обучения, построенный на базе DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) — платформа параллельных вычислений и модель API, разработанная Nvidia, которая позволяет использовать графические процессоры (GPU) для универсальной обработки.  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) — открытый формат для представления моделей машинного обучения, обеспечивающий совместимость между разными ML-фреймворками.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) — формат для представления и обновления моделей машинного обучения, особенно полезный для небольших языковых моделей, которые эффективно работают на CPU с 4-8-битной квантизацией.

## DirectML

DirectML — это низкоуровневый API, который позволяет использовать аппаратное ускорение для машинного обучения. Он построен на базе DirectX 12, чтобы задействовать GPU, и не зависит от производителя оборудования, то есть не требует изменений в коде для работы с разными GPU. В основном применяется для обучения моделей и выполнения выводов на GPU.

Что касается поддержки оборудования, DirectML разработан для работы с широким спектром GPU, включая интегрированные и дискретные GPU AMD, интегрированные GPU Intel и дискретные GPU NVIDIA. Он является частью платформы Windows AI и поддерживается в Windows 10 и 11, что позволяет обучать модели и выполнять выводы на любом устройстве под Windows.

Были обновления и новые возможности, связанные с DirectML, например, поддержка до 150 операторов ONNX и использование в ONNX runtime и WinML. За проектом стоят крупные производители аппаратного обеспечения (IHV), каждый из которых реализует различные метакоманды.

## CUDA

CUDA (Compute Unified Device Architecture) — это платформа параллельных вычислений и модель API, созданная Nvidia. Она позволяет разработчикам использовать GPU с поддержкой CUDA для универсальной обработки, подход, известный как GPGPU (General-Purpose computing on Graphics Processing Units). CUDA является ключевым инструментом для ускорения вычислений на GPU Nvidia и широко применяется в таких областях, как машинное обучение, научные вычисления и обработка видео.

Поддержка оборудования для CUDA ограничена GPU Nvidia, так как это проприетарная технология компании. Каждая архитектура GPU поддерживает определённые версии CUDA toolkit, который предоставляет необходимые библиотеки и инструменты для разработки и запуска приложений на CUDA.

## ONNX

ONNX (Open Neural Network Exchange) — открытый формат для представления моделей машинного обучения. Он описывает расширяемую модель вычислительного графа, а также определяет встроенные операторы и стандартные типы данных. ONNX позволяет разработчикам переносить модели между разными ML-фреймворками, обеспечивая совместимость и упрощая создание и развертывание AI-приложений.

Phi3 mini может работать с ONNX Runtime на CPU и GPU на различных устройствах, включая серверные платформы, Windows, Linux и Mac десктопы, а также мобильные CPU.  
Оптимизированные конфигурации, которые мы добавили:

- ONNX модели для int4 DML: квантизация в int4 с помощью AWQ  
- ONNX модель для fp16 CUDA  
- ONNX модель для int4 CUDA: квантизация в int4 с помощью RTN  
- ONNX модель для int4 CPU и Mobile: квантизация в int4 с помощью RTN

## Llama.cpp

Llama.cpp — это библиотека с открытым исходным кодом, написанная на C++. Она выполняет вывод для различных больших языковых моделей (LLM), включая Llama. Разрабатываемая вместе с библиотекой ggml (универсальная тензорная библиотека), llama.cpp стремится обеспечить более быструю работу и меньший расход памяти по сравнению с оригинальной реализацией на Python. Поддерживает аппаратную оптимизацию, квантизацию и предлагает простой API с примерами. Если вас интересует эффективный вывод LLM, стоит обратить внимание на llama.cpp, так как Phi3 может запускать Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) — формат для представления и обновления моделей машинного обучения. Особенно полезен для небольших языковых моделей (SLM), которые эффективно работают на CPU с 4-8-битной квантизацией. GGUF удобен для быстрого прототипирования и запуска моделей на периферийных устройствах или в пакетных задачах, таких как CI/CD пайплайны.

**Отказ от ответственности**:  
Этот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия обеспечить точность, просим учитывать, что автоматический перевод может содержать ошибки или неточности. Оригинальный документ на исходном языке следует считать авторитетным источником. Для критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неверные толкования, возникшие в результате использования данного перевода.