<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:40:24+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "ru"
}
-->
# Основные упомянутые технологии

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) — низкоуровневый API для аппаратного ускорения машинного обучения, построенный на базе DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) — платформа параллельных вычислений и модель API, разработанная Nvidia, позволяющая выполнять универсальную обработку на графических процессорах (GPU).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) — открытый формат для представления моделей машинного обучения, обеспечивающий совместимость между разными ML-фреймворками.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) — формат для представления и обновления моделей машинного обучения, особенно полезный для небольших языковых моделей, которые эффективно работают на CPU с 4-8-битной квантизацией.

## DirectML

DirectML — это низкоуровневый API, который обеспечивает аппаратное ускорение машинного обучения. Он построен на базе DirectX 12 для использования ускорения на GPU и не зависит от производителя, то есть не требует изменений в коде для работы с GPU разных вендоров. В основном применяется для обучения моделей и выполнения инференса на GPU.

Что касается поддержки оборудования, DirectML разработан для работы с широким спектром GPU, включая интегрированные и дискретные GPU AMD, интегрированные GPU Intel и дискретные GPU NVIDIA. Он является частью Windows AI Platform и поддерживается в Windows 10 и 11, что позволяет обучать модели и выполнять инференс на любом устройстве с Windows.

Были выпущены обновления и расширены возможности DirectML, например, поддержка до 150 операторов ONNX и использование как в ONNX runtime, так и в WinML. За проектом стоят крупные производители аппаратного обеспечения (IHV), каждый из которых реализует различные метакоманды.

## CUDA

CUDA (Compute Unified Device Architecture) — это платформа параллельных вычислений и модель API, созданная Nvidia. Она позволяет разработчикам использовать GPU с поддержкой CUDA для универсальной обработки — подход, известный как GPGPU (General-Purpose computing on Graphics Processing Units). CUDA является ключевым элементом ускорения на GPU Nvidia и широко применяется в таких областях, как машинное обучение, научные вычисления и обработка видео.

Поддержка оборудования для CUDA ограничена GPU Nvidia, так как это проприетарная технология компании. Каждая архитектура поддерживает определённые версии CUDA toolkit, который предоставляет необходимые библиотеки и инструменты для разработки и запуска CUDA-приложений.

## ONNX

ONNX (Open Neural Network Exchange) — открытый формат для представления моделей машинного обучения. Он описывает расширяемую модель вычислительного графа, а также определяет встроенные операторы и стандартные типы данных. ONNX позволяет разработчикам переносить модели между разными ML-фреймворками, обеспечивая совместимость и упрощая создание и развертывание AI-приложений.

Phi3 mini может работать с ONNX Runtime на CPU и GPU на различных устройствах, включая серверные платформы, Windows, Linux и Mac на настольных компьютерах, а также мобильные CPU.  
Оптимизированные конфигурации, которые мы добавили:

- ONNX модели для int4 DML: квантизированы до int4 с помощью AWQ  
- ONNX модель для fp16 CUDA  
- ONNX модель для int4 CUDA: квантизирована до int4 с помощью RTN  
- ONNX модель для int4 CPU и Mobile: квантизирована до int4 с помощью RTN

## Llama.cpp

Llama.cpp — это библиотека с открытым исходным кодом, написанная на C++. Она выполняет инференс на различных больших языковых моделях (LLM), включая Llama. Разработанная вместе с библиотекой ggml (универсальная библиотека для работы с тензорами), llama.cpp нацелена на более быструю работу и меньший расход памяти по сравнению с оригинальной реализацией на Python. Она поддерживает аппаратную оптимизацию, квантизацию и предоставляет простой API с примерами. Если вас интересует эффективный инференс LLM, llama.cpp стоит изучить, так как Phi3 может запускать Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) — формат для представления и обновления моделей машинного обучения. Особенно полезен для небольших языковых моделей (SLM), которые эффективно работают на CPU с 4-8-битной квантизацией. GGUF удобен для быстрого прототипирования и запуска моделей на периферийных устройствах или в пакетных задачах, таких как CI/CD пайплайны.

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия по обеспечению точности, просим учитывать, что автоматический перевод может содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется обращаться к профессиональному переводу, выполненному человеком. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования данного перевода.