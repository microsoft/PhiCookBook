<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-07T15:00:10+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "ru"
}
-->
# Начало работы с Phi-3 локально

Это руководство поможет вам настроить локальную среду для запуска модели Phi-3 с помощью Ollama. Вы можете запустить модель несколькими способами, включая использование GitHub Codespaces, VS Code Dev Containers или вашей локальной среды.

## Настройка окружения

### GitHub Codespaces

Вы можете запустить этот шаблон виртуально, используя GitHub Codespaces. Кнопка откроет веб-версию VS Code прямо в вашем браузере:

1. Откройте шаблон (это может занять несколько минут):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Откройте терминал

### VS Code Dev Containers

⚠️ Этот вариант будет работать только если Docker Desktop выделено не менее 16 ГБ ОЗУ. Если у вас меньше 16 ГБ, попробуйте вариант с [GitHub Codespaces](../../../../../md/01.Introduction/01) или [настройку локально](../../../../../md/01.Introduction/01).

Связанный вариант — VS Code Dev Containers, который откроет проект в вашем локальном VS Code с помощью [расширения Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Запустите Docker Desktop (установите, если еще не установлен)
2. Откройте проект:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. В открывшемся окне VS Code, как только появятся файлы проекта (это может занять несколько минут), откройте терминал.
4. Продолжите с [шагами развертывания](../../../../../md/01.Introduction/01)

### Локальная среда

1. Убедитесь, что установлены следующие инструменты:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Тестирование модели

1. Попросите Ollama скачать и запустить модель phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Загрузка модели займет несколько минут.

2. Как только в выводе появится "success", вы сможете отправлять сообщения этой модели через командную строку.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Через несколько секунд вы увидите поток ответа от модели.

4. Чтобы узнать о различных техниках работы с языковыми моделями, откройте Python-ноутбук [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) и выполните каждую ячейку. Если вы использовали модель, отличную от 'phi3:mini', измените `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` в начале файла по необходимости, а также можете изменить системное сообщение или добавить примеры few-shot, если хотите.

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия по обеспечению точности, пожалуйста, учитывайте, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется использовать профессиональный перевод, выполненный человеком. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования данного перевода.