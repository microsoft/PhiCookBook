<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "839ccc4b3886ef10cfd4e64977f5792d",
  "translation_date": "2026-01-04T07:04:21+00:00",
  "source_file": "md/01.Introduction/01/01.AISafety.md",
  "language_code": "ru"
}
-->
# Безопасность ИИ для моделей Phi
Семейство моделей Phi было разработано в соответствии с [Стандарт ответственного ИИ Microsoft](https://www.microsoft.com/ai/principles-and-approach#responsible-ai-standard), который представляет собой набор требований для всей компании, основанный на следующих шести принципах: подотчетность, прозрачность, справедливость, надежность и безопасность, конфиденциальность и защита, а также инклюзивность, которые формируют [Принципы ответственного ИИ Microsoft](https://www.microsoft.com/ai/responsible-ai). 

Как и предыдущие модели Phi, был применен многоаспектный подход к оценке безопасности и посттренировочным мерам безопасности, с дополнительными мерами с учетом многоязычных возможностей этого выпуска. Наш подход к обучению безопасности и оценкам, включая тестирование на нескольких языках и по категориям риска, изложен в [Статье по пост-тренировочной безопасности Phi](https://arxiv.org/abs/2407.13833). Хотя модели Phi выигрывают от этого подхода, разработчики должны применять передовые практики ответственного ИИ, включая картирование, измерение и смягчение рисков, связанных с их конкретным вариантом использования и культурным и языковым контекстом.

## Лучшие практики

Как и другие модели, семейство моделей Phi потенциально может вести себя несправедливо, ненадежно или оскорбительно.

Некоторые ограничивающие поведения SLM и LLM, о которых вам следует знать, включают:

- **Качество обслуживания:** Модели Phi обучены в первую очередь на текстах на английском языке. Языки, отличные от английского, будут демонстрировать худшую производительность. Варианты английского языка с меньшим представлением в обучающих данных могут показывать худшие результаты по сравнению со стандартным американским английским.
- **Представление вреда и закрепление стереотипов:** Эти модели могут чрезмерно или недостаточно представлять группы людей, стирать представительство некоторых групп или усиливать унизительные или негативные стереотипы. Несмотря на посттренировочную безопасность, эти ограничения все еще могут присутствовать из-за различного уровня представительства разных групп или распространенности примеров негативных стереотипов в обучающих данных, которые отражают реальные модели и общественные предубеждения.
- **Неподходящий или оскорбительный контент:** Эти модели могут генерировать и другие типы неподходящего или оскорбительного контента, что может сделать их неподходящими для развертывания в чувствительных контекстах без дополнительных мер смягчения, специфичных для варианта использования.
Информационная надежность: языковые модели могут генерировать бессмысленный контент или выдумывать материалы, которые могут звучать разумно, но являются неточными или устаревшими.
- **Ограниченный охват для кода:** Большая часть обучающих данных Phi-3 основана на Python и использует общие пакеты, такие как "typing, math, random, collections, datetime, itertools". Если модель генерирует скрипты на Python, которые используют другие пакеты или скрипты на других языках, мы настоятельно рекомендуем пользователям вручную проверять все обращения к API.

Разработчики должны применять передовые практики ответственного ИИ и несут ответственность за обеспечение соответствия конкретного варианта использования применимым законам и нормативам (например, конфиденциальность, торговля и т. д.). 

## Соображения по ответственному ИИ

Как и другие языковые модели, модели серии Phi потенциально могут вести себя несправедливо, ненадежно или оскорбительно. Некоторые ограничивающие поведения, о которых следует знать, включают:

**Качество обслуживания:** Модели Phi обучены в первую очередь на текстах на английском языке. Языки, отличные от английского, будут демонстрировать худшую производительность. Варианты английского языка с меньшим представлением в обучающих данных могут показывать худшие результаты по сравнению со стандартным американским английским.

**Представление вреда и закрепление стереотипов:** Эти модели могут чрезмерно или недостаточно представлять группы людей, стирать представительство некоторых групп или усиливать унизительные или негативные стереотипы. Несмотря на посттренировочную безопасность, эти ограничения все еще могут присутствовать из-за различного уровня представительства разных групп или распространенности примеров негативных стереотипов в обучающих данных, которые отражают реальные модели и общественные предубеждения.

**Неподходящий или оскорбительный контент:** Эти модели могут генерировать и другие типы неподходящего или оскорбительного контента, что может сделать их неподходящими для развертывания в чувствительных контекстах без дополнительных мер смягчения, специфичных для варианта использования.
Информационная надежность: языковые модели могут генерировать бессмысленный контент или выдумывать материалы, которые могут звучать разумно, но являются неточными или устаревшими.

**Ограниченный охват для кода:** Большая часть обучающих данных Phi-3 основана на Python и использует общие пакеты, такие как "typing, math, random, collections, datetime, itertools". Если модель генерирует скрипты на Python, которые используют другие пакеты или скрипты на других языках, мы настоятельно рекомендуем пользователям вручную проверять все обращения к API.

Разработчики должны применять передовые практики ответственного ИИ и несут ответственность за обеспечение соответствия конкретного варианта использования применимым законам и нормативам (например, конфиденциальность, торговля и т. д.). Важные области для рассмотрения включают:

**Распределение:** Модели могут быть непригодны для сценариев, которые могут иметь существенное влияние на юридический статус или распределение ресурсов или жизненных возможностей (например: жильё, трудоустройство, кредит и т. д.) без дальнейших оценок и дополнительных техник по устранению смещений.

**Сценарии высокого риска:** Разработчики должны оценить пригодность использования моделей в сценариях высокого риска, где несправедливые, ненадежные или оскорбительные ответы могут иметь чрезвычайно высокую стоимость или привести к вреду. Это включает предоставление советов в чувствительных или экспертных областях, где точность и надежность критически важны (например: юридические или медицинские консультации). На уровне приложения должны быть реализованы дополнительные меры безопасности в соответствии с контекстом развертывания.

**Дезинформация:** Модели могут генерировать неточную информацию. Разработчики должны следовать лучшим практикам прозрачности и информировать конечных пользователей о том, что они взаимодействуют с системой ИИ. На уровне приложения разработчики могут строить механизмы обратной связи и конвейеры для обоснования ответов в контекстно-специфичной информации для варианта использования, техника, известная как Retrieval Augmented Generation (RAG).

**Генерация вредоносного контента:** Разработчикам следует оценивать выходные данные с точки зрения их контекста и использовать доступные классификаторы безопасности или кастомные решения, подходящие для их варианта использования.

**Злоупотребление:** Возможны другие формы злоупотребления, такие как мошенничество, спам или создание вредоносного ПО, и разработчики должны обеспечивать, чтобы их приложения не нарушали применимые законы и нормативные акты.

### Дообучение и безопасность контента ИИ

После дообучения модели мы настоятельно рекомендуем использовать меры [Службы Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) для мониторинга контента, генерируемого моделями, выявления и блокировки потенциальных рисков, угроз и проблем качества.

![Безопасность Phi3](../../../../../translated_images/01.phi3aisafety.c0d7fc42f5a5c405.ru.png)

[Служба Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) поддерживает как текстовый, так и визуальный контент. Ее можно развернуть в облаке, в отключенных контейнерах и на периферийных/встроенных устройствах.

## Обзор службы Azure AI Content Safety

Служба Azure AI Content Safety не является универсальным решением; ее можно настроить в соответствии со специфическими политиками бизнеса. Кроме того, ее многоязычные модели позволяют ей понимать несколько языков одновременно.

![Безопасность контента ИИ](../../../../../translated_images/01.AIcontentsafety.a288819b8ce8da1a.ru.png)

- **Служба Azure AI Content Safety**
- **Microsoft для разработчиков**
- **5 видео**

Служба Azure AI Content Safety обнаруживает вредоносный контент, созданный пользователями и ИИ, в приложениях и службах. Она включает API для текста и изображений, позволяющие обнаруживать вредоносные или неподходящие материалы.

[Плейлист по безопасности контента ИИ](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
Отказ от ответственности:
Этот документ был переведен с помощью сервиса автоматического перевода на основе ИИ [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия по обеспечению точности, учтите, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для критически важной информации рекомендуется профессиональный перевод, выполненный человеком. Мы не несем ответственности за любые недоразумения или неверные толкования, возникшие в результате использования этого перевода.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->