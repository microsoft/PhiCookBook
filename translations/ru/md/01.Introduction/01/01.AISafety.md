<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c8273672cc57df2be675407a1383aaf0",
  "translation_date": "2025-07-16T17:42:05+00:00",
  "source_file": "md/01.Introduction/01/01.AISafety.md",
  "language_code": "ru"
}
-->
# Безопасность ИИ для моделей Phi  
Семейство моделей Phi разработано в соответствии с [Microsoft Responsible AI Standard](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFl) — корпоративным набором требований, основанных на шести принципах: подотчетность, прозрачность, справедливость, надежность и безопасность, конфиденциальность и защита, а также инклюзивность, которые формируют [принципы ответственного ИИ Microsoft](https://www.microsoft.com/ai/responsible-ai).

Как и в предыдущих моделях Phi, применялся многоаспектный подход к оценке безопасности и посттренировочной доработке с дополнительными мерами, учитывающими многоязычные возможности этой версии. Наш подход к обучению безопасности и оценкам, включая тестирование на нескольких языках и в различных категориях рисков, описан в [статье Phi Safety Post-Training Paper](https://arxiv.org/abs/2407.13833). Несмотря на преимущества этого подхода, разработчикам рекомендуется применять лучшие практики ответственного ИИ, включая картирование, измерение и снижение рисков, связанных с конкретным случаем использования и культурно-языковым контекстом.

## Лучшие практики

Как и другие модели, семейство Phi может иногда вести себя несправедливо, ненадежно или оскорбительно.

Некоторые ограничения моделей SLM и LLM, о которых стоит знать:

- **Качество обслуживания:** Модели Phi в основном обучены на английском языке. Для других языков производительность будет ниже. Разновидности английского с меньшим представлением в обучающих данных могут работать хуже, чем стандартный американский английский.  
- **Представление вреда и закрепление стереотипов:** Модели могут пере- или недопредставлять определённые группы людей, стирать представление некоторых групп или усиливать уничижительные или негативные стереотипы. Несмотря на посттренировочные меры безопасности, эти ограничения могут сохраняться из-за разного уровня представления групп или наличия примеров негативных стереотипов в обучающих данных, отражающих реальные социальные паттерны и предубеждения.  
- **Неподходящий или оскорбительный контент:** Модели могут генерировать и другие виды неподходящего или оскорбительного контента, что делает их использование в чувствительных контекстах без дополнительных мер предосторожности нежелательным.  
- **Надежность информации:** Языковые модели могут создавать бессмысленный контент или выдумывать информацию, которая звучит правдоподобно, но является неточной или устаревшей.  
- **Ограничения в области кода:** Большая часть обучающих данных Phi-3 основана на Python и использует распространённые пакеты, такие как "typing, math, random, collections, datetime, itertools". Если модель генерирует скрипты на Python с использованием других пакетов или скрипты на других языках, настоятельно рекомендуется вручную проверять все вызовы API.

Разработчики должны применять лучшие практики ответственного ИИ и нести ответственность за соответствие конкретного случая использования действующим законам и нормативам (например, в области конфиденциальности, торговли и т. д.).

## Вопросы ответственного ИИ

Как и другие языковые модели, модели серии Phi могут вести себя несправедливо, ненадежно или оскорбительно. Основные ограничения, о которых стоит помнить:

**Качество обслуживания:** Модели Phi преимущественно обучены на английском языке. Для других языков производительность будет ниже. Разновидности английского с меньшим представлением в обучающих данных могут работать хуже, чем стандартный американский английский.

**Представление вреда и закрепление стереотипов:** Модели могут пере- или недопредставлять определённые группы людей, стирать представление некоторых групп или усиливать уничижительные или негативные стереотипы. Несмотря на посттренировочные меры безопасности, эти ограничения могут сохраняться из-за разного уровня представления групп или наличия примеров негативных стереотипов в обучающих данных, отражающих реальные социальные паттерны и предубеждения.

**Неподходящий или оскорбительный контент:** Модели могут генерировать и другие виды неподходящего или оскорбительного контента, что делает их использование в чувствительных контекстах без дополнительных мер предосторожности нежелательным.  
Надежность информации: Языковые модели могут создавать бессмысленный контент или выдумывать информацию, которая звучит правдоподобно, но является неточной или устаревшей.

**Ограничения в области кода:** Большая часть обучающих данных Phi-3 основана на Python и использует распространённые пакеты, такие как "typing, math, random, collections, datetime, itertools". Если модель генерирует скрипты на Python с использованием других пакетов или скрипты на других языках, настоятельно рекомендуется вручную проверять все вызовы API.

Разработчики должны применять лучшие практики ответственного ИИ и нести ответственность за соответствие конкретного случая использования действующим законам и нормативам (например, в области конфиденциальности, торговли и т. д.). Важные аспекты для рассмотрения включают:

**Распределение:** Модели могут быть неподходящими для сценариев, которые могут существенно повлиять на юридический статус или распределение ресурсов и жизненных возможностей (например, жильё, трудоустройство, кредитование и т. д.) без дополнительных оценок и методов устранения предвзятости.

**Сценарии с высоким риском:** Разработчикам следует оценить пригодность моделей для использования в сценариях с высоким риском, где несправедливые, ненадежные или оскорбительные результаты могут привести к серьёзным последствиям или вреду. Это включает предоставление советов в чувствительных или экспертных областях, где важны точность и надежность (например, юридические или медицинские консультации). Дополнительные меры безопасности должны внедряться на уровне приложений с учётом контекста развертывания.

**Дезинформация:** Модели могут генерировать неточную информацию. Разработчикам рекомендуется соблюдать лучшие практики прозрачности и информировать конечных пользователей о том, что они взаимодействуют с ИИ-системой. На уровне приложений можно создавать механизмы обратной связи и каналы, которые позволяют опираться на контекстно-специфичную информацию, известную как Retrieval Augmented Generation (RAG).

**Генерация вредоносного контента:** Разработчикам следует оценивать результаты с учётом контекста и использовать доступные классификаторы безопасности или собственные решения, подходящие для конкретного случая использования.

**Злоупотребления:** Возможны и другие формы злоупотреблений, такие как мошенничество, спам или создание вредоносного ПО, поэтому разработчики должны гарантировать, что их приложения не нарушают применимые законы и нормативы.

### Тонкая настройка и безопасность контента ИИ

После тонкой настройки модели настоятельно рекомендуется использовать меры [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) для мониторинга генерируемого контента, выявления и блокировки потенциальных рисков, угроз и проблем с качеством.

![Phi3AISafety](../../../../../translated_images/01.phi3aisafety.c0d7fc42f5a5c40507c5e8be556615b8377a63b8764865d057d4faac3757a478.ru.png)

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview) поддерживает как текстовый, так и графический контент. Его можно развернуть в облаке, в изолированных контейнерах и на периферийных/встроенных устройствах.

## Обзор Azure AI Content Safety

Azure AI Content Safety — это не универсальное решение; его можно настраивать в соответствии с политиками конкретного бизнеса. Кроме того, его многоязычные модели позволяют одновременно работать с несколькими языками.

![AIContentSafety](../../../../../translated_images/01.AIcontentsafety.a288819b8ce8da1a56cf708aff010a541799d002ae7ae84bb819b19ab8950591.ru.png)

- **Azure AI Content Safety**  
- **Microsoft Developer**  
- **5 видео**

Сервис Azure AI Content Safety обнаруживает вредоносный контент, созданный пользователями и ИИ, в приложениях и сервисах. Включает API для текста и изображений, позволяющие выявлять вредоносные или неподходящие материалы.

[AI Content Safety Playlist](https://www.youtube.com/playlist?list=PLlrxD0HtieHjaQ9bJjyp1T7FeCbmVcPkQ)

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия по обеспечению точности, просим учитывать, что автоматический перевод может содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется обращаться к профессиональному человеческому переводу. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования данного перевода.