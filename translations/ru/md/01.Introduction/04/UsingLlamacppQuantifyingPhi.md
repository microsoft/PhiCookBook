<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e5bb9190ef9d149d28037a768c6b62b2",
  "translation_date": "2025-03-27T08:38:15+00:00",
  "source_file": "md\\01.Introduction\\04\\UsingLlamacppQuantifyingPhi.md",
  "language_code": "ru"
}
-->
# **Квантование семейства Phi с использованием llama.cpp**

## **Что такое llama.cpp**

llama.cpp — это библиотека с открытым исходным кодом, написанная преимущественно на C++, предназначенная для выполнения вывода на различных моделях больших языков (LLMs), таких как Llama. Основная цель этой библиотеки — обеспечить передовую производительность вывода LLM на широком спектре оборудования с минимальными настройками. Также доступны Python-биндинги для этой библиотеки, предоставляющие высокоуровневый API для завершения текста и веб-сервер, совместимый с OpenAI.

Главная задача llama.cpp — обеспечить минимальную настройку и передовую производительность вывода LLM на разнообразном оборудовании, как локально, так и в облаке.

- Реализация на чистом C/C++ без зависимостей
- Apple Silicon — приоритетная платформа, оптимизированная с использованием ARM NEON, Accelerate и Metal Frameworks
- Поддержка AVX, AVX2 и AVX512 для архитектур x86
- Квантование с использованием целых чисел 1.5-бит, 2-бит, 3-бит, 4-бит, 5-бит, 6-бит и 8-бит для ускорения вывода и уменьшения использования памяти
- Пользовательские CUDA-ядра для работы с LLM на GPU NVIDIA (поддержка GPU AMD через HIP)
- Поддержка бэкэндов Vulkan и SYCL
- Гибридный вывод CPU+GPU для частичного ускорения моделей, превышающих общий объем VRAM

## **Квантование Phi-3.5 с использованием llama.cpp**

Модель Phi-3.5-Instruct может быть квантована с использованием llama.cpp, однако Phi-3.5-Vision и Phi-3.5-MoE пока не поддерживаются. Формат, преобразованный llama.cpp, — это gguf, который также является наиболее широко используемым форматом квантования.

На Hugging Face доступно большое количество моделей в квантованном формате GGUF. AI Foundry, Ollama и LlamaEdge используют llama.cpp, поэтому модели GGUF также часто применяются.

### **Что такое GGUF**

GGUF — это бинарный формат, оптимизированный для быстрого загрузки и сохранения моделей, что делает его крайне эффективным для вывода. GGUF разработан для использования с GGML и другими исполнителями. GGUF был создан @ggerganov, который также является разработчиком llama.cpp, популярного фреймворка вывода LLM на C/C++. Модели, изначально разработанные в таких фреймворках, как PyTorch, могут быть преобразованы в формат GGUF для использования с этими движками.

### **ONNX vs GGUF**

ONNX — это традиционный формат машинного обучения/глубокого обучения, который хорошо поддерживается различными AI-фреймворками и имеет удачные сценарии использования на периферийных устройствах. GGUF же основан на llama.cpp и можно сказать, что он появился в эпоху GenAI. Оба формата имеют схожие области применения. Если вам нужна лучшая производительность на встроенном оборудовании и прикладных уровнях, ONNX может быть вашим выбором. Если вы используете производные фреймворка и технологии llama.cpp, то GGUF может быть предпочтительнее.

### **Квантование Phi-3.5-Instruct с использованием llama.cpp**

**1. Настройка окружения**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. Квантование**

С использованием llama.cpp преобразовать Phi-3.5-Instruct в FP16 GGUF


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Квантование Phi-3.5 в INT4


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. Тестирование**

Установить llama-cpp-python


```bash

pip install llama-cpp-python -U

```

***Примечание*** 

Если вы используете Apple Silicon, установите llama-cpp-python следующим образом


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

Тестирование 


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **Ресурсы**

1. Узнать больше о llama.cpp [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. Узнать больше о GGUF [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**Отказ от ответственности**:  
Этот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Хотя мы стремимся к точности, пожалуйста, учитывайте, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования данного перевода.