<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "462bddc47427d8785f3c9fd817b346fe",
  "translation_date": "2025-05-07T14:51:04+00:00",
  "source_file": "md/01.Introduction/04/UsingLlamacppQuantifyingPhi.md",
  "language_code": "ru"
}
-->
# **Квантование семейства Phi с использованием llama.cpp**

## **Что такое llama.cpp**

llama.cpp — это библиотека с открытым исходным кодом, написанная преимущественно на C++, которая выполняет вывод на различных больших языковых моделях (LLM), таких как Llama. Основная задача — обеспечить передовую производительность вывода LLM на широком спектре аппаратного обеспечения с минимальной настройкой. Кроме того, для этой библиотеки доступны Python-обертки, которые предоставляют высокоуровневый API для автодополнения текста и веб-сервер, совместимый с OpenAI.

Главная цель llama.cpp — сделать вывод LLM максимально простым в настройке и высокопроизводительным на самых разных устройствах — как локально, так и в облаке.

- Чистая реализация на C/C++ без зависимостей
- Apple Silicon — полноценная поддержка с оптимизацией через ARM NEON, Accelerate и Metal
- Поддержка AVX, AVX2 и AVX512 для архитектур x86
- Квантование целых чисел с разрядностью 1.5, 2, 3, 4, 5, 6 и 8 бит для ускорения вывода и снижения использования памяти
- Пользовательские CUDA-ядра для запуска LLM на GPU NVIDIA (поддержка AMD GPU через HIP)
- Поддержка Vulkan и SYCL
- Гибридный вывод CPU+GPU для частичного ускорения моделей, превышающих общий объем VRAM

## **Квантование Phi-3.5 с помощью llama.cpp**

Модель Phi-3.5-Instruct можно квантовать с использованием llama.cpp, однако Phi-3.5-Vision и Phi-3.5-MoE пока не поддерживаются. Формат, в который конвертирует llama.cpp, называется gguf — это также самый распространённый формат квантования.

На Hugging Face доступно большое количество моделей в квантованном формате GGUF. AI Foundry, Ollama и LlamaEdge используют llama.cpp, поэтому модели GGUF часто применяются в этих решениях.

### **Что такое GGUF**

GGUF — это бинарный формат, оптимизированный для быстрой загрузки и сохранения моделей, что делает его очень эффективным для вывода. GGUF предназначен для использования с GGML и другими исполнительными движками. GGUF разработан @ggerganov, который также является автором llama.cpp — популярного фреймворка для вывода LLM на C/C++. Модели, изначально разработанные в таких фреймворках, как PyTorch, можно конвертировать в формат GGUF для использования с этими движками.

### **ONNX vs GGUF**

ONNX — традиционный формат для машинного и глубокого обучения, который хорошо поддерживается в различных AI-фреймворках и широко используется в edge-устройствах. GGUF же основан на llama.cpp и является продуктом эпохи GenAI. Оба формата имеют схожие области применения. Если вам важна лучшая производительность на встроенном оборудовании и в прикладных слоях, возможно, стоит выбрать ONNX. Если вы используете производные фреймворки и технологии llama.cpp, то GGUF может подойти лучше.

### **Квантование Phi-3.5-Instruct с помощью llama.cpp**

**1. Настройка окружения**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. Квантование**

Конвертация Phi-3.5-Instruct в FP16 GGUF с помощью llama.cpp


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Квантование Phi-3.5 в INT4


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. Тестирование**

Установка llama-cpp-python


```bash

pip install llama-cpp-python -U

```

***Примечание*** 

Если вы используете Apple Silicon, установите llama-cpp-python следующим образом


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

Тестирование 


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **Ресурсы**

1. Подробнее о llama.cpp [https://github.com/ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)
2. Подробнее о onnxruntime [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)
3. Подробнее о GGUF [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия по обеспечению точности, имейте в виду, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования данного перевода.