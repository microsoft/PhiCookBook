<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "462bddc47427d8785f3c9fd817b346fe",
  "translation_date": "2025-07-16T22:05:51+00:00",
  "source_file": "md/01.Introduction/04/UsingLlamacppQuantifyingPhi.md",
  "language_code": "ru"
}
-->
# **Квантование семейства Phi с помощью llama.cpp**

## **Что такое llama.cpp**

llama.cpp — это библиотека с открытым исходным кодом, написанная преимущественно на C++, которая выполняет инференс на различных больших языковых моделях (LLM), таких как Llama. Основная цель — обеспечить передовую производительность инференса LLM на широком спектре оборудования с минимальной настройкой. Кроме того, для этой библиотеки доступны Python-обертки, предоставляющие высокоуровневый API для автозаполнения текста и веб-сервер, совместимый с OpenAI.

Главная задача llama.cpp — позволить запускать инференс LLM с минимальными усилиями и максимальной производительностью на разнообразном оборудовании — как локально, так и в облаке.

- Чистая реализация на C/C++ без внешних зависимостей
- Apple silicon поддерживается на высоком уровне — оптимизация через ARM NEON, Accelerate и Metal
- Поддержка AVX, AVX2 и AVX512 для архитектур x86
- Квантование с 1.5, 2, 3, 4, 5, 6 и 8 битами для ускорения инференса и снижения использования памяти
- Собственные CUDA-ядра для запуска LLM на GPU NVIDIA (поддержка AMD GPU через HIP)
- Поддержка бэкендов Vulkan и SYCL
- Гибридный инференс CPU+GPU для частичного ускорения моделей, превышающих общий объем VRAM

## **Квантование Phi-3.5 с помощью llama.cpp**

Модель Phi-3.5-Instruct можно квантовать с помощью llama.cpp, однако Phi-3.5-Vision и Phi-3.5-MoE пока не поддерживаются. Формат, в который конвертирует llama.cpp — gguf, который также является самым распространённым форматом квантования.

На Hugging Face доступно множество моделей в квантованном формате GGUF. AI Foundry, Ollama и LlamaEdge используют llama.cpp, поэтому модели GGUF часто применяются и там.

### **Что такое GGUF**

GGUF — это бинарный формат, оптимизированный для быстрой загрузки и сохранения моделей, что делает его очень эффективным для инференса. GGUF разработан для использования с GGML и другими движками. Формат GGUF был создан @ggerganov, который также является разработчиком llama.cpp — популярного фреймворка для инференса LLM на C/C++. Модели, изначально разработанные в таких фреймворках, как PyTorch, можно конвертировать в формат GGUF для использования с этими движками.

### **ONNX против GGUF**

ONNX — это традиционный формат для машинного и глубокого обучения, который хорошо поддерживается в различных AI-фреймворках и широко используется на edge-устройствах. GGUF же основан на llama.cpp и можно сказать, что он появился в эпоху GenAI. Оба формата имеют схожие области применения. Если вам нужна лучшая производительность на встроенном оборудовании и в прикладных слоях, возможно, стоит выбрать ONNX. Если вы используете производные фреймворки и технологии llama.cpp, тогда GGUF будет предпочтительнее.

### **Квантование Phi-3.5-Instruct с помощью llama.cpp**

**1. Настройка окружения**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. Квантование**

Конвертация Phi-3.5-Instruct в FP16 GGUF с помощью llama.cpp


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Квантование Phi-3.5 в INT4


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. Тестирование**

Установка llama-cpp-python


```bash

pip install llama-cpp-python -U

```

***Примечание*** 

Если вы используете Apple Silicon, установите llama-cpp-python следующим образом


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

Тестирование 


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **Ресурсы**

1. Подробнее о llama.cpp [https://github.com/ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)  
2. Подробнее о onnxruntime [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)  
3. Подробнее о GGUF [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия по обеспечению точности, просим учитывать, что автоматический перевод может содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется обращаться к профессиональному переводу, выполненному человеком. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования данного перевода.