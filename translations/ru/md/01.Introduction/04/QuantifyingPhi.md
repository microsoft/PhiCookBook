<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-04T07:07:34+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ru"
}
-->
# **Квантование семейства Phi**

Квантование модели относится к процессу отображения параметров (таких как веса и значения активаций) в нейронной сети из большого диапазона значений (обычно непрерывного диапазона) в меньший конечный диапазон значений. Эта технология может уменьшить размер и вычислительную сложность модели и повысить эффективность работы модели в условиях ограниченных ресурсов, таких как мобильные устройства или встраиваемые системы. Квантование модели достигает сжатия за счёт уменьшения точности параметров, но также вводит определённую потерю точности. Поэтому в процессе квантования необходимо балансировать размер модели, вычислительную сложность и точность. Распространённые методы квантования включают фиксированно-точечное квантование, плавающую точку и т.д. Вы можете выбрать подходящую стратегию квантования в зависимости от конкретного сценария и потребностей.

Мы надеемся развернуть модели GenAI на граничных устройствах и позволить большему числу устройств войти в сценарии GenAI, такие как мобильные устройства, AI PC/Copilot+PC и традиционные устройства IoT. Через квантованные модели мы можем развертывать их на различных граничных устройствах в зависимости от устройства. В сочетании с фреймворком ускорения моделей и квантованными моделями, предоставляемыми производителями аппаратного обеспечения, мы можем строить лучшие варианты применения SLM.

В сценарии квантования у нас есть разные точности (INT4, INT8, FP16, FP32). Ниже приведено объяснение часто используемых точностей квантования

### **INT4**

Квантование INT4 — это радикальный метод квантования, который переводит веса и значения активаций модели в 4-битные целые числа. Квантование INT4 обычно приводит к большей потере точности из-за меньшего диапазона представления и более низкой точности. Однако по сравнению с INT8 квантование INT4 может ещё больше сократить требования к хранению и вычислительную сложность модели. Следует отметить, что квантование INT4 относительно редко применяется на практике, поскольку слишком низкая точность может вызвать значительное ухудшение производительности модели. Кроме того, не всё аппаратное обеспечение поддерживает операции INT4, поэтому при выборе метода квантования необходимо учитывать совместимость с оборудованием.

### **INT8**

Квантование INT8 — это процесс преобразования весов и активаций модели из чисел с плавающей запятой в 8-битные целые числа. Хотя числовой диапазон, представляемый целыми числами INT8, меньше и менее точен, это может значительно снизить требования к хранению и вычислениям. При квантовании INT8 веса и значения активаций модели проходят процесс квантования, включающий масштабирование и смещение, чтобы максимально сохранить исходную информацию с плавающей запятой. Во время инференса эти квантизованные значения будут деквантизироваться обратно в числа с плавающей запятой для вычислений, а затем снова квантизироваться в INT8 для следующего шага. Этот метод может обеспечить достаточную точность в большинстве приложений при сохранении высокой вычислительной эффективности.

### **FP16**

Формат FP16, то есть 16-битные числа с плавающей запятой (float16), сокращает объём памяти вдвое по сравнению с 32-битными числами с плавающей запятой (float32), что имеет значительные преимущества в масштабных приложениях глубокого обучения. Формат FP16 позволяет загружать более крупные модели или обрабатывать больше данных в пределах тех же ограничений памяти GPU. Поскольку современное GPU-оборудование продолжает поддерживать операции FP16, использование формата FP16 также может привести к улучшению скорости вычислений. Однако формат FP16 также имеет свои недостатки, а именно более низкую точность, что в некоторых случаях может привести к численной нестабильности или потере точности.

### **FP32**

Формат FP32 обеспечивает более высокую точность и может точно представлять широкий диапазон значений. В сценариях, где выполняются сложные математические операции или требуются результаты высокой точности, предпочтительно использовать формат FP32. Однако высокая точность также означает большее потребление памяти и более длительное время вычислений. Для масштабных моделей глубокого обучения, особенно когда существует много параметров модели и огромный объём данных, формат FP32 может привести к недостатку памяти GPU или снижению скорости инференса.

На мобильных устройствах или устройствах IoT мы можем конвертировать модели Phi-3.x в INT4, в то время как AI PC / Copilot PC могут использовать более высокую точность, такую как INT8, FP16, FP 32.

В настоящее время разные производители аппаратного обеспечения имеют разные фреймворки для поддержки генеративных моделей, такие как OpenVINO от Intel, QNN от Qualcomm, MLX от Apple и CUDA от Nvidia и т.д., в сочетании с квантованием модели для локального развёртывания.

С технической точки зрения после квантования у нас есть поддержка разных форматов, таких как форматы PyTorch / TensorFlow, GGUF и ONNX. Я провёл сравнение форматов и сценариев применения между GGUF и ONNX. Здесь я рекомендую формат квантования ONNX, который хорошо поддерживается от фреймворка модели до аппаратного обеспечения. В этой главе мы сосредоточимся на ONNX Runtime для GenAI, OpenVINO и Apple MLX для выполнения квантования модели (если у вас есть более хороший способ, вы также можете прислать его нам, отправив PR)

**В этой главе**

1. [Квантование Phi-3.5 / 4 с помощью llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Квантование Phi-3.5 / 4 с помощью расширений Generative AI для onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Квантование Phi-3.5 / 4 с помощью Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Квантование Phi-3.5 / 4 с помощью фреймворка Apple MLX](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
Отказ от ответственности:
Этот документ был переведен с помощью сервиса машинного перевода на основе ИИ [Co-op Translator](https://github.com/Azure/co-op-translator). Хотя мы стремимся к точности, пожалуйста, имейте в виду, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для критически важной информации рекомендуется профессиональный перевод, выполненный человеком. Мы не несем ответственности за любые недопонимания или неверные толкования, возникающие в результате использования этого перевода.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->