<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-07-16T21:41:45+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ru"
}
-->
# **Квантование семейства Phi**

Квантование модели — это процесс преобразования параметров (таких как веса и значения активации) нейронной сети из большого диапазона значений (обычно непрерывного) в меньший конечный диапазон. Эта технология позволяет уменьшить размер и вычислительную сложность модели, а также повысить её эффективность в условиях ограниченных ресурсов, например, на мобильных устройствах или встроенных системах. Квантование достигает сжатия за счёт снижения точности параметров, но при этом возникает определённая потеря точности. Поэтому в процессе квантования важно найти баланс между размером модели, вычислительной сложностью и точностью. Распространённые методы квантования включают фиксированную точку, плавающую точку и другие. Выбор подходящей стратегии зависит от конкретного сценария и требований.

Мы стремимся развернуть модели GenAI на устройствах на периферии и позволить большему числу устройств участвовать в сценариях GenAI, таких как мобильные устройства, AI PC/Copilot+PC и традиционные IoT-устройства. С помощью квантованных моделей мы можем адаптировать их под разные периферийные устройства. В сочетании с фреймворками ускорения моделей и квантованными моделями, предоставляемыми производителями аппаратного обеспечения, мы можем создавать более эффективные сценарии применения SLM.

В сценариях квантования используются разные уровни точности (INT4, INT8, FP16, FP32). Ниже приведено описание наиболее часто используемых форматов квантования.

### **INT4**

Квантование INT4 — это радикальный метод, при котором веса и значения активации модели преобразуются в 4-битные целые числа. Из-за меньшего диапазона представления и низкой точности INT4 обычно приводит к более значительной потере точности. Однако по сравнению с INT8 оно позволяет ещё сильнее сократить требования к хранению и вычислениям. Следует отметить, что INT4 встречается редко в практических задачах, так как слишком низкая точность может существенно ухудшить качество модели. Кроме того, не все аппаратные платформы поддерживают операции с INT4, поэтому при выборе метода квантования важно учитывать совместимость с оборудованием.

### **INT8**

Квантование INT8 — это процесс преобразования весов и активаций модели из чисел с плавающей точкой в 8-битные целые числа. Несмотря на меньший диапазон и точность, INT8 значительно снижает требования к памяти и вычислениям. При квантовании INT8 веса и активации проходят процедуру масштабирования и смещения, чтобы максимально сохранить исходную информацию с плавающей точкой. Во время инференса эти квантованные значения де-квантируются обратно в числа с плавающей точкой для вычислений, а затем снова квантируются в INT8 для следующего шага. Такой подход обеспечивает достаточную точность в большинстве приложений при высокой вычислительной эффективности.

### **FP16**

Формат FP16, то есть 16-битные числа с плавающей точкой (float16), сокращает объём памяти вдвое по сравнению с 32-битными числами (float32), что особенно выгодно для масштабных задач глубокого обучения. FP16 позволяет загружать более крупные модели или обрабатывать больше данных в рамках ограничений памяти GPU. Современное GPU-оборудование всё активнее поддерживает операции FP16, что может ускорить вычисления. Однако у FP16 есть и недостатки — более низкая точность, которая в некоторых случаях может привести к числовой нестабильности или потере точности.

### **FP32**

Формат FP32 обеспечивает высокую точность и способен точно представлять широкий диапазон значений. Его предпочитают в задачах с сложными математическими операциями или когда требуется высокая точность результатов. Однако высокая точность сопровождается большим потреблением памяти и увеличением времени вычислений. Для масштабных моделей глубокого обучения с большим количеством параметров и данных FP32 может привести к нехватке памяти GPU или снижению скорости инференса.

На мобильных или IoT-устройствах мы можем конвертировать модели Phi-3.x в INT4, тогда как AI PC / Copilot PC могут использовать более высокую точность, например INT8, FP16 или FP32.

В настоящее время разные производители аппаратного обеспечения предлагают собственные фреймворки для поддержки генеративных моделей, такие как OpenVINO от Intel, QNN от Qualcomm, MLX от Apple и CUDA от Nvidia, которые в сочетании с квантованием моделей позволяют реализовать локальное развертывание.

С технологической точки зрения после квантования поддерживаются разные форматы, например PyTorch / Tensorflow, GGUF и ONNX. Я провёл сравнение форматов и сценариев применения GGUF и ONNX. Рекомендую использовать формат квантования ONNX, который хорошо поддерживается как со стороны фреймворков моделей, так и аппаратного обеспечения. В этой главе мы сосредоточимся на ONNX Runtime для GenAI, OpenVINO и Apple MLX для выполнения квантования моделей (если у вас есть более эффективные методы, вы можете предложить их через PR).

**В этой главе:**

1. [Квантование Phi-3.5 / 4 с помощью llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Квантование Phi-3.5 / 4 с помощью расширений Generative AI для onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Квантование Phi-3.5 / 4 с помощью Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Квантование Phi-3.5 / 4 с помощью Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия по обеспечению точности, просим учитывать, что автоматический перевод может содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется обращаться к профессиональному переводу, выполненному человеком. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования данного перевода.