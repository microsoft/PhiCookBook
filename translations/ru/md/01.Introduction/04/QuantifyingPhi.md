<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-03-27T08:19:48+00:00",
  "source_file": "md\\01.Introduction\\04\\QuantifyingPhi.md",
  "language_code": "ru"
}
-->
# **Квантификация семейства Phi**

Квантификация модели — это процесс преобразования параметров (например, весов и значений активации) в модели нейронной сети из большого диапазона значений (обычно непрерывного диапазона) в меньший конечный диапазон значений. Эта технология позволяет уменьшить размер и вычислительную сложность модели, а также повысить её эффективность в средах с ограниченными ресурсами, таких как мобильные устройства или встроенные системы. Квантификация модели достигает сжатия за счёт снижения точности параметров, но при этом вводит определённую потерю точности. Поэтому в процессе квантификации необходимо находить баланс между размером модели, вычислительной сложностью и точностью. Среди распространённых методов квантификации — квантификация с фиксированной точкой, квантификация с плавающей точкой и др. Выбор подходящей стратегии квантификации зависит от конкретного сценария и потребностей.

Мы стремимся развернуть модели GenAI на периферийных устройствах, чтобы большее количество устройств могли участвовать в сценариях GenAI, таких как мобильные устройства, AI PC/Копилот+PC, а также традиционные IoT-устройства. Благодаря квантифицированным моделям мы можем адаптировать их для различных периферийных устройств, учитывая их особенности. В сочетании с фреймворками ускорения моделей и квантифицированными моделями, предоставляемыми производителями оборудования, мы можем создавать более эффективные приложения для SLM.

В рамках квантификации используются различные уровни точности (INT4, INT8, FP16, FP32). Ниже приведено описание наиболее часто используемых уровней точности квантификации.

### **INT4**

Квантификация INT4 — это радикальный метод, который преобразует веса и значения активации модели в 4-битные целые числа. INT4 обычно приводит к более значительной потере точности из-за меньшего диапазона представления и низкой точности. Однако по сравнению с INT8 квантификация INT4 позволяет ещё больше сократить требования к хранению и вычислительной сложности модели. Следует учитывать, что квантификация INT4 довольно редко используется на практике, так как слишком низкая точность может значительно ухудшить производительность модели. Кроме того, не всё оборудование поддерживает операции INT4, поэтому при выборе метода квантификации необходимо учитывать совместимость оборудования.

### **INT8**

Квантификация INT8 — это процесс преобразования весов и активаций модели из чисел с плавающей точкой в 8-битные целые числа. Хотя диапазон значений, представляемых 8-битными числами, меньше и менее точен, этот метод значительно снижает требования к хранению и вычислениям. В процессе квантификации INT8 веса и значения активации модели проходят этапы масштабирования и смещения, чтобы максимально сохранить исходную информацию чисел с плавающей точкой. Во время выполнения модели эти квантифицированные значения преобразуются обратно в числа с плавающей точкой для вычислений, а затем снова квантифицируются в INT8 для следующего шага. Этот метод обеспечивает достаточную точность для большинства приложений при высокой вычислительной эффективности.

### **FP16**

Формат FP16, то есть 16-битные числа с плавающей точкой (float16), сокращает объём памяти вдвое по сравнению с 32-битными числами с плавающей точкой (float32), что имеет значительные преимущества в крупномасштабных задачах глубокого обучения. FP16 позволяет загружать более крупные модели или обрабатывать больше данных в рамках ограничений памяти GPU. С развитием современного оборудования GPU, поддерживающего операции FP16, использование этого формата может также привести к ускорению вычислений. Однако FP16 имеет свои недостатки, такие как более низкая точность, что в некоторых случаях может вызвать численную нестабильность или потерю точности.

### **FP32**

Формат FP32 обеспечивает более высокую точность и позволяет точно представлять широкий диапазон значений. В сценариях, где выполняются сложные математические операции или требуется высокая точность, предпочтение отдаётся FP32. Однако высокая точность также означает больший объём памяти и более длительное время вычислений. Для крупномасштабных моделей глубокого обучения, особенно при большом количестве параметров и объёме данных, формат FP32 может привести к недостатку памяти GPU или снижению скорости выполнения.

На мобильных устройствах или IoT-устройствах мы можем преобразовать модели Phi-3.x в INT4, в то время как AI PC / Копилот PC могут использовать более высокую точность, такую как INT8, FP16, FP32.

В настоящее время различные производители оборудования предлагают свои фреймворки для поддержки генеративных моделей, такие как OpenVINO от Intel, QNN от Qualcomm, MLX от Apple и CUDA от Nvidia. В сочетании с квантификацией моделей они позволяют осуществлять локальное развертывание.

С технической точки зрения, после квантификации мы имеем поддержку различных форматов, таких как PyTorch / Tensorflow, GGUF и ONNX. Я провёл сравнение форматов GGUF и ONNX и их сценариев применения. Здесь я рекомендую формат квантификации ONNX, который хорошо поддерживается как фреймворками моделей, так и оборудованием. В этой главе мы сосредоточимся на ONNX Runtime для GenAI, OpenVINO и Apple MLX для выполнения квантификации моделей (если у вас есть более эффективный способ, вы можете отправить его нам через PR).

**Эта глава включает**

1. [Квантификация Phi-3.5 / 4 с использованием llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Квантификация Phi-3.5 / 4 с использованием расширений Generative AI для onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Квантификация Phi-3.5 / 4 с использованием Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Квантификация Phi-3.5 / 4 с использованием фреймворка Apple MLX](./UsingAppleMLXQuantifyingPhi.md)

**Отказ от ответственности**:  
Этот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Хотя мы стремимся к точности, пожалуйста, учитывайте, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникшие в результате использования данного перевода.