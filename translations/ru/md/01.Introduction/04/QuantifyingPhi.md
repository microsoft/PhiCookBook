# **Квантификация семейства Phi**

Квантификация модели относится к процессу отображения параметров (таких как веса и значения активации) нейронной сети из большого диапазона значений (обычно непрерывного диапазона) в меньший конечный диапазон значений. Эта технология может уменьшить размер и вычислительную сложность модели, а также повысить эффективность работы модели в условиях ограниченных ресурсов, таких как мобильные устройства или встроенные системы. Квантификация модели достигает сжатия за счёт уменьшения точности параметров, но при этом вводит определённую потерю точности. Поэтому в процессе квантификации необходимо сбалансировать размер модели, вычислительную сложность и точность. Распространённые методы квантификации включают фиксированную точку, плавающую точку и др. Вы можете выбрать подходящую стратегию квантификации в зависимости от конкретного сценария и потребностей.

Мы хотим развернуть GenAI модели на пограничных устройствах и позволить большему количеству устройств участвовать в GenAI сценариях, таких как мобильные устройства, AI ПК/Copilot+ПК и традиционные IoT устройства. С помощью квантифицированной модели мы можем развернуть её на различных пограничных устройствах в зависимости от конкретных устройств. В сочетании с фреймворком ускорения модели и квантифицированной моделью, предоставляемой производителями оборудования, мы можем создавать лучшие прикладные сценарии SLM.

В сценарии квантификации существуют различные точности (INT4, INT8, FP16, FP32). Ниже приведено объяснение наиболее часто используемых точностей квантификации.

### **INT4**

Квантификация INT4 — это радикальный метод квантификации, при котором веса и значения активации модели преобразуются в 4-битные целые числа. Квантификация INT4 обычно приводит к более значительной потере точности из-за меньшего диапазона представления и низкой точности. Однако по сравнению с квантификацией INT8, квантификация INT4 может ещё больше снизить требования к хранению и вычислительной сложности модели. Следует отметить, что квантификация INT4 относительно редка в практических применениях, поскольку слишком низкая точность может вызвать значительное ухудшение производительности модели. Кроме того, не все аппаратные средства поддерживают операции INT4, поэтому при выборе метода квантификации необходимо учитывать совместимость с оборудованием.

### **INT8**

Квантификация INT8 – это процесс преобразования весов и активаций модели из чисел с плавающей точкой в 8-битные целые числа. Хотя числовой диапазон, представляемый 8-битными целыми, меньше и менее точен, это может существенно сократить требования к хранению и вычислениям. При квантификации INT8 веса и значения активации модели проходят процесс квантификации, включающий масштабирование и сдвиг, чтобы максимально сохранить исходную информацию с плавающей точкой. Во время инференса эти квантизированные значения деквантизируются обратно в числа с плавающей точкой для расчёта, а затем снова квантизируются в INT8 для следующего шага. Этот метод обеспечивает достаточную точность в большинстве приложений при сохранении высокой вычислительной эффективности.

### **FP16**

Формат FP16, то есть 16-битные числа с плавающей точкой (float16), сокращает объём памяти вдвое по сравнению с 32-битными числами с плавающей точкой (float32), что даёт значительные преимущества в масштабных приложениях глубокого обучения. Формат FP16 позволяет загружать более крупные модели или обрабатывать больше данных в рамках тех же ограничений памяти GPU. По мере того как современное GPU-оборудование продолжает поддерживать операции FP16, использование формата FP16 может также привести к улучшению скорости вычислений. Однако у формата FP16 есть и свои внутренние недостатки — это более низкая точность, что в некоторых случаях может привести к числовой нестабильности или потере точности.

### **FP32**

Формат FP32 обеспечивает более высокую точность и может точно представлять широкий диапазон значений. В ситуациях, когда выполняются сложные математические операции или требуются высокоточные результаты, предпочтительнее использовать формат FP32. Однако высокая точность означает и большее использование памяти, и более длительное время вычисления. Для масштабных моделей глубокого обучения, особенно когда имеется много параметров модели и огромный объём данных, формат FP32 может привести к недостатку памяти GPU или снижению скорости инференса.

На мобильных устройствах или IoT-устройствах мы можем преобразовывать модели Phi-3.x в INT4, а AI ПК / Copilot ПК могут использовать более высокую точность, такую как INT8, FP16, FP32.

В настоящее время различные производители оборудования имеют свои фреймворки для поддержки генеративных моделей, такие как Intel OpenVINO, Qualcomm QNN, Apple MLX и Nvidia CUDA и др., в сочетании с квантификацией модели для выполнения локального развертывания.

В плане технологий после квантификации поддерживаются разные форматы, такие как PyTorch / TensorFlow форматы, GGUF и ONNX. Я провёл сравнение форматов и описание сценариев применения между GGUF и ONNX. Здесь я рекомендую формат квантификации ONNX, который имеет хорошую поддержку от фреймворка модели до оборудования. В этой главе мы будем рассматривать ONNX Runtime для GenAI, OpenVINO и Apple MLX для выполнения квантификации модели (если у вас есть лучший способ, вы также можете предоставить его, отправив PR).

**Эта глава включает**

1. [Квантификация Phi-3.5 / 4 с использованием llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Квантификация Phi-3.5 / 4 с использованием расширений Generative AI для onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Квантификация Phi-3.5 / 4 с использованием Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Квантификация Phi-3.5 / 4 с использованием Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Отказ от ответственности**:  
Данный документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия обеспечить точность, имейте в виду, что автоматический перевод может содержать ошибки или неточности. Оригинальный документ на языке оригинала следует считать авторитетным источником. Для получения важной информации рекомендуется использовать профессиональный человеческий перевод. Мы не несем ответственности за любые недоразумения или неправильное толкование, возникшие в результате использования этого перевода.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->