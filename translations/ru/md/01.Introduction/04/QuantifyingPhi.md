<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-05-07T14:48:50+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ru"
}
-->
# **Квантование семейства Phi**

Квантование модели — это процесс преобразования параметров (например, весов и значений активации) нейронной сети из широкого диапазона значений (обычно непрерывного) в меньший конечный диапазон. Эта технология позволяет уменьшить размер и вычислительную сложность модели, а также повысить эффективность работы модели в условиях ограниченных ресурсов, таких как мобильные устройства или встроенные системы. Квантование достигает сжатия за счёт снижения точности параметров, но при этом возникает определённая потеря точности. Поэтому в процессе квантования необходимо найти баланс между размером модели, вычислительной сложностью и точностью. Распространённые методы квантования включают фиксированную точку, плавающую точку и другие. Выбор подходящей стратегии квантования зависит от конкретного сценария и требований.

Мы стремимся развернуть модели GenAI на периферийных устройствах и позволить большему количеству устройств участвовать в сценариях GenAI, таких как мобильные устройства, AI PC/Copilot+PC и традиционные IoT-устройства. С помощью квантованных моделей мы можем развертывать их на разных периферийных устройствах в зависимости от типа устройства. В сочетании с фреймворками для ускорения моделей и квантованными моделями, предоставляемыми производителями аппаратного обеспечения, мы можем создавать более эффективные сценарии применения SLM.

В квантовании используются разные уровни точности (INT4, INT8, FP16, FP32). Ниже приведено объяснение наиболее часто используемых уровней квантования.

### **INT4**

Квантование INT4 — это радикальный метод, который преобразует веса и значения активации модели в 4-битные целые числа. INT4 обычно приводит к более значительной потере точности из-за меньшего диапазона представления и низкой точности. Однако по сравнению с INT8, INT4 позволяет ещё сильнее снизить требования к хранению и вычислительной сложности модели. Следует отметить, что INT4 применяется довольно редко, так как слишком низкая точность может существенно ухудшить производительность модели. Кроме того, не все аппаратные платформы поддерживают операции с INT4, поэтому при выборе метода квантования важно учитывать совместимость с оборудованием.

### **INT8**

Квантование INT8 — это процесс преобразования весов и активаций модели из чисел с плавающей точкой в 8-битные целые числа. Несмотря на то, что числовой диапазон и точность INT8 меньше, это существенно снижает требования к хранению и вычислениям. В процессе квантования INT8 веса и значения активации проходят через этапы масштабирования и смещения, чтобы максимально сохранить информацию исходных чисел с плавающей точкой. Во время инференса эти квантованные значения деквантируются обратно в числа с плавающей точкой для вычислений, а затем снова квантируются в INT8 для следующего шага. Этот метод обеспечивает достаточную точность в большинстве приложений при высокой вычислительной эффективности.

### **FP16**

Формат FP16, то есть 16-битные числа с плавающей точкой (float16), сокращает объём памяти вдвое по сравнению с 32-битными числами с плавающей точкой (float32), что является большим преимуществом для масштабных задач глубокого обучения. FP16 позволяет загружать более крупные модели или обрабатывать больше данных в рамках ограничений памяти GPU. Современное GPU-оборудование всё активнее поддерживает операции FP16, что может также ускорить вычисления. Однако у FP16 есть и недостатки — более низкая точность, которая в некоторых случаях может привести к числовой нестабильности или потере точности.

### **FP32**

Формат FP32 обеспечивает высокую точность и может точно представлять широкий диапазон значений. В сценариях, где выполняются сложные математические операции или требуется высокая точность, предпочтение отдаётся FP32. Однако высокая точность сопровождается большим расходом памяти и более длительным временем вычислений. Для масштабных моделей глубокого обучения с большим количеством параметров и огромными объёмами данных формат FP32 может привести к нехватке памяти GPU или снижению скорости инференса.

На мобильных устройствах или IoT-устройствах модели Phi-3.x можно конвертировать в INT4, тогда как AI PC / Copilot PC могут использовать более высокую точность, например INT8, FP16, FP32.

В настоящее время разные производители аппаратного обеспечения предлагают различные фреймворки для поддержки генеративных моделей, такие как OpenVINO от Intel, QNN от Qualcomm, MLX от Apple и CUDA от Nvidia, которые в сочетании с квантованием моделей позволяют осуществлять локальное развертывание.

С технической точки зрения после квантования поддерживаются разные форматы, например PyTorch / Tensorflow, GGUF и ONNX. Я провёл сравнение форматов и сценариев применения между GGUF и ONNX. Рекомендую использовать формат квантования ONNX, который хорошо поддерживается от фреймворка модели до аппаратного обеспечения. В этой главе мы сосредоточимся на ONNX Runtime для GenAI, OpenVINO и Apple MLX для выполнения квантования моделей (если у вас есть более эффективные методы, вы можете предложить их через PR).

**В этой главе рассмотрено**

1. [Квантование Phi-3.5 / 4 с использованием llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Квантование Phi-3.5 / 4 с использованием расширений Generative AI для onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Квантование Phi-3.5 / 4 с использованием Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Квантование Phi-3.5 / 4 с использованием Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

**Отказ от ответственности**:  
Этот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия обеспечить точность, просим учитывать, что автоматический перевод может содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неверные толкования, возникшие в результате использования данного перевода.