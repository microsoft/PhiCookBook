<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "be4101a30d98e95a71d42c276e8bcd37",
  "translation_date": "2025-03-27T07:25:37+00:00",
  "source_file": "md\\01.Introduction\\03\\Jetson_Inference.md",
  "language_code": "ru"
}
-->
# **Inference Phi-3 на Nvidia Jetson**

Nvidia Jetson — это серия встроенных вычислительных плат от Nvidia. Модели Jetson TK1, TX1 и TX2 оснащены процессором Tegra (или SoC) от Nvidia, который интегрирует центральный процессор (CPU) архитектуры ARM. Jetson — это энергоэффективная система, предназначенная для ускорения приложений машинного обучения. Nvidia Jetson используется профессиональными разработчиками для создания инновационных продуктов в области искусственного интеллекта во всех отраслях, а также студентами и энтузиастами для практического изучения ИИ и создания удивительных проектов. SLM развертывается на устройствах периферии, таких как Jetson, что обеспечивает более эффективное внедрение сценариев промышленного применения генеративного ИИ.

## Развертывание на NVIDIA Jetson:
Разработчики, работающие над автономной робототехникой и встроенными устройствами, могут использовать Phi-3 Mini. Небольшие размеры Phi-3 делают его идеальным для периферийного развертывания. Параметры были тщательно настроены во время обучения, что обеспечивает высокую точность ответов.

### Оптимизация TensorRT-LLM:
Библиотека [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM?WT.mc_id=aiml-138114-kinfeylo) от NVIDIA оптимизирует вывод больших языковых моделей. Она поддерживает длинное контекстное окно Phi-3 Mini, улучшая как пропускную способность, так и задержку. Оптимизации включают такие методы, как LongRoPE, FP8 и пакетирование в процессе выполнения.

### Доступность и развертывание:
Разработчики могут ознакомиться с Phi-3 Mini с контекстным окном на 128K на [NVIDIA AI](https://www.nvidia.com/en-us/ai-data-science/generative-ai/). Модель представлена в виде NVIDIA NIM — микросервиса со стандартным API, который можно развернуть где угодно. Также доступны [реализации TensorRT-LLM на GitHub](https://github.com/NVIDIA/TensorRT-LLM).

## **1. Подготовка**

a. Jetson Orin NX / Jetson NX

b. JetPack 5.1.2+
   
c. Cuda 11.8
   
d. Python 3.8+

## **2. Запуск Phi-3 на Jetson**

Мы можем выбрать [Ollama](https://ollama.com) или [LlamaEdge](https://llamaedge.com).

Если вы хотите использовать gguf одновременно в облаке и на периферийных устройствах, LlamaEdge можно рассматривать как WasmEdge (WasmEdge — это легковесная, высокопроизводительная, масштабируемая среда выполнения WebAssembly, подходящая для облачных, периферийных и децентрализованных приложений. Она поддерживает бессерверные приложения, встроенные функции, микросервисы, смарт-контракты и устройства IoT. Вы можете развернуть количественную модель gguf на периферийных устройствах и в облаке через LlamaEdge).

![llamaedge](../../../../../translated_images/llamaedge.1356a35c809c5e9d89d8168db0c92161e87f5e2c34831f2fad800f00fc4e74dc.ru.jpg)

Вот шаги использования:

1. Установите и скачайте соответствующие библиотеки и файлы.

```bash

curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install.sh | bash -s -- --plugin wasi_nn-ggml

curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

curl -LO https://github.com/LlamaEdge/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz

tar xzf chatbot-ui.tar.gz

```

**Примечание**: llama-api-server.wasm и chatbot-ui должны находиться в одной директории.

2. Запустите скрипты в терминале.

```bash

wasmedge --dir .:. --nn-preload default:GGML:AUTO:{Your gguf path} llama-api-server.wasm -p phi-3-chat

```

Вот результат выполнения:

![llamaedgerun](../../../../../translated_images/llamaedgerun.66eb2acd7f14e814437879522158b9531ae7c955014d48d0708d0e4ce6ac94a6.ru.png)

***Пример кода*** [Phi-3 mini WASM Notebook Sample](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm)

В заключение, Phi-3 Mini представляет собой значительный шаг вперед в языковом моделировании, сочетая эффективность, осведомленность о контексте и оптимизационные возможности NVIDIA. Независимо от того, создаете ли вы роботов или периферийные приложения, Phi-3 Mini — это мощный инструмент, о котором стоит знать.

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия по обеспечению точности, пожалуйста, учитывайте, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется использовать профессиональный перевод человеком. Мы не несём ответственности за любые недоразумения или неверные интерпретации, возникающие в результате использования данного перевода.