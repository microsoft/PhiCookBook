<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f1ff728038c4f554b660a36b76cbdd6e",
  "translation_date": "2025-07-16T21:06:48+00:00",
  "source_file": "md/01.Introduction/03/overview.md",
  "language_code": "ru"
}
-->
В контексте Phi-3-mini, инференс означает процесс использования модели для предсказаний или генерации результатов на основе входных данных. Позвольте рассказать подробнее о Phi-3-mini и его возможностях инференса.

Phi-3-mini является частью серии моделей Phi-3, выпущенной Microsoft. Эти модели созданы, чтобы переопределить возможности Малых Языковых Моделей (SLM).

Вот несколько ключевых моментов о Phi-3-mini и его возможностях инференса:

## **Обзор Phi-3-mini:**
- Размер параметров Phi-3-mini составляет 3,8 миллиарда.
- Он может работать не только на традиционных вычислительных устройствах, но и на edge-устройствах, таких как мобильные устройства и IoT-устройства.
- Выпуск Phi-3-mini позволяет частным лицам и компаниям разворачивать SLM на различных аппаратных платформах, особенно в условиях ограниченных ресурсов.
- Поддерживаются различные форматы моделей, включая традиционный формат PyTorch, квантизированную версию формата gguf и квантизированную версию на базе ONNX.

## **Доступ к Phi-3-mini:**
Для доступа к Phi-3-mini вы можете использовать [Semantic Kernel](https://github.com/microsoft/SemanticKernelCookBook?WT.mc_id=aiml-138114-kinfeylo) в приложении Copilot. Semantic Kernel обычно совместим с Azure OpenAI Service, открытыми моделями на Hugging Face и локальными моделями.  
Также можно использовать [Ollama](https://ollama.com) или [LlamaEdge](https://llamaedge.com) для вызова квантизированных моделей. Ollama позволяет отдельным пользователям вызывать разные квантизированные модели, а LlamaEdge обеспечивает кроссплатформенную доступность моделей GGUF.

## **Квантизированные модели:**
Многие пользователи предпочитают использовать квантизированные модели для локального инференса. Например, вы можете напрямую запускать Phi-3 через Ollama или настроить его офлайн с помощью Modelfile. В Modelfile указывается путь к файлу GGUF и формат подсказки.

## **Возможности генеративного ИИ:**
Сочетание SLM, таких как Phi-3-mini, открывает новые возможности для генеративного ИИ. Инференс — это лишь первый шаг; эти модели можно использовать для различных задач в условиях ограниченных ресурсов, низкой задержки и ограниченного бюджета.

## **Открывая генеративный ИИ с Phi-3-mini: руководство по инференсу и развертыванию**  
Узнайте, как использовать Semantic Kernel, Ollama/LlamaEdge и ONNX Runtime для доступа и инференса моделей Phi-3-mini, а также исследуйте возможности генеративного ИИ в различных сценариях применения.

**Особенности**  
Инференс модели phi3-mini в:

- [Semantic Kernel](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/semantickernel?WT.mc_id=aiml-138114-kinfeylo)  
- [Ollama](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ollama?WT.mc_id=aiml-138114-kinfeylo)  
- [LlamaEdge WASM](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm?WT.mc_id=aiml-138114-kinfeylo)  
- [ONNX Runtime](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/onnx?WT.mc_id=aiml-138114-kinfeylo)  
- [iOS](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ios?WT.mc_id=aiml-138114-kinfeylo)  

В итоге, Phi-3-mini позволяет разработчикам исследовать различные форматы моделей и использовать генеративный ИИ в самых разных сценариях применения.

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия по обеспечению точности, просим учитывать, что автоматический перевод может содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется обращаться к профессиональному переводу, выполненному человеком. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования данного перевода.