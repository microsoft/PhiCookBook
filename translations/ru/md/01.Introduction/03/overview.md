<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f1ff728038c4f554b660a36b76cbdd6e",
  "translation_date": "2025-03-27T07:55:15+00:00",
  "source_file": "md\\01.Introduction\\03\\overview.md",
  "language_code": "ru"
}
-->
В контексте Phi-3-mini, вывод (inference) относится к процессу использования модели для создания прогнозов или генерации результатов на основе входных данных. Позвольте мне рассказать подробнее о Phi-3-mini и его возможностях вывода.

Phi-3-mini является частью серии моделей Phi-3, выпущенной Microsoft. Эти модели предназначены для переосмысления возможностей Малых Языковых Моделей (SLMs).

Вот основные моменты о Phi-3-mini и его возможностях вывода:

## **Обзор Phi-3-mini:**
- Phi-3-mini имеет размер параметров 3.8 миллиарда.
- Модель может работать не только на традиционных вычислительных устройствах, но и на устройствах периферии, таких как мобильные и IoT-устройства.
- Выпуск Phi-3-mini позволяет как отдельным пользователям, так и предприятиям использовать SLMs на различных аппаратных устройствах, особенно в условиях ограниченных ресурсов.
- Модель поддерживает различные форматы, включая традиционный формат PyTorch, квантованный формат gguf и квантованный формат на основе ONNX.

## **Доступ к Phi-3-mini:**
Для доступа к Phi-3-mini можно использовать [Semantic Kernel](https://github.com/microsoft/SemanticKernelCookBook?WT.mc_id=aiml-138114-kinfeylo) в приложении Copilot. Semantic Kernel совместим с Azure OpenAI Service, открытыми моделями на Hugging Face и локальными моделями.
Также можно использовать [Ollama](https://ollama.com) или [LlamaEdge](https://llamaedge.com) для вызова квантованных моделей. Ollama позволяет отдельным пользователям вызывать различные квантованные модели, а LlamaEdge обеспечивает межплатформенную доступность для моделей GGUF.

## **Квантованные модели:**
Многие пользователи предпочитают использовать квантованные модели для локального вывода. Например, можно напрямую запустить Ollama run Phi-3 или настроить модель оффлайн с помощью Modelfile. Файл Modelfile определяет путь к файлу GGUF и формат запроса.

## **Возможности генеративного ИИ:**
Комбинирование SLMs, таких как Phi-3-mini, открывает новые возможности для генеративного ИИ. Вывод — это только первый шаг; эти модели могут использоваться для выполнения различных задач в условиях ограниченных ресурсов, минимальной задержки и сниженной стоимости.

## **Открытие генеративного ИИ с Phi-3-mini: Руководство по выводу и развертыванию**
Узнайте, как использовать Semantic Kernel, Ollama/LlamaEdge и ONNX Runtime для доступа к моделям Phi-3-mini и их вывода, а также исследуйте возможности генеративного ИИ в различных сценариях применения.

**Особенности**
Вывод модели phi3-mini возможен в:

- [Semantic Kernel](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/semantickernel?WT.mc_id=aiml-138114-kinfeylo)
- [Ollama](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ollama?WT.mc_id=aiml-138114-kinfeylo)
- [LlamaEdge WASM](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm?WT.mc_id=aiml-138114-kinfeylo)
- [ONNX Runtime](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/onnx?WT.mc_id=aiml-138114-kinfeylo)
- [iOS](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ios?WT.mc_id=aiml-138114-kinfeylo)

В итоге, Phi-3-mini позволяет разработчикам изучать различные форматы моделей и использовать генеративный ИИ в разнообразных сценариях применения.

**Отказ от ответственности**:  
Этот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Хотя мы стремимся к точности, пожалуйста, учитывайте, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке должен считаться авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникшие в результате использования данного перевода.