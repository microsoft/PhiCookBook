<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f1ff728038c4f554b660a36b76cbdd6e",
  "translation_date": "2025-05-07T14:41:40+00:00",
  "source_file": "md/01.Introduction/03/overview.md",
  "language_code": "ru"
}
-->
В контексте Phi-3-mini инференс означает процесс использования модели для предсказаний или генерации результатов на основе входных данных. Позвольте рассказать подробнее о Phi-3-mini и её возможностях инференса.

Phi-3-mini входит в серию моделей Phi-3, выпущенных Microsoft. Эти модели созданы, чтобы переопределить возможности Малых Языковых Моделей (SLM).

Вот ключевые моменты о Phi-3-mini и её возможностях инференса:

## **Обзор Phi-3-mini:**
- Размер параметров Phi-3-mini составляет 3,8 миллиарда.
- Она может работать не только на традиционных вычислительных устройствах, но и на edge-устройствах, таких как мобильные устройства и IoT.
- Выпуск Phi-3-mini позволяет как частным лицам, так и компаниям запускать SLM на различных аппаратных платформах, особенно в условиях ограниченных ресурсов.
- Поддерживаются различные форматы моделей, включая традиционный формат PyTorch, квантизированную версию формата gguf и квантизированную версию на основе ONNX.

## **Доступ к Phi-3-mini:**
Для доступа к Phi-3-mini можно использовать [Semantic Kernel](https://github.com/microsoft/SemanticKernelCookBook?WT.mc_id=aiml-138114-kinfeylo) в приложении Copilot. Semantic Kernel в целом совместим с Azure OpenAI Service, open-source моделями на Hugging Face и локальными моделями.
Также можно использовать [Ollama](https://ollama.com) или [LlamaEdge](https://llamaedge.com) для вызова квантизированных моделей. Ollama позволяет отдельным пользователям запускать различные квантизированные модели, а LlamaEdge обеспечивает кроссплатформенную поддержку моделей GGUF.

## **Квантизированные модели:**
Многие пользователи предпочитают использовать квантизированные модели для локального инференса. Например, можно напрямую запустить Phi-3 через Ollama или настроить его офлайн с помощью Modelfile. В Modelfile указывается путь к файлу GGUF и формат prompt.

## **Возможности генеративного ИИ:**
Комбинация SLM, таких как Phi-3-mini, открывает новые возможности для генеративного ИИ. Инференс — это лишь первый шаг; эти модели можно применять для различных задач в условиях ограниченных ресурсов, низкой задержки и ограниченного бюджета.

## **Открывая генеративный ИИ с Phi-3-mini: руководство по инференсу и развертыванию**
Узнайте, как использовать Semantic Kernel, Ollama/LlamaEdge и ONNX Runtime для доступа и инференса моделей Phi-3-mini, а также исследуйте возможности генеративного ИИ в различных сценариях применения.

**Особенности**
Инференс модели phi3-mini в:

- [Semantic Kernel](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/semantickernel?WT.mc_id=aiml-138114-kinfeylo)
- [Ollama](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ollama?WT.mc_id=aiml-138114-kinfeylo)
- [LlamaEdge WASM](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm?WT.mc_id=aiml-138114-kinfeylo)
- [ONNX Runtime](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/onnx?WT.mc_id=aiml-138114-kinfeylo)
- [iOS](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ios?WT.mc_id=aiml-138114-kinfeylo)

В итоге, Phi-3-mini даёт разработчикам возможность работать с разными форматами моделей и использовать генеративный ИИ в разнообразных сценариях.

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия по обеспечению точности, пожалуйста, имейте в виду, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется использовать профессиональный перевод, выполненный человеком. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования данного перевода.