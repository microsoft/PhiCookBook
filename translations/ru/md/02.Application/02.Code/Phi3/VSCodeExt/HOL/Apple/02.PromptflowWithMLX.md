<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3dbbf568625b1ee04b354c2dc81d3248",
  "translation_date": "2025-07-17T04:22:41+00:00",
  "source_file": "md/02.Application/02.Code/Phi3/VSCodeExt/HOL/Apple/02.PromptflowWithMLX.md",
  "language_code": "ru"
}
-->
# **Лабораторная работа 2 - Запуск Prompt flow с Phi-3-mini в AIPC**

## **Что такое Prompt flow**

Prompt flow — это набор инструментов для разработки, созданный для упрощения полного цикла разработки AI-приложений на базе LLM: от идеи, прототипирования, тестирования и оценки до развертывания в продакшн и мониторинга. Он значительно облегчает работу с prompt-инжинирингом и позволяет создавать LLM-приложения с качеством, готовым к промышленному использованию.

С помощью Prompt flow вы сможете:

- Создавать потоки, которые связывают LLM, промпты, Python-код и другие инструменты в исполняемый рабочий процесс.

- Отлаживать и улучшать ваши потоки, особенно взаимодействие с LLM, без лишних сложностей.

- Оценивать ваши потоки, рассчитывать метрики качества и производительности на больших наборах данных.

- Интегрировать тестирование и оценку в вашу CI/CD систему для обеспечения качества потока.

- Развёртывать потоки на выбранной платформе обслуживания или легко интегрировать их в код вашей программы.

- (Опционально, но настоятельно рекомендуется) Работать в команде, используя облачную версию Prompt flow в Azure AI.

## **Создание потоков генерации кода на Apple Silicon**

***Примечание***: Если вы ещё не завершили установку окружения, пожалуйста, посетите [Лабораторная работа 0 - Установка](./01.Installations.md)

1. Откройте расширение Prompt flow в Visual Studio Code и создайте пустой проект потока

![create](../../../../../../../../../translated_images/pf_create.bde888dc83502eba082a058175bbf1eee6791219795393a386b06fd3043ec54d.ru.png)

2. Добавьте параметры Inputs и Outputs, а также добавьте Python-код как новый поток

![flow](../../../../../../../../../translated_images/pf_flow.520824c0969f2a94f17e947f86bdc4b4c6c88a2efa394fe3bcfb58c0dbc578a7.ru.png)

Вы можете использовать эту структуру (flow.dag.yaml) для построения вашего потока

```yaml

inputs:
  prompt:
    type: string
    default: Write python code for Fibonacci serie. Please use markdown as output
outputs:
  result:
    type: string
    reference: ${gen_code_by_phi3.output}
nodes:
- name: gen_code_by_phi3
  type: python
  source:
    type: code
    path: gen_code_by_phi3.py
  inputs:
    prompt: ${inputs.prompt}


```

3. Квантификация phi-3-mini

Мы хотим лучше запускать SLM на локальных устройствах. Обычно мы квантифицируем модель (INT4, FP16, FP32)

```bash

python -m mlx_lm.convert --hf-path microsoft/Phi-3-mini-4k-instruct

```

**Примечание:** папка по умолчанию — mlx_model

4. Добавьте код в ***Chat_With_Phi3.py***

```python


from promptflow import tool

from mlx_lm import load, generate


# The inputs section will change based on the arguments of the tool function, after you save the code
# Adding type to arguments and return value will help the system show the types properly
# Please update the function name/signature per need
@tool
def my_python_tool(prompt: str) -> str:

    model_id = './mlx_model_phi3_mini'

    model, tokenizer = load(model_id)

    # <|user|>\nWrite python code for Fibonacci serie. Please use markdown as output<|end|>\n<|assistant|>

    response = generate(model, tokenizer, prompt="<|user|>\n" + prompt  + "<|end|>\n<|assistant|>", max_tokens=2048, verbose=True)

    return response


```

4. Вы можете протестировать поток через Debug или Run, чтобы проверить корректность генерации кода

![RUN](../../../../../../../../../translated_images/pf_run.4239e8a0b420a58284edf6ee1471c1697c345670313c8e7beac0edaee15b9a9d.ru.png)

5. Запустите поток как API для разработки в терминале

```

pf flow serve --source ./ --port 8080 --host localhost   

```

Вы можете протестировать его в Postman / Thunder Client

### **Примечания**

1. Первый запуск занимает много времени. Рекомендуется скачать модель phi-3 через Hugging face CLI.

2. Учитывая ограниченные вычислительные возможности Intel NPU, рекомендуется использовать Phi-3-mini-4k-instruct.

3. Мы используем Intel NPU Acceleration для квантификации с конверсией в INT4, но если вы перезапускаете сервис, необходимо удалить папки cache и nc_workshop.

## **Ресурсы**

1. Изучить Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Изучить Intel NPU Acceleration [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Пример кода, скачать [Local NPU Agent Sample Code](../../../../../../../../../code/07.Lab/01/AIPC/local-npu-agent)

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия по обеспечению точности, просим учитывать, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется обращаться к профессиональному человеческому переводу. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования данного перевода.