<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "92e7dac1e5af0dd7c94170fdaf6860fe",
  "translation_date": "2025-07-17T02:56:52+00:00",
  "source_file": "md/02.Application/01.TextAndChat/Phi3/UsingPromptFlowWithONNX.md",
  "language_code": "ru"
}
-->
# Использование Windows GPU для создания решения Prompt flow с Phi-3.5-Instruct ONNX

В этом документе приведён пример использования PromptFlow с ONNX (Open Neural Network Exchange) для разработки AI-приложений на базе моделей Phi-3.

PromptFlow — это набор инструментов для разработки, который упрощает полный цикл создания AI-приложений на основе LLM (Large Language Model) — от идеи и прототипирования до тестирования и оценки.

Интегрируя PromptFlow с ONNX, разработчики могут:

- Оптимизировать производительность модели: использовать ONNX для эффективного вывода и развертывания моделей.
- Упростить разработку: применять PromptFlow для управления рабочим процессом и автоматизации рутинных задач.
- Улучшить сотрудничество: обеспечить удобную совместную работу команды через единое рабочее окружение.

**Prompt flow** — это набор инструментов, который упрощает полный цикл разработки AI-приложений на базе LLM: от идеи, прототипирования, тестирования и оценки до развертывания в продакшн и мониторинга. Он значительно облегчает prompt engineering и позволяет создавать LLM-приложения с качеством, готовым к промышленному использованию.

Prompt flow может подключаться к OpenAI, Azure OpenAI Service и настраиваемым моделям (Huggingface, локальные LLM/SLM). Мы планируем развернуть квантизированную ONNX-модель Phi-3.5 в локальных приложениях. Prompt flow поможет нам лучше спланировать бизнес и реализовать локальные решения на базе Phi-3.5. В этом примере мы объединим ONNX Runtime GenAI Library для создания решения Prompt flow на базе Windows GPU.

## **Установка**

### **ONNX Runtime GenAI для Windows GPU**

Прочитайте это руководство по настройке ONNX Runtime GenAI для Windows GPU [нажмите здесь](./ORTWindowGPUGuideline.md)

### **Настройка Prompt flow в VSCode**

1. Установите расширение Prompt flow для VS Code

![pfvscode](../../../../../../translated_images/ru/pfvscode.eff93dfc66a42cbe.png)

2. После установки расширения Prompt flow для VS Code, кликните по нему и выберите **Installation dependencies**, следуйте этому руководству для установки Prompt flow SDK в вашем окружении

![pfsetup](../../../../../../translated_images/ru/pfsetup.b46e93096f5a254f.png)

3. Скачайте [пример кода](../../../../../../code/09.UpdateSamples/Aug/pf/onnx_inference_pf) и откройте его в VS Code

![pfsample](../../../../../../translated_images/ru/pfsample.8d89e70584ffe7c4.png)

4. Откройте **flow.dag.yaml** и выберите ваше Python-окружение

![pfdag](../../../../../../translated_images/ru/pfdag.264a77f7366458ff.png)

   Откройте **chat_phi3_ort.py** и укажите путь к вашей модели Phi-3.5-instruct ONNX

![pfphi](../../../../../../translated_images/ru/pfphi.72da81d74244b45f.png)

5. Запустите prompt flow для тестирования

Откройте **flow.dag.yaml**, нажмите на визуальный редактор

![pfv](../../../../../../translated_images/ru/pfv.ba8a81f34b20f603.png)

после этого запустите выполнение для теста

![pfflow](../../../../../../translated_images/ru/pfflow.4e1135a089b1ce1b.png)

1. Вы также можете запускать пакетные задания в терминале для получения дополнительных результатов


```bash

pf run create --file batch_run.yaml --stream --name 'Your eval qa name'    

```

Результаты можно просмотреть в вашем браузере по умолчанию


![pfresult](../../../../../../translated_images/ru/pfresult.c22c826f8062d7cb.png)

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия по обеспечению точности, просим учитывать, что автоматический перевод может содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется обращаться к профессиональному переводу, выполненному человеком. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования данного перевода.