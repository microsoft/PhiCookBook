# Интерактивный чатбот Phi 3 Mini 4K Instruct с Whisper

## Обзор

Интерактивный чатбот Phi 3 Mini 4K Instruct — это инструмент, который позволяет пользователям взаимодействовать с демонстрацией Microsoft Phi 3 Mini 4K instruct с помощью текстового или голосового ввода. Чатбот может использоваться для различных задач, таких как перевод, обновления погоды и сбор общей информации.

### Начало работы

Чтобы использовать этот чатбот, просто выполните следующие инструкции:

1. Откройте новый [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb)
2. В основном окне ноутбука вы увидите интерфейс чатбокса с полем для ввода текста и кнопкой «Send».
3. Чтобы использовать текстовый чатбот, просто введите ваше сообщение в поле для ввода текста и нажмите кнопку «Send». Чатбот ответит аудиофайлом, который можно воспроизвести прямо из ноутбука.

**Примечание**: Для работы этого инструмента требуется GPU и доступ к моделям Microsoft Phi-3 и OpenAI Whisper, которые используются для распознавания речи и перевода.

### Требования к GPU

Для запуска этой демонстрации вам потребуется 12 ГБ памяти GPU.

Требования к памяти для запуска демонстрации **Microsoft-Phi-3-Mini-4K instruct** на GPU зависят от нескольких факторов, таких как размер входных данных (аудио или текст), язык перевода, скорость модели и доступная память на GPU.

В целом модель Whisper разработана для работы на GPU. Рекомендуемый минимальный объем памяти GPU для работы модели Whisper — 8 ГБ, но она может использовать и больший объем памяти при необходимости.

Важно отметить, что обработка больших объемов данных или большого количества запросов на модели может потребовать больше памяти GPU и/или вызвать проблемы с производительностью. Рекомендуется протестировать ваш случай использования с разными настройками и контролировать использование памяти, чтобы определить оптимальные параметры для ваших конкретных задач.

## Пример E2E для интерактивного чатбота Phi 3 Mini 4K Instruct с Whisper

Ноутбук Jupyter под названием [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) демонстрирует, как использовать демонстрацию Microsoft Phi 3 Mini 4K instruct для генерации текста из аудио или текстового ввода. В ноутбуке определены несколько функций:

1. `tts_file_name(text)`: Эта функция создает имя файла на основе вводимого текста для сохранения сгенерированного аудиофайла.
1. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: Эта функция использует API Edge TTS для генерации аудиофайла из списка фрагментов текста. Входные параметры – список фрагментов, скорость речи, имя голоса и путь для сохранения сгенерированного аудиофайла.
1. `talk(input_text)`: Эта функция генерирует аудиофайл с помощью API Edge TTS и сохраняет его под случайным именем в каталоге /content/audio. Входной параметр — текст для преобразования в речь.
1. `run_text_prompt(message, chat_history)`: Эта функция использует демонстрацию Microsoft Phi 3 Mini 4K instruct для создания аудиофайла из входящего сообщения и добавляет его в историю чата.
1. `run_audio_prompt(audio, chat_history)`: Эта функция преобразует аудиофайл в текст с помощью API модели Whisper и передает его функции `run_text_prompt()`.
1. Код запускает приложение Gradio, которое позволяет пользователям взаимодействовать с демонстрацией Phi 3 Mini 4K instruct, вводя сообщения или загружая аудиофайлы. Вывод отображается в виде текстового сообщения внутри приложения.

## Устранение неполадок

Установка драйверов Cuda для GPU

1. Убедитесь, что ваше приложение Linux обновлено

    ```bash
    sudo apt update
    ```

1. Установите драйверы Cuda

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. Зарегистрируйте расположение драйвера cuda

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. Проверка объема памяти Nvidia GPU (Требуется 12 ГБ памяти GPU)

    ```bash
    nvidia-smi
    ```

1. Очистка кэша: если вы используете PyTorch, вызовите torch.cuda.empty_cache(), чтобы освободить всю неиспользуемую кэшированную память для использования другими приложениями GPU

    ```python
    torch.cuda.empty_cache() 
    ```

1. Проверка Nvidia Cuda

    ```bash
    nvcc --version
    ```

1. Выполните следующие действия для создания токена Hugging Face.

    - Перейдите на страницу [Настройки токенов Hugging Face](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo).
    - Выберите **New token** (Новый токен).
    - Введите **Name** (Название) проекта, который хотите использовать.
    - Выберите **Type** (Тип) — **Write** (Запись).

> [!NOTE]
> 
> Если вы сталкиваетесь со следующей ошибкой:
> 
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> Чтобы устранить её, введите следующую команду в вашем терминале.
> 
> ```bash
> sudo ldconfig
> ```

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Отказ от ответственности**:  
Этот документ был переведен с помощью сервисa автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на то, что мы стремимся к точности, имейте в виду, что автоматический перевод может содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется обращаться к услугам профессионального переводчика. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования данного перевода.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->