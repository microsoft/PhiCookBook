<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "006e8cf75211d3297f24e1b22e38955f",
  "translation_date": "2025-03-27T10:47:26+00:00",
  "source_file": "md\\02.Application\\01.TextAndChat\\Phi3\\E2E_Phi-3-mini_with_whisper.md",
  "language_code": "ru"
}
-->
# Интерактивный чат-бот Phi 3 Mini 4K Instruct с Whisper

## Обзор

Интерактивный чат-бот Phi 3 Mini 4K Instruct — это инструмент, который позволяет пользователям взаимодействовать с демонстрацией Microsoft Phi 3 Mini 4K Instruct с помощью текстового или голосового ввода. Чат-бот может использоваться для различных задач, таких как перевод, обновления погоды и сбор общей информации.

### Начало работы

Чтобы использовать этот чат-бот, выполните следующие шаги:

1. Откройте новый [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb)
2. В главном окне ноутбука вы увидите интерфейс чат-бота с текстовым полем ввода и кнопкой «Send».
3. Чтобы использовать чат-бот с текстовым вводом, просто введите сообщение в текстовое поле и нажмите кнопку «Send». Чат-бот ответит аудиофайлом, который можно воспроизвести прямо из ноутбука.

**Примечание**: Для работы этого инструмента требуется GPU и доступ к моделям Microsoft Phi-3 и OpenAI Whisper, которые используются для распознавания речи и перевода.

### Требования к GPU

Для запуска этой демонстрации требуется 12 ГБ памяти GPU.

Требования к памяти GPU для запуска демонстрации **Microsoft-Phi-3-Mini-4K Instruct** зависят от нескольких факторов, таких как размер входных данных (аудио или текст), язык перевода, скорость модели и доступная память на GPU.

В общем, модель Whisper разработана для работы на GPU. Рекомендуемое минимальное количество памяти GPU для запуска модели Whisper составляет 8 ГБ, но она может использовать больше памяти, если это необходимо.

Важно отметить, что обработка большого объема данных или высокого количества запросов на модели может потребовать больше памяти GPU и/или вызвать проблемы с производительностью. Рекомендуется протестировать ваш сценарий использования с различными конфигурациями и отслеживать использование памяти для определения оптимальных настроек для ваших конкретных нужд.

## Пример E2E для интерактивного чат-бота Phi 3 Mini 4K Instruct с Whisper

Ноутбук под названием [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) демонстрирует, как использовать демонстрацию Microsoft Phi 3 Mini 4K Instruct для генерации текста из аудио или текстового ввода. Ноутбук определяет несколько функций:

1. `tts_file_name(text)`: Эта функция генерирует имя файла на основе входного текста для сохранения созданного аудиофайла.
2. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: Эта функция использует API Edge TTS для генерации аудиофайла из списка фрагментов входного текста. Входные параметры: список фрагментов, скорость речи, имя голоса и путь для сохранения созданного аудиофайла.
3. `talk(input_text)`: Эта функция создает аудиофайл, используя API Edge TTS и сохраняет его с случайным именем файла в директории /content/audio. Входной параметр — текст для преобразования в речь.
4. `run_text_prompt(message, chat_history)`: Эта функция использует демонстрацию Microsoft Phi 3 Mini 4K Instruct для создания аудиофайла из входного сообщения и добавляет его в историю чата.
5. `run_audio_prompt(audio, chat_history)`: Эта функция преобразует аудиофайл в текст с использованием API модели Whisper и передает его в функцию `run_text_prompt()`.
6. Код запускает приложение Gradio, которое позволяет пользователям взаимодействовать с демонстрацией Phi 3 Mini 4K Instruct, вводя сообщения или загружая аудиофайлы. Вывод отображается как текстовое сообщение в приложении.

## Устранение неполадок

Установка драйверов Cuda GPU

1. Убедитесь, что ваше приложение Linux обновлено.

    ```bash
    sudo apt update
    ```

2. Установите драйверы Cuda.

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

3. Зарегистрируйте местоположение драйвера Cuda.

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

4. Проверка размера памяти Nvidia GPU (требуется 12 ГБ памяти GPU).

    ```bash
    nvidia-smi
    ```

5. Очистка кэша: Если вы используете PyTorch, вы можете вызвать torch.cuda.empty_cache(), чтобы освободить всю неиспользуемую кэшированную память, чтобы она могла быть использована другими приложениями GPU.

    ```python
    torch.cuda.empty_cache() 
    ```

6. Проверка Nvidia Cuda.

    ```bash
    nvcc --version
    ```

7. Выполните следующие шаги, чтобы создать токен Hugging Face.

    - Перейдите на страницу [настроек токенов Hugging Face](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo).
    - Выберите **New token**.
    - Введите **Name** проекта, который вы хотите использовать.
    - Выберите **Type** как **Write**.

> **Примечание**
>
> Если вы столкнулись с ошибкой:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> Чтобы устранить её, выполните следующую команду в терминале.
>
> ```bash
> sudo ldconfig
> ```

**Отказ от ответственности**:  
Этот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия обеспечить точность перевода, пожалуйста, учитывайте, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неверные интерпретации, возникающие в результате использования данного перевода.