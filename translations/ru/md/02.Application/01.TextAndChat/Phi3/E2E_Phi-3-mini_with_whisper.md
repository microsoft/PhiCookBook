<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "006e8cf75211d3297f24e1b22e38955f",
  "translation_date": "2025-05-07T14:10:39+00:00",
  "source_file": "md/02.Application/01.TextAndChat/Phi3/E2E_Phi-3-mini_with_whisper.md",
  "language_code": "ru"
}
-->
# Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper

## Обзор

Interactive Phi 3 Mini 4K Instruct Chatbot — это инструмент, который позволяет пользователям взаимодействовать с демонстрацией Microsoft Phi 3 Mini 4K Instruct с помощью текстового или голосового ввода. Чатбот можно использовать для различных задач, таких как перевод, получение прогноза погоды и сбор общей информации.

### Начало работы

Чтобы использовать этот чатбот, просто выполните следующие шаги:

1. Откройте новый [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb)
2. В главном окне ноутбука вы увидите интерфейс чатбокса с полем для ввода текста и кнопкой «Send».
3. Чтобы использовать текстовый чатбот, просто введите сообщение в поле для текста и нажмите кнопку «Send». Чатбот ответит аудиофайлом, который можно воспроизвести прямо внутри ноутбука.

**Note**: Для работы этого инструмента требуется GPU и доступ к моделям Microsoft Phi-3 и OpenAI Whisper, которые используются для распознавания речи и перевода.

### Требования к GPU

Для запуска этой демонстрации требуется 12 ГБ памяти GPU.

Объем памяти, необходимый для запуска демонстрации **Microsoft-Phi-3-Mini-4K instruct** на GPU, зависит от нескольких факторов: размера входных данных (аудио или текст), языка перевода, скорости модели и доступной памяти на GPU.

В целом, модель Whisper разработана для работы на GPU. Рекомендуемый минимум памяти GPU для запуска модели Whisper — 8 ГБ, но она может использовать и больше памяти при необходимости.

Важно учитывать, что обработка большого объема данных или большого количества запросов может потребовать больше памяти GPU и/или вызвать проблемы с производительностью. Рекомендуется протестировать ваш сценарий с разными настройками и отслеживать использование памяти, чтобы определить оптимальные параметры под ваши нужды.

## Пример E2E для Interactive Phi 3 Mini 4K Instruct Chatbot с Whisper

Jupyter ноутбук под названием [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) демонстрирует, как использовать Microsoft Phi 3 Mini 4K Instruct Demo для генерации текста из аудио или текстового ввода. В ноутбуке определены несколько функций:

1. `tts_file_name(text)`: Функция генерирует имя файла на основе входного текста для сохранения сгенерированного аудиофайла.
1. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: Функция использует Edge TTS API для создания аудиофайла из списка частей входного текста. Входные параметры — список частей, скорость речи, имя голоса и путь для сохранения аудиофайла.
1. `talk(input_text)`: Функция генерирует аудиофайл с помощью Edge TTS API и сохраняет его под случайным именем в директории /content/audio. Входной параметр — текст для преобразования в речь.
1. `run_text_prompt(message, chat_history)`: Функция использует демонстрацию Microsoft Phi 3 Mini 4K Instruct для генерации аудиофайла из входящего сообщения и добавляет его в историю чата.
1. `run_audio_prompt(audio, chat_history)`: Функция преобразует аудиофайл в текст с помощью API модели Whisper и передает результат в функцию `run_text_prompt()`.
1. Код запускает приложение Gradio, которое позволяет пользователям взаимодействовать с демонстрацией Phi 3 Mini 4K Instruct, вводя сообщения или загружая аудиофайлы. Результат отображается в виде текстового сообщения в приложении.

## Устранение неполадок

Установка драйверов Cuda GPU

1. Убедитесь, что ваше Linux-приложение обновлено

    ```bash
    sudo apt update
    ```

1. Установите драйверы Cuda

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. Зарегистрируйте расположение драйвера cuda

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. Проверка объема памяти Nvidia GPU (требуется 12 ГБ памяти GPU)

    ```bash
    nvidia-smi
    ```

1. Очистка кеша: если вы используете PyTorch, вызовите torch.cuda.empty_cache(), чтобы освободить всю неиспользуемую кэшированную память, чтобы её могли использовать другие приложения GPU

    ```python
    torch.cuda.empty_cache() 
    ```

1. Проверка Nvidia Cuda

    ```bash
    nvcc --version
    ```

1. Выполните следующие действия для создания токена Hugging Face.

    - Перейдите на страницу [Hugging Face Token Settings](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo).
    - Выберите **New token**.
    - Введите имя проекта (**Name**), которое хотите использовать.
    - Выберите **Type** — **Write**.

> **Note**
>
> Если вы столкнулись с ошибкой:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> Чтобы решить проблему, введите следующую команду в терминале.
>
> ```bash
> sudo ldconfig
> ```

**Отказ от ответственности**:  
Этот документ был переведен с помощью сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия по обеспечению точности, просим учитывать, что автоматический перевод может содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для критически важной информации рекомендуется использовать профессиональный перевод, выполненный человеком. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования данного перевода.