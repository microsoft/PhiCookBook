{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime_genai as og\n",
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = og.Model('..\\..\\..\\..\\Models\\Phi-3-mini-4k-instruct-onnx\\directml\\directml-int4-awq-block-128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = og.Tokenizer(model)\n",
    "tokenizer_stream = tokenizer.create_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_options = {\"max_length\": 1024,\"temperature\":0.6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = og.GeneratorParams(model)\n",
    "params.try_use_cuda_graph_with_max_batch_size(1)\n",
    "params.set_search_options(**search_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<|system|>You are a helpful AI assistant.<|end|><|user|>Can you introduce yourself?<|end|><|assistant|>\"\n",
    "input_tokens = tokenizer.encode(prompt)\n",
    "params.input_ids = input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generator = og.Generator(model, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Of course! I'm an AI developed by Microsoft, designed to assist and provide information to users like you. My purpose is to help answer your questions, provide guidance, and offer support in various areas such as general knowledge, research, and problem-solving. I'm powered by advanced machine learning algorithms, enabling me to understand and respond to your queries effectively. While I don't have personal experiences or emotions, I'm here to make your interactions as helpful and informative as possible. How can I assist you today?"
     ]
    }
   ],
   "source": [
    "while not generator.is_done():\n",
    "                generator.compute_logits()\n",
    "                generator.generate_next_token()\n",
    "\n",
    "                new_token = generator.get_next_tokens()[0]\n",
    "                print(tokenizer_stream.decode(new_token), end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\nഡിസ്ക്ലെയിമർ:\nഈ രേഖ AI വിവർത്തന സേവനം [Co-op Translator](https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് വിവർത്തനം ചെയ്തതാണ്. ഞങ്ങൾ കൃത്യത കൈവരിക്കാൻ ശ്രമിച്ചുകൊണ്ടിരിക്കുകയാണെങ്കിലും, യാന്ത്രിക (ഓട്ടോമേറ്റഡ്) വിവർത്തനങ്ങളിൽ പിശകുകളോ അപരിഷ്ക്കാരങ്ങളോ ഉണ്ടായിരിക്കാമെന്ന് ദയവായി ശ്രദ്ധിക്കുക. മൂല രേഖയുടെ സ്വദേശഭാഷയിലെ പതിപ്പിനെ അധികാരപരമായ സ്രോതസായി പരിഗണിക്കണം. നിർണ്ണായകമായ വിവരങ്ങളുടെ കാര്യത്തിൽ പ്രൊഫഷണൽ മനുഷ്യ വിവർത്തനം ശുപാർശ ചെയ്യപ്പെടുന്നു. ഈ വിവർത്തനത്തിന്റെ ഉപയോഗത്തിൽ നിന്നുണ്ടാകുന്ന ഏതെങ്കിലും തെറ്റിദ്ധാരണകളിലോ വ്യാഖ്യാനപരമായ പിശകുകളിലോ ഞങ്ങൾ ഉത്തരവാദികളല്ല.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "coopTranslator": {
   "original_hash": "016e5cff42352953a5e2cbb687ea247e",
   "translation_date": "2025-12-22T06:34:23+00:00",
   "source_file": "code/03.Inference/AIPC/AIPC_DirectML_DEMO.ipynb",
   "language_code": "ml"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}