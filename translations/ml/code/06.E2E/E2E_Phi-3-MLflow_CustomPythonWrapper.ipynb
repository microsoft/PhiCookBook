{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow-ൽ Phi-3 മോഡൽ ഓടിക്കൽ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "ഈ നോട്ട്ബുക്ക് Phi-3 mini 4K Instruct എന്ന ഉദാഹരണത്തിൽ ഒരു കസ്റ്റം Python റാപ്പറിന്റെ ഉപയോഗത്തെ വിശദീകരിക്കുന്നു.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import onnxruntime_genai as og"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi-3 Mini 4K മോഡലിനായി ഇഷ്‌ടാനുസൃത പൈത്തൺ ക്ലാസ് നിർവചിക്കൽ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining custom PythonModel class\n",
    "class Phi3Model(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        # Retrieving model from the artifacts\n",
    "        model_path = context.artifacts[\"phi3-mini-onnx\"]\n",
    "        model_options = {\n",
    "             \"max_length\": 300,\n",
    "             \"temperature\": 0.2,         \n",
    "        }\n",
    "    \n",
    "        # Defining the model\n",
    "        self.phi3_model = og.Model(model_path)\n",
    "        self.params = og.GeneratorParams(self.phi3_model)\n",
    "        self.params.set_search_options(**model_options)\n",
    "        \n",
    "        # Defining the tokenizer\n",
    "        self.tokenizer = og.Tokenizer(self.phi3_model)\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # Retrieving prompt from the input\n",
    "        prompt = model_input[\"prompt\"][0]\n",
    "        self.params.input_ids = self.tokenizer.encode(prompt)\n",
    "\n",
    "        # Generating the model's response\n",
    "        response = self.phi3_model.generate(self.params)\n",
    "\n",
    "        return self.tokenizer.decode(response[0][len(self.params.input_ids):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLFlow ആർട്ടിഫാക്ട് സൃഷ്ടിക്കുന്നു\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/18 14:17:34 INFO mlflow.models.utils: We convert input dictionaries to pandas DataFrames such that each key represents a column, collectively constituting a single row of data. If you would like to save data as multiple rows, please convert your data to a pandas DataFrame before passing to input_example.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c43e3ebd0d74c18997ec6bddf560e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generating the MLflow model with the custom Python model\n",
    "input_example = {\"prompt\": \"<|system|>You are a helpful AI assistant.<|end|><|user|>What is the capital of Spain?<|end|><|assistant|>\"}\n",
    "artifact_path = \"phi3_mlflow_model\"\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path = artifact_path,\n",
    "        python_model = Phi3Model(),\n",
    "        artifacts = {\n",
    "            \"phi3-mini-onnx\": \"cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4\",\n",
    "        },\n",
    "        input_example = input_example,\n",
    "        signature = infer_signature(input_example, [\"Run\"]),\n",
    "        extra_pip_requirements = [\"torch\", \"onnxruntime_genai\", \"numpy\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi-3നെ MLFlow മോഡലായി ഓടിക്കൽ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Phi-3 MLFlow model\n",
    "loaded_model = mlflow.pyfunc.load_model(\n",
    "    model_uri = model_info.model_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  ['prompt': string (required)]\n",
       "outputs: \n",
       "  [string (required)]\n",
       "params: \n",
       "  None"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking MLFlow model's signature\n",
    "loaded_model.metadata.signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Alright, here's a little atom-related joke for you!\n",
      "\n",
      "Why don't electrons ever play hide and seek with protons?\n",
      "\n",
      "Because good luck finding them when they're always \"sharing\" their electrons!\n",
      "\n",
      "Remember, this is all in good fun, and we're just having a little atomic-level humor!\n"
     ]
    }
   ],
   "source": [
    "# Testing the loaded model\n",
    "response = loaded_model.predict(\n",
    "    {\"prompt\": \"<|system|>You are a stand-up comedian.<|end|><|user|>Tell me a joke about atom<|end|><|assistant|>\",}\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\nഡിസ്ക്ലെയിമർ:\nഈ ഡോക്യുമെന്റ് AI വിവർത്തന സേവനമായ [Co-op Translator](https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് പരിഭാഷചെയ്തതാണ്. നാം കൃത്യതയ്ക്ക് ശ്രമിക്കുന്നുവെങ്കിലും, യാന്ത്രിക (ഓട്ടോമേറ്റഡ്) വിവർത്തനങ്ങളിൽ പിശകുകളും തړېകുറവുകളും ഉണ്ടാകാമെന്ന点 ദയവായി ശ്രദ്ധിക്കുക. മൂലഭാഷയിലെ ഒറിജിനൽ ഡോക്യുമെന്റ് പ്രാമാണിക സ്രോതസ്സായി പരിഗണിക്കണം. അത്യാവശ്യ വിവരങ്ങൾക്ക് പ്രൊഫഷണൽ മനുഷ്യ വിവർത്തനം ശിപാർശിക്കുന്നു. ഈ വിവർത്തനം ഉപയോഗിക്കുന്നതിൽ നിന്നുണ്ടാകുന്ന ഏതെങ്കിലും തെറ്റിദ്ധാരണകളുടെയും തെറ്റായ വ്യാഖ്യാനങ്ങളുടെയും ഉത്തരവാദിത്തം ഞങ്ങൾ ഏറ്റെടുക്കുന്നില്ല.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "d821cb8bd6a7041484fa62b88482b2fc",
   "translation_date": "2025-12-22T05:12:33+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-MLflow_CustomPythonWrapper.ipynb",
   "language_code": "ml"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}