<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-12-21T23:04:15+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "ml"
}
-->
# പ്രമേയമായി പരാമർശിച്ച പ്രധാന സാങ്കേതികവിദ്യകൾ

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12 ന്റെ മുകളിൽ നിർമ്മിച്ചിരിക്കുന്ന, ഹാർഡ്‌വെയർ-അസിസ്റ്റഡ് മെഷീൻ ലേണിംഗിനുള്ള ഒരു ലോ-ലെവൽ API.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia വികസിപ്പിച്ച രണ്ട് സാദൃശ്യമുള്ള കംപ്യൂട്ടിങ് പ്ലാറ്റ്ഫോം மற்றும் ആപ്‌ളിക്കേഷൻ പ്രോഗ്രാമിംഗ് ഇന്റർഫേസ് (API) മോഡൽ, GPUകളിൽ ജനറൽ-പർപ്പസ് പ്രോസസ്സിംഗ് സാധ്യമാക്കുന്നു.
3. [ONNX](https://onnx.ai/) (ഓപ്പൺ ന്യുറൽ നെറ്റ്‌വർക്ക് എക്സ്ചേഞ്ച്) - മെഷീൻ ലേണിംഗ് മോഡലുകൾ പ്രതിനിധീകരിക്കാൻ രൂപകൽപ്പന ചെയ്ത ഒരു തുറന്ന ഫോർമാറ്റ്, വിവിധ ML ഫ്രെയിംവർക്കുകൾക്കിടയിൽ ഇന്റർഓപ്പറബിലിറ്റി നൽകുന്നു.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (ജനറിക് ഗ്രാഫ് അപ്‌ഡേറ്റ് ഫോർമാറ്റ്) - മെഷീൻ ലേണിംഗ് മോഡലുകൾ പ്രതിനിധീകരിക്കുകയും അപ്‌ഡേറ്റ് ചെയ്യുകയും ചെയ്യുന്നതിനുള്ള ഒരു ഫോർമാറ്റ്; പ്രത്യേകിച്ച് 4-8 ബിറ്റ് ക്വാൻറൈസേഷൻ ഉപയോഗിച്ച് CPUs-ൽ ഫലപ്രദമായി പ്രവർത്തിക്കാവുന്ന ചെറിയ ഭാഷാ മോഡലുകൾക്കു ഏറെ പ്രയോജനകരമാണ്.

## DirectML

DirectML ഹാർഡ്‌വെയർ-അസിസ്റ്റഡ് മെഷീൻ ലേണിംഗ് സാധ്യമാക്കുന്ന ഒരു ലോ-ലെവൽ API ആണ്. GPU ആക്സിലറേഷൻ ഉപയോഗിക്കാൻ ഇത് DirectX 12 ന്റെ മുകളിൽ നിർമ്മിച്ചിരിക്കുകയാണ്, കൂടാതെ വ്യത്യസ്ത GPU വിതരണക്കാരുമായെല്ലാം പ്രവർത്തിക്കാൻ കോഡ് മാറ്റങ്ങൾ ആവശ്യമില്ല എന്ന നിലയിൽ vendor-agnostic ആണ്. ഇത് പ്രധാനമായും GPUകളിൽ മോഡൽ പരിശീലനത്തിനും ഇൻഫറൻസിംഗിനും ഉപയോഗിക്കുന്നു.

ഹാർഡ്വെയർ പിന്തുണയെ സംബന്ധിച്ച്, DirectML വിവിധതരം GPUകളുമായി പ്രവർത്തിക്കാൻ രൂപകൽപ്പന ചെയ്‌തതാണ്, ഇതിൽ AMD ഇന്റഗ്രേറ്റഡ് અને ഡിസ്ക്രീറ്റ് GPUകൾ, Intel ഇന്റഗ്രേറ്റഡ് GPUകൾ, NVIDIA ഡിസ്ക്രീറ്റ് GPUകൾ എന്നിവ ഉൾപ്പെടുന്നു. ഇത് Windows AI പ്ലാറ്റ്ഫോമിന്റെ ഭാഗമായി ആണ് ಮತ್ತು Windows 10 & 11 ൽ പിന്തുണയ്ക്കപ്പെടുന്നു, അതിലൂടെ ഏതെങ്കിലും Windows ഡിവൈസിൽ മോഡൽ പരിശീലനവും ഇൻഫറൻസും നടത്താൻ സാധ്യമാണ്.

DirectML-നെ സംബന്ധിച്ച ചില അപ്ഡേറ്റുകളും അവസരങ്ങളും ഉണ്ടായിട്ടുണ്ട്, ഉദാഹരണത്തിന് 150 ഓൺഎഎൻഎക്സ് ഓപ്പറേറ്ററുകൾ വരെ പിന്തുണയ്ക്കൽ എന്നിവയും ONNX runtime-നും WinML-നും ഉപയോഗിച്ചുവരുന്നത് പോലുള്ളവയും. ഇത് പ്രമുഖ ഇന്റഗ്രേറ്റഡ് ഹാർഡ്വെയർ വെണ്ടറുകൾ (IHVs) പിന്തുണയ്ക്കുന്നു, ഓരോന്നും വിവിധ മെറ്റാകമാൻഡുകൾ നടപ്പിലാക്കി വരുന്നു.

## CUDA

CUDA (Compute Unified Device Architecture) Nvidia എത്തിച്ചെടുത്ത ഒരു പരലൽ കംപ്യൂട്ടിങ് പ്ലാറ്റ്ഫോംയും API മോഡലും ആണ്. CUDA-സാധ്യമായ GPU ഉപയോഗിച്ച് സോഫ്റ്റ്വേർ ഡവലപ്പർമാർ ജനറൽ-പർപ്പസ് പ്രോസസ്സിംഗ് നടത്താൻ സാധിക്കുകയും ചെയ്യുന്നു — ഇതിനെ GPGPU (General-Purpose computing on Graphics Processing Units) എന്നാണ് വിളിക്കുന്നത്. CUDA Nvidia-യുടെ GPU ആക്സിലറേഷന്റെ പ്രധാന സഹായിയാണ്, മെഷീൻ ലേണിംഗ്, ശാസ്ത്രീയ കണക്കുകൂട്ടൽ, വീഡിയോ പ്രോസസിംഗ് തുടങ്ങിയ വിവിധ മേഖലയിലായാണ് വ്യാപകമായി ഉപയോഗിക്കുന്നത്.

CUDA-യ്ക്ക് ഹാർഡ്വെയർ പിന്തുണ Nvidia 的 GPUകളെ അടിസ്ഥാനമാക്കി പ്രത്യേകിച്ച് ഉണ്ടാവുന്നു, കാരണം ഇത് Nvidia വികസിപ്പിച്ച ഒരു പ്രോപ്രൈറ്ററി സാങ്കേതികവിദ്യയാണ്. ഓരോ ആർക്കിടെക്ചറും CUDA ടൂൾകിറ്റിന്റെ പ്രത്യേക പതിപ്പുകൾക്കാണ് പിന്തുണ നൽകുന്നത്, അവ ഡവലപ്പർമാർക്ക് CUDA ആപ്ലിക്കേഷനുകൾ നിർമ്മിക്കാൻ ആവശ്യമായ ലൈബ്രറികളും ടൂൾസും നൽകുന്നു.

## ONNX

ONNX (Open Neural Network Exchange) മെഷീൻ ലേണിംഗ് മോഡലുകൾ പ്രതിനിധീകരിക്കാൻ രൂപകൽപ്പന ചെയ്ത ഒരു തുറന്ന ഫോർമാറ്റാണ്. ഇത് വിപുലീകരിക്കാവുന്ന കംപ്യൂട്ടേഷൻ ഗ്രാഫ് മോഡലിന്റെ നിർവചനവും, തയ്യാറാക്കിയ ഇൻ-বിൽട്ട് ഓപ്പറേറ്ററുകളുടെ നിർവചനങ്ങളുമും സ്റ്റാൻഡേർഡ് ഡാറ്റാ ടൈപ്പുകളുടെ നിർവചനങ്ങളുമാണ് നൽകുന്നത്. ONNX ഡെവലപ്പർമാർക്ക് മോഡലുകൾ വിവിധ ML ഫ്രെയിംവർക്കുകളിലായി മാറ്റാൻ അനുവദിച്ച് ഇന്റർഓപ്പറബിലിറ്റിയും AI ആപ്ലിക്കേഷനുകൾ സൃഷ്ടിക്കുകയും ഡിപ്ലോയ് ചെയ്യുകയും ചെയ്യാൻ എളുപ്പമാക്കുന്നു.

Phi3 mini ONNX Runtime ഉപയോഗിച്ച് CPU യിലും GPU യിലും വ്യത്യസ്ഥ ഡിവൈസുകളിൽ പ്രവർത്തിക്കാം, സർക്കുലർ പ്ലാറ്റ്ഫോമുകൾ ഉൾപ്പെടെ Windows, Linux, Mac ഡെസ്‌ക്ടോപ്പുകൾ, മൊബൈൽ CPUകൾ എന്നിവയ്ക്കു മൈത്രീയമാണ്.
ഞങ്ങൾ ചേർത്ത tốiേപ്പെടുത്തിയ കോൺഫിഗറേഷനുകൾ ആണ്

- ONNX models for int4 DML: AWQ മുഖാന്തിരം int4 ആയി ക്വാൻറൈസ് ചെയ്തത്
- ONNX model for fp16 CUDA
- ONNX model for int4 CUDA: RTN മുഖാന്തിരം int4 ആയി ക്വാൻറൈസ് ചെയ്തത്
- ONNX model for int4 CPU and Mobile: RTN മുഖാന്തിരം int4 ആയി ക്വാൻറൈസ് ചെയ്തത്

## Llama.cpp

Llama.cpp C++യില്‍ എഴുതപ്പെട്ട ഒരു ഓപ്പൺ-സോഴ്‌സ് സോഫ്‌റ്റ്‌വെയർ ലൈബ്രറിയാണ്. ഇത് Llama ഉൾപ്പെടെയുള്ള വിവിധ വലുതായായ ഭാഷാ മോഡലുകളിൽ ഇൻഫറൻസ് നടത്തുന്നു. ജനറൽ-പർപ്പസ് ടെൻസർ ലൈബ്രറിയായ ggml നൊപ്പം വികസിപ്പിച്ചെടുത്ത Llama.cpp মূল Python നടപ്പിലാക്കലിനേക്കാൾ വേഗത്തിൽ ഇൻഫറൻസ് നടത്താനും കുറഞ്ഞ മെമ്മറി ഉപയോഗിക്കാനുമാണ് ലക്ഷ്യം. ഇത് ഹാർഡ്‌വെയർ ഓപ്റ്റിമൈസേഷൻ, ക്വാന്തൈസേഷൻ എന്നിവയ്ക്കും പിന്തുണ നൽകുന്നു, കൂടാതെ ഒരു ലളിതമായ APIയും examples3 പോലുള്ള ഉദാഹരണങ്ങളും നൽകുന്നു. ഫലപ്രദമായ LLM ഇൻഫറൻസിൽ ആകർഷണമുണ്ടെങ്കിൽ, Phi3-ൽ Llama.cpp പ്രവർത്തിക്കാനാകുന്നുണ്ടെന്നതിനാൽ Llama.cpp പരിശോധിക്കാൻ വാക്കുതീരും.

## GGUF

GGUF (ജനറിക് ഗ്രാഫ് അപ്‌ഡേറ്റ് ഫോർമാറ്റ്) മെഷീൻ ലേണിംഗ് മോഡലുകൾ പ്രതിനിധീകരിക്കുകയും അപ്‌ഡേറ്റ് ചെയ്യുകയും ചെയ്യുന്നതിനുള്ള ഒരു ഫോർമാറ്റ് ആണ്. ഇത് சிறുചുരുക്കമുള്ള ഭാഷാ മോഡലുകൾക്കായി പ്രത്യേകിച്ച് പ്രയോജനകരമാണ്, അവ 4-8 ബിറ്റ് ക്വാൻറൈസേഷൻ ഉപയോഗിച്ച് CPUs-ൽ ഫലപ്രദമായി പ്രവർത്തിക്കാറുണ്ട്. GGUF മാർഗ്ഗം റാപ്പിഡ് പ്രോട്ടോട്ടൈപ്പിംഗിനും എഡ്ജ് ഡിവൈസുകളിലോ CI/CD പോലുള്ള ബാച്ച് ജോബുകളിലോ മോഡലുകൾ ഓടിക്കുന്നതിനും ഉപകരിച്ചു പോകുന്നു.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
പ്രത്യാഖ്യാനം:
ഈ രേഖ AI പരിഭാഷാ സേവനമായ Co‑op Translator (https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് വിവർത്തനം ചെയ്തതാണ്. ഞങ്ങൾ കൃത്യതയ്ക്ക് പരിശ്രമിച്ചാൽ പോലും, സ്വയംചാലിതമായ പരിഭാഷകളിൽ പിശകുകളോ അസത്യതകളോ ഉണ്ടായിരിക്കാമെന്നതിൽ ദയവായി ശ്രദ്ധിക്കുക. മൗലികരൂപം അതിന്റെ മാതൃഭാഷയിലുള്ളതെന്തെന്നത് അധികൃത സ്രോതസ്സായി കണക്കാക്കണം. പ്രധാനപ്പെട്ട വിവരങ്ങളെ ബന്ധപ്പെട്ടാണ് പ്രൊഫഷണൽ മാനവപരിഭാഷ നിർദ്ദേശിക്കുന്നത്. ഈ പരിഭാഷ ഉപയോഗിച്ചതിൽ നിന്നുണ്ടാകുന്ന ഏതൊരു തെറ്റിദ്ധാരണത്തിനോ തെറ്റായ വ്യാഖ്യാനത്തിനോ ഞങ്ങൾ ഉത്തരവാദികളല്ല.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->