# Phi-3 ലോക്കലായി ആരംഭിക്കുക

ഈ ഗൈഡ് Ollama ഉപയോഗിച്ച് Phi-3 മോഡൽ പ്രവർത്തിപ്പിക്കാൻ നിങ്ങളുടെ ലോക്കൽ പരിസ്ഥിതി ക്രമീകരിക്കാൻ സഹായിക്കും. മോഡൽ GitHub Codespaces, VS Code Dev Containers, അല്ലെങ്കിൽ നിങ്ങളുടെ ലോക്കൽ പരിസ്ഥിതി ഉൾപ്പെടെ ചില വ്യത്യസ്ത മാർഗങ്ങളിലാണ് നിങ്ങൾക്ക് പ്രവർത്തിപ്പിക്കാനാകുന്നത്.

## പരിസ്ഥിതി ക്രമീകരണം

### GitHub Codespaces

GitHub Codespaces ഉപയോഗിച്ചുകൊണ്ട് നിങ്ങൾ ഈ ടെംപ്ലേറ്റ് വെർച്ച്വലായി പ്രവൃത്തിപ്പിക്കാം. ബട്ടൺ നിങ്ങളുടെ ബ്രൗസറിലെ വെബ്-ബേസ്ഡ് VS Code ഇൻസ്റ്റൻസ് തുറക്കും:

1. ടෙംപ്ലേറ്റ് തുറക്കുക (ഇത് ചില മിനിറ്റുകൾ എടുക്കാം):

    [![GitHub Codespaces-ൽ തുറക്കുക](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. ഒരു ടർമിനൽ വിൻഡോ തുറക്കുക

### VS Code Dev Containers

⚠️ ഈ ഓപ്ഷൻ Docker Desktop-ന് കുറഞ്ഞത് 16 GB RAM നീക്കമേറ്റിരിക്കണമെന്ന് മാത്രം പ്രവർത്തിക്കും. നിങ്ങളുടെ কাছে 16 GB-ൽ കുറവ് RAM ഉണ്ടെങ്കിൽ, നിങ്ങൾക്ക് [GitHub Codespaces ഓപ്ഷൻ](../../../../../md/01.Introduction/01) ശ്രമിക്കാവുന്നതാണ് അല്ലെങ്കിൽ [ലോക്കലായി സജ്ജീകരിക്കുക](../../../../../md/01.Introduction/01).

സംബന്ധപ്പെടുന്ന ഒരു ഓപ്ഷൻ VS Code Dev Containers ആണ്, ഇത് നിങ്ങളുടെ ലോക്കൽ VS Code-ൽ പ്രോജക്ട് തുറക്കാൻ [Dev Containers വിപുലീകരണം](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) ഉപയോഗിക്കും:

1. Docker Desktop ആരംഭിക്കുക (ഇതുവരെ ഇൻസ്റ്റാൾ ചെയ്തിട്ടില്ലെങ്കിൽ ഇൻസ്റ്റാൾ ചെയ്യുക)
2. പ്രോജക്ട് തുറക്കുക:

    [![Dev Containers-ൽ തുറക്കുക](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. തുറക്കുന്ന VS Code വിൻഡോയിൽ, പ്രോജക്ട് ഫയലുകൾ പ്രദർശിതമായതിനു ശേഷം (ഇത് ചില മിനിറ്റുകൾ എടുക്കാം), ഒരു ടർമിനൽ വിൻഡോ തുറക്കുക.
4. ഡെപ്ലോയ്‌മെന്റ് ഘട്ടങ്ങളുമായി തുടരുക: [deployment steps](../../../../../md/01.Introduction/01)

### ലോക്കൽ പരിസ്ഥിതി

1. താഴെപ്പറയുന്ന ടൂളുകൾ ഇൻസ്റ്റാൾ ചെയ്തിട്ടുണ്ടെന്ന് ഉറപ്പാക്കുക:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## മോഡൽ പരീക്ഷിക്കുക

1. Ollama-നെ phi3:mini മോഡൽ ഡൗൺലോഡ് ചെയ്ത് പ്രവർത്തിപ്പിക്കാൻ അപേക്ഷിക്കുക:

    ```shell
    ollama run phi3:mini
    ```

    മോഡൽ ഡൗൺലോഡ് ചെയ്യാൻ ഇതിന് ചില മിനിറ്റുകൾ എടുക്കും.

2. ഔട്ട്‌പുട്ടിൽ "success" എന്ന് കാണാനുടൻ, നിങ്ങൾ പ്രോംപ്റ്റിൽ നിന്നുള്ള ആ മോഡലിന് സന്ദേശം അയക്കാം.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. ചില സെക്കൻഡുകൾ കഴിഞ്ഞാൽ, മോഡലിൽ നിന്ന് ഒരു റെസ്‌പോൺസ് സ്ട്രീം കാണാനാകും.

4. ഭാഷാ മോഡലുകളുമായി ഉപയോഗിക്കുന്ന വിവിധ സാങ്കേതികവിദ്യകൾ പഠിക്കാൻ, Python നോട്ട്‌ബുക്ക് [ollama.ipynb](../../../code/01.Introduce/ollama.ipynb) തുറക്കുകയും ഓരോ സെല്ലും പ്രവർത്തിപ്പിക്കുകയും ചെയ്യുക. നിങ്ങൾ 'phi3:mini' അല്ലാത്ത മറ്റൊരു മോഡൽ ഉപയോഗിച്ചിരുന്നെങ്കിൽ, ആദ്യ സെല്ലിലുള്ള `MODEL_NAME` മാറ്റിക്കുക.

5. Python-ൽ നിന്ന് phi3:mini മോഡലുമായി സംഭാഷണം നടത്താൻ, Python ഫയൽ [chat.py](../../../../../code/01.Introduce/chat.py) തുറക്കുക এবং അത് റൺ ചെയ്യുക. ആവശ്യത്തിനു ഫയലിന്റെ മുകളിൽ ഉള്ള `MODEL_NAME` മാറ്റാം, കൂടാതെ സിസ്റ്റം മെസ്സേജ് മാറ്റുകയോ few-shot ഉദാഹരണങ്ങൾ ചേർക്കുകയോ ചെയ്യാവുന്നതാണ്.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
ഡിസ്‌ക്ലെയിമർ:
ഈ രേഖ AI വിവർത്തനസേവനമായ [Co-op Translator](https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് വിവർത്തനം ചെയ്തതാണ്. നാം കൃത്യതയ്ക്കായി ശ്രമിച്ചിട്ടുണ്ടെങ്കിലും, യന്ത്രപരിഭാഷയിൽ തെറ്റുകൾ അല്ലെങ്കിൽ അസാധുതകൾ ഉണ്ടാകാമെന്ന് ദയവായി ശ്രദ്ധിക്കുക. മാതൃഭാഷയിലുള്ള മൂല രേഖ അധികാരപരമായ ഉറവിടമെന്ന നിലയിൽ പരിഗണിക്കപ്പെട്ടുകൂടതാണ്. നിർണ്ണായകമായ വിവരങ്ങൾക്ക് പ്രൊഫഷണൽ മനുഷ്യപരിഭാഷ ശുപാർശ ചെയ്യുന്നു. ഈ പരിഭാഷ ഉപയോഗിക്കുന്നതിൽ നിന്നുണ്ടാകുന്ന ഏതെങ്കിലും തെറ്റിദ്ധകരണങ്ങളിലോ വ്യാഖ്യാനഭ്രംസങ്ങളിലോ ഞങ്ങൾക്ക് ഉത്തരവാദിത്വം ഇല്ല.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->