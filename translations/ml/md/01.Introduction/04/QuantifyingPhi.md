<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-12-22T01:52:02+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "ml"
}
-->
# **Phi കുടുംബത്തിന്റെ അളക്കൽ**

model quantization എന്നത് ന്യൂറൽ നെറ്റ്‌വര്‍ക്ക് മോഡലിലെ പാരാമീറ്ററുകള്‍ (വെയ്റ്റുകള്‍ மற்றும் ആക്ടിവേഷന്‍ മൂല്യങ്ങള്‍ പോലുള്ളവ) വലിയ മൂല്യപരിധിയില്‍ നിന്ന് (സാധാരണയായി ഒരു തുടർച്ചയായ മൂല്യപരിധി) ചെറിയ ഫിനിറ്റ് മൂല്യപരിധിയിലേക്ക് മാപ്പുചെയ്യുന്ന പ്രക്രിയയെ സൂചിപ്പിക്കുന്നു. ഈ സാങ്കേതിക വിഷയം മോഡലിന്റെ വലുപ്പവും കംപ്യൂട്ടേഷണല്‍സങ്കീര്‍ണ്ണതയും കുറയ്ക്കാനും മൊബൈല്‍ ഡിവൈസുകള്‍ അല്ലെങ്കില്‍ എംബെഡഡ് സിസ്റ്റങ്ങള്‍ പോലുള്ള വിഭവ-പരിമിത ഉള്ള പരിസരങ്ങളില്‍ മോഡലിന്റെ പ്രവര്‍ത്തനക്ഷമത മെച്ചപ്പെടുത്താനും സഹായിക്കുന്നു. മോഡല്‍ ക്വാണ്ടൈസേഷന്‍ പാരാമീറ്ററുകളുടെ_PRECISION_ കുറയ്ക്കുന്ന വഴി കംപ്രഷന്‍ നേടുന്നു, എന്നാല്‍ ഇത് ഒരു നിശ്ചിത_PRECISION_ നഷ്ടവും ഉണ്ടാവാക്കും. അതിനാല്‍ ക്വാണ്ടൈസേഷന്‍ പ്രക്രിയയില്‍ മോഡല്‍ വലുപ്പം, കംപ്യൂട്ടേഷണല്‍ സങ്കീര്‍ണ്ണത,PRECISION എന്നിവ തമ്മില്‍ ബാലന്‍സ് ചെയ്യേണ്ടതുണ്ട്. സാധാരണ ക്വാണ്ടൈസേഷന്‍ നിയമഗതികള്‍ക്ക് ഫിക്‌സഡ് പോയിന്റ് ക്വാന്തൈസേഷന്‍, ഫ്ലോട്ടിങ്ങ് പോയിന്റ് ക്വാണ്ടൈസേഷന്‍ എന്നിവ ഉള്‍പ്പെടുന്നു. പ്രത്യേക സന്നിവേശങ്ങള്‍ക്കും ആവശ്യകതകള്‍ക്കും അനുസരിച്ച് അനുയോജ്യമായ ക്വാണ്ടൈസേഷന്‍ രണതന്ത്രം തിരഞ്ഞെടുക്കാവുന്നതാണ്.

ഞങ്ങള്‍ GenAI മോഡലുകള്‍ എഡ് ഡിവൈസുകളില്‍ വിന്യസിച്ചു കൂടുതല്‍ ഉപകരണങ്ങള്‍ GenAI ദൃശ്യങ്ങളില്‍ പ്രവേശിക്കട്ടെയെന്നതാണ് ആഗ്രഹം — ഉദാഹരണത്തിന് മൊബൈല്‍ ഡിവൈസുകള്‍, AI PC/Copilot+PC, പരമ്പരാഗത IoT ഉപകരണങ്ങള്‍ എന്നിവ. ക്വാണ്ടൈസുചെയ്ത മോഡലുകള്‍ ഉപയോഗിച്ച്, വ്യത്യസ്ത ഉപകരണങ്ങളുടെ അടിസ്ഥാനത്തില്‍ erine എഡ് ഡിവൈസുകളിലേക്ക് നാം വിന്യസിക്കാവുന്നതാണ്. ഹാര്‍ഡ്‌വെയര്‍ നിര്‍മ്മാതാക്കള്‍ നല്‍കുന്ന മോഡല്‍ അതിവേഗപ്പെടുത്തല്‍ ഫ്രെയിംവര്‍ക്കും ക്വാണ്ടൈസുചെയ്ത മോഡല്‍ കൂടിച്ച് നന്നായ SLM അപേക്ഷാ ദൃശ്യങ്ങള്‍ നിര്‍മിക്കാം.

ക്വാണ്ടൈസേഷന്‍ സാഹചര്യത്തില്‍ ഞങ്ങള്‍ക്കു വ്യത്യസ്ത_PRECISION_ ഗണ്യങ്ങളുണ്ട് (INT4, INT8, FP16, FP32). താഴെ പതിവായി ഉപയോഗിക്കുന്ന ക്വാണ്ടൈസേഷന്‍_PRECISION_കളുടെ വിശദീകരണം കൊടുക്കുന്നു

### **INT4**

INT4 ക്വാണ്ടൈസേഷന്‍ ഒരു തീവ്രമായ ക്വാന്തൈസേഷന്‍ രീതി ആണ്, മോഡലിന്റെ വെയ്റ്റുകളും ആക്ടിവേഷന്‍ മൂല്യങ്ങളും 4-ബിറ്റ് ഇന്റേജറുകളിലാക്കി ക്വാന്തൈസ് ചെയ്യുന്നു. പ്രതിനിധാന പരിധി ചെറുതായും_PRECISION_ കുറഞ്ഞതായും ഉള്ളതിനാല്‍ INT4 ക്വാണ്ടൈസേഷന്‍ സാധാരണയായി വലിയ_PRECISION_ നഷ്ടം ഉണ്ടാക്കും. എന്നിരുന്നാലും, INT8 താരതമ്യത്തില്‍ കണക്കെടുപ്പില്‍ INT4 ക്വuantൈസേഷന്‍ മോഡലിന്റെ സംഭരണം ആവശ്യകതകളും കംപ്യൂട്ടേഷണല്‍ സങ്കീര്‍ണ്ണതയും കൂടുതല്‍ കുറക്കാന്‍ സഹായിക്കുന്നു. ശ്രദ്ധിക്കേണ്ടത്, പ്രായോഗിക ഉപയോഗത്തില്‍ INT4 ക്വാണ്ടൈസേഷന്‍ 상대적으로 അപൂര്‍വ്വമാണ്, കാരണം അതിനുള്ള_PRECISION_ വളരെ താഴ്ന്നാല്‍ മോഡല്‍ പ്രകടനത്തില്‍ വലിയ വീഴ്ച വന്നേക്കാം. കൂടാതെ, എല്ലാ ഹാര്‍ഡ്‌വെയരും INT4 ഓപ്പറേഷനുകള്‍ പിന്തുണക്കുന്നില്ല; അതിനാല്‍ ക്വuantൈസേഷന്‍ രീതി തിരഞ്ഞെടുക്കുമ്പോള്‍ ഹാര്‍ഡ്‌വെയര്‍ അനുയോജ്യത പരിഗണിക്കേണ്ടതാണ്.

### **INT8**

INT8 ക്വuantൈസേഷന്‍ എന്നത് ഒരു മോഡലിന്റെ വെയ്റ്റുകളും ആക്ടിവേഷനുകളും ഫ്ലോട്ടിംഗ് പോയിന്റ് സംഖ്യകളില്‍ നിന്ന് 8-ബിറ്റ് ഇന്റേജറുകളിലായി മാറ്റുന്ന പ്രക്രിയയാണ്. INT8 ഇന്റേജറുകള്‍ പ്രതിനിധാനം ചെയ്യുന്ന സംഖ്യപരിധി ചെറിയതും കുറച്ച്_PRECISION_ഉം ആയിട്ടുള്ളതായിട്ടുണ്ടെങ്കിലും, ഇത് സംഭരണം மற்றும் കണക്കെടുപ്പ് ആവശ്യകതകള്‍ ഗണ്യമായി കുറക്കാന്‍ സഹായിക്കുന്നു. INT8 ക്വuantൈസേഷനില്‍, മോഡലിന്റെ വെയ്റ്റുകളെയും ആക്ടിവേഷന്‍ മൂല്യങ്ങളെയും സ്‌കെയിലിംഗ്, ഓഫ്സെറ്റ് എന്നിവ ഉള്‍പ്പെടുന്ന ഒരു ക്വuantൈസേഷന്‍ പ്രക്രിയയിലൂടെ കടന്നു اصل ഫ്ലോട്ടിംഗ് പോയിന്റ് വിവരങ്ങള്‍ όσο കഴിയുമത്രയും സംരക്ഷിക്കുന്നു. ഇന്‍ഫറന്‍സിനിടെ, ഈ ക്വാറ്റൈസ്ഡ് മൂല്യങ്ങള്‍ വീണ്ടും ഫ്ലോട്ടിംഗ് പോയിന്റ് സംഖ്യകളിലാക്കി കണക്കെടുപ്പിന് ഉപയോഗിച്ചശേഷം അടുത്ത ഘട്ടത്തിനായി വീണ്ടും INT8 ആയി ക്വuantൈസ് ചെയ്യപ്പെടും. ഈ രീതി പല ആവശ്യകതകളിലും മതിയായ_PRECISION_ നല്‍കുമ്പോഴും ഉയര്‍ന്ന കംപ്യൂട്ടേഷണല്‍ കാര്യക്ഷമത നിലനിർത്തുന്നു.

### **FP16**

FP16 ഫോര്‍മാറ്റ്, അഥവാ 16-ബിറ്റ് ഫ്ലോട്ടിംഗ് പോയിന്റ് സംഖ്യകള്‍ (float16), 32-ബിറ്റ് ഫ്ലോട്ടിംഗ് പോയിന്റ് സംഖ്യകള്‍ (float32) നെ അപേക്ഷിച്ച് മെമ്മറി ശബളഭാരത്തിന്റെ അരികിലേക്കു കുറയ്ക്കുന്നു, ഇത് വലിയ-തത്വത്തിലുള്ള ഡീപ് ലേണിംഗ് അപേക്ഷകളില്‍ ഗണ്യമായ ലാഭം നല്‍കുന്നു. FP16 ഫോര്‍മാറ്റ് സമാന GPU മെമ്മറി പരിമിതികളിൽ കൂടുതലായി മോഡലുകൾ ലോഡ് ചെയ്യാനും അല്ലെങ്കിൽ കൂടുതൽ ഡേറ്റ പ്രോസസ്സ് ചെയ്യാനും അനുവദിക്കുന്നു. ആധുനിക GPU ഹാർഡ്‌വെയർ FP16 ഓപ്പറേഷനുകൾ പിന്തുണയ്‌ക്കുന്നതോടെ, FP16 ഫോര്‍മാറ്റിന്റെ ഉപയോഗം കമ്പ്യൂട്ടിംഗ് വേഗതയിൽ മെച്ചം കൊണ്ടുവരാം. എന്നിരുന്നാലും, FP16 ഫോര്‍മാറ്റിന് സ്വന്തം ചില അപകീയതകളുണ്ട് — അത്_PRECISION_ കുറവായതിനാല്‍ ചില സാഹചര്യങ്ങളിൽ സംഖ്യാത്മക അസ്ഥിരതയോ_PRECISION_ നഷ്ടമോ ഉണ്ടാകാം.

### **FP32**

FP32 ഫോര്‍മാറ്റ് ഉയർന്ന_PRECISION_ നല്‍കുന്നു און വ്യാപകമായ മൂല്യങ്ങള്‍ നಿಖരമായി പ്രതിനിധീകരിക്കാന്‍ കഴിയും. സങ്കീര്‍ണ്ണ ഗണിതീയ ഒപറേഷനുകള്‍ നടത്തപ്പെടുന്ന സാഹചര്യങ്ങളിലോ ഉയർന്ന_PRECISION_ ഫലങ്ങള്‍ ആവശ്യമുണ്ടായിരിക്കുമ്പോള്‍ FP32 ഫോര്‍മാറ്റ് ചിത്രംമാകും. എന്നിരുന്നാലും, ഉയർന്ന_PRECISION_ കൂടിയിരിക്കുന്നതിന്റെ ഫലമായി മെമ്മറി ഉപയോഗവും കണക്കെടുപ്പ് സമയംയും കൂടും. വലിയ-തത്വ ഡീപ് ലേണിംഗ് മോഡലുകള്‍ especially മിക്കം മോഡല്‍ പാരാമീറ്ററുകളും വിപുലമായ ഡാറ്റയും ഉള്ളപ്പോള്‍ FP32 ഫോര്‍മാറ്റ് GPU മെമ്മറി അപര്യാപ്തതയും ഇന്‍ഫറന്‍സ് വേഗത കുറയലും ഉണ്ടാക്കാം.

മൊബൈല്‍ ഉപകരണങ്ങളിലോ IoT ഉപകരണങ്ങളിലോ Phi-3.x മോഡലുകള്‍ ഞങ്ങള്‍ INT4 ആയി മാറ്റാം, AI PC / Copilot PC പോലുള്ളവയ്ക്ക് INT8, FP16, FP32 പോലുള്ള ഉയർന്ന_PRECISION_ ഉപയോഗിക്കാം.

ഇപ്പോള്‍ വ്യത്യസ്ത ഹാര്‍ഡ്‍വെയര്‍ നിര്‍മ്മാതാക്കള്‍ക്ക് ജനേറേറ്റീവ് മോഡലുകള്‍ക്ക് പിന്തുണ നല്‍കുന്ന വ്യത്യസ്ത ഫ്രെയിംവര്‍ക്കുകൾ ഉണ്ട്, ഉദാഹരണത്തിന് Intel's OpenVINO, Qualcomm's QNN, Apple's MLX, Nvidia's CUDA എന്നിവ, ഇങ്ങനെ മോഡല്‍ ക്വuantൈസേഷന്‍ കൂടിച്ച് ലോക്കല്‍ വിന്യാസം പൂർത്തിയാക്കാം.

ടెక്നോളജിയുടെ ദിശയില്‍, ക്വuantൈസേഷന്റെ ശേഷമുള്ള формат പിന്തുണയ്ക്കലിന്റെ വ്യത്യാസങ്ങള്‍ ഉണ്ട്, ഉദാഹരണത്തിന് PyTorch / Tensorflow format, GGUF, ONNX എന്നിവ. GGUF ഉം ONNX ഉം തമ്മിലുള്ള ഫോര്‍മാറ്റ് താരതമ്യവും അപേക്ഷാ ദൃശ്യങ്ങളും ഞാൻ ചെയ്തിട്ടുണ്ട്. ഇവിടെ മോഡല്‍ ഫ്രെയിംവര്‍ക്കില്‍നിന്നും ഹാര്‍ഡ്‌വെയര്‍ വരെ മികച്ച പിന്തുണ ലഭിക്കുന്നതിനാല്‍ ONNX ക്വuantൈസേഷന്‍ ഫോര്‍മാറ്റ് ഞാന്‍ ശുപാര്‍ശ ചെയ്യുന്നു. ഈ അധ്യായത്തില്‍ നാം GenAIക്കായി ONNX Runtime, OpenVINO, Apple MLX എന്നിവ ഉപയോഗിച്ച് മോഡല്‍ ക്വuantൈസേഷൻ ചെയ്യുന്നത് üzerine ഊന്നി പ്രവര്‍ത്തിക്കും (നിങ്ങള്‍ക്ക് നല്ല ഒരു വഴിയെങ്കിൽ, PR സമര്‍പ്പിച്ച് അതേ намવાથી ഞങ്ങളോടൊപ്പം പങ്കുവെക്കാവുന്നതാണ്)

**ഈ അധ്യായത്തില്‍ ചേരുന്നത്**

1. [llama.cpp ഉപയോഗിച്ച് Phi-3.5 / 4 ക്വuantൈസ് ചെയ്യൽ](./UsingLlamacppQuantifyingPhi.md)

2. [Generative AI extensions కోసం onnxruntime ഉപയോഗിച്ച് Phi-3.5 / 4 ക്വuantൈസ് ചെയ്യൽ](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO ഉപയോഗിച്ച് Phi-3.5 / 4 ക്വuantൈസ് ചെയ്യൽ](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX Framework ഉപയോഗിച്ച് Phi-3.5 / 4 ക്വuantൈസ് ചെയ്യൽ](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
അസ്വീകാരം:
ഈ രേഖ AI വിവർത്തന സേവനം [Co-op Translator](https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് വിവർത്തനമാക്കിയതാണ്. നാം കൃത്യതയ്ക്ക് പരിശ്രമിച്ചിട്ടുണ്ടെങ്കിലും, ഓട്ടോമാറ്റഡ് വിവർത്തനങ്ങളിൽ പിശകുകൾ അല്ലെങ്കിൽ അപൃത്യതകൾ ഉണ്ടായിരിക്കാൻ സാധ്യതയുണ്ടെന്ന് ദയവായി ശ്രദ്ധിക്കുക. മൂല രേഖ അതിന്റെ മാതൃഭാഷയിൽ ഉള്ള പതിപ്പേയാണ് അധികാരപരമായ ഉറവിടം എന്ന് പരിഗണിക്കുക. നിർണായകമായ വിവരങ്ങൾക്ക് പ്രൊഫഷണൽ മനുഷ്യ വിവർത്തനം ശുപാർശ ചെയ്യപ്പെടുന്നു. ഈ വിവർത്തനത്തിന്റെ ഉപയോഗത്തിൽ നിന്നുണ്ടാകുന്ന ഏതെങ്കിലും തെറ്റിദ്ധാരണകളിലോ തെറ്റായ വ്യാഖ്യാനങ്ങളിലോ ഞങ്ങൾ ഉത്തരവാദികളല്ല.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->