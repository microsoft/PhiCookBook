<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "462bddc47427d8785f3c9fd817b346fe",
  "translation_date": "2025-12-22T01:59:07+00:00",
  "source_file": "md/01.Introduction/04/UsingLlamacppQuantifyingPhi.md",
  "language_code": "ml"
}
-->
# **llama.cpp ഉപയോഗിച്ച് Phi കുടുംബത്തെ ക്വാണ്ടൈസിംഗ്**

## **llama.cpp എന്താണ്**

llama.cpp C++ൽ പ്രധാനമായും എഴുതിയിട്ടുള്ള ഒരു ഓപ്പൺ-സോഴ്‌സ് സോഫ്റ്റ്‌വെയർ ലൈബ്രറിയാണ്, Llama പോലുള്ള വിവിധ വലിയ ഭാഷാ മോഡലുകളിൽ (LLMs) ഇൻഫെറൻസ് നടത്തുന്നത്. കുറഞ്ഞ സജ്ജീകരണത്തോടെ വ്യത്യസ്ത ഹാർഡ്‌വെയറുകളിൽ LLM ഇൻഫെറൻസിനുള്ള ആധുനിക പ്രകടനം നൽകുക എന്നതായിരുന്നു അതിന്റെ പ്രധാന ലക്ഷ്യം. കൂടാതെ, ഈ ലൈബ്രറിയ്ക്ക് വേണ്ടി Python ബൈൻഡിങുകളും ലഭ്യമാണ്, അവ ടെക്സ്റ്റ് പൂർത്തീകരണത്തിനുള്ള ഉയർന്ന നിരവേദ്യ APIയും OpenAI-സഹജമായ ഒരു വെബ് സെർവറും നൽകുന്നു.

llama.cppന്റെ പ്രധാന ലക്ഷ്യം കുറഞ്ഞ സജ്ജീകരണത്തോടെ വിവിധ ഹാർഡ്‌വെയറുകളിൽ — ലൊക്കലായിയും ക്ലൗഡ് രാജ്യങ്ങളിലുമായി — state-of-the-art പ്രകടനത്തോടെ LLM ഇൻഫെറൻസ് സാധ്യമാക്കുകയാണ്.

- ഡിപെൻഡൻസിമില്ലാത്ത ശുദ്ധ C/C++ നടപ്പാക്കൽ
- Apple silicon ഒരു പ്രധാന പ്ലാറ്റ്ഫോമാണ് - ARM NEON, Accelerate, Metal ഫ്രെയിംവർക്കുകളിലൂടെ ഓപ്റ്റിമൈസ് ചെയ്തത്
- x86 ആർക്കിടെക്ചറുകൾക്ക് AVX, AVX2, AVX512 പിന്തുണ
- വേഗത്തിൽ ഇൻഫെറൻസ് ചെയ്യാനും മെമ്മറി ഉപയോഗം കുറക്കാനും 1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, 8-bit ഇന്റിജർ ക്വാണ്ടൈസേഷൻ समर्थनം
- NVIDIA GPUകളിൽ LLMങ്ങൾ ഓടിക്കാൻ കസ്റ്റം CUDA കർണലുകൾ (AMD GPUs വേണ്ടി HIP വഴി പിന്തുണ)
- Vulkan ഉം SYCL ബാക്ക്എൻഡ് പിന്തുണയും
- മൊത്തം VRAM ശേഷിയേക്കാൾ വലിയ മോഡൽ ഭാഗികമായി ത്വരിതപ്പെടുത്താൻ CPU+GPU ഹൈബ്രിഡ് ഇൻഫെറൻസ്

## **llama.cpp ഉപയോഗിച്ച് Phi-3.5 ക്വாண்டൈസിംഗ്**

Phi-3.5-Instruct മോഡൽ llama.cpp ഉപയോഗിച്ച് ക്വാണ്ടൈസ് ചെയ്യാവുന്നതാണ്, പക്ഷേ Phi-3.5-Vision এবং Phi-3.5-MoE ഇതുവരെ പിന്തുണയ്ക്കുന്നില്ല. llama.cpp കൊണ്ടു മാറ്റിവെയ്ക്കുന്ന ഫോർമാറ്റ് gguf ആണ്, കൂടാതെ ഇത് ഏറ്റവും വ്യാപകമായി ഉപയോഗിക്കുന്ന ക്വാണ്ടൈസേഷൻ ഫോർമാറ്റാണ്.

Hugging face-ൽ നിരവധി ക്വാണ്ടൈസ്ഡ് GGUF ഫോർമാറ്റ് മോഡലുകൾ ലഭ്യമാണ്. AI Foundry, Ollama, LlamaEdge മുതലായവ llama.cpp-നാൽ ആശ്രയിക്കുന്നു, അതിനാൽ GGUF മോഡലുകളും പലപ്പോഴും ഉപയോഗിക്കപ്പെടുന്നു.

### **GGUF എന്താണ്**

GGUF ഒരു ബൈനറി ഫോർമാറ്റാണ്, മോഡലുകൾ വേഗത്തിൽ ലോഡ് ചെയ്യാനും സംരക്ഷിക്കാന്‍ ഒപ്ടിമൈസ് ചെയ്തത്, അത് ഇൻഫെറൻസിനായി വളരെ കാര്യക്ഷമമാക്കുന്നു. GGUF GGML ഉം മറ്റ് എക്സിക്യൂട്ടറുകൾക്കും ഉപയോഗിക്കാനായി രൂപകല്പന ചെയ്‌തതാണ്. GGUF വികസിപ്പിച്ചെടുത്തത് llama.cpp-യുടെ ഡെവലപ്പറും പ്രശസ്ത C/C++ LLM ഇൻഫെറൻസ് ഫ്രെയിംവർക്കിന്റെ രചയിതാവുമായ @ggerganov ആണ്. PyTorch പോലുള്ള ഫ്രെയിംവർക്കുകളിൽ ആദ്യം വികസിപ്പിച്ച മോഡലുകൾ GGUF ഫോർമാറ്റിലേക്ക് മാറ്റി ആ എഞ്ചിനുകളിൽ ഉപയോഗിക്കാം.

### **ONNX vs GGUF**

ONNX ഒരു പരമ്പരാഗത മെഷീൻ ലേർണിംഗ്/ഡിീപ് ലേർണിംഗ് ഫോർമാറ്റാണ്, വിവിധ AI ഫ്രെയിംവർക്കുകളിൽ അത് നല്ല പിന്തുണ ലഭ്യമാണ് കൂടാതെ എഡ്ജ് ഉപകരണങ്ങളിലും നല്ല ഉപയോഗ സാധ്യതകൾ ഉണ്ട്. GGUF llama.cpp-നെയാണ് അടിസ്ഥാനമാക്കുന്നത്, GenAI കാലഘട്ടത്തിൽ ഉത്പാദിപ്പിച്ചതായാണ് പറയപ്പെടുന്നത്. രണ്ടു ഫോർമാറ്റുകൾക്കും സമാനമായ ഉപയോഗങ്ങൾ ഉണ്ടാകും. കോൺമ്പണ്ടുചെയ്യുന്ന ഹാർഡ്‌വെയർ വിധാനങ്ങളിൽ നല്ല പ്രകടനം ആഗ്രഹിച്ചാൽ ONNX നിങ്ങൾക്ക് നല്ല തിരഞ്ഞെടുപ്പാകാം. llama.cpp-യുടെ ഡെറിവറ്റീവ് ഫ്രെയിംവർക്കും സാങ്കേതിക വിദ്യയും ഉപയോഗിക്കുന്നുണ്ടെങ്കിൽ GGUF മികച്ചതാകാം.

### **llama.cpp ഉപയോഗിച്ച് Phi-3.5-Instruct ക്വാണ്ടൈസേഷൻ**

**1. പരിസ്ഥിതി കോൺഫിഗറേഷൻ**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. ക്വാണ്ടൈസേഷൻ**

llama.cpp ഉപയോഗിച്ച് Phi-3.5-Instruct നെ FP16 GGUF ആയി മാറ്റുക


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Phi-3.5 നെ INT4 ആയി ക്വാണ്ടൈസ് ചെയ്യൽ


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. പരിശോധന**

llama-cpp-python ഇൻസ്റ്റാൾ ചെയ്യുക


```bash

pip install llama-cpp-python -U

```

***കുറിപ്പ്*** 

Apple Silicon ഉപയോഗിക്കുന്നുവെങ്കിൽ , ദയവായി ഇങ്ങനെ llama-cpp-python ഇൻസ്റ്റാൾ ചെയ്യുക


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

പരിശോധന 


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **സ്രോതസ്സുകൾ**

1. llama.cpp സംബന്ധിച്ച കൂടുതൽ വിവരങ്ങൾ [https://github.com/ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)
2. onnxruntime സംബന്ധിച്ച കൂടുതൽ വിവരങ്ങൾ [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)
3. GGUF സംബന്ധിച്ച കൂടുതൽ വിവരങ്ങൾ [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
അസ്വീകാര പ്രസ്താവന:
ഈ പ്രമാണം AI വിവർത്തന സേവനായ [Co-op Translator](https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് വിവർത്തനമായി ഒരുക്കിയതാണ്. ഞങ്ങൾ കൃത്യത ഉറപ്പാക്കാൻ പരിശ്രമിച്ചിരിക്കുന്നു എങ്കിലും, സ്വയം പ്രവർത്തിക്കുന്ന വിവർത്തനങ്ങളിൽ പിശകുകൾ അല്ലെങ്കിൽ തെറ്റുകൾ ഉണ്ടായിരിക്കാമെന്ന് ദയവായി ശ്രദ്ധിക്കുക. મૂળ രേഖ അതിന്റെ സംസ്ഥാനഭാഷയിൽ കോൺവിന്റേയുള്ള അതോറിറ്റേറ്റീവ് ഉറവിടമായി കണക്കാക്കപ്പെടണം. നിർണായകമായ വിവരങ്ങൾക്ക്, പ്രൊഫഷനൽ മാനവ വിവർത്തനത്തെ ശുപാർശ ചെയ്യുന്നു. ഈ വിവർത്തനത്തിന്റെ ഉപയോഗത്തിൽ നിന്നുണ്ടാകുന്ന ഏതെങ്കിലും തെറ്റിദ്ധാരണകൾക്കും തെറ്റായി വ്യാഖ്യാനിക്കപ്പെടലുകൾക്കും ഞങ്ങൾ ഉത്തരവാദിക്ക് ബാധകരല്ല.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->