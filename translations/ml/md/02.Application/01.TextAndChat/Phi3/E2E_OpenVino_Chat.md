<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a2a54312eea82ac654fb0f6d39b1f772",
  "translation_date": "2025-12-21T21:33:28+00:00",
  "source_file": "md/02.Application/01.TextAndChat/Phi3/E2E_OpenVino_Chat.md",
  "language_code": "ml"
}
-->
[OpenVino ചാറ്റ് ഉദാഹരണം](../../../../code/06.E2E/E2E_OpenVino_Chat_Phi3-instruct.ipynb)

ഈ കോഡ് ഒരു മോഡൽ OpenVINO ഫോർമാറ്റിലേക്ക് എക്സ്പോർട്ട് ചെയ്യുകയും, അത് ലോഡ് ചെയ്യുകയും, നൽകിയ പ്രോംപ്റ്റിനു മറുപടി ജനറേറ്റ് ചെയ്യാൻ അതിനെ ഉപയോഗിക്കുകയും ചെയ്യുന്നു. 

1. **മോഡൽ എക്സ്പോർട്ട് ചെയ്യൽ**:
   ```bash
   optimum-cli export openvino --model "microsoft/Phi-3-mini-4k-instruct" --task text-generation-with-past --weight-format int4 --group-size 128 --ratio 0.6 --sym --trust-remote-code ./model/phi3-instruct/int4
   ```
   - ഈ കമാൻഡ് മോഡൽ OpenVINO ഫോർമാറ്റിലേക്ക് എക്സ്പോർട്ട് ചെയ്യാൻ `optimum-cli` ടൂൾ ഉപയോഗിക്കുന്നു, ഇത് ഫലപ്രദമായ ഇൻഫറൻസിനായി ഒപ്റ്റിമൈസ് ചെയ്തതാണ്.
   - എക്സ്പോർട്ട് ചെയ്യപ്പെടുന്നത് `"microsoft/Phi-3-mini-4k-instruct"` മോഡലാണ്, ഇത് മുമ്പ് നൽകിയ സന്ദർഭത്തിന്റെ അടിസ്ഥാനത്തിൽ ടെക്സ്റ്റ് ജനറേറ്റ് ചെയ്യാനുള്ള ടാസ്ക്കിനായി ഒരുക്കപ്പെട്ടിരിക്കുന്നു.
   - മോഡലിന്റെ വെയ്റ്റുകൾ 4-ബിറ്റ് ഇന്റിജറുകളായിട്ട് (`int4`) ക്വാൻറ്റൈസ് ചെയ്തിരിക്കുന്നു, ഇത് മോഡലിന്റെ الحجم കുറക്കാനും പ്രോസസ്സിംഗ് വേഗത കൂട്ടാനുമാണ് സഹായിക്കുക.
   - മറ്റ് പാരാമീറ്ററുകൾ, ഉദാഹരണത്തിന് `group-size`, `ratio`, `sym`, ക്വാൻറ്റൈസേഷൻ പ്രക്രിയ സൂക്ഷ്മമായി ക്രമീകരിക്കാൻ ഉപയോഗിക്കുന്നു.
   - എക്സ്പോർട്ട് ചെയ്ത മോഡൽ `./model/phi3-instruct/int4` ഡയറക്ടറിയിൽ സേവ് ചെയ്യപ്പെടുന്നു.

2. **ആവശ്യമായ ലൈബ്രറികൾ ഇംപോർട്ട് ചെയ്യൽ**:
   ```python
   from transformers import AutoConfig, AutoTokenizer
   from optimum.intel.openvino import OVModelForCausalLM
   ```
   - ഈ വരികൾ മോഡൽ ലോഡ് ചെയ്ത് ഉപയോഗിക്കാൻ ആവശ്യമായ `transformers` ലൈബ്രറിയിൽ നിന്നും ക്ലാസുകളും `optimum.intel.openvino` മോഡ്യൂളിൽ നിന്നുള്ളവയും ഇംപോർട്ട് ചെയ്യുന്നു.

3. **മോഡൽ ഡയറക്ടറി మరియు കോൺഫിഗറേഷൻ ക്രമീകരിക്കൽ**:
   ```python
   model_dir = './model/phi3-instruct/int4'
   ov_config = {
       "PERFORMANCE_HINT": "LATENCY",
       "NUM_STREAMS": "1",
       "CACHE_DIR": ""
   }
   ```
   - `model_dir` മോഡൽ ഫയലുകൾ സൂക്ഷിച്ചിരിക്കുന്ന സ്ഥലം വ്യക്തമാക്കുന്നു.
   - `ov_config` ഒരു ഡിക്ഷണറിയാണ്, ഇത് OpenVINO മോഡലിന് കുറഞ്ഞ ലേറ്റൻസി പ്രാധാന്യം നൽകുന്നത്, ഒരു ഇൻഫറൻസ് സ്ട്രീം ഉപയോഗിക്കുന്നത്, കൂടാതെ കാഷെ ഡയറക്ടറി ഉപയോഗിക്കാതിരിക്കുക എന്നിങ്ങനെ ക്രമീകരിക്കുന്നു.

4. **മോഡൽ ലോഡ് ചെയ്യൽ**:
   ```python
   ov_model = OVModelForCausalLM.from_pretrained(
       model_dir,
       device='GPU.0',
       ov_config=ov_config,
       config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True),
       trust_remote_code=True,
   )
   ```
   - ഈ ലൈനിൽ മുൻകൂട്ടി നിർവചിച്ച ഡയറക്ടറിയിൽ നിന്ന് കോൻഫിഗറേഷൻ സജ്ജീകരണങ്ങൾ ഉപയോഗിച്ച് മോഡൽ ലോഡ് ചെയ്യുന്നു. ആവശ്യമായാൽ റിമോട്ട് കോഡ് എക്സിക്ക്യൂഷൻ അനുവദിക്കുന്നു.

5. **ടോക്കണൈസർ ലോഡ് ചെയ്യൽ**:
   ```python
   tok = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
   ```
   - ഈ ലൈനിൽ ടെക്സ്റ്റിനെ മോഡൽ മനസ്സിലാക്കാവുന്ന ടോക്കണുകളായി മാറ്റുന്നതിനുള്ള ടോക്കണൈസർ ലോഡ് ചെയ്യുന്നു.

6. **ടോക്കണൈസറിന്റെ ആർഗ്യൂമെന്റുകൾ ക്രമീകരിക്കൽ**:
   ```python
   tokenizer_kwargs = {
       "add_special_tokens": False
   }
   ```
   - ഈ ഡിക്ഷണറി ടോക്കനൈസ്ഡ് ഔട്ട്പുട്ടിലേക്ക് പ്രത്യേക ടോക്കണുകൾ ചേർക്കരുതെന്നു വ്യക്തമാക്കുന്നു.

7. **പ്രോംപ്റ്റ് നിർവചിക്കൽ**:
   ```python
   prompt = "<|system|>You are a helpful AI assistant.<|end|><|user|>can you introduce yourself?<|end|><|assistant|>"
   ```
   - ഈ സ്ട്രിംഗ് ഒരു സംഭാഷണ പ്രോംപ്റ്റ് സജ്ജമാക്കുന്നു, ഇതിൽ ഉപയോക്താവ് AI അസിസ്റ്റന്റിനെ സ്വന്തം പരിചയം പറയാൻ ആവശ്യപ്പെടുന്നു.

8. **പ്രോംപ്റ്റ് ടോക്കനൈസ് ചെയ്യൽ**:
   ```python
   input_tokens = tok(prompt, return_tensors="pt", **tokenizer_kwargs)
   ```
   - ഈ ലൈനിൽ പ്രോംപ്റ്റ് മോഡൽ പ്രോസസ് ചെയ്യാവുന്ന ടോക്കണുകളായി മാറ്റിയിട്ട് ഫലമായി PyTorch ടെൻസറുകൾ ആയി തിരികെ നൽകുന്നു.

9. **പ്രതികരണം ജനറേറ്റ് ചെയ്യൽ**:
   ```python
   answer = ov_model.generate(**input_tokens, max_new_tokens=1024)
   ```
   - ഈ ലൈനിൽ ഇൻപുട്ട് ടോക്കണുകൾ അടിസ്ഥാനമാക്കി മോഡൽ ഉപയോഗിച്ച് പ്രതികരണം ജനറേറ്റ് ചെയ്യുന്നു, പരമാവധി 1024 പുതിയ ടോക്കണുകൾ വരെയാണ് അനുവദിക്കുക.

10. **പ്രതികരണം ഡീകോഡ് ചെയ്യൽ**:
    ```python
    decoded_answer = tok.batch_decode(answer, skip_special_tokens=True)[0]
    ```
    - ഈ ലൈനിൽ ജനറേറ്റുചെയ്‌ത ടോക്കണുകൾ മനുഷ്യൻ വായിക്കാൻ ആകുന്ന സ്ട്രിംഗിലേക്ക് തിരിച്ചു മാറ്റുന്നു, പ്രത്യേക ടോക്കണുകൾ ഒഴിവാക്കി, ആദ്യ ഫലമെടുത്ത് കാണിക്കുന്നു.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
ഡിസ്ക്ലെയിമർ:

ഈ രേഖ AI വിവർത്തന സേവനമായ [Co-op Translator](https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് വിവർത്തനം ചെയ്തതാണ്. നാം കൃത്യതയ്ക്കായി ശ്രമിച്ചുവെങ്കിലും, ഓട്ടോമേറ്റഡ് വിവർത്തനങ്ങളിൽ തെറ്റുകളും തെറ്റായ വ്യാഖ്യാനങ്ങളും ഉണ്ടാകാവുന്നതാണ്. രേഖയുടെ മാതൃഭാഷയിലെ മൂല പതിപ്പ് പ്രാമാണിക ഉറവിടമായി പരിഗണിക്കണമാണ്. നിർണായക വിവരങ്ങൾക്ക് പ്രൊഫഷണൽ മാനവ വിവർത്തനം ശുപാർശിക്കപ്പെടുന്നു. ഈ വിവർത്തനം ഉപയോഗിച്ചതിൽ നിന്നുണ്ടാകുന്ന ഏതെങ്കിലും തെറ്റിദ്ധാരണകൾക്കും വ്യാഖ്യാനപരമായ തെറ്റുകൾക്കുമായി ഞങ്ങൾ ഉത്തരവാദികളല്ല.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->