# Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper

## ওভারভিউ

ইন্টারঅ্যাকটিভ Phi 3 Mini 4K Instruct Chatbot একটি টুল যা ব্যবহারকারীদের মাইক্রোসফট Phi 3 Mini 4K instruct ডেমো-র সাথে টেক্সট বা অডিও ইনপুটের মাধ্যমে ইন্টারঅ্যাক্ট করার অনুমতি দেয়। এই চ্যাটবটটি বিভিন্ন কাজের জন্য ব্যবহার করা যেতে পারে, যেমন অনুবাদ, আবহাওয়া আপডেট, এবং সাধারণ তথ্য সংগ্রহ।

### শুরু করা

এই চ্যাটবটটি ব্যবহার করতে, নিম্নলিখিত নির্দেশাবলি অনুসরণ করুন:

1. একটি নতুন [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) খুলুন
2. নোটবুকের প্রধান উইন্ডোতে, আপনি একটি চ্যাটবক্স ইন্টারফেস দেখতে পাবেন যেখানে একটি টেক্সট ইনপুট বক্স এবং একটি "Send" বোতাম রয়েছে।
3. টেক্সট-ভিত্তিক চ্যাটবট ব্যবহার করতে, আপনার মেসেজটি টেক্সট ইনপুট বক্সে টাইপ করুন এবং "Send" বোতামে ক্লিক করুন। চ্যাটবট একটি অডিও ফাইলের মাধ্যমে উত্তর দেবে যেটি সরাসরি নোটবুক থেকে প্লে করা যাবে।

**দ্রষ্টব্য**: এই টুলটির জন্য একটি GPU এবং মাইক্রোসফট Phi-3 ও OpenAI Whisper মডেলের অ্যাক্সেস প্রয়োজন, যেটি বক্তৃতা স্বীকৃতি ও অনুবাদের জন্য ব্যবহৃত হয়।

### GPU এর প্রয়োজনীয়তা

এই ডেমো চালানোর জন্য আপনার কাছে 12GB GPU মেমোরি থাকা আবশ্যক।

**Microsoft-Phi-3-Mini-4K instruct** ডেমোটি GPU-তে চালানোর জন্য মেমোরি প্রয়োজনীয়তা ইনপুট ডেটার (অডিও অথবা টেক্সট) সাইজ, অনুবাদের জন্য ব্যবহৃত ভাষা, মডেলের গতি এবং GPU তে উপলব্ধ মেমোরির উপর নির্ভর করবে।

সাধারণত, Whisper মডেলটি GPU-তে চালানোর জন্য ডিজাইন করা হয়েছে। Whisper মডেল চালানোর জন্য সুপারিশকৃত GPU মেমোরির ন্যূনতম পরিমাণ ৮ জিবি, তবে প্রয়োজনে এটি আরও বড় মেমোরি পরিচালনা করতে পারে।

বেশি ভলিউমে ডেটা বা উচ্চ পরিমাণের অনুরোধ মডেলে চালানোতে অতিরিক্ত GPU মেমোরি প্রয়োজন হতে পারে এবং/অথবা পারফরম্যান্সের সমস্যা হতে পারে। নির্দিষ্ট প্রয়োজনের জন্য আপনার ব্যবহার পরীক্ষা করতে এবং মেমোরি ব্যবহারে নজর রাখতে বিভিন্ন কনফিগারেশন পরীক্ষা করার পরামর্শ দেওয়া হয়।

## ইন্টারঅ্যাকটিভ Phi 3 Mini 4K Instruct Chatbot with Whisper এর E2E স্যাম্পল

[Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) নামক জুপিটার নোটবুকটি প্রদর্শন করে কিভাবে মাইক্রোসফট Phi 3 Mini 4K instruct ডেমো ব্যবহার করে অডিও বা লেখা টেক্সট ইনপুট থেকে টেক্সট তৈরি করা যায়। নোটবুকটি কয়েকটি ফাংশন ডিফাইন করেছে:

1. `tts_file_name(text)`: এই ফাংশনটি ইনপুট টেক্সটের ভিত্তিতে একটি ফাইল নাম তৈরি করে যাতে তৈরি করা অডিও ফাইলটি সংরক্ষণ করা যায়।
1. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: এই ফাংশনটি Edge TTS API ব্যবহার করে ইনপুট টেক্সটের অংশগুলোর একটি তালিকা থেকে একটি অডিও ফাইল তৈরি করে। ইনপুট প্যারামিটারসমূহ হল অংশের তালিকা, বক্তৃতার গতি, ভয়েসের নাম, এবং তৈরি অডিও ফাইল সংরক্ষণের পাথ।
1. `talk(input_text)`: এই ফাংশনটি Edge TTS API ব্যবহার করে একটি অডিও ফাইল তৈরি করে এবং এটি /content/audio ডিরেক্টরিতে একটি র‍্যান্ডম ফাইল নাম দিয়ে সংরক্ষণ করে। ইনপুট প্যারামিটারটি এমন টেক্সট যেটি বক্তৃতায় রূপান্তরিত করা হবে।
1. `run_text_prompt(message, chat_history)`: এই ফাংশনটি Microsoft Phi 3 Mini 4K instruct ডেমো ব্যবহার করে একটি মেসেজ ইনপুট থেকে অডিও ফাইল তৈরি করে এবং এটি চ্যাট হিস্টোরিতে যোগ করে।
1. `run_audio_prompt(audio, chat_history)`: এই ফাংশনটি Whisper মডেল API ব্যবহার করে একটি অডিও ফাইলকে টেক্সটে রূপান্তরিত করে এবং এটি `run_text_prompt()` ফাংশনে পাঠায়।
1. কোডটি একটি Gradio অ্যাপ চালু করে যা ব্যবহারকারীদের মেসেজ টাইপ করে অথবা অডিও ফাইল আপলোড করে Phi 3 Mini 4K instruct ডেমোর সাথে ইন্টারঅ্যাক্ট করার অনুমতি দেয়। আউটপুট অ্যাপের মধ্যে টেক্সট মেসেজ হিসেবে প্রদর্শিত হয়।

## সমস্যার সমাধান

Cuda GPU ড্রাইভার ইনস্টল করা

1. নিশ্চিত করুন যে আপনার লিনাক্স অ্যাপ্লিকেশন আপডেট আছে

    ```bash
    sudo apt update
    ```

1. Cuda ড্রাইভার ইনস্টল করুন

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. cuda ড্রাইভার অবস্থান নিবন্ধন করুন

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. Nvidia GPU মেমোরি সাইজ পরীক্ষা করুন (12GB GPU মেমোরি প্রয়োজন)

    ```bash
    nvidia-smi
    ```

1. ক্যাশে ফাঁকা করুন: আপনি যদি PyTorch ব্যবহার করেন, তাহলে torch.cuda.empty_cache() কল করে সকল অনাবশ্যক ক্যাশ করা মেমোরি মুক্ত করতে পারেন যাতে অন্যান্য GPU অ্যাপ্লিকেশন এটি ব্যবহার করতে পারে

    ```python
    torch.cuda.empty_cache() 
    ```

1. Nvidia Cuda পরীক্ষা করুন

    ```bash
    nvcc --version
    ```

1. একটি Hugging Face টোকেন তৈরি করতে নিম্নলিখিত কাজগুলো সম্পন্ন করুন।

    - [Hugging Face Token Settings পেজে](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo) যান।
    - **New token** সিলেক্ট করুন।
    - আপনি যে প্রকল্পের জন্য ব্যবহার করতে চান তার **Name** লিখুন।
    - **Type** নির্বাচন করুন **Write**।

> [!NOTE]
>
> যদি আপনি নিম্নলিখিত ত্রুটির মুখোমুখি হন:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> এই সমাধানের জন্য, আপনার টার্মিনালে নিম্নলিখিত কমান্ডটি টাইপ করুন।
>
> ```bash
> sudo ldconfig
> ```

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**অস্বীকৃতি**:  
এই নথিটি AI অনুবাদ সেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনূদিত হয়েছে। আমরা যথাসাধ্য সঠিকতার জন্য চেষ্টা করি, তবে অনুগ্রহ করে লক্ষ্য করুন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা ভুল থাকতে পারে। মূল নথি তার স্থানীয় ভাষায় সবচেয়ে বিশ্বাসযোগ্য উৎস হিসেবে বিবেচিত হওয়া উচিত। গুরুত্বপূর্ণ তথ্যের জন্য পেশাদার মানব অনুবাদের পরামর্শ দেওয়া হয়। এই অনুবাদের ব্যবহার থেকে উদ্ভূত কোনো ভুল ধরা বা ভুল ব্যাখ্যার জন্য আমরা দায়বদ্ধ নই।
<!-- CO-OP TRANSLATOR DISCLAIMER END -->