<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-05T02:00:44+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "bn"
}
-->
# **ফাই পরিবারের কোয়ান্টাইজেশন**

মডেল কোয়ান্টাইজেশন বলতে নিউরাল নেটওয়ার্ক মডেলের প্যারামিটারগুলো (যেমন ওয়েট এবং অ্যাক্টিভেশন মান) বড় মানের রেঞ্জ (সাধারণত একটি ধারাবাহিক মানের রেঞ্জ) থেকে একটি ছোট সীমিত মানের রেঞ্জে ম্যাপ করার প্রক্রিয়াকে বোঝায়। এই প্রযুক্তি মডেলের আকার এবং গণনাগত জটিলতা কমাতে পারে এবং মোবাইল ডিভাইস বা এমবেডেড সিস্টেমের মতো সীমিত সংস্থানযুক্ত পরিবেশে মডেলের কার্যকরী দক্ষতা বাড়াতে পারে। মডেল কোয়ান্টাইজেশন প্যারামিটারগুলোর প্রিসিশন কমিয়ে কমপ্রেশন করে, তবে এটি একটি নির্দিষ্ট পরিমাণ প্রিসিশন ক্ষতিও নিয়ে আসে। তাই কোয়ান্টাইজেশন প্রক্রিয়ায় মডেল আকার, গণনাগত জটিলতা এবং প্রিসিশনের মধ্যে সঠিক সামঞ্জস্য বজায় রাখা প্রয়োজন। সাধারণ কোয়ান্টাইজেশন পদ্ধতির মধ্যে রয়েছে ফিক্সড-পয়েন্ট কোয়ান্টাইজেশন, ফ্লোটিং-পয়েন্ট কোয়ান্টাইজেশন ইত্যাদি। নির্দিষ্ট দৃশ্যপট ও চাহিদা অনুযায়ী উপযুক্ত কোয়ান্টাইজেশন কৌশল নির্বাচন করা যেতে পারে।

আমরা আশা করি GenAI মডেলগুলোকে এজ ডিভাইসে ডিপ্লয় করে আরও বেশি ডিভাইসকে GenAI দৃশ্যপটে আনতে পারব, যেমন মোবাইল ডিভাইস, AI PC/Copilot+PC, এবং প্রচলিত IoT ডিভাইস। কোয়ান্টাইজড মডেলের মাধ্যমে আমরা বিভিন্ন ডিভাইসের জন্য বিভিন্ন এজ ডিভাইসে ডিপ্লয় করতে পারি। হার্ডওয়্যার নির্মাতাদের প্রদত্ত মডেল অ্যাকসেলারেশন ফ্রেমওয়ার্ক এবং কোয়ান্টাইজড মডেলের সাথে মিলিয়ে আমরা আরও উন্নত SLM অ্যাপ্লিকেশন দৃশ্যপট গড়ে তুলতে পারি।

কোয়ান্টাইজেশন দৃশ্যপটে, আমাদের বিভিন্ন প্রিসিশন রয়েছে (INT4, INT8, FP16, FP32). নিচে সাধারণত ব্যবহৃত কোয়ান্টাইজেশন প্রিসিশনগুলোর বর্ণনা দেওয়া হল

### **INT4**

INT4 কোয়ান্টাইজেশন একটি চরম কোয়ান্টাইজেশন পদ্ধতি যা মডেলের ওয়েট এবং অ্যাক্টিভেশন মানগুলোকে 4-বিট পূর্ণসংখ্যায় কোয়ান্টাইজ করে। ছোট প্রদর্শন রেঞ্জ এবং কম প্রিসিশনের কারণে INT4 কোয়ান্টাইজেশন সাধারণত বড় প্রিসিশন ক্ষতি ডেকে আনে। তবুও, INT8 কোয়ান্টাইজণের তুলনায় INT4 কোয়ান্টাইজেশন মডেলের স্টোরেজ প্রয়োজনীয়তা এবং গণনাগত জটিলতা আরও কমাতে পারে। লক্ষ্য রাখার মতো বিষয় হল যে বাস্তব ব্যবহারিক ক্ষেত্রে INT4 কোয়ান্টাইজেশন তুলনামূলকভাবে বিরল, কারণ অত্যন্ত কম প্রিসিশন মডেল পারফরম্যান্সে উল্লেখযোগ্য অবনতি ঘটাতে পারে। পাশাপাশি, সকল হার্ডওয়্যার INT4 অপারেশন সমর্থন করে না, তাই কোয়ান্টাইজেশন পদ্ধতি নির্বাচন করার সময় হার্ডওয়্যার সঙ্গততা বিবেচনা করা প্রয়োজন।

### **INT8**

INT8 কোয়ান্টাইজেশন হচ্ছে মডেলের ওয়েট এবং অ্যাক্টিভেশনগুলোকে ফ্লোটিং পয়েন্ট সংখ্যার কাছ থেকে 8-বিট পূর্ণসংখ্যায় রূপান্তর করার প্রক্রিয়া। INT8 পূর্ণসংখ্যা দ্বারা প্রতিনিধিত্ব করা সংখ্যার পরিসর ছোট এবং কম সুনির্দিষ্ট হলেও, এটি স্টোরেজ এবং গণনার প্রয়োজনীয়তা উল্লেখযোগ্যভাবে হ্রাস করতে পারে। INT8 কোয়ান্টাইজেশনে মডেলের ওয়েট এবং অ্যাক্টিভেশন মানগুলো একটি কোয়ান্টাইজেশন প্রক্রিয়ার মধ্য দিয়ে যায়, যার মধ্যে স্কেলিং এবং অফসেট থাকে, যাতে সম্ভব হলে আসল ফ্লোটিং পয়েন্ট তথ্য সংরক্ষণ করা যায়। ইনফারেন্সের সময় এই কোয়ান্টাইজড মানগুলো আবার গণনার জন্য ফ্লোটিং পয়েন্ট সংখ্যায় ডিকোয়ান্টাইজ করা হয়, এবং পরবর্তী ধাপের জন্য পুনরায় INT8 এ কোয়ান্টাইজ করা হয়। এই পদ্ধতিটি বেশিরভাগ অ্যাপ্লিকেশনে পর্যাপ্ত সঠিকতা প্রদান করতে পারে এবং একই সাথে উচ্চ গণনাগত দক্ষতা বজায় রাখে।

### **FP16**

FP16 ফরম্যাট, অর্থাৎ 16-বিট ফ্লোটিং পয়েন্ট সংখ্যা (float16), 32-বিট ফ্লোটিং পয়েন্ট সংখ্যা (float32) তুলনায় মেমরি ফুটপ্রিংট অর্ধেক করে, যা বড়-স্কেলের ডিপ লার্নিং অ্যাপ্লিকেশনগুলিতে উল্লেখযোগ্য সুবিধা রয়েছে। FP16 ফরম্যাট একই GPU মেমরি সীমার মধ্যে বড় মডেল লোড করা বা বেশি ডেটা প্রসেস করার অনুমতি দেয়। আধুনিক GPU হার্ডওয়্যার FP16 অপারেশন সমর্থন চালিয়ে যাওয়ার কারণে FP16 ফরম্যাট ব্যবহার করলে কম্পিউটিং গতিতেও উন্নতি হতে পারে। তবে, FP16 ফরম্যাটের ন্যূনতম প্রিসিশন হওয়ায় কিছু ক্ষেত্রে সাংখ্যিক অস্থিতিশীলতা বা প্রিসিশন ক্ষতি দেখা যেতে পারে।

### **FP32**

FP32 ফরম্যাট বেশি প্রিসিশন প্রদান করে এবং বিস্তৃত মানের পরিসর সঠিকভাবে উপস্থাপন করতে সক্ষম। যেখানে জটিল গণিতীয় অপারেশন করা হয় বা উচ্চ-প্রিসিশন ফলাফল প্রয়োজন, সেখানে FP32 ফরম্যাটই পছন্দ করা হয়। তবে উচ্চ সঠিকতা মানে বেশি মেমরি ব্যবহার এবং দীর্ঘতর গণনা সময়ও। বড়-স্কেলের ডিপ লার্নিং মডেলগুলির ক্ষেত্রে, বিশেষত যখন অনেক মডেল প্যারামিটার এবং প্রচুর ডেটা থাকে, তখন FP32 ফরম্যাট GPU মেমরির অপর্যাপ্ততা বা ইনফারেন্স গতির হ্রাস সৃষ্টি করতে পারে।

On mobile devices or IoT devices, we can convert Phi-3.x models to INT4, while AI PC / Copilot PC can use higher precision such as INT8, FP16, FP 32.

At present, different hardware manufacturers have different frameworks to support generative models, such as Intel's OpenVINO, Qualcomm's QNN, Apple's MLX, and Nvidia's CUDA, etc., combined with model quantization to complete local deployment.

In terms of technology, we have different format support after quantization, such as PyTorch / TensorFlow format, GGUF, and ONNX. I have done a format comparison and application scenarios between GGUF and ONNX. Here I recommend the ONNX quantization format, which has good support from the model framework to the hardware. In this chapter, we will focus on ONNX Runtime for GenAI, OpenVINO, and Apple MLX to perform model quantization (if you have a better way, you can also give it to us by submitting PR)

**এই অধ্যায়ে অন্তর্ভুক্ত আছে**

1. [llama.cpp ব্যবহার করে Phi-3.5 / 4 কোয়ান্টাইজেশন](./UsingLlamacppQuantifyingPhi.md)

2. [Generative AI এক্সটেনশন ব্যবহার করে onnxruntime-এ Phi-3.5 / 4 কোয়ান্টাইজেশন](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO ব্যবহার করে Phi-3.5 / 4 কোয়ান্টাইজেশন](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX ফ্রেমওয়ার্ক ব্যবহার করে Phi-3.5 / 4 কোয়ান্টাইজেশন](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
দায়-অস্বীকৃতি:
এই নথিটি AI অনুবাদ সেবা Co-op Translator (https://github.com/Azure/co-op-translator) ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসম্ভব সঠিকতার চেষ্টা করি; তবু অনুগ্রহ করে মনে রাখুন যে স্বয়ংক্রিয় অনুবাদে ভুল বা অসম্পূর্ণতা থাকতে পারে। মূল নথিটিকে তার নিজস্ব ভাষায়ই কর্তৃত্বপূর্ণ উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের ক্ষেত্রে পেশাদার মানব অনুবাদ গ্রহণ করার পরামর্শ দেওয়া হয়। এই অনুবাদ ব্যবহারের ফলে সৃষ্ট কোনো ভুল বোঝাবুঝি বা ভ্রান্ত ব্যাখ্যার জন্য আমরা দায়ী নই।
<!-- CO-OP TRANSLATOR DISCLAIMER END -->