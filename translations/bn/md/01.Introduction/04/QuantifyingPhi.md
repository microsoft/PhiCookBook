<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-05T16:41:38+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "bn"
}
-->
# **ফাই পরিবারকে পরিমাপ করা**

মডেল কোয়ান্টাইজেশন বলতে এমন একটি প্রক্রিয়াকে বোঝায় যেখানে একটি নিউরাল নেটওয়ার্ক মডেলের প্যারামিটারগুলি (যেমন ওজন এবং অ্যাক্টিভেশন মান) একটি বৃহৎ মানের পরিসর (সাধারণত একটি ধারাবাহিক মানের পরিসর) থেকে একটি ছোট সীমিত মানের পরিসরে ম্যাপ করা হয়। এই প্রযুক্তি মডেলের আকার এবং গাণিতিক জটিলতা কমাতে পারে এবং মোবাইল ডিভাইস বা এমবেডেড সিস্টেমের মতো সীমাবদ্ধ সম্পদের পরিবেশে মডেলের কার্যকরী দক্ষতা উন্নত করে। মডেল কোয়ান্টাইজেশন প্যারামিটারগুলির নির্ভুলতা কমিয়ে কম্প্রেশন অর্জন করে, তবে এতে কিছু নির্ভুলতার ক্ষতিও ঘটে। অতএব, কোয়ান্টাইজেশন প্রক্রিয়ায় মডেলের আকার, গাণিতিক জটিলতা এবং নির্ভুলতার মধ্যে সামঞ্জস্য বজায় রাখা প্রয়োজন। সাধারণ কোয়ান্টাইজেশন পদ্ধতির মধ্যে রয়েছে ফিক্সড-পয়েন্ট কোয়ান্টাইজেশন, ফ্লোটিং-পয়েন্ট কোয়ান্টাইজেশন ইত্যাদি। নির্দিষ্ট পরিস্থিতি এবং প্রয়োজন অনুযায়ী উপযুক্ত কোয়ান্টাইজেশন কৌশল নির্বাচন করতে পারেন।

আমরা আশা করি জেনএআই মডেলকে এজ ডিভাইসে স্থাপন করতে এবং আরো ডিভাইসকে জেনএআই ব্যবহারের সুযোগ দিতে, যেমন মোবাইল ডিভাইস, AI PC/Copilot+PC, এবং প্রচলিত IoT ডিভাইস। কোয়ান্টাইজড মডেলের মাধ্যমে আমরা এটি বিভিন্ন এজ ডিভাইসে ডিপ্লয় করতে পারি, যা ভিন্ন ভিন্ন ডিভাইসের উপর ভিত্তি করে। হার্ডওয়্যার নির্মাতাদের দ্বারা প্রদত্ত মডেল অ্যাক্সিলারেশন ফ্রেমওয়ার্ক এবং কোয়ান্টাইজড মডেলের সাথে মিলিয়ে আমরা আরও ভালো SLM অ্যাপ্লিকেশন পরিবেশ তৈরি করতে পারি।

কোয়ান্টাইজেশন পরিস্থিতিতে আমাদের কাছে বিভিন্ন নির্ভুলতা আছে (INT4, INT8, FP16, FP32)। নিচে প্রচলিত কোয়ান্টাইজেশন নির্ভুলতার ব্যাখ্যা দেওয়া হলো।

### **INT4**

INT4 কোয়ান্টাইজেশন একটি কড়া কোয়ান্টাইজেশন পদ্ধতি যা মডেলের ওজন এবং অ্যাক্টিভেশন মানকে ৪-বিট পূর্ণসংখ্যায় রূপান্তর করে। INT4 কোয়ান্টাইজেশনে সাধারণত একটি বড় নির্ভুলতা ক্ষতি হয় কারণ এর উপস্থাপনার পরিসর ছোট এবং নির্ভুলতা কম। তবে, INT8 কোয়ান্টাইজেশনের তুলনায় INT4 আরও বেশি সংরক্ষণ এবং গাণিতিক জটিলতা কমাতে পারে। লক্ষ্যনীয় যে, বাস্তব ব্যবহারে INT4 কোয়ান্টাইজেশন তুলনামূলক খুব কম ব্যবহৃত হয়, কারণ অতিমাত্রায় কম নির্ভুলতা মডেলের কর্মদক্ষতায় উল্লেখযোগ্য অবনতি ঘটাতে পারে। এছাড়া, সব হার্ডওয়্যার INT4 অপারেশন সমর্থন করে না, তাই কোয়ান্টাইজেশন পদ্ধতি বাছাই করার সময় হার্ডওয়্যার সামঞ্জস্য বিবেচনা করতে হবে।

### **INT8**

INT8 কোয়ান্টাইজেশন হলো মডেলের ওজন এবং অ্যাক্টিভেশনকে ফ্লোটিং পয়েন্ট সংখ্যার থেকে ৮-বিট পূর্ণসংখ্যায় রূপান্তর করার প্রক্রিয়া। যদিও INT8 পূর্ণসংখ্যার প্রতিনিধিত্ব পরিসর ছোট এবং কম নির্ভুল, এটি উল্লেখযোগ্যভাবে সংরক্ষণ এবং গণনার প্রয়োজনীয়তা হ্রাস করে। INT8 কোয়ান্টাইজেশনে মডেলের ওজন এবং অ্যাক্টিভেশন মান কোয়ান্টাইজেশন প্রক্রিয়ায় যায়, যার মধ্যে স্কেলিং এবং অফসেট অন্তর্ভুক্ত থাকে, যাতে মূল ফ্লোটিং পয়েন্ট তথ্য যতটা সম্ভব সংরক্ষিত থাকে। অনুমান সময় এই কোয়ান্টাইজড মানগুলি আবার ফ্লোটিং পয়েন্টে ডিকোয়ান্টাইজ করা হয় গণনার জন্য, এবং পরবর্তী ধাপের জন্য আবার INT8 এ কোয়ান্টাইজ করা হয়। এই পদ্ধতি বেশিরভাগ ব্যবহারে যথেষ্ট নির্ভুলতা প্রদান করে, এবং গাণিতিক দক্ষতাও বজায় রাখে।

### **FP16**

FP16 ফরম্যাট অর্থাৎ 16-বিট ফ্লোটিং পয়েন্ট সংখ্যা (float16), 32-বিট ফ্লোটিং পয়েন্ট সংখ্যার (float32) তুলনায় মেমরি আকার অর্ধেক হ্রাস করে, যা বড় স্কেলের ডিপ লার্নিং প্রয়োগে গুরুত্বপূর্ণ সুবিধা আসে। FP16 ফরম্যাট একই GPU মেমরি সীমাবদ্ধতার মধ্যে বড় মডেল লোড বা বেশি ডাটা প্রসেস করতে সক্ষম করে। আধুনিক GPU হার্ডওয়্যার FP16 অপারেশন সমর্থন করে থাকার কারণে, FP16 ফরম্যাট ব্যবহার করা গাণিতিক গতিশীলতা উন্নত করতেও সাহায্য করতে পারে। তবে, FP16 ফরম্যাটের নিজস্ব সীমাবদ্ধতা রয়েছে, যেমন অনিয়ন্ত্রিত নির্ভুলতা, যা কিছু ক্ষেত্রে সংখ্যাগত অস্থিরতা বা নির্ভুলতা ক্ষতি ঘটাতে পারে।

### **FP32**

FP32 ফরম্যাট উচ্চ নির্ভুলতা প্রদান করে এবং বিস্তৃত মানকে সঠিকভাবে উপস্থাপন করতে পারে। যেখানে জটিল গাণিতিক অপারেশন করা হয় বা উচ্চ নির্ভুল ফলাফল আবশ্যক, সেখানে FP32 ফরম্যাট পছন্দ করা হয়। তবে, উচ্চ নির্ভুলতার মানে হচ্ছে বেশি মেমরি ব্যবহার এবং দীর্ঘ গণনা সময়। বড় স্কেলে ডিপ লার্নিং মডেলের ক্ষেত্রে, বিশেষত যখন অনেক মডেল প্যারামিটার এবং বিশাল ডাটা থাকে, FP32 ফরম্যাট GPU মেমরি সংকট বা অনুমান গতি হ্রাস ঘটাতে পারে।

মোবাইল ডিভাইস বা IoT ডিভাইসে আমরা Phi-3.x মডেলগুলোকে INT4 এ রূপান্তর করতে পারি, যেখানে AI PC / Copilot PC-তে উচ্চ নির্ভুলতা যেমন INT8, FP16, FP32 ব্যবহার করা যায়।

বর্তমানে বিভিন্ন হার্ডওয়্যার নির্মাতাদের বিভিন্ন ফ্রেমওয়ার্ক আছে জেনারেটিভ মডেলকে সমর্থন করার জন্য, যেমন Intel-এর OpenVINO, Qualcomm-এর QNN, Apple-এর MLX, এবং Nvidia-এর CUDA ইত্যাদি, যা মডেল কোয়ান্টাইজেশনের সাথে মিলিয়ে স্থানীয় ডিপ্লয়মেন্ট সম্পন্ন করে।

প্রযুক্তিগত দিক থেকে, কোয়ান্টাইজেশনের পর আমরা বিভিন্ন ফরম্যাট সমর্থন করি, যেমন PyTorch / TensorFlow ফরম্যাট, GGUF, এবং ONNX। আমি GGUF এবং ONNX-এর মধ্যে ফরম্যাট তুলনা ও অ্যাপ্লিকেশন পরিস্থিতি করেছি। এখানে আমি ONNX কোয়ান্টাইজেশন ফরম্যাটটি সুপারিশ করছি, যা মডেল ফ্রেমওয়ার্ক থেকে হার্ডওয়্যার পর্যন্ত ভাল সমর্থন রয়েছে। এই অধ্যায়ে আমরা ONNX Runtime for GenAI, OpenVINO, এবং Apple MLX দিয়ে মডেল কোয়ান্টাইজেশন করার উপর দৃষ্টি নিবদ্ধ করব (যদি আপনার কোন ভালো পদ্ধতি থাকে, তবে PR জমা দিয়ে আমাদের সাথে শেয়ার করতে পারেন)।

**এই অধ্যায় অন্তর্ভুক্ত**

1. [llama.cpp ব্যবহার করে Phi-3.5 / 4 কোয়ান্টাইজ করা](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntime-এর জেনারেটিভ AI এক্সটেনশনের মাধ্যমে Phi-3.5 / 4 কোয়ান্টাইজ করা](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO ব্যবহার করে Phi-3.5 / 4 কোয়ান্টাইজ করা](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX ফ্রেমওয়ার্ক ব্যবহার করে Phi-3.5 / 4 কোয়ান্টাইজ করা](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**অস্বীকৃতি**:
এই দলিলটি AI অনুবাদ সেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনূদিত হয়েছে। আমরা যথাসাধ্য সঠিকতার চেষ্টা করি, তবে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। মূল ভাষায় অবস্থিত দলিলটিই সর্বাধিক বিশ্বাসযোগ্য উৎস হিসেবে বিবেচিত হবে। গুরুত্বপূর্ণ তথ্যের জন্য পেশাদার মান মানুষের অনুবাদ গ্রহণ করার পরামর্শ দেওয়া হয়। এই অনুবাদের ব্যবহারে সৃষ্ট কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যার জন্য আমরা দায়বদ্ধ নই।
<!-- CO-OP TRANSLATOR DISCLAIMER END -->