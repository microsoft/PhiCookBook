<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:17:19+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "bn"
}
-->
# উল্লেখযোগ্য প্রযুক্তিগুলো অন্তর্ভুক্ত

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12 এর উপরে নির্মিত একটি নিম্নস্তরের API যা হার্ডওয়্যার-ত্বরিত মেশিন লার্নিং সক্ষম করে।
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia কর্তৃক তৈরি একটি প্যারালাল কম্পিউটিং প্ল্যাটফর্ম এবং অ্যাপ্লিকেশন প্রোগ্রামিং ইন্টারফেস (API) মডেল, যা গ্রাফিক্স প্রসেসিং ইউনিট (GPU) গুলিতে সাধারণ উদ্দেশ্যের প্রসেসিং চালানোর সুযোগ দেয়।
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - একটি ওপেন ফরম্যাট যা মেশিন লার্নিং মডেলগুলোকে উপস্থাপন করার জন্য ডিজাইন করা হয়েছে এবং বিভিন্ন ML ফ্রেমওয়ার্কের মধ্যে ইন্টারঅপারেবিলিটি প্রদান করে।
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - মেশিন লার্নিং মডেল উপস্থাপন এবং আপডেট করার জন্য ব্যবহৃত একটি ফরম্যাট, যা বিশেষ করে ছোট ভাষার মডেলগুলোর জন্য উপযোগী, যেগুলো CPU তে ৪-৮ বিট কোয়ান্টাইজেশন ব্যবহার করে কার্যকরভাবে চালানো যায়।

## DirectML

DirectML একটি নিম্নস্তরের API যা হার্ডওয়্যার-ত্বরিত মেশিন লার্নিং সম্ভব করে। এটি DirectX 12 এর উপরে তৈরি এবং GPU ত্বরণ ব্যবহার করে, এবং ভেন্ডর-নিরপেক্ষ, অর্থাৎ বিভিন্ন GPU ভেন্ডরের মধ্যে কাজ করার জন্য কোড পরিবর্তনের প্রয়োজন হয় না। এটি প্রধানত GPU তে মডেল ট্রেনিং এবং ইনফারেন্স কাজের জন্য ব্যবহৃত হয়।

হার্ডওয়্যার সমর্থনের দিক থেকে, DirectML বিস্তৃত GPU সমর্থন করে, যার মধ্যে AMD ইন্টিগ্রেটেড এবং ডিসক্রিট GPU, Intel ইন্টিগ্রেটেড GPU এবং NVIDIA ডিসক্রিট GPU অন্তর্ভুক্ত। এটি Windows AI প্ল্যাটফর্মের অংশ এবং Windows 10 ও 11 এ সমর্থিত, যেকোন Windows ডিভাইসে মডেল ট্রেনিং এবং ইনফারেন্স করার সুযোগ দেয়।

DirectML সম্পর্কিত আপডেট এবং সুযোগ রয়েছে, যেমন ১৫০ টি ONNX অপারেটর পর্যন্ত সমর্থন এবং ONNX রানটাইম ও WinML দ্বারা ব্যবহৃত হওয়া। এটি প্রধান Integrated Hardware Vendors (IHVs) দ্বারা সমর্থিত, যারা বিভিন্ন মেটাকমান্ড বাস্তবায়ন করে।

## CUDA

CUDA, যার পূর্ণরূপ Compute Unified Device Architecture, Nvidia কর্তৃক তৈরি একটি প্যারালাল কম্পিউটিং প্ল্যাটফর্ম এবং API মডেল। এটি সফটওয়্যার ডেভেলপারদের CUDA-সক্ষম GPU ব্যবহার করে সাধারণ উদ্দেশ্যের প্রসেসিং করার সুযোগ দেয়, যা GPGPU (General-Purpose computing on Graphics Processing Units) নামে পরিচিত। CUDA Nvidia এর GPU ত্বরণের মূল চালিকা শক্তি এবং বিভিন্ন ক্ষেত্রে ব্যাপকভাবে ব্যবহৃত, যেমন মেশিন লার্নিং, বৈজ্ঞানিক কম্পিউটিং, এবং ভিডিও প্রসেসিং।

CUDA এর হার্ডওয়্যার সমর্থন শুধুমাত্র Nvidia এর GPU গুলোর জন্য প্রযোজ্য, কারণ এটি Nvidia এর নিজস্ব প্রযুক্তি। প্রতিটি আর্কিটেকচার CUDA টুলকিটের নির্দিষ্ট সংস্করণগুলো সমর্থন করে, যা ডেভেলপারদের CUDA অ্যাপ্লিকেশন তৈরি এবং চালানোর জন্য প্রয়োজনীয় লাইব্রেরি এবং সরঞ্জাম সরবরাহ করে।

## ONNX

ONNX (Open Neural Network Exchange) একটি ওপেন ফরম্যাট যা মেশিন লার্নিং মডেল উপস্থাপনের জন্য ডিজাইন করা হয়েছে। এটি একটি সম্প্রসারিত কম্পিউটেশন গ্রাফ মডেলের সংজ্ঞা প্রদান করে, পাশাপাশি বিল্ট-ইন অপারেটর এবং স্ট্যান্ডার্ড ডেটা টাইপের সংজ্ঞাও দেয়। ONNX ডেভেলপারদের মডেলগুলো বিভিন্ন ML ফ্রেমওয়ার্কের মধ্যে স্থানান্তর করার সুযোগ দেয়, যা ইন্টারঅপারেবিলিটি নিশ্চিত করে এবং AI অ্যাপ্লিকেশন তৈরি ও ডিপ্লয়মেন্ট সহজ করে।

Phi3 mini ONNX Runtime ব্যবহার করে CPU এবং GPU তে বিভিন্ন ডিভাইসে চলতে পারে, যার মধ্যে সার্ভার প্ল্যাটফর্ম, Windows, Linux এবং Mac ডেস্কটপ, এবং মোবাইল CPU অন্তর্ভুক্ত।

আমরা যে অপ্টিমাইজড কনফিগারেশনগুলো যুক্ত করেছি সেগুলো হলো

- int4 DML এর জন্য ONNX মডেল: AWQ এর মাধ্যমে int4 কোয়ান্টাইজড
- fp16 CUDA এর জন্য ONNX মডেল
- int4 CUDA এর জন্য ONNX মডেল: RTN এর মাধ্যমে int4 কোয়ান্টাইজড
- int4 CPU এবং মোবাইলের জন্য ONNX মডেল: RTN এর মাধ্যমে int4 কোয়ান্টাইজড

## Llama.cpp

Llama.cpp একটি ওপেন-সোর্স সফটওয়্যার লাইব্রেরি যা C++ এ লেখা। এটি বিভিন্ন বড় ভাষার মডেলে (LLMs) ইনফারেন্স করে, যার মধ্যে Llama অন্তর্ভুক্ত। ggml লাইব্রেরির সাথে একসাথে উন্নত, যা একটি সাধারণ উদ্দেশ্যের টেনসর লাইব্রেরি, llama.cpp মূল Python ইমপ্লিমেন্টেশনের তুলনায় দ্রুত ইনফারেন্স এবং কম মেমরি ব্যবহার প্রদান করে। এটি হার্ডওয়্যার অপ্টিমাইজেশন, কোয়ান্টাইজেশন সাপোর্ট করে এবং একটি সহজ API ও উদাহরণ সরবরাহ করে। যদি আপনি দক্ষ LLM ইনফারেন্সে আগ্রহী হন, তাহলে llama.cpp পরীক্ষা করা উচিত, কারণ Phi3 Llama.cpp চালাতে পারে।

## GGUF

GGUF (Generic Graph Update Format) একটি ফরম্যাট যা মেশিন লার্নিং মডেল উপস্থাপন এবং আপডেট করার জন্য ব্যবহৃত হয়। এটি বিশেষ করে ছোট ভাষার মডেলগুলোর জন্য উপযোগী, যেগুলো CPU তে ৪-৮ বিট কোয়ান্টাইজেশন ব্যবহার করে কার্যকরভাবে চালানো যায়। GGUF দ্রুত প্রোটোটাইপ তৈরি এবং এজ ডিভাইস বা CI/CD পাইপলাইন মত ব্যাচ জবের মাধ্যমে মডেল চালানোর জন্য উপকারী।

**অস্বীকৃতি**:  
এই নথিটি AI অনুবাদ সেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনূদিত হয়েছে। আমরা যথাসাধ্য সঠিকতার চেষ্টা করি, তবে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা ভুল থাকতে পারে বলে অনুগ্রহ করে সতর্ক থাকুন। মূল নথিটি তার নিজ ভাষায়ই কর্তৃত্বপূর্ণ উৎস হিসেবে বিবেচিত হওয়া উচিত। গুরুত্বপূর্ণ তথ্যের জন্য পেশাদার মানব অনুবাদ গ্রহণ করার পরামর্শ দেওয়া হয়। এই অনুবাদের ব্যবহারে সৃষ্ট কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যার জন্য আমরা দায়বদ্ধ নই।