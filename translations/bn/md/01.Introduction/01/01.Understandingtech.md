# উল্লেখযোগ্য প্রযুক্তিগুলো অন্তর্ভুক্ত

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12 এর ওপর ভিত্তি করে নির্মিত একটি নিম্নস্তরের API যা হার্ডওয়্যার-ত্বরিত মেশিন লার্নিং সক্ষম করে।
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia দ্বারা তৈরি একটি প্যারালাল কম্পিউটিং প্ল্যাটফর্ম এবং অ্যাপ্লিকেশন প্রোগ্রামিং ইন্টারফেস (API) মডেল, যা গ্রাফিক্স প্রসেসিং ইউনিট (GPU) গুলোতে সাধারণ উদ্দেশ্যের প্রসেসিং সক্ষম করে।
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - একটি ওপেন ফরম্যাট যা মেশিন লার্নিং মডেলগুলো উপস্থাপন করার জন্য ডিজাইন করা হয়েছে এবং বিভিন্ন ML ফ্রেমওয়ার্কের মধ্যে ইন্টারঅপারেবিলিটি প্রদান করে।
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - একটি ফরম্যাট যা মেশিন লার্নিং মডেল উপস্থাপন এবং আপডেট করার জন্য ব্যবহৃত হয়, বিশেষ করে ছোট ভাষা মডেলগুলোর জন্য যা 4-8bit কোয়ান্টাইজেশনের মাধ্যমে CPU তে কার্যকরভাবে চলতে পারে।

## DirectML

DirectML একটি নিম্নস্তরের API যা হার্ডওয়্যার-ত্বরিত মেশিন লার্নিং সক্ষম করে। এটি DirectX 12 এর ওপর নির্মিত, GPU ত্বরণ ব্যবহার করে এবং ভেন্ডর-নিরপেক্ষ, অর্থাৎ বিভিন্ন GPU ভেন্ডরের মধ্যে কাজ করার জন্য কোড পরিবর্তনের প্রয়োজন হয় না। এটি প্রধানত GPU তে মডেল ট্রেনিং এবং ইনফারেন্সের জন্য ব্যবহৃত হয়।

হার্ডওয়্যার সাপোর্টের ক্ষেত্রে, DirectML বিভিন্ন GPU এর সাথে কাজ করার জন্য ডিজাইন করা হয়েছে, যার মধ্যে রয়েছে AMD ইন্টিগ্রেটেড এবং ডিসক্রিট GPU, Intel ইন্টিগ্রেটেড GPU, এবং NVIDIA ডিসক্রিট GPU। এটি Windows AI প্ল্যাটফর্মের অংশ এবং Windows 10 ও 11 এ সমর্থিত, যা যেকোনো Windows ডিভাইসে মডেল ট্রেনিং এবং ইনফারেন্সের সুযোগ দেয়।

DirectML সম্পর্কিত আপডেট এবং সুযোগ রয়েছে, যেমন ১৫০ টিরও বেশি ONNX অপারেটর সাপোর্ট করা এবং ONNX runtime ও WinML উভয়ের দ্বারা ব্যবহৃত হওয়া। এটি প্রধান ইন্টিগ্রেটেড হার্ডওয়্যার ভেন্ডরদের (IHVs) দ্বারা সমর্থিত, যারা বিভিন্ন মেটাকমান্ড বাস্তবায়ন করে।

## CUDA

CUDA, যার পূর্ণরূপ Compute Unified Device Architecture, Nvidia দ্বারা তৈরি একটি প্যারালাল কম্পিউটিং প্ল্যাটফর্ম এবং API মডেল। এটি সফটওয়্যার ডেভেলপারদের CUDA-সক্ষম GPU ব্যবহার করে সাধারণ উদ্দেশ্যের প্রসেসিং করার সুযোগ দেয় — যাকে GPGPU (General-Purpose computing on Graphics Processing Units) বলা হয়। CUDA Nvidia এর GPU ত্বরণের মূল চালিকা শক্তি এবং এটি মেশিন লার্নিং, বৈজ্ঞানিক কম্পিউটিং, ভিডিও প্রসেসিংসহ বিভিন্ন ক্ষেত্রে ব্যাপকভাবে ব্যবহৃত হয়।

CUDA এর হার্ডওয়্যার সাপোর্ট শুধুমাত্র Nvidia এর GPU গুলোর জন্য নির্দিষ্ট, কারণ এটি Nvidia এর মালিকানাধীন প্রযুক্তি। প্রতিটি আর্কিটেকচার CUDA টুলকিটের নির্দিষ্ট সংস্করণ সাপোর্ট করে, যা ডেভেলপারদের CUDA অ্যাপ্লিকেশন তৈরি ও চালানোর জন্য প্রয়োজনীয় লাইব্রেরি ও টুল সরবরাহ করে।

## ONNX

ONNX (Open Neural Network Exchange) একটি ওপেন ফরম্যাট যা মেশিন লার্নিং মডেল উপস্থাপনের জন্য ডিজাইন করা হয়েছে। এটি একটি সম্প্রসারিত কম্পিউটেশন গ্রাফ মডেলের সংজ্ঞা দেয়, পাশাপাশি বিল্ট-ইন অপারেটর এবং স্ট্যান্ডার্ড ডেটা টাইপের সংজ্ঞাও প্রদান করে। ONNX ডেভেলপারদের বিভিন্ন ML ফ্রেমওয়ার্কের মধ্যে মডেল স্থানান্তর করার সুযোগ দেয়, যা ইন্টারঅপারেবিলিটি নিশ্চিত করে এবং AI অ্যাপ্লিকেশন তৈরি ও ডিপ্লয় করা সহজ করে।

Phi3 mini ONNX Runtime ব্যবহার করে CPU এবং GPU তে বিভিন্ন ডিভাইসে চলতে পারে, যার মধ্যে সার্ভার প্ল্যাটফর্ম, Windows, Linux ও Mac ডেস্কটপ এবং মোবাইল CPU অন্তর্ভুক্ত।

আমরা যে অপ্টিমাইজড কনফিগারেশনগুলো যোগ করেছি সেগুলো হলো

- int4 DML এর জন্য ONNX মডেল: AWQ এর মাধ্যমে int4 কোয়ান্টাইজড
- fp16 CUDA এর জন্য ONNX মডেল
- int4 CUDA এর জন্য ONNX মডেল: RTN এর মাধ্যমে int4 কোয়ান্টাইজড
- int4 CPU এবং মোবাইলের জন্য ONNX মডেল: RTN এর মাধ্যমে int4 কোয়ান্টাইজড

## Llama.cpp

Llama.cpp একটি ওপেন-সোর্স সফটওয়্যার লাইব্রেরি যা C++ এ লেখা। এটি বিভিন্ন বড় ভাষা মডেলের (LLMs) উপর ইনফারেন্স করে, যার মধ্যে Llama অন্তর্ভুক্ত। ggml লাইব্রেরির (একটি সাধারণ উদ্দেশ্যের টেনসর লাইব্রেরি) সাথে একসাথে উন্নত, llama.cpp মূল Python ইমপ্লিমেন্টেশনের তুলনায় দ্রুততর ইনফারেন্স এবং কম মেমরি ব্যবহার নিশ্চিত করে। এটি হার্ডওয়্যার অপ্টিমাইজেশন, কোয়ান্টাইজেশন সাপোর্ট করে এবং একটি সহজ API ও উদাহরণ প্রদান করে। যদি আপনি দক্ষ LLM ইনফারেন্সে আগ্রহী হন, তাহলে llama.cpp পরীক্ষা করা উচিত কারণ Phi3 Llama.cpp চালাতে পারে।

## GGUF

GGUF (Generic Graph Update Format) একটি ফরম্যাট যা মেশিন লার্নিং মডেল উপস্থাপন এবং আপডেট করার জন্য ব্যবহৃত হয়। এটি বিশেষ করে ছোট ভাষা মডেলগুলোর জন্য উপযোগী, যেগুলো 4-8bit কোয়ান্টাইজেশনের মাধ্যমে CPU তে কার্যকরভাবে চলতে পারে। GGUF দ্রুত প্রোটোটাইপ তৈরি এবং এজ ডিভাইস বা CI/CD পাইপলাইনের মতো ব্যাচ জবগুলোতে মডেল চালানোর জন্য উপকারী।

**অস্বীকৃতি**:  
এই নথিটি AI অনুবাদ সেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনূদিত হয়েছে। আমরা যথাসাধ্য সঠিকতার চেষ্টা করি, তবে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। মূল নথিটি তার নিজস্ব ভাষায়ই কর্তৃত্বপূর্ণ উৎস হিসেবে বিবেচিত হওয়া উচিত। গুরুত্বপূর্ণ তথ্যের জন্য পেশাদার মানব অনুবাদ গ্রহণ করার পরামর্শ দেওয়া হয়। এই অনুবাদের ব্যবহারে সৃষ্ট কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যার জন্য আমরা দায়ী নই।