# Phi-3 লোকালি শুরু করুন

এই গাইডটি আপনাকে Ollama ব্যবহার করে Phi-3 মডেলটি আপনার লোকাল পরিবেশে চালানোর জন্য সেটআপ করতে সাহায্য করবে। আপনি মডেলটি বিভিন্ন উপায়ে চালাতে পারেন, যেমন GitHub Codespaces, VS Code Dev Containers, অথবা আপনার লোকাল পরিবেশ থেকে।

## পরিবেশ সেটআপ

### GitHub Codespaces

GitHub Codespaces ব্যবহার করে আপনি এই টেমপ্লেটটি ভার্চুয়ালি চালাতে পারেন। নিচের বাটনটি আপনার ব্রাউজারে একটি ওয়েব-ভিত্তিক VS Code ইনস্ট্যান্স খুলবে:

1. টেমপ্লেটটি খুলুন (এটি কয়েক মিনিট সময় নিতে পারে):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. একটি টার্মিনাল উইন্ডো খুলুন

### VS Code Dev Containers

⚠️ এই অপশনটি তখনই কাজ করবে যখন আপনার Docker Desktop-এ কমপক্ষে ১৬ জিবি RAM বরাদ্দ থাকবে। যদি আপনার RAM ১৬ জিবির কম হয়, তাহলে আপনি [GitHub Codespaces অপশন](../../../../../md/01.Introduction/01) চেষ্টা করতে পারেন অথবা [লোকালি সেটআপ করতে পারেন](../../../../../md/01.Introduction/01)।

একটি সম্পর্কিত অপশন হলো VS Code Dev Containers, যা আপনার লোকাল VS Code-তে [Dev Containers এক্সটেনশন](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) ব্যবহার করে প্রজেক্টটি খুলবে:

1. Docker Desktop চালু করুন (যদি না থাকে তাহলে ইনস্টল করুন)
2. প্রজেক্টটি খুলুন:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. VS Code উইন্ডোতে, যখন প্রজেক্ট ফাইলগুলো লোড হবে (এটি কয়েক মিনিট সময় নিতে পারে), তখন একটি টার্মিনাল উইন্ডো খুলুন।
4. [ডিপ্লয়মেন্ট ধাপগুলো](../../../../../md/01.Introduction/01) অনুসরণ করুন

### লোকাল পরিবেশ

1. নিশ্চিত করুন নিচের টুলগুলো ইনস্টল করা আছে:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## মডেল পরীক্ষা করুন

1. Ollama-কে phi3:mini মডেলটি ডাউনলোড ও চালানোর জন্য বলুন:

    ```shell
    ollama run phi3:mini
    ```

    মডেলটি ডাউনলোড করতে কয়েক মিনিট সময় লাগবে।

2. আউটপুটে "success" দেখার পর, আপনি প্রম্পট থেকে মডেলটিতে মেসেজ পাঠাতে পারবেন।

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. কয়েক সেকেন্ড পর, মডেল থেকে একটি রেসপন্স স্ট্রিম দেখতে পাবেন।

4. ভাষা মডেলগুলোর বিভিন্ন কৌশল সম্পর্কে জানতে, Python নোটবুক [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) খুলুন এবং প্রতিটি সেল রান করুন। যদি আপনি 'phi3:mini' ছাড়া অন্য মডেল ব্যবহার করে থাকেন, তাহলে প্রথম সেলে `MODEL_NAME` পরিবর্তন করুন।

5. Python থেকে phi3:mini মডেলের সাথে কথোপকথন করতে, Python ফাইল [chat.py](../../../../../code/01.Introduce/chat.py) খুলুন এবং রান করুন। প্রয়োজনে ফাইলের উপরের দিকে `MODEL_NAME` পরিবর্তন করতে পারেন, এবং সিস্টেম মেসেজ বা few-shot উদাহরণ যোগ করতেও পারেন।

**অস্বীকৃতি**:  
এই নথিটি AI অনুবাদ সেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনূদিত হয়েছে। আমরা যথাসাধ্য সঠিকতার চেষ্টা করি, তবে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। মূল নথিটি তার নিজস্ব ভাষায়ই কর্তৃত্বপূর্ণ উৎস হিসেবে বিবেচিত হওয়া উচিত। গুরুত্বপূর্ণ তথ্যের জন্য পেশাদার মানব অনুবাদ গ্রহণ করার পরামর্শ দেওয়া হয়। এই অনুবাদের ব্যবহারে সৃষ্ট কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যার জন্য আমরা দায়ী নই।