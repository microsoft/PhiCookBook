{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ইন্টারঅ্যাকটিভ ফাই ৩ মিনি ৪কে ইনস্ট্রাক্ট চ্যাটবট উইথ হুইসপার\n",
    "\n",
    "### পরিচিতি:\n",
    "ইন্টারঅ্যাকটিভ ফাই ৩ মিনি ৪কে ইনস্ট্রাক্ট চ্যাটবট একটি টুল যা ব্যবহারকারীদের মাইক্রোসফট ফাই ৩ মিনি ৪কে ইনস্ট্রাক্ট ডেমোর সাথে টেক্সট বা অডিও ইনপুট ব্যবহার করে যোগাযোগ করতে দেয়। এই চ্যাটবট বিভিন্ন কাজের জন্য ব্যবহার করা যেতে পারে, যেমন অনুবাদ, আবহাওয়ার আপডেট এবং সাধারণ তথ্য সংগ্রহ।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "আপনার Huggingface অ্যাক্সেস টোকেন তৈরি করুন\n",
    "\n",
    "একটি নতুন টোকেন তৈরি করুন  \n",
    "একটি নতুন নাম দিন  \n",
    "লিখার অনুমতি নির্বাচন করুন  \n",
    "টোকেনটি কপি করুন এবং এটি নিরাপদ স্থানে সংরক্ষণ করুন\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "নিম্নলিখিত পাইথন কোড দুটি প্রধান কাজ সম্পাদন করে: `os` মডিউল আমদানি করা এবং একটি পরিবেশ ভেরিয়েবল সেট করা।\n",
    "\n",
    "1. `os` মডিউল আমদানি করা:\n",
    "   - পাইথনের `os` মডিউল অপারেটিং সিস্টেমের সাথে যোগাযোগ করার একটি উপায় প্রদান করে। এটি বিভিন্ন অপারেটিং সিস্টেম-সম্পর্কিত কাজ সম্পাদন করতে সাহায্য করে, যেমন পরিবেশ ভেরিয়েবল অ্যাক্সেস করা, ফাইল এবং ডিরেক্টরি নিয়ে কাজ করা ইত্যাদি।\n",
    "   - এই কোডে, `os` মডিউলটি `import` স্টেটমেন্ট ব্যবহার করে আমদানি করা হয়েছে। এই স্টেটমেন্টটি বর্তমান পাইথন স্ক্রিপ্টে `os` মডিউলের কার্যকারিতা ব্যবহারযোগ্য করে তোলে।\n",
    "\n",
    "2. একটি পরিবেশ ভেরিয়েবল সেট করা:\n",
    "   - পরিবেশ ভেরিয়েবল হলো একটি মান যা অপারেটিং সিস্টেমে চলমান প্রোগ্রামগুলো দ্বারা অ্যাক্সেস করা যায়। এটি কনফিগারেশন সেটিংস বা অন্যান্য তথ্য সংরক্ষণের একটি উপায় যা একাধিক প্রোগ্রাম দ্বারা ব্যবহার করা যেতে পারে।\n",
    "   - এই কোডে, একটি নতুন পরিবেশ ভেরিয়েবল `os.environ` ডিকশনারি ব্যবহার করে সেট করা হচ্ছে। ডিকশনারির কী হলো `'HF_TOKEN'`, এবং মানটি `HUGGINGFACE_TOKEN` ভেরিয়েবল থেকে অ্যাসাইন করা হয়েছে।\n",
    "   - `HUGGINGFACE_TOKEN` ভেরিয়েবলটি এই কোড স্নিপেটের ঠিক উপরে সংজ্ঞায়িত করা হয়েছে, এবং এটি `#@param` সিনট্যাক্স ব্যবহার করে একটি স্ট্রিং মান `\"hf_**************\"` অ্যাসাইন করা হয়েছে। এই সিনট্যাক্সটি প্রায়শই জুপিটার নোটবুকে ব্যবহার করা হয়, যাতে ব্যবহারকারী ইনপুট এবং প্যারামিটার কনফিগারেশন সরাসরি নোটবুক ইন্টারফেসে করতে পারে।\n",
    "   - `'HF_TOKEN'` পরিবেশ ভেরিয়েবল সেট করার মাধ্যমে, এটি প্রোগ্রামের অন্যান্য অংশ বা একই অপারেটিং সিস্টেমে চলমান অন্যান্য প্রোগ্রাম দ্বারা অ্যাক্সেসযোগ্য হয়।\n",
    "\n",
    "সারসংক্ষেপে, এই কোডটি `os` মডিউল আমদানি করে এবং `'HF_TOKEN'` নামে একটি পরিবেশ ভেরিয়েবল সেট করে, যার মান `HUGGINGFACE_TOKEN` ভেরিয়েবল থেকে প্রদান করা হয়েছে।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এই কোড স্নিপেটটি একটি ফাংশন clear_output সংজ্ঞায়িত করে, যা Jupyter Notebook বা IPython-এ বর্তমান সেলের আউটপুট মুছে ফেলার জন্য ব্যবহৃত হয়। চলুন কোডটি বিশ্লেষণ করি এবং এর কার্যকারিতা বুঝি:\n",
    "\n",
    "ফাংশন clear_output একটি প্যারামিটার গ্রহণ করে, যার নাম wait, যা একটি বুলিয়ান মান। ডিফল্টভাবে, wait-এর মান False সেট করা থাকে। এই প্যারামিটার নির্ধারণ করে যে ফাংশনটি কি নতুন আউটপুট পাওয়া পর্যন্ত অপেক্ষা করবে, বিদ্যমান আউটপুট মুছে ফেলার আগে।\n",
    "\n",
    "ফাংশনটি নিজেই বর্তমান সেলের আউটপুট মুছে ফেলার জন্য ব্যবহৃত হয়। Jupyter Notebook বা IPython-এ, যখন একটি সেল আউটপুট তৈরি করে, যেমন প্রিন্ট করা টেক্সট বা গ্রাফিকাল প্লট, সেই আউটপুট সেলের নিচে প্রদর্শিত হয়। clear_output ফাংশন আপনাকে সেই আউটপুট মুছে ফেলার সুযোগ দেয়।\n",
    "\n",
    "ফাংশনের বাস্তবায়ন কোড স্নিপেটে সরবরাহ করা হয়নি, যা এলিপসিস (...) দ্বারা নির্দেশিত হয়েছে। এলিপসিস মূলত সেই কোডের জন্য একটি প্লেসহোল্ডার যা আউটপুট মুছে ফেলার কাজ সম্পন্ন করে। ফাংশনের বাস্তবায়ন সম্ভবত Jupyter Notebook বা IPython API-এর সাথে ইন্টারঅ্যাক্ট করে সেল থেকে বিদ্যমান আউটপুট সরিয়ে ফেলার প্রক্রিয়া সম্পন্ন করে।\n",
    "\n",
    "সামগ্রিকভাবে, এই ফাংশনটি Jupyter Notebook বা IPython-এ বর্তমান সেলের আউটপুট মুছে ফেলার একটি সুবিধাজনক উপায় প্রদান করে, যা ইন্টারঅ্যাক্টিভ কোডিং সেশনের সময় প্রদর্শিত আউটপুট পরিচালনা এবং আপডেট করা সহজ করে তোলে।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "এজ টিটিএস পরিষেবা ব্যবহার করে টেক্সট-টু-স্পিচ (TTS) সম্পাদন করুন। আসুন প্রাসঙ্গিক ফাংশনগুলির বাস্তবায়ন একে একে দেখি:\n",
    "\n",
    "1. `calculate_rate_string(input_value)`: এই ফাংশনটি একটি ইনপুট মান গ্রহণ করে এবং TTS ভয়েসের জন্য গতি স্ট্রিং গণনা করে। ইনপুট মানটি বক্তৃতার পছন্দসই গতি উপস্থাপন করে, যেখানে 1 মানে স্বাভাবিক গতি। ফাংশনটি ইনপুট মান থেকে 1 বিয়োগ করে, 100 দ্বারা গুণ করে এবং তারপর চিহ্ন নির্ধারণ করে যে ইনপুট মানটি 1 এর সমান বা তার বেশি কিনা। ফাংশনটি \"{sign}{rate}\" ফরম্যাটে গতি স্ট্রিংটি ফেরত দেয়।\n",
    "\n",
    "2. `make_chunks(input_text, language)`: এই ফাংশনটি ইনপুট টেক্সট এবং একটি ভাষা প্যারামিটার হিসাবে গ্রহণ করে। এটি ভাষা-নির্দিষ্ট নিয়ম অনুযায়ী ইনপুট টেক্সটকে অংশে বিভক্ত করে। এই বাস্তবায়নে, যদি ভাষাটি \"English\" হয়, ফাংশনটি প্রতিটি পিরিয়ড (\".\") এ টেক্সটকে বিভক্ত করে এবং যেকোনো শুরুর বা শেষের ফাঁকা স্থান সরিয়ে দেয়। এটি প্রতিটি অংশে একটি পিরিয়ড যোগ করে এবং ফিল্টার করা অংশগুলির তালিকা ফেরত দেয়।\n",
    "\n",
    "3. `tts_file_name(text)`: এই ফাংশনটি ইনপুট টেক্সটের উপর ভিত্তি করে TTS অডিও ফাইলের জন্য একটি ফাইল নাম তৈরি করে। এটি টেক্সটের উপর কয়েকটি রূপান্তর সম্পাদন করে: একটি শেষের পিরিয়ড (যদি থাকে) সরানো, টেক্সটকে ছোট হাতের অক্ষরে রূপান্তর করা, শুরুর এবং শেষের ফাঁকা স্থান সরানো, এবং স্পেসগুলিকে আন্ডারস্কোর দিয়ে প্রতিস্থাপন করা। তারপর এটি টেক্সটটি সর্বাধিক 25 অক্ষরে সংক্ষিপ্ত করে (যদি দীর্ঘ হয়) অথবা পূর্ণ টেক্সট ব্যবহার করে যদি এটি খালি থাকে। অবশেষে, এটি [`uuid`] মডিউল ব্যবহার করে একটি র্যান্ডম স্ট্রিং তৈরি করে এবং সংক্ষিপ্ত টেক্সটের সাথে এটি সংযুক্ত করে ফাইল নাম তৈরি করে \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\" ফরম্যাটে।\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`: এই ফাংশনটি একাধিক অডিও ফাইলকে একক অডিও ফাইলে একত্রিত করে। এটি অডিও ফাইলের পথের একটি তালিকা এবং একটি আউটপুট পথ প্যারামিটার হিসাবে গ্রহণ করে। ফাংশনটি একটি খালি `AudioSegment` অবজেক্ট [`merged_audio`] দিয়ে শুরু করে। তারপর এটি প্রতিটি অডিও ফাইলের পথের মধ্য দিয়ে পুনরাবৃত্তি করে, `pydub` লাইব্রেরির `AudioSegment.from_file()` পদ্ধতি ব্যবহার করে অডিও ফাইল লোড করে এবং বর্তমান অডিও ফাইলটি [`merged_audio`] অবজেক্টে যোগ করে। অবশেষে, এটি MP3 ফরম্যাটে নির্দিষ্ট আউটপুট পথে একত্রিত অডিওটি রপ্তানি করে।\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`: এই ফাংশনটি এজ টিটিএস পরিষেবা ব্যবহার করে TTS অপারেশন সম্পাদন করে। এটি টেক্সট অংশগুলির একটি তালিকা, বক্তৃতার গতি, ভয়েস নাম এবং সংরক্ষণ পথ প্যারামিটার হিসাবে গ্রহণ করে। যদি অংশগুলির সংখ্যা 1 এর বেশি হয়, ফাংশনটি পৃথক অংশ অডিও ফাইল সংরক্ষণের জন্য একটি ডিরেক্টরি তৈরি করে। তারপর এটি প্রতিটি অংশের মধ্য দিয়ে পুনরাবৃত্তি করে, `calculate_rate_string()` ফাংশন, ভয়েস নাম এবং অংশ টেক্সট ব্যবহার করে একটি এজ টিটিএস কমান্ড তৈরি করে এবং `os.system()` ফাংশন ব্যবহার করে কমান্ডটি কার্যকর করে। যদি কমান্ড কার্যকর সফল হয়, এটি তৈরি করা অডিও ফাইলের পথটি একটি তালিকায় যোগ করে। সমস্ত অংশ প্রক্রিয়াকরণের পরে, এটি `merge_audio_files()` ফাংশন ব্যবহার করে পৃথক অডিও ফাইলগুলিকে একত্রিত করে এবং নির্দিষ্ট সংরক্ষণ পথে একত্রিত অডিওটি সংরক্ষণ করে। যদি শুধুমাত্র একটি অংশ থাকে, এটি সরাসরি এজ টিটিএস কমান্ড তৈরি করে এবং অডিওটি সংরক্ষণ পথে সংরক্ষণ করে। অবশেষে, এটি তৈরি করা অডিও ফাইলের সংরক্ষণ পথটি ফেরত দেয়।\n",
    "\n",
    "6. `random_audio_name_generate()`: এই ফাংশনটি [`uuid`] মডিউল ব্যবহার করে একটি র্যান্ডম অডিও ফাইল নাম তৈরি করে। এটি একটি র্যান্ডম UUID তৈরি করে, এটিকে একটি স্ট্রিংয়ে রূপান্তর করে, প্রথম 8 অক্ষর গ্রহণ করে, \".mp3\" এক্সটেনশন যোগ করে এবং র্যান্ডম অডিও ফাইল নামটি ফেরত দেয়।\n",
    "\n",
    "7. `talk(input_text)`: এই ফাংশনটি TTS অপারেশন সম্পাদনের প্রধান এন্ট্রি পয়েন্ট। এটি একটি ইনপুট টেক্সট প্যারামিটার হিসাবে গ্রহণ করে। এটি প্রথমে ইনপুট টেক্সটের দৈর্ঘ্য পরীক্ষা করে নির্ধারণ করে যে এটি একটি দীর্ঘ বাক্য (600 অক্ষরের সমান বা তার বেশি) কিনা। দৈর্ঘ্য এবং `translate_text_flag` ভেরিয়েবলের মানের উপর ভিত্তি করে, এটি ভাষা নির্ধারণ করে এবং `make_chunks()` ফাংশন ব্যবহার করে টেক্সট অংশগুলির তালিকা তৈরি করে। তারপর এটি `random_audio_name_generate()` ফাংশন ব্যবহার করে অডিও ফাইলের জন্য একটি সংরক্ষণ পথ তৈরি করে। অবশেষে, এটি `edge_free_tts()` ফাংশনটি কল করে TTS অপারেশন সম্পাদন করে এবং তৈরি করা অডিও ফাইলের সংরক্ষণ পথটি ফেরত দেয়।\n",
    "\n",
    "সামগ্রিকভাবে, এই ফাংশনগুলি একসাথে কাজ করে ইনপুট টেক্সটকে অংশে বিভক্ত করতে, অডিও ফাইলের জন্য একটি ফাইল নাম তৈরি করতে, এজ টিটিএস পরিষেবা ব্যবহার করে TTS অপারেশন সম্পাদন করতে এবং পৃথক অডিও ফাইলগুলিকে একক অডিও ফাইলে একত্রিত করতে।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "দুটি ফাংশন: convert_to_text এবং run_text_prompt এর বাস্তবায়ন, এবং দুটি ক্লাস: str এবং Audio এর ঘোষণা।\n",
    "\n",
    "convert_to_text ফাংশনটি একটি audio_path ইনপুট হিসেবে গ্রহণ করে এবং একটি মডেল যার নাম whisper_model ব্যবহার করে অডিওকে টেক্সটে রূপান্তর করে। প্রথমে ফাংশনটি পরীক্ষা করে gpu ফ্ল্যাগটি True কিনা। যদি এটি True হয়, তাহলে whisper_model ব্যবহার করা হয় কিছু নির্দিষ্ট প্যারামিটার সহ যেমন word_timestamps=True, fp16=True, language='English', এবং task='translate'। যদি gpu ফ্ল্যাগটি False হয়, তাহলে whisper_model ব্যবহার করা হয় fp16=False সহ। প্রাপ্ত ট্রান্সক্রিপশনটি 'scan.txt' নামে একটি ফাইলে সংরক্ষণ করা হয় এবং টেক্সট হিসেবে ফেরত দেওয়া হয়।\n",
    "\n",
    "run_text_prompt ফাংশনটি একটি message এবং একটি chat_history ইনপুট হিসেবে গ্রহণ করে। এটি phi_demo ফাংশন ব্যবহার করে ইনপুট message এর উপর ভিত্তি করে একটি চ্যাটবট থেকে একটি প্রতিক্রিয়া তৈরি করে। তৈরি করা প্রতিক্রিয়াটি talk ফাংশনে পাঠানো হয়, যা প্রতিক্রিয়াটিকে একটি অডিও ফাইলে রূপান্তর করে এবং ফাইলের পথ ফেরত দেয়। Audio ক্লাসটি অডিও ফাইলটি প্রদর্শন এবং প্লে করতে ব্যবহৃত হয়। অডিওটি IPython.display মডিউলের display ফাংশন ব্যবহার করে প্রদর্শিত হয়, এবং Audio অবজেক্টটি autoplay=True প্যারামিটার সহ তৈরি করা হয়, যাতে অডিওটি স্বয়ংক্রিয়ভাবে প্লে শুরু হয়। chat_history আপডেট করা হয় ইনপুট message এবং তৈরি করা প্রতিক্রিয়ার সাথে, এবং একটি খালি স্ট্রিং এবং আপডেট করা chat_history ফেরত দেওয়া হয়।\n",
    "\n",
    "str ক্লাসটি পাইথনের একটি বিল্ট-ইন ক্লাস যা অক্ষরের একটি সিকোয়েন্স উপস্থাপন করে। এটি স্ট্রিং ম্যানিপুলেট এবং কাজ করার জন্য বিভিন্ন পদ্ধতি প্রদান করে, যেমন capitalize, casefold, center, count, encode, endswith, expandtabs, find, format, index, isalnum, isalpha, isascii, isdecimal, isdigit, isidentifier, islower, isnumeric, isprintable, isspace, istitle, isupper, join, ljust, lower, lstrip, partition, replace, removeprefix, removesuffix, rfind, rindex, rjust, rpartition, rsplit, rstrip, split, splitlines, startswith, strip, swapcase, title, translate, upper, zfill, এবং আরও অনেক কিছু। এই পদ্ধতিগুলি আপনাকে স্ট্রিং অনুসন্ধান, প্রতিস্থাপন, ফরম্যাটিং এবং ম্যানিপুলেট করার মতো অপারেশন সম্পাদন করতে দেয়।\n",
    "\n",
    "Audio ক্লাসটি একটি কাস্টম ক্লাস যা একটি অডিও অবজেক্ট উপস্থাপন করে। এটি Jupyter Notebook পরিবেশে একটি অডিও প্লেয়ার তৈরি করতে ব্যবহৃত হয়। ক্লাসটি বিভিন্ন প্যারামিটার গ্রহণ করে যেমন data, filename, url, embed, rate, autoplay, এবং normalize। data প্যারামিটারটি একটি numpy array, নমুনার তালিকা, একটি ফাইলনাম বা URL উপস্থাপনকারী একটি স্ট্রিং, অথবা raw PCM data হতে পারে। filename প্যারামিটারটি স্থানীয় ফাইল থেকে অডিও ডেটা লোড করার জন্য নির্দিষ্ট করতে ব্যবহৃত হয়, এবং url প্যারামিটারটি অডিও ডেটা ডাউনলোড করার জন্য একটি URL নির্দিষ্ট করতে ব্যবহৃত হয়। embed প্যারামিটারটি নির্ধারণ করে অডিও ডেটা একটি data URI ব্যবহার করে এম্বেড করা হবে কিনা বা মূল উৎস থেকে রেফারেন্স করা হবে। rate প্যারামিটারটি অডিও ডেটার স্যাম্পলিং রেট নির্দিষ্ট করে। autoplay প্যারামিটারটি নির্ধারণ করে অডিও স্বয়ংক্রিয়ভাবে প্লে শুরু করবে কিনা। normalize প্যারামিটারটি নির্ধারণ করে অডিও ডেটা সর্বোচ্চ সম্ভাব্য রেঞ্জে rescale করা হবে কিনা। Audio ক্লাসটি reload এর মতো পদ্ধতি প্রদান করে, যা ফাইল বা URL থেকে অডিও ডেটা পুনরায় লোড করতে ব্যবহৃত হয়, এবং src_attr, autoplay_attr, এবং element_id_attr এর মতো অ্যাট্রিবিউট প্রদান করে, যা HTML এ অডিও এলিমেন্টের জন্য সংশ্লিষ্ট অ্যাট্রিবিউটগুলি পুনরুদ্ধার করতে ব্যবহৃত হয়।\n",
    "\n",
    "সার্বিকভাবে, এই ফাংশন এবং ক্লাসগুলি অডিওকে টেক্সটে রূপান্তর করতে, চ্যাটবট থেকে অডিও প্রতিক্রিয়া তৈরি করতে, এবং Jupyter Notebook পরিবেশে অডিও প্রদর্শন এবং প্লে করতে ব্যবহৃত হয়।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**অস্বীকৃতি**:  \nএই নথিটি AI অনুবাদ পরিষেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসম্ভব সঠিকতার জন্য চেষ্টা করি, তবে অনুগ্রহ করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। মূল ভাষায় থাকা নথিটিকে প্রামাণিক উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য, পেশাদার মানব অনুবাদ সুপারিশ করা হয়। এই অনুবাদ ব্যবহারের ফলে কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যা হলে আমরা দায়বদ্ধ থাকব না।\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T19:53:56+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "bn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}