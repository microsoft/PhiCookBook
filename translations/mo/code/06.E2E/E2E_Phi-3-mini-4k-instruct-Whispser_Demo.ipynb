{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 互動式 Phi 3 Mini 4K 指令聊天機器人搭配 Whisper\n",
    "\n",
    "### 介紹：\n",
    "互動式 Phi 3 Mini 4K 指令聊天機器人是一個工具，讓使用者可以透過文字或語音輸入與 Microsoft Phi 3 Mini 4K 指令示範進行互動。此聊天機器人可用於多種任務，例如翻譯、天氣更新以及一般資訊收集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Atl_WEmtR0Yd"
   },
   "outputs": [],
   "source": [
    "#Install required Python Packages\n",
    "!pip install accelerate\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
    "!pip install transformers\n",
    "!pip install wheel\n",
    "!pip install gradio\n",
    "!pip install pydub==0.25.1\n",
    "!pip install edge-tts\n",
    "!pip install openai-whisper==20231117\n",
    "!pip install ffmpeg==1.4\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if Cuda support is available \n",
    "# Output True = Cuda\n",
    "# Output False = No Cuda (installing Cuda will be required to run the model on GPU)\n",
    "import os \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKAUp20H4ZXl"
   },
   "source": [
    "[建立您的 Huggingface 存取權杖](https://huggingface.co/settings/tokens)\n",
    "\n",
    "建立一個新的權杖  \n",
    "提供一個新的名稱  \n",
    "選擇寫入權限  \n",
    "複製權杖並將其保存在安全的地方\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是 Python 程式碼的主要功能：匯入 `os` 模組以及設定環境變數。\n",
    "\n",
    "1. 匯入 `os` 模組：\n",
    "   - Python 中的 `os` 模組提供與作業系統互動的方式。它允許執行各種與作業系統相關的操作，例如存取環境變數、操作檔案和目錄等。\n",
    "   - 在這段程式碼中，使用 `import` 語句匯入了 `os` 模組。這個語句使得 `os` 模組的功能可以在目前的 Python 腳本中使用。\n",
    "\n",
    "2. 設定環境變數：\n",
    "   - 環境變數是一種可以被作業系統上執行的程式存取的值。它是一種儲存設定或其他資訊的方法，這些資訊可以被多個程式使用。\n",
    "   - 在這段程式碼中，使用 `os.environ` 字典設定了一個新的環境變數。字典的鍵是 `'HF_TOKEN'`，其值是從 `HUGGINGFACE_TOKEN` 變數中取得的。\n",
    "   - `HUGGINGFACE_TOKEN` 變數在這段程式碼的上方被定義，並使用 `#@param` 語法賦值為字串 `\"hf_**************\"`。這種語法通常用於 Jupyter notebook，允許使用者直接在 notebook 介面中輸入參數或進行設定。\n",
    "   - 設定 `'HF_TOKEN'` 環境變數後，程式的其他部分或在同一作業系統上執行的其他程式都可以存取這個變數。\n",
    "\n",
    "總結來說，這段程式碼匯入了 `os` 模組，並設定了一個名為 `'HF_TOKEN'` 的環境變數，其值來自 `HUGGINGFACE_TOKEN` 變數。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "N5r2ikbwR68c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the Hugging Face Token from \n",
    "# add the Hugging Face Token to the environment variables\n",
    "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
    "os.environ['HF_TOKEN']HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此程式碼片段定義了一個名為 `clear_output` 的函式，用於清除 Jupyter Notebook 或 IPython 中當前儲存格的輸出。以下是對程式碼的逐步解析及其功能的理解：\n",
    "\n",
    "函式 `clear_output` 接受一個名為 `wait` 的參數，該參數是一個布林值。預設情況下，`wait` 的值為 `False`。此參數決定函式是否應等待新的輸出可用以取代現有輸出後再進行清除。\n",
    "\n",
    "此函式的主要用途是清除當前儲存格的輸出。在 Jupyter Notebook 或 IPython 中，當一個儲存格產生輸出（例如列印的文字或圖形繪圖）時，該輸出會顯示在儲存格下方。`clear_output` 函式允許您清除這些輸出。\n",
    "\n",
    "程式碼片段中並未提供函式的具體實現，僅以省略號（...）表示。省略號代表實際執行清除輸出的程式碼的佔位符。函式的具體實現可能涉及與 Jupyter Notebook 或 IPython 的 API 互動，以移除儲存格中的現有輸出。\n",
    "\n",
    "總的來說，這個函式提供了一種方便的方法來清除 Jupyter Notebook 或 IPython 中當前儲存格的輸出，使得在互動式編程過程中更容易管理和更新顯示的輸出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nmXm0dxuRinA"
   },
   "outputs": [],
   "source": [
    "# Download Phi-3-mini-4k-instruct model & Whisper Tiny\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "#whisper for speech to text()\n",
    "import whisper\n",
    "select_model =\"tiny\" # ['tiny', 'base']\n",
    "whisper_model = whisper.load_model(select_model)\n",
    "\n",
    "#from IPython.display import clear_output\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 Edge TTS 服務進行文字轉語音 (TTS)。以下是相關功能的逐步實現：\n",
    "\n",
    "1. `calculate_rate_string(input_value)`：此函數接收一個輸入值並計算 TTS 語音的速度字串。輸入值表示所需的語音速度，其中 1 代表正常速度。函數通過將輸入值減去 1，乘以 100，然後根據輸入值是否大於或等於 1 確定符號來計算速度字串。函數以 \"{sign}{rate}\" 格式返回速度字串。\n",
    "\n",
    "2. `make_chunks(input_text, language)`：此函數接收輸入文字和語言作為參數。它根據語言特定的規則將輸入文字分割成多個片段。在此實現中，如果語言是 \"English\"，函數會在每個句號 (\".\") 處分割文字，並移除任何前後的空白。接著，它會在每個片段後附加句號並返回過濾後的片段列表。\n",
    "\n",
    "3. `tts_file_name(text)`：此函數根據輸入文字生成 TTS 音頻檔案的檔名。它對文字進行多項轉換：移除尾部的句號（如果存在）、將文字轉換為小寫、去除前後空白、並將空格替換為底線。然後，它將文字截斷至最多 25 個字元（如果超過）或使用完整文字（如果文字為空）。最後，它使用 [`uuid`] 模組生成隨機字串，並將其與截斷後的文字結合，生成檔名格式為 \"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"。\n",
    "\n",
    "4. `merge_audio_files(audio_paths, output_path)`：此函數將多個音頻檔案合併為一個音頻檔案。它接收音頻檔案路徑列表和輸出路徑作為參數。函數初始化一個空的 `AudioSegment` 對象，稱為 [`merged_audio`]。接著，它遍歷每個音頻檔案路徑，使用 `pydub` 庫的 `AudioSegment.from_file()` 方法載入音頻檔案，並將當前音頻檔案附加到 [`merged_audio`] 對象。最後，它將合併後的音頻以 MP3 格式匯出到指定的輸出路徑。\n",
    "\n",
    "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`：此函數使用 Edge TTS 服務執行 TTS 操作。它接收文字片段列表、語音速度、語音名稱和保存路徑作為參數。如果片段數量大於 1，函數會為存儲個別片段音頻檔案創建目錄。接著，它遍歷每個片段，使用 `calculate_rate_string()` 函數、語音名稱和片段文字構建 Edge TTS 命令，並使用 `os.system()` 函數執行命令。如果命令執行成功，它會將生成的音頻檔案路徑附加到列表中。在處理所有片段後，它使用 `merge_audio_files()` 函數合併個別音頻檔案，並將合併後的音頻保存到指定的保存路徑。如果只有一個片段，它會直接生成 Edge TTS 命令並將音頻保存到保存路徑。最後，它返回生成的音頻檔案保存路徑。\n",
    "\n",
    "6. `random_audio_name_generate()`：此函數使用 [`uuid`] 模組生成隨機音頻檔案名稱。它生成一個隨機 UUID，轉換為字串，取前 8 個字元，附加 \".mp3\" 擴展名，並返回隨機音頻檔案名稱。\n",
    "\n",
    "7. `talk(input_text)`：此函數是執行 TTS 操作的主要入口。它接收輸入文字作為參數。首先，它檢查輸入文字的長度以確定是否為長句（大於或等於 600 個字元）。根據文字長度和 `translate_text_flag` 變數的值，它確定語言並使用 `make_chunks()` 函數生成文字片段列表。接著，它使用 `random_audio_name_generate()` 函數生成音頻檔案的保存路徑。最後，它調用 `edge_free_tts()` 函數執行 TTS 操作並返回生成的音頻檔案保存路徑。\n",
    "\n",
    "總體而言，這些函數協同工作，將輸入文字分割成片段，生成音頻檔案名稱，使用 Edge TTS 服務執行 TTS 操作，並將個別音頻檔案合併為一個音頻檔案。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "Mv4WVhNUz4IL",
    "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
   },
   "outputs": [],
   "source": [
    "#@title Edge TTS\n",
    "def calculate_rate_string(input_value):\n",
    "    rate = (input_value - 1) * 100\n",
    "    sign = '+' if input_value >= 1 else '-'\n",
    "    return f\"{sign}{abs(int(rate))}\"\n",
    "\n",
    "\n",
    "def make_chunks(input_text, language):\n",
    "    language=\"English\"\n",
    "    if language == \"English\":\n",
    "      temp_list = input_text.strip().split(\".\")\n",
    "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
    "      if temp_list[-1].strip():\n",
    "          filtered_list.append(temp_list[-1].strip())\n",
    "      return filtered_list\n",
    "\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "def tts_file_name(text):\n",
    "    if text.endswith(\".\"):\n",
    "        text = text[:-1]\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace(\" \",\"_\")\n",
    "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
    "    random_string = uuid.uuid4().hex[:8].upper()\n",
    "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
    "    return file_name\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import shutil\n",
    "import os\n",
    "def merge_audio_files(audio_paths, output_path):\n",
    "    # Initialize an empty AudioSegment\n",
    "    merged_audio = AudioSegment.silent(duration=0)\n",
    "\n",
    "    # Iterate through each audio file path\n",
    "    for audio_path in audio_paths:\n",
    "        # Load the audio file using Pydub\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "        # Append the current audio file to the merged_audio\n",
    "        merged_audio += audio\n",
    "\n",
    "    # Export the merged audio to the specified output path\n",
    "    merged_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
    "  # print(chunks_list)\n",
    "  if len(chunks_list)>1:\n",
    "    chunk_audio_list=[]\n",
    "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
    "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
    "    os.mkdir(\"/content/edge_tts_voice\")\n",
    "    k=1\n",
    "    for i in chunks_list:\n",
    "      print(i)\n",
    "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
    "      print(edge_command)\n",
    "      var1=os.system(edge_command)\n",
    "      if var1==0:\n",
    "        pass\n",
    "      else:\n",
    "        print(f\"Failed: {i}\")\n",
    "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
    "      k+=1\n",
    "    # print(chunk_audio_list)\n",
    "    merge_audio_files(chunk_audio_list, save_path)\n",
    "  else:\n",
    "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
    "    print(edge_command)\n",
    "    var2=os.system(edge_command)\n",
    "    if var2==0:\n",
    "      pass\n",
    "    else:\n",
    "      print(f\"Failed: {chunks_list[0]}\")\n",
    "  return save_path\n",
    "\n",
    "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
    "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
    "Language = \"English\" # @param ['English']\n",
    "# Gender of voice simply change from male to female and choose the voice you want to use\n",
    "Gender = \"Female\"# @param ['Male', 'Female']\n",
    "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
    "speed = 1  # @param {type: \"number\"}\n",
    "translate_text_flag  = False\n",
    "if len(text)>=600:\n",
    "  long_sentence = True\n",
    "else:\n",
    "  long_sentence = False\n",
    "\n",
    "# long_sentence = False # @param {type:\"boolean\"}\n",
    "save_path = ''  # @param {type: \"string\"}\n",
    "if len(save_path)==0:\n",
    "  save_path=tts_file_name(text)\n",
    "if Language == \"English\" :\n",
    "  if Gender==\"Male\":\n",
    "    voice_name=\"en-US-ChristopherNeural\"\n",
    "  if Gender==\"Female\":\n",
    "    voice_name=female_voice\n",
    "    # voice_name=\"en-US-AriaNeural\"\n",
    "\n",
    "\n",
    "if translate_text_flag:\n",
    "  input_text=text\n",
    "  # input_text=translate_text(text, Language)\n",
    "  # print(\"Translateting\")\n",
    "else:\n",
    "  input_text=text\n",
    "if long_sentence==True and translate_text_flag==True:\n",
    "  chunks_list=make_chunks(input_text,Language)\n",
    "elif long_sentence==True and translate_text_flag==False:\n",
    "  chunks_list=make_chunks(input_text,\"English\")\n",
    "else:\n",
    "  chunks_list=[input_text]\n",
    "# print(chunks_list)\n",
    "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "# from IPython.display import clear_output\n",
    "# clear_output()\n",
    "# from IPython.display import Audio\n",
    "# Audio(edge_save_path, autoplay=True)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Audio\n",
    "if not os.path.exists(\"/content/audio\"):\n",
    "    os.mkdir(\"/content/audio\")\n",
    "import uuid\n",
    "def random_audio_name_generate():\n",
    "  random_uuid = uuid.uuid4()\n",
    "  audio_extension = \".mp3\"\n",
    "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
    "  return random_audio_name\n",
    "def talk(input_text):\n",
    "  global translate_text_flag,Language,speed,voice_name\n",
    "  if len(input_text)>=600:\n",
    "    long_sentence = True\n",
    "  else:\n",
    "    long_sentence = False\n",
    "\n",
    "  if long_sentence==True and translate_text_flag==True:\n",
    "    chunks_list=make_chunks(input_text,Language)\n",
    "  elif long_sentence==True and translate_text_flag==False:\n",
    "    chunks_list=make_chunks(input_text,\"English\")\n",
    "  else:\n",
    "    chunks_list=[input_text]\n",
    "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
    "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
    "  return edge_save_path\n",
    "\n",
    "\n",
    "edge_save_path=talk(text)\n",
    "Audio(edge_save_path, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "實現了兩個函數：convert_to_text 和 run_text_prompt，以及宣告了兩個類別：str 和 Audio。\n",
    "\n",
    "convert_to_text 函數接收一個 audio_path 作為輸入，並使用名為 whisper_model 的模型將音頻轉錄為文字。該函數首先檢查 gpu 標誌是否設置為 True。如果是，則使用 whisper_model 並設置一些參數，例如 word_timestamps=True、fp16=True、language='English' 和 task='translate'。如果 gpu 標誌為 False，則使用 whisper_model 並設置 fp16=False。轉錄結果會保存到名為 'scan.txt' 的文件中，並返回轉錄的文字。\n",
    "\n",
    "run_text_prompt 函數接收一個 message 和一個 chat_history 作為輸入。它使用 phi_demo 函數根據輸入的 message 從聊天機器人生成回應。生成的回應會傳遞給 talk 函數，該函數將回應轉換為音頻文件並返回文件路徑。Audio 類別用於顯示和播放音頻文件。音頻通過 IPython.display 模組的 display 函數顯示，並使用 autoplay=True 參數創建 Audio 對象，使音頻自動播放。chat_history 會更新為包含輸入的 message 和生成的回應，並返回一個空字串和更新後的 chat_history。\n",
    "\n",
    "str 類別是 Python 中的內建類別，用於表示字符序列。它提供了多種方法來操作和處理字串，例如 capitalize、casefold、center、count、encode、endswith、expandtabs、find、format、index、isalnum、isalpha、isascii、isdecimal、isdigit、isidentifier、islower、isnumeric、isprintable、isspace、istitle、isupper、join、ljust、lower、lstrip、partition、replace、removeprefix、removesuffix、rfind、rindex、rjust、rpartition、rsplit、rstrip、split、splitlines、startswith、strip、swapcase、title、translate、upper、zfill 等等。這些方法允許執行搜尋、替換、格式化和操作字串等操作。\n",
    "\n",
    "Audio 類別是一個自定義類別，用於表示音頻對象。它用於在 Jupyter Notebook 環境中創建音頻播放器。該類別接受多種參數，例如 data、filename、url、embed、rate、autoplay 和 normalize。data 參數可以是 numpy 陣列、樣本列表、表示文件名或 URL 的字串，或者是原始 PCM 數據。filename 參數用於指定從本地文件加載音頻數據，url 參數用於指定從 URL 下載音頻數據。embed 參數決定音頻數據是使用數據 URI 嵌入還是引用原始來源。rate 參數指定音頻數據的採樣率。autoplay 參數決定音頻是否自動播放。normalize 參數指定音頻數據是否應該被標準化（重新縮放到最大可能範圍）。Audio 類別還提供了 reload 方法，用於從文件或 URL 重新加載音頻數據，以及 src_attr、autoplay_attr 和 element_id_attr 等屬性，用於檢索 HTML 中音頻元素的相應屬性。\n",
    "\n",
    "總的來說，這些函數和類別用於將音頻轉錄為文字、從聊天機器人生成音頻回應，以及在 Jupyter Notebook 環境中顯示和播放音頻。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6aTA6mk7Gi",
    "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
   },
   "outputs": [],
   "source": [
    "#@title Run gradio app\n",
    "def convert_to_text(audio_path):\n",
    "  gpu=True\n",
    "  if gpu:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
    "  else:\n",
    "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
    "  with open('scan.txt', 'w') as file:\n",
    "    file.write(str(result))\n",
    "  return result[\"text\"]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "from IPython.display import Audio, display\n",
    "def run_text_prompt(message, chat_history):\n",
    "    bot_message = phi_demo(message)\n",
    "    edge_save_path=talk(bot_message)\n",
    "    # print(edge_save_path)\n",
    "    display(Audio(edge_save_path, autoplay=True))\n",
    "\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def run_audio_prompt(audio, chat_history):\n",
    "    if audio is None:\n",
    "        return None, chat_history\n",
    "    print(audio)\n",
    "    message_transcription = convert_to_text(audio)\n",
    "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
    "    return None, chat_history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask anything\")\n",
    "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "\n",
    "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
    "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
    "\n",
    "demo.launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n---\n\n**免責聲明**：  \n本文件已使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。雖然我們致力於提供準確的翻譯，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於關鍵資訊，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或誤釋不承擔責任。\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "coopTranslator": {
   "original_hash": "751cbc4b70dda9c27b60003cc36ce794",
   "translation_date": "2025-09-12T19:53:14+00:00",
   "source_file": "code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb",
   "language_code": "mo"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}