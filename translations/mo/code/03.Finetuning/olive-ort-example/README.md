<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "aed7639909ebbd1960507880cff2ae4c",
  "translation_date": "2025-04-04T11:25:40+00:00",
  "source_file": "code\\03.Finetuning\\olive-ort-example\\README.md",
  "language_code": "mo"
}
-->
# Fine-tune Phi3 ye Olive laa k…îr…õ

Esi y…õ example la, wo y…õ Olive la kp…îk…õ:

1. Fine-tune LoRA adapter la k…õ phrases ye kp…î Sad, Joy, Fear, Surprise.
1. Merge adapter weights k…î base model me.
1. Optimize k…õ Quantize model la w…î `int4`.

D…î ko wo y…õ n…î s…õ…õ fine-tuned model la inference w…î ONNX Runtime (ORT) Generate API kp…î.

> **‚ö†Ô∏è Fine-tuning w…î y…õ, wo y…õ GPU la k…õ w…î y…õ - n…î y…õ A10, V100, A100.**

## üíæ Install

Y…õ Python virtual environment fofo (n…î y…õ `conda`):

```bash
conda create -n olive-ai python=3.11
conda activate olive-ai
```

Esi y…õ Olive k…õ dependencies w…î fine-tuning workflow y…õ:

```bash
cd Phi-3CookBook/code/04.Finetuning/olive-ort-example
pip install olive-ai[gpu]
pip install -r requirements.txt
```

## üß™ Fine-tune Phi3 ye Olive
[Olive configuration file](../../../../../code/03.Finetuning/olive-ort-example/phrase-classification.json) la y…õ *workflow* w…î *passes* n…î kp…î:

Phi3 -> LoRA -> MergeAdapterWeights -> ModelBuilder

Y…õ high-level la, workflow la y…õ:

1. Fine-tune Phi3 (150 steps la, wo y…õ modify) w…î [dataset/data-classification.json](../../../../../code/03.Finetuning/olive-ort-example/dataset/dataset-classification.json) data la kp…î.
1. Merge LoRA adapter weights k…î base model me. N…î y…õ single model artifact w…î ONNX format.
1. Model Builder y…õ model la optimize w…î ONNX runtime *k…õ* quantize model la w…î `int4`.

W…î y…õ workflow la execute, y…õ:

```bash
olive run --config phrase-classification.json
```

Esi Olive y…õ complete, optimized `int4` fine-tuned Phi3 model la w…î: `code/04.Finetuning/olive-ort-example/models/lora-merge-mb/gpu-cuda_model`.

## üßë‚Äçüíª Integrate fine-tuned Phi3 w…î wo application me 

W…î app la run:

```bash
python app/app.py --phrase "cricket is a wonderful sport!" --model-path models/lora-merge-mb/gpu-cuda_model
```

Response la y…õ single word classification w…î phrase (Sad/Joy/Fear/Surprise).

It seems like "mo" might refer to a language or abbreviation, but it's not clear which specific language or context you're referring to. Could you clarify what "mo" means? For example, are you asking for a translation into Maori, Mongolian, or another language?