<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:41:29+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "mo"
}
-->
# 主要提及的關鍵技術包括

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - 一個基於 DirectX 12 的低階 API，用於硬體加速的機器學習。
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - 由 Nvidia 開發的平行運算平台及應用程式介面（API）模型，能在圖形處理器（GPU）上進行通用運算。
3. [ONNX](https://onnx.ai/)（Open Neural Network Exchange）- 一種開放格式，用於表示機器學習模型，提供不同機器學習框架間的互通性。
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)（Generic Graph Update Format）- 一種用於表示和更新機器學習模型的格式，特別適合能在 CPU 上以 4-8 位元量化有效運行的小型語言模型。

## DirectML

DirectML 是一個低階 API，能實現硬體加速的機器學習。它建立於 DirectX 12 之上，利用 GPU 加速，且不依賴特定廠商，意味著在不同 GPU 廠商間無需修改程式碼即可運作。它主要用於 GPU 上的模型訓練與推論工作負載。

在硬體支援方面，DirectML 設計能支援多種 GPU，包括 AMD 的整合與獨立 GPU、Intel 的整合 GPU 以及 NVIDIA 的獨立 GPU。它是 Windows AI 平台的一部分，支援 Windows 10 與 11，允許在任何 Windows 裝置上進行模型訓練與推論。

DirectML 也持續有更新與機會，例如支援多達 150 個 ONNX 運算子，並被 ONNX runtime 與 WinML 使用。它由主要的整合硬體廠商（IHVs）支持，各自實作多種元命令。

## CUDA

CUDA（Compute Unified Device Architecture）是 Nvidia 創建的平行運算平台及應用程式介面（API）模型。它允許軟體開發者使用支援 CUDA 的圖形處理器（GPU）進行通用運算，這種方法稱為 GPGPU（圖形處理器通用運算）。CUDA 是 Nvidia GPU 加速的關鍵技術，廣泛應用於機器學習、科學運算及影片處理等領域。

CUDA 的硬體支援專屬於 Nvidia 的 GPU，因為它是 Nvidia 的專有技術。每個架構支援特定版本的 CUDA 工具包，該工具包提供開發者建置與執行 CUDA 應用所需的函式庫與工具。

## ONNX

ONNX（Open Neural Network Exchange）是一種開放格式，用於表示機器學習模型。它定義了可擴充的計算圖模型，以及內建運算子和標準資料類型的定義。ONNX 讓開發者能在不同機器學習框架間移動模型，促進互通性，並簡化 AI 應用的建立與部署。

Phi3 mini 可在 CPU 與 GPU 上使用 ONNX Runtime 運行，支援多種裝置，包括伺服器平台、Windows、Linux 與 Mac 桌面，以及行動 CPU。
我們新增的優化配置包括：

- 用於 int4 DML 的 ONNX 模型：透過 AWQ 量化為 int4
- 用於 fp16 CUDA 的 ONNX 模型
- 用於 int4 CUDA 的 ONNX 模型：透過 RTN 量化為 int4
- 用於 int4 CPU 與行動裝置的 ONNX 模型：透過 RTN 量化為 int4

## Llama.cpp

Llama.cpp 是一個以 C++ 編寫的開源軟體庫。它能對多種大型語言模型（LLMs）進行推論，包括 Llama。該專案與 ggml 函式庫（通用張量庫）一同開發，目標是提供比原始 Python 實作更快的推論速度與更低的記憶體使用量。它支援硬體優化、量化，並提供簡單的 API 與範例。如果你對高效的 LLM 推論有興趣，llama.cpp 值得一試，因為 Phi3 可運行 Llama.cpp。

## GGUF

GGUF（Generic Graph Update Format）是一種用於表示和更新機器學習模型的格式。它特別適合能在 CPU 上以 4-8 位元量化有效運行的小型語言模型（SLMs）。GGUF 對於快速原型開發以及在邊緣裝置或批次作業（如 CI/CD 流程）中執行模型非常有用。

**免責聲明**：  
本文件係使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。雖然我們致力於確保準確性，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應視為權威來源。對於重要資訊，建議採用專業人工翻譯。我們不對因使用本翻譯而產生的任何誤解或誤釋承擔責任。