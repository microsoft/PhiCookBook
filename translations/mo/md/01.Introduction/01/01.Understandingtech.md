<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-07T14:59:39+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "mo"
}
-->
# Key technologies mentioned include

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - API a nivel bajo para aprendizaje automático acelerado por hardware construido sobre DirectX 12.  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - plataforma de computación paralela y modelo de interfaz de programación de aplicaciones (API) desarrollado por Nvidia, que permite procesamiento de propósito general en unidades de procesamiento gráfico (GPUs).  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - formato abierto diseñado para representar modelos de aprendizaje automático que facilita la interoperabilidad entre diferentes frameworks de ML.  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - formato usado para representar y actualizar modelos de aprendizaje automático, especialmente útil para modelos de lenguaje pequeños que pueden funcionar eficientemente en CPUs con cuantización de 4-8 bits.

## DirectML

DirectML es una API de bajo nivel que permite el aprendizaje automático acelerado por hardware. Está construido sobre DirectX 12 para aprovechar la aceleración por GPU y es independiente del fabricante, lo que significa que no requiere cambios en el código para funcionar con diferentes proveedores de GPU. Se usa principalmente para cargas de trabajo de entrenamiento e inferencia de modelos en GPUs.

En cuanto al soporte de hardware, DirectML está diseñado para funcionar con una amplia variedad de GPUs, incluyendo GPUs integradas y discretas de AMD, GPUs integradas de Intel y GPUs discretas de NVIDIA. Forma parte de la Plataforma de IA de Windows y es compatible con Windows 10 y 11, permitiendo el entrenamiento y la inferencia de modelos en cualquier dispositivo con Windows.

Ha habido actualizaciones y oportunidades relacionadas con DirectML, como el soporte para hasta 150 operadores ONNX y su uso tanto en ONNX runtime como en WinML. Está respaldado por los principales proveedores de hardware integrados (IHVs), cada uno implementando diversos metacomandos.

## CUDA

CUDA, que significa Compute Unified Device Architecture, es una plataforma de computación paralela y un modelo de interfaz de programación de aplicaciones (API) creado por Nvidia. Permite a los desarrolladores usar una unidad de procesamiento gráfico (GPU) compatible con CUDA para procesamiento de propósito general, un enfoque conocido como GPGPU (computación de propósito general en GPUs). CUDA es un habilitador clave de la aceleración por GPU de Nvidia y se utiliza ampliamente en diversos campos, incluyendo aprendizaje automático, computación científica y procesamiento de video.

El soporte de hardware para CUDA es específico de las GPUs de Nvidia, ya que es una tecnología propietaria desarrollada por Nvidia. Cada arquitectura soporta versiones específicas del toolkit CUDA, que proporciona las bibliotecas y herramientas necesarias para que los desarrolladores construyan y ejecuten aplicaciones CUDA.

## ONNX

ONNX (Open Neural Network Exchange) es un formato abierto diseñado para representar modelos de aprendizaje automático. Proporciona una definición de un modelo de grafo computacional extensible, así como definiciones de operadores integrados y tipos de datos estándar. ONNX permite a los desarrolladores mover modelos entre diferentes frameworks de ML, facilitando la interoperabilidad y haciendo más sencillo crear y desplegar aplicaciones de IA.

Phi3 mini puede ejecutarse con ONNX Runtime en CPU y GPU en diversos dispositivos, incluyendo plataformas servidor, Windows, Linux y Mac de escritorio, así como CPUs móviles.  
Las configuraciones optimizadas que hemos añadido son:

- Modelos ONNX para int4 DML: cuantizados a int4 mediante AWQ  
- Modelo ONNX para fp16 CUDA  
- Modelo ONNX para int4 CUDA: cuantizado a int4 mediante RTN  
- Modelo ONNX para int4 CPU y móvil: cuantizado a int4 mediante RTN

## Llama.cpp

Llama.cpp es una biblioteca de software de código abierto escrita en C++. Realiza inferencia en varios Modelos de Lenguaje Grande (LLMs), incluyendo Llama. Desarrollada junto con la biblioteca ggml (una biblioteca tensorial de propósito general), llama.cpp busca ofrecer inferencia más rápida y menor uso de memoria comparado con la implementación original en Python. Soporta optimización de hardware, cuantización y ofrece una API sencilla junto con ejemplos. Si te interesa la inferencia eficiente de LLMs, llama.cpp vale la pena explorarlo, ya que Phi3 puede ejecutar Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) es un formato usado para representar y actualizar modelos de aprendizaje automático. Es especialmente útil para modelos de lenguaje pequeños (SLMs) que pueden funcionar eficazmente en CPUs con cuantización de 4-8 bits. GGUF es beneficioso para prototipos rápidos y para ejecutar modelos en dispositivos edge o en trabajos por lotes como pipelines de CI/CD.

**Disclaimer**:  
This document has been translated using AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator). While we strive for accuracy, please be aware that automated translations may contain errors or inaccuracies. The original document in its native language should be considered the authoritative source. For critical information, professional human translation is recommended. We are not liable for any misunderstandings or misinterpretations arising from the use of this translation.

---

Could you please clarify what language or code "mo" refers to? There are several possibilities (e.g., Moldovan, a language code, or a fictional/constructed language). Once I know which "mo" you mean, I can provide the translation.