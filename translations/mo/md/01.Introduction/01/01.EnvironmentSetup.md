<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3edae6aebc3d0143037109e8af58f1ac",
  "translation_date": "2025-05-07T15:00:53+00:00",
  "source_file": "md/01.Introduction/01/01.EnvironmentSetup.md",
  "language_code": "mo"
}
-->
# Phi-3 के साथ स्थानीय रूप से शुरू करें

यह मार्गदर्शिका आपको Ollama का उपयोग करके Phi-3 मॉडल चलाने के लिए अपने स्थानीय वातावरण को सेटअप करने में मदद करेगी। आप मॉडल को कई तरीकों से चला सकते हैं, जिनमें GitHub Codespaces, VS Code Dev Containers, या आपका स्थानीय वातावरण शामिल है।

## पर्यावरण सेटअप

### GitHub Codespaces

आप GitHub Codespaces का उपयोग करके इस टेम्प्लेट को आभासी रूप से चला सकते हैं। बटन आपके ब्राउज़र में वेब-आधारित VS Code इंस्टेंस खोलेगा:

1. टेम्प्लेट खोलें (इसमें कुछ मिनट लग सकते हैं):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. एक टर्मिनल विंडो खोलें

### VS Code Dev Containers

⚠️ यह विकल्प तभी काम करेगा जब आपके Docker Desktop को कम से कम 16 GB RAM आवंटित की गई हो। यदि आपके पास 16 GB से कम RAM है, तो आप [GitHub Codespaces विकल्प](../../../../../md/01.Introduction/01) आज़मा सकते हैं या [स्थानीय रूप से सेटअप कर सकते हैं](../../../../../md/01.Introduction/01)।

एक संबंधित विकल्प VS Code Dev Containers है, जो आपके स्थानीय VS Code में प्रोजेक्ट खोलेगा [Dev Containers एक्सटेंशन](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) का उपयोग करके:

1. Docker Desktop शुरू करें (यदि पहले से इंस्टॉल नहीं है तो इंस्टॉल करें)
2. प्रोजेक्ट खोलें:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. खुलने वाले VS Code विंडो में, जब प्रोजेक्ट फाइलें दिखाई दें (इसमें कुछ मिनट लग सकते हैं), एक टर्मिनल विंडो खोलें।
4. [डिप्लॉयमेंट स्टेप्स](../../../../../md/01.Introduction/01) के साथ जारी रखें

### स्थानीय वातावरण

1. सुनिश्चित करें कि निम्न टूल इंस्टॉल हैं:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## मॉडल का परीक्षण करें

1. Ollama से phi3:mini मॉडल डाउनलोड और चलाने के लिए कहें:

    ```shell
    ollama run phi3:mini
    ```

    मॉडल डाउनलोड करने में कुछ मिनट लगेंगे।

2. जब आउटपुट में "success" दिखाई दे, तब आप प्रॉम्प्ट से उस मॉडल को संदेश भेज सकते हैं।

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. कुछ सेकंड के बाद, आपको मॉडल से प्रतिक्रिया स्ट्रीम दिखाई देगी।

4. भाषा मॉडलों के साथ उपयोग की जाने वाली विभिन्न तकनीकों के बारे में जानने के लिए, Python नोटबुक [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) खोलें और प्रत्येक सेल चलाएं। यदि आपने 'phi3:mini' के अलावा कोई मॉडल उपयोग किया है, तो फ़ाइल के शीर्ष पर `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` को आवश्यकतानुसार बदलें, और आप सिस्टम संदेश को भी संशोधित कर सकते हैं या आवश्यकतानुसार कुछ शॉट उदाहरण जोड़ सकते हैं।

**Disclaimer**:  
This document has been translated using AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator). While we strive for accuracy, please be aware that automated translations may contain errors or inaccuracies. The original document in its native language should be considered the authoritative source. For critical information, professional human translation is recommended. We are not liable for any misunderstandings or misinterpretations arising from the use of this translation.

---

I’m sorry, but I don’t have the capability to translate into "mo" as it is not clear which language or dialect "mo" refers to. Could you please specify the language or provide more details?