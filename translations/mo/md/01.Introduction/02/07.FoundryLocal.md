<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "52973a5680a65a810aa80b7036afd31f",
  "translation_date": "2025-07-16T19:43:09+00:00",
  "source_file": "md/01.Introduction/02/07.FoundryLocal.md",
  "language_code": "mo"
}
-->
## 在 Foundry Local 中開始使用 Phi-Family 模型

### Foundry Local 簡介

Foundry Local 是一個強大的本地裝置 AI 推理解決方案，將企業級 AI 能力直接帶到您的本地硬體。這份教學將引導您如何設定並使用 Foundry Local 的 Phi-Family 模型，讓您在保持隱私和降低成本的同時，完全掌控您的 AI 工作負載。

Foundry Local 透過在您的裝置上本地運行 AI 模型，提供效能、隱私、自訂化和成本上的優勢。它能透過直覺的 CLI、SDK 和 REST API 無縫整合到您現有的工作流程和應用程式中。

![arch](../../../../../translated_images/zh-MO/foundry-local-arch.8823e321dd8258d7.webp)

### 為什麼選擇 Foundry Local？

了解 Foundry Local 的優點，能幫助您做出更明智的 AI 部署決策：

- **本地推理：** 在您自己的硬體上本地運行模型，降低成本，同時確保所有資料都留在您的裝置上。

- **模型自訂：** 從預設模型中選擇，或使用您自己的模型以符合特定需求和使用情境。

- **成本效益：** 利用現有硬體，消除持續的雲端服務費用，讓 AI 更加普及。

- **無縫整合：** 透過 SDK、API 端點或 CLI 與您的應用程式連接，並可隨需求成長輕鬆擴展至 Azure AI Foundry。

> **開始使用提示：** 本教學將著重於透過 CLI 和 SDK 介面使用 Foundry Local。您將學習兩種方法，幫助您選擇最適合的使用方式。

## 第一部分：設定 Foundry Local CLI

### 步驟 1：安裝

Foundry Local CLI 是您管理和本地運行 AI 模型的入口。讓我們先在系統上安裝它。

**支援平台：** Windows 和 macOS

詳細安裝說明請參考 [官方 Foundry Local 文件](https://github.com/microsoft/Foundry-Local/blob/main/README.md)。

### 步驟 2：探索可用模型

安裝 Foundry Local CLI 後，您可以查看有哪些模型適合您的使用情境。以下指令會顯示所有支援的模型：

```bash
foundry model list
```

### 步驟 3：了解 Phi Family 模型

Phi Family 提供多款針對不同使用情境和硬體配置優化的模型。以下是 Foundry Local 中可用的 Phi 模型：

**可用的 Phi 模型：**

- **phi-3.5-mini** - 適合基本任務的精簡模型
- **phi-3-mini-128k** - 支援更長對話的擴展上下文版本
- **phi-3-mini-4k** - 一般用途的標準上下文模型
- **phi-4** - 具備更強能力的進階模型
- **phi-4-mini** - Phi-4 的輕量版本
- **phi-4-mini-reasoning** - 專門用於複雜推理任務

> **硬體相容性：** 每款模型可根據您的系統能力設定不同的硬體加速（CPU、GPU）。

### 步驟 4：運行您的第一個 Phi 模型

讓我們從實作範例開始。這裡將運行 `phi-4-mini-reasoning` 模型，它擅長逐步解決複雜問題。

**運行模型的指令：**

```bash
foundry model run Phi-4-mini-reasoning-generic-cpu
```

> **首次設定說明：** 第一次運行模型時，Foundry Local 會自動將模型下載到您的本地裝置。下載時間會依網路速度而異，請耐心等待初次設定完成。

### 步驟 5：用實際問題測試模型

現在讓我們用一個經典邏輯問題來測試模型的逐步推理能力：

**範例問題：**

```txt
Please calculate the following step by step: Now there are pheasants and rabbits in the same cage, there are thirty-five heads on top and ninety-four legs on the bottom, how many pheasants and rabbits are there?
```

**預期行為：** 模型應該會將問題拆解成邏輯步驟，利用野雞有兩條腿、兔子有四條腿的事實，來解這組方程式。

**結果：**

![cli](../../../../../translated_images/zh-MO/cli.862ec6b55c2b5d91.webp)

## 第二部分：使用 Foundry Local SDK 建立應用程式

### 為什麼使用 SDK？

CLI 非常適合測試和快速互動，而 SDK 則讓您能以程式化方式將 Foundry Local 整合到應用程式中。這為您開啟了以下可能性：

- 建立自訂的 AI 應用程式
- 創建自動化工作流程
- 將 AI 功能整合到現有系統
- 開發聊天機器人和互動工具

### 支援的程式語言

Foundry Local 提供多種程式語言的 SDK，以符合您的開發偏好：

**📦 可用 SDK：**

- **C# (.NET)：** [SDK 文件與範例](https://github.com/microsoft/Foundry-Local/tree/main/sdk/cs)
- **Python：** [SDK 文件與範例](https://github.com/microsoft/Foundry-Local/tree/main/sdk/python)
- **JavaScript：** [SDK 文件與範例](https://github.com/microsoft/Foundry-Local/tree/main/sdk/js)
- **Rust：** [SDK 文件與範例](https://github.com/microsoft/Foundry-Local/tree/main/sdk/rust)

### 下一步

1. **根據您的開發環境選擇合適的 SDK**
2. **參考 SDK 專屬文件，了解詳細實作指南**
3. **從簡單範例開始，逐步建立複雜應用**
4. **探索各 SDK 倉庫中提供的範例程式碼**

## 結語

您現在已學會如何：
- ✅ 安裝並設定 Foundry Local CLI
- ✅ 探索並運行 Phi Family 模型
- ✅ 用實際問題測試模型
- ✅ 了解 SDK 選項以進行應用程式開發

Foundry Local 為您提供強大的基礎，將 AI 能力直接帶到本地環境，讓您掌控效能、隱私與成本，同時保有未來擴展至雲端解決方案的彈性。

**免責聲明**：  
本文件係使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。雖然我們致力於確保準確性，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應視為權威來源。對於重要資訊，建議採用專業人工翻譯。我們不對因使用本翻譯而產生的任何誤解或誤釋負責。