<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "b65fb1164cd818b78a83ac6b8021e4b4",
  "translation_date": "2025-04-04T12:51:43+00:00",
  "source_file": "md\\02.Application\\02.Code\\Phi3\\VSCodeExt\\HOL\\AIPC\\02.PromptflowWithNPU.md",
  "language_code": "mo"
}
-->
# **Lab 2 - Run Prompt flow with Phi-3-mini in AIPC**

## **What's Prompt flow**

Prompt flow is a set of development tools designed to simplify the complete development process of AI applications based on large language models (LLMs), including ideation, prototyping, testing, evaluation, production deployment, and monitoring. It makes prompt engineering significantly easier and allows you to create high-quality LLM applications.

With Prompt flow, you can:

- Design workflows that integrate LLMs, prompts, Python code, and other tools into executable processes.

- Debug and refine your workflows, especially interactions with LLMs, effortlessly.

- Assess your workflows and calculate performance and quality metrics using larger datasets.

- Incorporate testing and evaluation into your CI/CD pipeline to ensure the quality of your workflows.

- Deploy your workflows to your chosen serving platform or seamlessly integrate them into your application's codebase.

- (Optional but highly recommended) Collaborate with your team by utilizing the cloud-based version of Prompt flow on Azure AI.

## **What's AIPC**

An AI PC includes a CPU, a GPU, and an NPU, each offering specialized AI acceleration capabilities. The NPU, or neural processing unit, is a dedicated accelerator that processes artificial intelligence (AI) and machine learning (ML) tasks directly on your PC instead of relying on cloud processing. While the GPU and CPU can also handle these tasks, the NPU is particularly efficient for low-power AI computations. The AI PC represents a significant evolution in computing, not as a solution to a nonexistent problem, but as a major improvement to everyday PC use.

How does it work? Unlike generative AI and massive LLMs trained on extensive public datasets, the AI running on your PC is much more approachable. It is simpler to understand, and since it is trained on your personal data without needing cloud access, its advantages are immediately more appealing to a wider audience.

In the short term, AI PCs focus on personal assistants and smaller AI models operating directly on your computer. These models use your data to provide private, secure, and personalized enhancements for tasks you perform dailyâ€”such as taking meeting notes, organizing a fantasy football league, automating photo and video editing improvements, or planning an ideal family reunion itinerary based on everyone's travel schedules.

## **Building generation code flows on AIPC**

***Note***: If you have not completed the environment installation, please visit [Lab 0 - Installations](./01.Installations.md).

1. Open the Prompt flow Extension in Visual Studio Code and create an empty flow project.

![create](../../../../../../../../../translated_images/pf_create.d6172d8277a78a7fa82cd6ff727ed44e037fa78b662f1f62d5963f36d712d229.mo.png)

2. Add input and output parameters, and incorporate Python code as a new flow.

![flow](../../../../../../../../../translated_images/pf_flow.d5646a323fb7f444c0b98b4521057a592325c583e7ba18bc31500bc0415e9ef3.mo.png)

You can use this structure (flow.dag.yaml) as a reference to construct your flow:

```yaml

inputs:
  question:
    type: string
    default: how to write Bubble Algorithm
outputs:
  answer:
    type: string
    reference: ${Chat_With_Phi3.output}
nodes:
- name: Chat_With_Phi3
  type: python
  source:
    type: code
    path: Chat_With_Phi3.py
  inputs:
    question: ${inputs.question}


```

3. Add code to ***Chat_With_Phi3.py***.

```python


from promptflow.core import tool

# import torch
from transformers import AutoTokenizer, pipeline,TextStreamer
import intel_npu_acceleration_library as npu_lib

import warnings

import asyncio
import platform

class Phi3CodeAgent:
    
    model = None
    tokenizer = None
    text_streamer = None
    
    model_id = "microsoft/Phi-3-mini-4k-instruct"

    @staticmethod
    def init_phi3():
        
        if Phi3CodeAgent.model is None or Phi3CodeAgent.tokenizer is None or Phi3CodeAgent.text_streamer is None:
            Phi3CodeAgent.model = npu_lib.NPUModelForCausalLM.from_pretrained(
                                    Phi3CodeAgent.model_id,
                                    torch_dtype="auto",
                                    dtype=npu_lib.int4,
                                    trust_remote_code=True
                                )
            Phi3CodeAgent.tokenizer = AutoTokenizer.from_pretrained(Phi3CodeAgent.model_id)
            Phi3CodeAgent.text_streamer = TextStreamer(Phi3CodeAgent.tokenizer, skip_prompt=True)

    

    @staticmethod
    def chat_with_phi3(prompt):
        
        Phi3CodeAgent.init_phi3()

        messages = "<|system|>You are a AI Python coding assistant. Please help me to generate code in Python.The answer only genertated Python code, but any comments and instructions do not need to be generated<|end|><|user|>" + prompt +"<|end|><|assistant|>"



        generation_args = {
            "max_new_tokens": 1024,
            "return_full_text": False,
            "temperature": 0.3,
            "do_sample": False,
            "streamer": Phi3CodeAgent.text_streamer,
        }

        pipe = pipeline(
            "text-generation",
            model=Phi3CodeAgent.model,
            tokenizer=Phi3CodeAgent.tokenizer,
            # **generation_args
        )

        result = ''

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            response = pipe(messages, **generation_args)
            result =response[0]['generated_text']
            return result


@tool
def my_python_tool(question: str) -> str:
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    return Phi3CodeAgent.chat_with_phi3(question)


```

4. Test the flow using Debug or Run to verify whether the code generation works correctly.

![RUN](../../../../../../../../../translated_images/pf_run.d918637dc00f61e9bdeec37d4cc9646f77d270ac9203bcce13569f3157202b6e.mo.png)

5. Execute the flow as a development API in the terminal.

```

pf flow serve --source ./ --port 8080 --host localhost   

```

You can test the API using Postman or Thunder Client.

### **Note**

1. The first execution takes a significant amount of time. It is recommended to download the Phi-3 model via the Hugging Face CLI.

2. Given the limited computing capabilities of Intel NPU, it is advised to use Phi-3-mini-4k-instruct.

3. Intel NPU Acceleration employs INT4 quantization for optimization. However, if you restart the service, you need to delete the cache and nc_workshop folders.

## **Resources**

1. Learn Prompt flow: [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Learn Intel NPU Acceleration: [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Sample Code: Download [Local NPU Agent Sample Code](../../../../../../../../../code/07.Lab/01/AIPC)

It seems you would like the text translated to "mo." Could you clarify what "mo" refers to? Are you asking for translation into Maori, Mongolian, or another language?