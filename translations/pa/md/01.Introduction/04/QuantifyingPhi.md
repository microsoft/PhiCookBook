<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-07-16T21:44:51+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "pa"
}
-->
# **Phi ਪਰਿਵਾਰ ਦੀ ਮਾਤਰਾ ਨਿਰਧਾਰਿਤ ਕਰਨਾ**

ਮਾਡਲ ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਦਾ ਮਤਲਬ ਹੈ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਮਾਡਲ ਵਿੱਚ ਪੈਰਾਮੀਟਰਾਂ (ਜਿਵੇਂ ਕਿ ਵਜ਼ਨ ਅਤੇ ਐਕਟੀਵੇਸ਼ਨ ਮੁੱਲ) ਨੂੰ ਵੱਡੇ ਮੁੱਲ ਦੀ ਸੀਮਾ (ਆਮ ਤੌਰ 'ਤੇ ਲਗਾਤਾਰ ਮੁੱਲ ਦੀ ਸੀਮਾ) ਤੋਂ ਛੋਟੀ ਸੀਮਿਤ ਮੁੱਲ ਦੀ ਸੀਮਾ ਵਿੱਚ ਮੈਪ ਕਰਨ ਦੀ ਪ੍ਰਕਿਰਿਆ। ਇਹ ਤਕਨਾਲੋਜੀ ਮਾਡਲ ਦੇ ਆਕਾਰ ਅਤੇ ਗਣਨਾਤਮਕ ਜਟਿਲਤਾ ਨੂੰ ਘਟਾ ਸਕਦੀ ਹੈ ਅਤੇ ਮੋਬਾਈਲ ਡਿਵਾਈਸਾਂ ਜਾਂ ਐਂਬੈਡਿਡ ਸਿਸਟਮਾਂ ਵਰਗੇ ਸਰੋਤ-ਸੀਮਿਤ ਵਾਤਾਵਰਣਾਂ ਵਿੱਚ ਮਾਡਲ ਦੀ ਕਾਰਗੁਜ਼ਾਰੀ ਨੂੰ ਸੁਧਾਰ ਸਕਦੀ ਹੈ। ਮਾਡਲ ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਪੈਰਾਮੀਟਰਾਂ ਦੀ ਸਹੀਤਾ ਨੂੰ ਘਟਾ ਕੇ ਕੰਪ੍ਰੈਸ਼ਨ ਪ੍ਰਾਪਤ ਕਰਦਾ ਹੈ, ਪਰ ਇਸ ਨਾਲ ਕੁਝ ਸਹੀਤਾ ਦੀ ਹਾਨੀ ਵੀ ਹੁੰਦੀ ਹੈ। ਇਸ ਲਈ, ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਪ੍ਰਕਿਰਿਆ ਵਿੱਚ ਮਾਡਲ ਦੇ ਆਕਾਰ, ਗਣਨਾਤਮਕ ਜਟਿਲਤਾ ਅਤੇ ਸਹੀਤਾ ਦੇ ਵਿਚਕਾਰ ਸੰਤੁਲਨ ਬਣਾਉਣਾ ਜਰੂਰੀ ਹੁੰਦਾ ਹੈ। ਆਮ ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਤਰੀਕੇ ਵਿੱਚ ਫਿਕਸਡ-ਪੌਇੰਟ ਕੁਆੰਟਾਈਜੇਸ਼ਨ, ਫਲੋਟਿੰਗ-ਪੌਇੰਟ ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਆਦਿ ਸ਼ਾਮਲ ਹਨ। ਤੁਸੀਂ ਵਿਸ਼ੇਸ਼ ਸਥਿਤੀ ਅਤੇ ਜ਼ਰੂਰਤਾਂ ਦੇ ਅਨੁਸਾਰ ਉਚਿਤ ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਰਣਨੀਤੀ ਚੁਣ ਸਕਦੇ ਹੋ।

ਅਸੀਂ GenAI ਮਾਡਲ ਨੂੰ ਐਜ ਡਿਵਾਈਸਾਂ 'ਤੇ ਤਾਇਨਾਤ ਕਰਨ ਦੀ ਆਸ ਕਰਦੇ ਹਾਂ ਅਤੇ ਹੋਰ ਜ਼ਿਆਦਾ ਡਿਵਾਈਸਾਂ ਨੂੰ GenAI ਸਥਿਤੀਆਂ ਵਿੱਚ ਲਿਆਉਣਾ ਚਾਹੁੰਦੇ ਹਾਂ, ਜਿਵੇਂ ਕਿ ਮੋਬਾਈਲ ਡਿਵਾਈਸ, AI PC/Copilot+PC, ਅਤੇ ਪਰੰਪਰਾਗਤ IoT ਡਿਵਾਈਸ। ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਮਾਡਲ ਰਾਹੀਂ, ਅਸੀਂ ਇਸਨੂੰ ਵੱਖ-ਵੱਖ ਐਜ ਡਿਵਾਈਸਾਂ 'ਤੇ ਤਾਇਨਾਤ ਕਰ ਸਕਦੇ ਹਾਂ ਜੋ ਵੱਖ-ਵੱਖ ਡਿਵਾਈਸਾਂ 'ਤੇ ਆਧਾਰਿਤ ਹਨ। ਹਾਰਡਵੇਅਰ ਨਿਰਮਾਤਿਆਂ ਵੱਲੋਂ ਦਿੱਤੇ ਮਾਡਲ ਤੇਜ਼ੀ ਫਰੇਮਵਰਕ ਅਤੇ ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਮਾਡਲ ਦੇ ਨਾਲ ਮਿਲ ਕੇ, ਅਸੀਂ ਬਿਹਤਰ SLM ਐਪਲੀਕੇਸ਼ਨ ਸਥਿਤੀਆਂ ਤਿਆਰ ਕਰ ਸਕਦੇ ਹਾਂ।

ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਸਥਿਤੀ ਵਿੱਚ, ਸਾਡੇ ਕੋਲ ਵੱਖ-ਵੱਖ ਸਹੀਤਾਵਾਂ ਹਨ (INT4, INT8, FP16, FP32)। ਹੇਠਾਂ ਆਮ ਤੌਰ 'ਤੇ ਵਰਤੇ ਜਾਣ ਵਾਲੇ ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਸਹੀਤਾਵਾਂ ਦੀ ਵਿਆਖਿਆ ਦਿੱਤੀ ਗਈ ਹੈ।

### **INT4**

INT4 ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਇੱਕ ਤੀਬਰ ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਤਰੀਕਾ ਹੈ ਜੋ ਮਾਡਲ ਦੇ ਵਜ਼ਨਾਂ ਅਤੇ ਐਕਟੀਵੇਸ਼ਨ ਮੁੱਲਾਂ ਨੂੰ 4-ਬਿਟ ਇੰਟੀਜਰਾਂ ਵਿੱਚ ਕੁਆੰਟਾਈਜ਼ ਕਰਦਾ ਹੈ। INT4 ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਛੋਟੀ ਪ੍ਰਤੀਨਿਧੀ ਸੀਮਾ ਅਤੇ ਘੱਟ ਸਹੀਤਾ ਕਾਰਨ ਆਮ ਤੌਰ 'ਤੇ ਵੱਡੀ ਸਹੀਤਾ ਦੀ ਹਾਨੀ ਦਾ ਕਾਰਨ ਬਣਦਾ ਹੈ। ਪਰ, INT8 ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਨਾਲ ਤੁਲਨਾ ਕਰਨ 'ਤੇ, INT4 ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਮਾਡਲ ਦੀ ਸਟੋਰੇਜ ਲੋੜਾਂ ਅਤੇ ਗਣਨਾਤਮਕ ਜਟਿਲਤਾ ਨੂੰ ਹੋਰ ਘਟਾ ਸਕਦਾ ਹੈ। ਇਹ ਧਿਆਨ ਵਿੱਚ ਰੱਖਣਾ ਚਾਹੀਦਾ ਹੈ ਕਿ ਅਮਲੀ ਵਰਤੋਂ ਵਿੱਚ INT4 ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਕਾਫੀ ਘੱਟ ਮਿਲਦਾ ਹੈ, ਕਿਉਂਕਿ ਬਹੁਤ ਘੱਟ ਸਹੀਤਾ ਮਾਡਲ ਦੀ ਕਾਰਗੁਜ਼ਾਰੀ ਵਿੱਚ ਵੱਡਾ ਘਟਾਅ ਕਰ ਸਕਦੀ ਹੈ। ਇਸ ਤੋਂ ਇਲਾਵਾ, ਸਾਰੇ ਹਾਰਡਵੇਅਰ INT4 ਆਪਰੇਸ਼ਨਾਂ ਨੂੰ ਸਹਿਯੋਗ ਨਹੀਂ ਦਿੰਦੇ, ਇਸ ਲਈ ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਤਰੀਕੇ ਦੀ ਚੋਣ ਕਰਦੇ ਸਮੇਂ ਹਾਰਡਵੇਅਰ ਦੀ ਅਨੁਕੂਲਤਾ ਨੂੰ ਧਿਆਨ ਵਿੱਚ ਰੱਖਣਾ ਜਰੂਰੀ ਹੈ।

### **INT8**

INT8 ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਮਾਡਲ ਦੇ ਵਜ਼ਨਾਂ ਅਤੇ ਐਕਟੀਵੇਸ਼ਨਾਂ ਨੂੰ ਫਲੋਟਿੰਗ ਪੌਇੰਟ ਨੰਬਰਾਂ ਤੋਂ 8-ਬਿਟ ਇੰਟੀਜਰਾਂ ਵਿੱਚ ਬਦਲਣ ਦੀ ਪ੍ਰਕਿਰਿਆ ਹੈ। ਹਾਲਾਂਕਿ INT8 ਇੰਟੀਜਰਾਂ ਦੁਆਰਾ ਦਰਸਾਈ ਗਈ ਗਿਣਤੀ ਸੀਮਾ ਛੋਟੀ ਅਤੇ ਘੱਟ ਸਹੀਤਾ ਵਾਲੀ ਹੁੰਦੀ ਹੈ, ਇਹ ਸਟੋਰੇਜ ਅਤੇ ਗਣਨਾ ਦੀਆਂ ਲੋੜਾਂ ਨੂੰ ਕਾਫੀ ਘਟਾ ਸਕਦੀ ਹੈ। INT8 ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਵਿੱਚ, ਮਾਡਲ ਦੇ ਵਜ਼ਨ ਅਤੇ ਐਕਟੀਵੇਸ਼ਨ ਮੁੱਲ ਇੱਕ ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਪ੍ਰਕਿਰਿਆ ਵਿੱਚੋਂ ਲੰਘਦੇ ਹਨ, ਜਿਸ ਵਿੱਚ ਸਕੇਲਿੰਗ ਅਤੇ ਆਫਸੈਟ ਸ਼ਾਮਲ ਹੁੰਦੇ ਹਨ, ਤਾਂ ਜੋ ਮੂਲ ਫਲੋਟਿੰਗ ਪੌਇੰਟ ਜਾਣਕਾਰੀ ਨੂੰ ਜ਼ਿਆਦਾ ਤੋਂ ਜ਼ਿਆਦਾ ਬਚਾਇਆ ਜਾ ਸਕੇ। ਇੰਫਰੈਂਸ ਦੌਰਾਨ, ਇਹ ਕੁਆੰਟਾਈਜ਼ ਕੀਤੇ ਮੁੱਲ ਫਿਰ ਫਲੋਟਿੰਗ ਪੌਇੰਟ ਨੰਬਰਾਂ ਵਿੱਚ ਵਾਪਸ ਬਦਲੇ ਜਾਂਦੇ ਹਨ ਗਣਨਾ ਲਈ, ਅਤੇ ਫਿਰ ਅਗਲੇ ਕਦਮ ਲਈ ਮੁੜ INT8 ਵਿੱਚ ਕੁਆੰਟਾਈਜ਼ ਕੀਤੇ ਜਾਂਦੇ ਹਨ। ਇਹ ਤਰੀਕਾ ਜ਼ਿਆਦਾਤਰ ਐਪਲੀਕੇਸ਼ਨਾਂ ਵਿੱਚ ਕਾਫੀ ਸਹੀਤਾ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ ਅਤੇ ਉੱਚ ਗਣਨਾਤਮਕ ਕੁਸ਼ਲਤਾ ਨੂੰ ਬਰਕਰਾਰ ਰੱਖਦਾ ਹੈ।

### **FP16**

FP16 ਫਾਰਮੈਟ, ਜਿਸਨੂੰ 16-ਬਿਟ ਫਲੋਟਿੰਗ ਪੌਇੰਟ ਨੰਬਰ (float16) ਕਿਹਾ ਜਾਂਦਾ ਹੈ, 32-ਬਿਟ ਫਲੋਟਿੰਗ ਪੌਇੰਟ ਨੰਬਰਾਂ (float32) ਨਾਲੋਂ ਅੱਧਾ ਮੈਮੋਰੀ ਖਪਤ ਕਰਦਾ ਹੈ, ਜੋ ਵੱਡੇ ਪੱਧਰ ਦੇ ਡੀਪ ਲਰਨਿੰਗ ਐਪਲੀਕੇਸ਼ਨਾਂ ਵਿੱਚ ਮਹੱਤਵਪੂਰਨ ਫਾਇਦੇ ਰੱਖਦਾ ਹੈ। FP16 ਫਾਰਮੈਟ ਇੱਕੋ ਜਿਹੇ GPU ਮੈਮੋਰੀ ਸੀਮਾਵਾਂ ਵਿੱਚ ਵੱਡੇ ਮਾਡਲ ਲੋਡ ਕਰਨ ਜਾਂ ਵੱਧ ਡੇਟਾ ਪ੍ਰੋਸੈਸ ਕਰਨ ਦੀ ਆਗਿਆ ਦਿੰਦਾ ਹੈ। ਜਿਵੇਂ ਕਿ ਆਧੁਨਿਕ GPU ਹਾਰਡਵੇਅਰ FP16 ਆਪਰੇਸ਼ਨਾਂ ਨੂੰ ਸਹਿਯੋਗ ਦੇ ਰਹੇ ਹਨ, FP16 ਫਾਰਮੈਟ ਦੀ ਵਰਤੋਂ ਨਾਲ ਗਣਨਾ ਦੀ ਗਤੀ ਵਿੱਚ ਵੀ ਸੁਧਾਰ ਆ ਸਕਦਾ ਹੈ। ਪਰ FP16 ਫਾਰਮੈਟ ਦੀਆਂ ਕੁਝ ਆਪਣੀਆਂ ਖਾਮੀਆਂ ਵੀ ਹਨ, ਜਿਵੇਂ ਕਿ ਘੱਟ ਸਹੀਤਾ, ਜੋ ਕੁਝ ਹਾਲਤਾਂ ਵਿੱਚ ਗਿਣਤੀ ਸਥਿਰਤਾ ਜਾਂ ਸਹੀਤਾ ਦੀ ਹਾਨੀ ਦਾ ਕਾਰਨ ਬਣ ਸਕਦੀ ਹੈ।

### **FP32**

FP32 ਫਾਰਮੈਟ ਉੱਚ ਸਹੀਤਾ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ ਅਤੇ ਵੱਡੀ ਗਿਣਤੀ ਸੀਮਾ ਨੂੰ ਸਹੀ ਤਰੀਕੇ ਨਾਲ ਦਰਸਾ ਸਕਦਾ ਹੈ। ਜਿੱਥੇ ਜਟਿਲ ਗਣਿਤੀ ਕਾਰਵਾਈਆਂ ਕੀਤੀਆਂ ਜਾਂਦੀਆਂ ਹਨ ਜਾਂ ਉੱਚ ਸਹੀਤਾ ਵਾਲੇ ਨਤੀਜੇ ਲੋੜੀਂਦੇ ਹਨ, ਉਥੇ FP32 ਫਾਰਮੈਟ ਨੂੰ ਤਰਜੀਹ ਦਿੱਤੀ ਜਾਂਦੀ ਹੈ। ਪਰ, ਉੱਚ ਸਹੀਤਾ ਦਾ ਮਤਲਬ ਵੱਧ ਮੈਮੋਰੀ ਖਪਤ ਅਤੇ ਲੰਬਾ ਗਣਨਾ ਸਮਾਂ ਵੀ ਹੁੰਦਾ ਹੈ। ਵੱਡੇ ਪੱਧਰ ਦੇ ਡੀਪ ਲਰਨਿੰਗ ਮਾਡਲਾਂ ਲਈ, ਖਾਸ ਕਰਕੇ ਜਦੋਂ ਮਾਡਲ ਪੈਰਾਮੀਟਰਾਂ ਦੀ ਗਿਣਤੀ ਜ਼ਿਆਦਾ ਹੋਵੇ ਅਤੇ ਡੇਟਾ ਦੀ ਮਾਤਰਾ ਬਹੁਤ ਵੱਡੀ ਹੋਵੇ, FP32 ਫਾਰਮੈਟ GPU ਮੈਮੋਰੀ ਦੀ ਕਮੀ ਜਾਂ ਇੰਫਰੈਂਸ ਦੀ ਗਤੀ ਵਿੱਚ ਕਮੀ ਦਾ ਕਾਰਨ ਬਣ ਸਕਦਾ ਹੈ।

ਮੋਬਾਈਲ ਡਿਵਾਈਸਾਂ ਜਾਂ IoT ਡਿਵਾਈਸਾਂ 'ਤੇ, ਅਸੀਂ Phi-3.x ਮਾਡਲਾਂ ਨੂੰ INT4 ਵਿੱਚ ਬਦਲ ਸਕਦੇ ਹਾਂ, ਜਦਕਿ AI PC / Copilot PC ਵੱਧ ਸਹੀਤਾ ਵਾਲੇ ਜਿਵੇਂ ਕਿ INT8, FP16, FP32 ਵਰਤ ਸਕਦੇ ਹਨ।

ਹੁਣ, ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਨਿਰਮਾਤਿਆਂ ਕੋਲ ਜਨਰੇਟਿਵ ਮਾਡਲਾਂ ਨੂੰ ਸਹਿਯੋਗ ਦੇਣ ਲਈ ਵੱਖ-ਵੱਖ ਫਰੇਮਵਰਕ ਹਨ, ਜਿਵੇਂ ਕਿ Intel ਦਾ OpenVINO, Qualcomm ਦਾ QNN, Apple ਦਾ MLX, ਅਤੇ Nvidia ਦਾ CUDA ਆਦਿ, ਜੋ ਮਾਡਲ ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਨਾਲ ਮਿਲ ਕੇ ਸਥਾਨਕ ਤਾਇਨਾਤੀ ਨੂੰ ਪੂਰਾ ਕਰਦੇ ਹਨ।

ਤਕਨਾਲੋਜੀ ਦੇ ਮਾਮਲੇ ਵਿੱਚ, ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਤੋਂ ਬਾਅਦ ਸਾਡੇ ਕੋਲ ਵੱਖ-ਵੱਖ ਫਾਰਮੈਟ ਸਹਿਯੋਗ ਹਨ, ਜਿਵੇਂ ਕਿ PyTorch / Tensorflow ਫਾਰਮੈਟ, GGUF, ਅਤੇ ONNX। ਮੈਂ GGUF ਅਤੇ ONNX ਵਿਚਕਾਰ ਫਾਰਮੈਟ ਤੁਲਨਾ ਅਤੇ ਐਪਲੀਕੇਸ਼ਨ ਸਥਿਤੀਆਂ ਕੀਤੀਆਂ ਹਨ। ਇੱਥੇ ਮੈਂ ONNX ਕੁਆੰਟਾਈਜੇਸ਼ਨ ਫਾਰਮੈਟ ਦੀ ਸਿਫਾਰਸ਼ ਕਰਦਾ ਹਾਂ, ਜਿਸਨੂੰ ਮਾਡਲ ਫਰੇਮਵਰਕ ਤੋਂ ਲੈ ਕੇ ਹਾਰਡਵੇਅਰ ਤੱਕ ਚੰਗਾ ਸਹਿਯੋਗ ਮਿਲਦਾ ਹੈ। ਇਸ ਅਧਿਆਇ ਵਿੱਚ, ਅਸੀਂ ONNX Runtime for GenAI, OpenVINO, ਅਤੇ Apple MLX 'ਤੇ ਮਾਡਲ ਕੁਆੰਟਾਈਜੇਸ਼ਨ 'ਤੇ ਧਿਆਨ ਦੇਵਾਂਗੇ (ਜੇ ਤੁਹਾਡੇ ਕੋਲ ਕੋਈ ਵਧੀਆ ਤਰੀਕਾ ਹੈ, ਤਾਂ ਤੁਸੀਂ PR ਸਬਮਿਟ ਕਰਕੇ ਸਾਨੂੰ ਦੇ ਸਕਦੇ ਹੋ)।

**ਇਸ ਅਧਿਆਇ ਵਿੱਚ ਸ਼ਾਮਲ ਹੈ**

1. [llama.cpp ਦੀ ਵਰਤੋਂ ਕਰਕੇ Phi-3.5 / 4 ਦੀ ਕੁਆੰਟਾਈਜੇਸ਼ਨ](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntime ਲਈ Generative AI ਐਕਸਟੈਂਸ਼ਨ ਦੀ ਵਰਤੋਂ ਕਰਕੇ Phi-3.5 / 4 ਦੀ ਕੁਆੰਟਾਈਜੇਸ਼ਨ](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO ਦੀ ਵਰਤੋਂ ਕਰਕੇ Phi-3.5 / 4 ਦੀ ਕੁਆੰਟਾਈਜੇਸ਼ਨ](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX Framework ਦੀ ਵਰਤੋਂ ਕਰਕੇ Phi-3.5 / 4 ਦੀ ਕੁਆੰਟਾਈਜੇਸ਼ਨ](./UsingAppleMLXQuantifyingPhi.md)

**ਅਸਵੀਕਾਰੋਪਣ**:  
ਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦਿਤ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀਅਤ ਲਈ ਕੋਸ਼ਿਸ਼ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਰੱਖੋ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸਮਰਥਤਾਵਾਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼ ਆਪਣੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਪ੍ਰਮਾਣਿਕ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਉਤਪੰਨ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।