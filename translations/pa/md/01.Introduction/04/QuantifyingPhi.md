<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f4cbbe7bf3e764de52d64a96d97b3c35",
  "translation_date": "2026-01-05T02:43:32+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "pa"
}
-->
# **Phi ਪਰਿਵਾਰ ਦਾ ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ**

Model quantization ਮਤਲਬ ਹੈ ਕਿ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਮਾਡਲ ਵਿੱਚ ਪੈਰਾਮੀਟਰਾਂ (ਜਿਵੇਂ ਕਿ ਵੇਟ ਅਤੇ ਐਕਟਿਵੇਸ਼ਨ ਮੁੱਲ) ਨੂੰ ਵੱਡੇ ਮੁੱਲ ਰੇਂਜ (ਆਮ ਤੌਰ 'ਤੇ ਇਕ ਲਗਾਤਾਰ ਮੁੱਲ ਰੇਂਜ) ਤੋਂ ਇਕ ਛੋਟੇ ਸਵੀਮਿਤ ਮੁੱਲ ਰੇਂਜ ਵਿੱਚ ਨਕਸ਼ਾ ਕੀਤਾ ਜਾਣਾ। ਇਹ ਤਕਨੀਕ ਮਾਡਲ ਦੇ ਆਕਾਰ ਅਤੇ ਗਣਨਾਤਮਕ ਜਟਿਲਤਾ ਨੂੰ ਘਟਾ ਸਕਦੀ ਹੈ ਅਤੇ ਐਸੇ ਸੰਸਾਧਨ-ਸੀਮਿਤ ਵਾਤਾਵਰਨਾਂ ਜਿਵੇਂ ਕਿ ਮੋਬਾਈਲ ਡਿਵਾਈਸ ਜਾਂ ਐਂਬੈੱਡਡ ਸਿਸਟਮਾਂ ਵਿੱਚ ਮਾਡਲ ਦੀ ਚਾਲੂ ਕੁਸ਼ਲਤਾ ਨੂੰ ਸੁਧਾਰ ਸਕਦੀ ਹੈ। ਮਾਡਲ ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਪੈਰਾਮੀਟਰਾਂ ਦੀ ਪ੍ਰਿਸੀਜ਼ਨ ਘਟਾ ਕੇ ਕੰਪਰੈਸ਼ਨ ਪ੍ਰਾਪਤ ਕਰਦਾ ਹੈ, ਪਰ ਇਹ ਕੁਝ ਪ੍ਰਿਸੀਜ਼ਨ ਖੋਹ ਵੀ ਲਿਆਉਂਦਾ ਹੈ। ਇਸ ਲਈ, ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਪ੍ਰਕਿਰਿਆ ਵਿੱਚ ਮਾਡਲ ਆਕਾਰ, ਗਣਨਾਤਮਕ ਜਟਿਲਤਾ ਅਤੇ ਪ੍ਰਿਸੀਜ਼ਨ ਦੇ ਬੀਚ ਸਹੀ ਸੰਤੁਲਨ ਬਣਾਉਣਾ ਜ਼ਰੂਰੀ ਹੁੰਦਾ ਹੈ। ਆਮ ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਤਰੀਕੇ ਵਿੱਚ fixed-point quantization, floating-point quantization ਆਦਿ ਸ਼ਾਮِل ਹਨ। ਤੁਸੀਂ ਵਿਸ਼ੇਸ਼ ਸਨੈਰੀਓ ਅਤੇ ਲੋੜਾਂ ਦੇ ਅਨੁਸਾਰ ਯੋਗ ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਰਣਨੀਤੀ ਚੁਣ ਸਕਦੇ ਹੋ।

ਅਸੀਂ ਉਮੀਦ ਕਰਦੇ ਹਾਂ ਕਿ GenAI ਮਾਡਲਾਂ ਨੂੰ ਐਜ ਡਿਵਾਈਸز 'ਤੇ ਡਿਪਲੌਇ ਕੀਤਾ ਜਾਵੇ ਅਤੇ ਹੋਰ ਜ਼ਿਆਦਾ ਡਿਵਾਈਸ GenAI ਸਨੈਰੀਓਜ਼ ਵਿੱਚ ਸ਼ਾਮِل ਹੋਣ, ਜਿਵੇਂ ਕਿ ਮੋਬਾਈਲ ਡਿਵਾਈਸ, AI PC/Copilot+PC ਅਤੇ ਪਰੰਪਰਾਗਤ IoT ਡਿਵਾਈਸ। ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਮਾਡਲ ਰਾਹੀਂ ਅਸੀਂ ਵੱਖ-ਵੱਖ ਡਿਵਾਈਸਾਂ ਅਨੁਸਾਰ ਉਨ੍ਹਾਂ ਨੂੰ ਵੱਖ-ਵੱਖ ਐਜ ਡਿਵਾਈਸز 'ਤੇ ਡਿਪਲੌਇ ਕਰ ਸਕਦੇ ਹਾਂ। hardware ਨਿਰਮਾਤਾ ਦੁਆਰਾ ਮੁਹੱਈਆ model acceleration ਫਰੇਮਵਰਕ ਅਤੇ ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਮਾਡਲ ਨਾਲ ਮਿਲ ਕੇ ਅਸੀਂ ਵਧੀਆ SLM ਐਪਲੀਕੇਸ਼ਨ ਸਨੈਰੀਓ ਬਣਾਉ ਸਕਦੇ ਹਾਂ।

ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਸਨੈਰੀਓ ਵਿੱਚ ਸਾਡੇ ਕੋਲ ਵੱਖ-ਵੱਖ ਪ੍ਰਿਸੀਜ਼ਨ ਹੁੰਦੀਆਂ ਹਨ (INT4, INT8, FP16, FP32)। ਹੇਠਾਂ ਆਮ ਤੌਰ 'ਤੇ ਵਰਤੀਆਂ ਜਾਣ ਵਾਲੀਆਂ ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਪ੍ਰਿਸੀਜ਼ਨਾਂ ਦੀ ਵਿਆਖਿਆ ਦਿੱਤੀ ਗਈ ਹੈ

### **INT4**

INT4 ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਇੱਕ ਕਠੋਰ ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਤਰੀਕਾ ਹੈ ਜੋ ਮਾਡਲ ਦੇ ਵੇਟਾਂ ਅਤੇ ਐਕਟਿਵੇਸ਼ਨ ਮੁੱਲਾਂ ਨੂੰ 4-ਬਿਟ ਇੰਟੀਜਰਾਂ ਵਿੱਚ ਕੁਆਂਟਾਈਜ਼ ਕਰਦਾ ਹੈ। ਛੋਟੀ ਪ੍ਰਤੀਨਿਧੀ ਰੇਂਜ ਅਤੇ ਘੱਟ ਪ੍ਰਿਸੀਜ਼ਨ ਕਾਰਨ INT4 ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਅਕਸਰ ਵੱਡੀ ਪ੍ਰਿਸੀਜ਼ਨ ਹਾਨੀ ਦੇ ਨਤੀਜੇ ਵਜੋਂ ਨਿਕਲਦਾ ਹੈ। ਹਾਲਾਂਕਿ, INT8 ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਨਾਲ ਤੁਲਨਾ ਕਰਨ 'ਤੇ, INT4 ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਮਾਡਲ ਦੀ ਸਟੋਰੇਜ ਲੋੜਾਂ ਅਤੇ ਗਣਨਾਤਮਕ ਜਟਿਲਤਾ ਨੂੰ ਹੋਰ ਘਟਾ ਸਕਦਾ ਹੈ। ਧਿਆਨ ਯੋਗ ਗੱਲ ਇਹ ਹੈ ਕਿ ਵਿਹਾਰਕ ਅਰਜ਼ੀ ਵਿੱਚ INT4 ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਕਾਫੀ ਖੱਟ ਮਿਲਦੀ ਹੈ, ਕਿਉਂਕਿ ਬਹੁਤ ਘੱਟ ਸਟੇਟਿਕ ਸੀ ਹੋਣ ਕਾਰਨ ਮਾਡਲ ਪर्फਾਰਮੈਂਸ ਵਿੱਚ ਮਹੱਤਵਪੂਰਣ ਘਟਾਅ ਹੋ ਸਕਦਾ ਹੈ। ਇਸਦੇ ਇਲਾਵਾ, ਸਭ ਹਾਰਡਵੇਅਰ INT4 ਓਪਰੇਸ਼ਨ ਨੂੰ ਸਮਰਥਨ ਨਹੀਂ ਕਰਦੇ, ਇਸ ਲਈ ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਤਰੀਕੇ ਦੇ ਚੋਣ ਦੌਰਾਨ ਹਾਰਡਵੇਅਰ ਅਨੁਕੂਲਤਾ 'ਤੇ ਧਿਆਨ ਦੇਣਾ ਲਾਜ਼ਮੀ ਹੈ।

### **INT8**

INT8 ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਮਾਡਲ ਦੇ ਵੇਟਾਂ ਅਤੇ ਐਕਟਿਵੇਸ਼ਨਾਂ ਨੂੰ ਫਲੋਟਿੰਗ ਪੁਆਇੰਟ ਨੰਬਰਾਂ ਤੋਂ 8-ਬਿਟ ਇੰਟੀਜਰਾਂ ਵਿੱਚ ਬਦਲਣ ਦੀ ਪ੍ਰਕਿਰਿਆ ਹੈ। ਹਾਲਾਂਕਿ INT8 ਇੰਟੀਜਰਾਂ ਦੁਆਰਾ ਪ੍ਰਤੀਨਿਧਿਤ ਨੰਬਰਾਤਮਕ ਰੇਂਜ ਛੋਟੀ ਅਤੇ ਘੱਟ ਪ੍ਰਿਸੀਜ਼ਨ ਵਾਲੀ ਹੁੰਦੀ ਹੈ, ਇਹ ਸਟੋਰੇਜ ਅਤੇ ਗਣਨਾ ਦੀਆਂ ਲੋੜਾਂ ਨੂੰ ਮਹੱਤਵਪੂਰਵਕ ਘਟਾ ਸਕਦੀ ਹੈ। INT8 ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਵਿੱਚ, ਮਾਡਲ ਦੇ ਵੇਟ ਅਤੇ ਐਕਟਿਵੇਸ਼ਨ ਮੁੱਲਾਂ ਨੂੰ ਇੱਕ ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਪ੍ਰਕਿਰਿਆ ਤੋਂ ਗੁਜ਼ਾਰਿਆ ਜਾਂਦਾ ਹੈ, ਜਿਸ ਵਿੱਚ ਸਕੇਲਿੰਗ ਅਤੇ ਆਫਸੈਟ ਸ਼ਾਮِل ਹੁੰਦੇ ਹਨ, ਤਾਂ ਜੋ ਮੂਲ ਫਲੋਟਿੰਗ ਪੁਆਇੰਟ ਜਾਣਕਾਰੀ ਨੂੰ ਸੰਭਾਲਿਆ ਜਾ ਸਕੇ। ਇਨਫਰੰਸ ਦੌਰਾਨ, ਇਹ ਕੁਆਂਟਾਈਜ਼ਡ ਮੁੱਲ ਮੁੜ ਫਲੋਟਿੰਗ ਪੁਆਇੰਟ ਨੰਬਰਾਂ ਵਿੱਚ ਡੀਕੁਆਂਟਾਈਜ਼ ਕੀਤੇ ਜਾਂਦੇ ਹਨ ਗਣਨਾ ਲਈ, ਅਤੇ ਫਿਰ ਅਗਲੇ ਕਦਮ ਲਈ ਮੁੜ INT8 ਵਿੱਚ ਕੁਆਂਟਾਈਜ਼ ਕੀਤੇ ਜਾਂਦੇ ਹਨ। ਇਹ ਤਰੀਕਾ ਬਹੁਤ ਸਾਰੀਆਂ ਰਲਾਈਆਂ ਵਿੱਚ ਯੋਗ ਪ੍ਰਿਸੀਜ਼ਨ ਦੇ ਸਕਦੀ ਹੈ ਜਦੋਂ ਕਿ ਉੱਚ ਗਣਨਾਤਮਕ ਕੁਸ਼ਲਤਾ ਵੀ ਬਰਕਰਾਰ ਰਹਿੰਦੀ ਹੈ।

### **FP16**

FP16 ਫਾਰਮੈਟ, ਅਰਥਾਤ 16-ਬਿਟ ਫਲੋਟਿੰਗ ਪੁਆਇੰਟ ਨੰਬਰ (float16), 32-ਬਿਟ ਫਲੋਟਿੰਗ ਪੁਆਇੰਟ ਨੰਬਰਾਂ (float32) ਨਾਲ ਤੁਲਨਾ ਕਰਨ 'ਤੇ ਮੈਮੋਰੀ ਉਪਭੋਗਤਾ ਨੂੰ ਅੱਧਾ ਕਰ ਦਿੰਦਾ ਹੈ, ਜੋ ਕਿ ਵੱਡੇ ਪੱਧਰ ਦੇ ਡੀਪ ਲਰਨਿੰਗ ਐਪਲੀਕੇਸ਼ਨਾਂ ਵਿੱਚ ਮਹੱਤਵਪੂਰਣ ਫਾਇਦੇ ਲਿਆਉਂਦਾ ਹੈ। FP16 ਫਾਰਮੈਟ ਇੱਕੋ GPU ਮੈਮੋਰੀ ਸੀਮਾਵਾਂ ਦੇ ਅੰਦਰ ਵੱਡੇ ਮਾਡਲ ਲੋਡ ਕਰਨ ਜਾਂ ਹੋਰ ਡੇਟਾ ਪ੍ਰੋਸੈਸ ਕਰਨ ਦੀ ਆਗਿਆ ਦਿੰਦਾ ਹੈ। ਜਿਵੇਂ-जਿਵੇਂ ਆਧੁਨਿਕ GPU ਹਾਰਡਵੇਅਰ FP16 ਓਪਰੇਸ਼ਨਾਂ ਨੂੰ ਸਮਰਥਨ ਕਰਦਾ ਜਾ ਰਿਹਾ ਹੈ, FP16 ਫਾਰਮੈਟ ਵਖ-ਵਖ ਹਾਲਤਾਂ ਵਿੱਚ ਗਣਨਾ ਦੀ ਰਫ਼ਤਾਰ ਵਿੱਚ ਸੁਧਾਰ ਵੀ ਲਿਆ ਸਕਦਾ ਹੈ। ਹਾਲਾਂਕਿ, FP16 ਫਾਰਮੈਟ ਦੀਆਂ ਕੁਝ ਅੰਦਰੂਨੀ ਖਾਮੀਆਂ ਵੀ ਹਨ, ਜਿਵੇਂ ਘੱਟ ਪ੍ਰਿਸੀਜ਼ਨ, ਜੋ ਕਿ ਕੁਝ ਕੇਸز ਵਿੱਚ ਨੰਬਰਾਤਮਕ ਅਸਥਿਰਤਾ ਜਾਂ ਪ੍ਰਿਸੀਜ਼ਨ ਘਟਾ ਦੇ ਸਕਦੀ ਹੈ।

### **FP32**

FP32 ਫਾਰਮੈਟ ਉੱਚ ਪ੍ਰਿਸੀਜ਼ਨ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ ਅਤੇ ਵਿਆਪਕ ਰੇਂਜ ਦੇ ਮੁੱਲਾਂ ਨੂੰ ਸਹੀ ਤਰੀਕੇ ਨਾਲ ਦਰਸਾ ਸਕਦਾ ਹੈ। ਜਦੋਂ ਜਟਿਲ ਗਣਿਤੀਕ ਓਪਰੇਸ਼ਨ ਕੀਤੇ ਜਾਂਦੇ ਹਨ ਜਾਂ ਉੱਚ-ਪ੍ਰਿਸੀਜ਼ਨ ਨਤੀਜੇ ਲਾਜ਼ਮੀ ਹੁੰਦੇ ਹਨ, FP32 ਫਾਰਮੈਟ ਵਰਤਣਾ ਬਿਹਤਰ ਰਹਿੰਦਾ ਹੈ। ਹਾਲਾਂਕਿ, ਉੱਚ ਪ੍ਰਿਸੀਜ਼ਨ ਦਾ ਮਤਲਬ ਵੱਧ ਮੈਮੋਰੀ ਉਪਭੋਗਤਾ ਅਤੇ ਲੰਮੀ ਗਣਨਾ ਸਮਾਂ ਵੀ ਹੁੰਦਾ ਹੈ। ਵੱਡੇ ਪੱਧਰ ਦੇ ਡੀਪ ਲਰਨਿੰਗ ਮਾਡਲਾਂ ਲਈ, ਖਾਸ ਕਰਕੇ ਜਦੋਂ ਬਹੁਤ ਸਾਰੇ ਮਾਡਲ ਪੈਰਾਮੀਟਰ ਅਤੇ ਵੱਡੀ ਮਾਤਰਾ ਵਿੱਚ ਡੇਟਾ ਹੋਵੇ, FP32 ਫਾਰਮੈਟ GPU ਮੈਮੋਰੀ ਦੀ ਘਾਟ ਜਾਂ ਇੰਫਰੰਸ ਰਫ਼ਤਾਰ ਵਿੱਚ ਕਮੀ ਕਾਰਨ ਬਣ ਸਕਦਾ ਹੈ।

On mobile devices ਜਾਂ IoT ਡਿਵਾਈਸز 'ਤੇ, ਅਸੀਂ Phi-3.x ਮਾਡਲਾਂ ਨੂੰ INT4 ਵਿੱਚ ਬਦਲ ਸਕਦੇ ਹਾਂ, ਜਦਕਿ AI PC / Copilot PC ਉੱਚ ਪ੍ਰਿਸੀਜ਼ਨ ਜਿਵੇਂ ਕਿ INT8, FP16, FP32 ਵਰਤ ਸਕਦੇ ਹਨ।

ਫਿਲਹਾਲ, ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਨਿਰਮਾਤਾ ਜਨਰੇਟਿਵ ਮਾਡਲਾਂ ਦਾ ਸਹਿਯੋਗ ਕਰਨ ਲਈ ਵੱਖ-ਵੱਖ ਫਰੇਮਵਰਕ ਰੱਖਦੇ ਹਨ, ਜਿਵੇਂ ਕਿ Intel's OpenVINO, Qualcomm's QNN, Apple's MLX, ਅਤੇ Nvidia's CUDA ਆਦਿ, ਜੋ ਮਾਡਲ ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਨਾਲ ਮਿਲ ਕੇ ਲੋਕਲ ਡਿਪਲੌਇਮੈਂਟ ਨੂੰ ਪੂਰਾ ਕਰਦੇ ਹਨ।

ਤਕਨੀਕੀ ਪੱਖ ਤੋਂ, ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਦੇ ਬਾਅਦ ਸਾਡੇ ਕੋਲ ਵੱਖ-ਵੱਖ ਫਾਰਮੈਟ ਸਹਿਯੋਗ ਹੁੰਦਾ ਹੈ, ਜਿਵੇਂ PyTorch / TensorFlow ਫਾਰਮੈਟ, GGUF ਅਤੇ ONNX। ਮੈਂ GGUF ਅਤੇ ONNX ਵਿਚਕਾਰ ਫਾਰਮੈਟ ਤੁਲਨਾ ਅਤੇ ਐਪਲੀਕੇਸ਼ਨ ਸਨੈਰੀਓ ਕੀਤੇ ਹਨ। ਇੱਥੇ ਮੈਂ ONNX ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਫਾਰਮੈਟ ਦੀ ਸਿਫ਼ਾਰਸ਼ ਕਰਦਾ ਹਾਂ, ਜਿਸਨੂੰ ਮਾਡਲ ਫਰੇਮਵਰਕ ਤੋਂ ਲੈ ਕੇ ਹਾਰਡਵੇਅਰ ਤੱਕ ਚੰਗਾ ਸਹਿਯੋਗ ਮਿਲਦਾ ਹੈ। ਇਸ ਅਧਿਆਇ ਵਿੱਚ, ਅਸੀਂ ONNX Runtime for GenAI, OpenVINO, ਅਤੇ Apple MLX 'ਤੇ ਮਾਡਲ ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ 'ਤੇ ਧਿਆਨ ਦੇਵਾਂਗੇ (ਜੇ ਤੁਹਾਡੇ ਕੋਲ ਕੋਈ ਬਿਹਤਰ ਤਰੀਕਾ ਹੈ, ਤਾਂ ਤੁਸੀਂ PR ਜਮ੍ਹਾਂ ਕਰਕੇ ਸਾਨੂੰ ਦੱਸ ਸਕਦੇ ਹੋ)

**This chapter includes**

1. [Quantizing Phi-3.5 / 4 using llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Quantizing Phi-3.5 / 4 using Generative AI extensions for onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Quantizing Phi-3.5 / 4 using Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Quantizing Phi-3.5 / 4 using Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
ਅਸਵੀਕਾਰਨ:
ਇਸ ਦਸਤਾਵੇਜ਼ ਦਾ ਅਨੁਵਾਦ ਏਆਈ ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਕੀਤਾ ਗਿਆ ਹੈ। ਅਸੀਂ ਸ਼ੁੱਧਤਾ ਲਈ ਕੋਸ਼ਿਸ਼ ਕਰਦੇ ਹਾਂ, ਪਰ ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਰੱਖੋ ਕਿ ਆਟੋਮੇਟਿਕ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਤਰੁਟੀਆਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਉਸ ਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਹੀ ਅਧਿਕਾਰਤ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਿਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਅਸੀਂ ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਨਾਲ ਪੈਦਾ ਹੋਣ ਵਾਲੀਆਂ ਕਿਸੇ ਵੀ ਗਲਤਫਹਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।
<!-- CO-OP TRANSLATOR DISCLAIMER END -->