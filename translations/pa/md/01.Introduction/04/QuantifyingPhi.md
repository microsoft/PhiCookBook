<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "d658062de70b131ef4c0bff69b5fc70e",
  "translation_date": "2025-05-09T13:22:22+00:00",
  "source_file": "md/01.Introduction/04/QuantifyingPhi.md",
  "language_code": "pa"
}
-->
# **ਫਾਈ ਪਰਿਵਾਰ ਦੀ ਮਾਤਰਾ ਨਿਰਧਾਰਿਤ ਕਰਨਾ**

ਮਾਡਲ ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਦਾ ਮਤਲਬ ਹੈ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਮਾਡਲ ਵਿੱਚ ਪੈਰਾਮੀਟਰਾਂ (ਜਿਵੇਂ ਕਿ ਵਜ਼ਨ ਅਤੇ ਐਕਟੀਵੇਸ਼ਨ ਮੁੱਲ) ਨੂੰ ਵੱਡੇ ਮੁੱਲਾਂ ਦੇ ਰੇਂਜ (ਆਮ ਤੌਰ 'ਤੇ ਲਗਾਤਾਰ ਮੁੱਲਾਂ ਦਾ ਰੇਂਜ) ਤੋਂ ਛੋਟੇ ਸੀਮਿਤ ਮੁੱਲਾਂ ਦੇ ਰੇਂਜ ਵਿੱਚ ਬਦਲਣਾ। ਇਹ ਤਕਨੀਕ ਮਾਡਲ ਦੇ ਆਕਾਰ ਅਤੇ ਗਣਨਾਤਮਕ ਜਟਿਲਤਾ ਨੂੰ ਘਟਾ ਸਕਦੀ ਹੈ ਅਤੇ ਮੋਬਾਈਲ ਡਿਵਾਈਸ ਜਾਂ ਐਂਬੇਡਿਡ ਸਿਸਟਮ ਵਰਗੇ ਸਰੋਤ-ਸੀਮਿਤ ਵਾਤਾਵਰਣਾਂ ਵਿੱਚ ਮਾਡਲ ਦੀ ਕਾਰਗੁਜ਼ਾਰੀ ਨੂੰ ਸੁਧਾਰ ਸਕਦੀ ਹੈ। ਮਾਡਲ ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਪੈਰਾਮੀਟਰਾਂ ਦੀ ਸਹੀਤਾ ਨੂੰ ਘਟਾ ਕੇ ਸੰਕੋਚਨ ਪ੍ਰਾਪਤ ਕਰਦਾ ਹੈ, ਪਰ ਇਸ ਨਾਲ ਕੁਝ ਹੱਦ ਤੱਕ ਸਹੀਤਾ ਵਿੱਚ ਕਮੀ ਵੀ ਹੁੰਦੀ ਹੈ। ਇਸ ਲਈ, ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਪ੍ਰਕਿਰਿਆ ਵਿੱਚ ਮਾਡਲ ਦੇ ਆਕਾਰ, ਗਣਨਾਤਮਕ ਜਟਿਲਤਾ ਅਤੇ ਸਹੀਤਾ ਦੇ ਵਿਚਕਾਰ ਸੰਤੁਲਨ ਬਣਾਉਣਾ ਜਰੂਰੀ ਹੁੰਦਾ ਹੈ। ਆਮ ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਤਰੀਕੇ ਵਿੱਚ ਫਿਕਸਡ-ਪੌਇੰਟ ਕੁਆਨਟਾਈਜੇਸ਼ਨ, ਫਲੋਟਿੰਗ-ਪੌਇੰਟ ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਆਦਿ ਸ਼ਾਮਲ ਹਨ। ਤੁਸੀਂ ਵਿਸ਼ੇਸ਼ ਸਥਿਤੀ ਅਤੇ ਲੋੜਾਂ ਅਨੁਸਾਰ ਉਚਿਤ ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਰਣਨੀਤੀ ਚੁਣ ਸਕਦੇ ਹੋ।

ਅਸੀਂ ਚਾਹੁੰਦੇ ਹਾਂ ਕਿ GenAI ਮਾਡਲ ਨੂੰ ਏਜ ਡਿਵਾਈਸਾਂ 'ਤੇ ਤੈਅ ਕੀਤਾ ਜਾਵੇ ਅਤੇ ਹੋਰ ਜ਼ਿਆਦਾ ਡਿਵਾਈਸਾਂ ਨੂੰ GenAI ਸਥਿਤੀਆਂ ਵਿੱਚ ਲਿਆਇਆ ਜਾਵੇ, ਜਿਵੇਂ ਕਿ ਮੋਬਾਈਲ ਡਿਵਾਈਸ, AI PC/Copilot+PC, ਅਤੇ ਰਵਾਇਤੀ IoT ਡਿਵਾਈਸ। ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਮਾਡਲ ਰਾਹੀਂ, ਅਸੀਂ ਇਸਨੂੰ ਵੱਖ-ਵੱਖ ਏਜ ਡਿਵਾਈਸਾਂ 'ਤੇ ਤੈਅ ਕਰ ਸਕਦੇ ਹਾਂ ਜੋ ਵੱਖ-ਵੱਖ ਡਿਵਾਈਸਾਂ ਦੇ ਅਧਾਰ 'ਤੇ ਹਨ। ਹਾਰਡਵੇਅਰ ਨਿਰਮਾਤਿਆਂ ਵੱਲੋਂ ਦਿੱਤੇ ਮਾਡਲ ਤੇਜ਼ੀ ਫਰੇਮਵਰਕ ਅਤੇ ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਮਾਡਲ ਨਾਲ ਮਿਲ ਕੇ, ਅਸੀਂ ਬਿਹਤਰ SLM ਐਪਲੀਕੇਸ਼ਨ ਸਥਿਤੀਆਂ ਬਣਾ ਸਕਦੇ ਹਾਂ।

ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਸਥਿਤੀ ਵਿੱਚ, ਸਾਡੇ ਕੋਲ ਵੱਖ-ਵੱਖ ਸਹੀਤਾਵਾਂ (INT4, INT8, FP16, FP32) ਹਨ। ਹੇਠਾਂ ਆਮ ਤੌਰ 'ਤੇ ਵਰਤੇ ਜਾਂਦੇ ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਸਹੀਤਾਵਾਂ ਦੀ ਵਿਆਖਿਆ ਦਿੱਤੀ ਗਈ ਹੈ।

### **INT4**

INT4 ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਇੱਕ ਕਾਫ਼ੀ ਤੀਬਰ ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਤਰੀਕਾ ਹੈ ਜੋ ਮਾਡਲ ਦੇ ਵਜ਼ਨਾਂ ਅਤੇ ਐਕਟੀਵੇਸ਼ਨ ਮੁੱਲਾਂ ਨੂੰ 4-ਬਿੱਟ ਇੰਟੀਜਰਾਂ ਵਿੱਚ ਬਦਲਦਾ ਹੈ। INT4 ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਛੋਟੇ ਪ੍ਰਤੀਨਿਧਾਨ ਰੇਂਜ ਅਤੇ ਘੱਟ ਸਹੀਤਾ ਕਰਕੇ ਵੱਡੀ ਸਹੀਤਾ ਘਾਟ ਦਾ ਕਾਰਨ ਬਣਦਾ ਹੈ। ਪਰ, INT8 ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਨਾਲੋਂ, INT4 ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਮਾਡਲ ਦੀ ਸਟੋਰੇਜ ਲੋੜਾਂ ਅਤੇ ਗਣਨਾ ਜਟਿਲਤਾ ਨੂੰ ਹੋਰ ਘਟਾ ਸਕਦਾ ਹੈ। ਧਿਆਨ ਦੇਣ ਯੋਗ ਗੱਲ ਇਹ ਹੈ ਕਿ ਵਾਸਤਵਿਕ ਲਾਗੂਆਮਲਾਂ ਵਿੱਚ INT4 ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਕਾਫ਼ੀ ਘੱਟ ਵਰਤਿਆ ਜਾਂਦਾ ਹੈ, ਕਿਉਂਕਿ ਬਹੁਤ ਘੱਟ ਸਹੀਤਾ ਮਾਡਲ ਦੀ ਕਾਰਗੁਜ਼ਾਰੀ ਵਿੱਚ ਵੱਡਾ ਨੁਕਸਾਨ ਪਹੁੰਚਾ ਸਕਦੀ ਹੈ। ਇਸਦੇ ਨਾਲ-ਨਾਲ, ਸਾਰੇ ਹਾਰਡਵੇਅਰ INT4 ਆਪਰੇਸ਼ਨਾਂ ਨੂੰ ਸਮਰਥਨ ਨਹੀਂ ਕਰਦੇ, ਇਸ ਲਈ ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਤਰੀਕੇ ਦੀ ਚੋਣ ਕਰਦਿਆਂ ਹਾਰਡਵੇਅਰ ਅਨੁਕੂਲਤਾ ਦਾ ਧਿਆਨ ਰੱਖਣਾ ਜਰੂਰੀ ਹੈ।

### **INT8**

INT8 ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਮਾਡਲ ਦੇ ਵਜ਼ਨ ਅਤੇ ਐਕਟੀਵੇਸ਼ਨ ਨੂੰ ਫਲੋਟਿੰਗ-ਪੌਇੰਟ ਨੰਬਰਾਂ ਤੋਂ 8-ਬਿੱਟ ਇੰਟੀਜਰਾਂ ਵਿੱਚ ਬਦਲਣ ਦੀ ਪ੍ਰਕਿਰਿਆ ਹੈ। ਹਾਲਾਂਕਿ INT8 ਇੰਟੀਜਰਾਂ ਵੱਲੋਂ ਦਰਸਾਏ ਜਾਣ ਵਾਲੇ ਅੰਕੜੇ ਦੀ ਸੀਮਾ ਛੋਟੀ ਅਤੇ ਘੱਟ ਸਹੀਤਾ ਵਾਲੀ ਹੁੰਦੀ ਹੈ, ਪਰ ਇਹ ਸਟੋਰੇਜ ਅਤੇ ਗਣਨਾ ਦੀਆਂ ਲੋੜਾਂ ਨੂੰ ਕਾਫ਼ੀ ਘਟਾ ਸਕਦਾ ਹੈ। INT8 ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਵਿੱਚ, ਮਾਡਲ ਦੇ ਵਜ਼ਨ ਅਤੇ ਐਕਟੀਵੇਸ਼ਨ ਮੁੱਲ ਕੁਆਨਟਾਈਜ਼ੇਸ਼ਨ ਪ੍ਰਕਿਰਿਆ ਵਿੱਚੋਂ ਲੰਘਦੇ ਹਨ, ਜਿਸ ਵਿੱਚ ਸਕੇਲਿੰਗ ਅਤੇ ਓਫਸੈਟ ਸ਼ਾਮਲ ਹੁੰਦੇ ਹਨ, ਤਾਂ ਜੋ ਮੂਲ ਫਲੋਟਿੰਗ-ਪੌਇੰਟ ਜਾਣਕਾਰੀ ਨੂੰ ਜ਼ਿਆਦਾ ਤੋਂ ਜ਼ਿਆਦਾ ਬਚਾਇਆ ਜਾ ਸਕੇ। ਇਨਫਰੰਸ ਦੌਰਾਨ, ਇਹ ਕੁਆਨਟਾਈਜ਼ਡ ਮੁੱਲ ਫਿਰ ਫਲੋਟਿੰਗ-ਪੌਇੰਟ ਨੰਬਰਾਂ ਵਿੱਚ ਵਾਪਸ ਬਦਲੇ ਜਾਂਦੇ ਹਨ ਗਣਨਾ ਲਈ, ਅਤੇ ਫਿਰ ਅਗਲੇ ਕਦਮ ਲਈ ਦੁਬਾਰਾ INT8 ਵਿੱਚ ਕੁਆਨਟਾਈਜ਼ ਕੀਤੇ ਜਾਂਦੇ ਹਨ। ਇਹ ਤਰੀਕਾ ਜ਼ਿਆਦਾਤਰ ਐਪਲੀਕੇਸ਼ਨਾਂ ਵਿੱਚ ਕਾਫ਼ੀ ਸਹੀਤਾ ਦੇ ਸਕਦਾ ਹੈ ਅਤੇ ਉੱਚ ਗਣਨਾਤਮਕ ਕੁਸ਼ਲਤਾ ਨੂੰ ਬਰਕਰਾਰ ਰੱਖਦਾ ਹੈ।

### **FP16**

FP16 ਫਾਰਮੈਟ, ਜਿਸਨੂੰ 16-ਬਿੱਟ ਫਲੋਟਿੰਗ-ਪੌਇੰਟ ਨੰਬਰ (float16) ਕਿਹਾ ਜਾਂਦਾ ਹੈ, 32-ਬਿੱਟ ਫਲੋਟਿੰਗ-ਪੌਇੰਟ ਨੰਬਰਾਂ (float32) ਨਾਲੋਂ ਯਾਦਾਸ਼ਤ ਦੀ ਖਪਤ ਨੂੰ ਅੱਧਾ ਕਰਦਾ ਹੈ, ਜੋ ਵੱਡੇ ਪੱਧਰ ਦੇ ਡੀਪ ਲਰਨਿੰਗ ਐਪਲੀਕੇਸ਼ਨਾਂ ਵਿੱਚ ਮਹੱਤਵਪੂਰਨ ਫਾਇਦੇ ਦਿੰਦਾ ਹੈ। FP16 ਫਾਰਮੈਟ ਨਾਲ ਇੱਕੋ GPU ਯਾਦਾਸ਼ਤ ਸੀਮਾਵਾਂ ਵਿੱਚ ਵੱਡੇ ਮਾਡਲ ਲੋਡ ਕਰਨ ਜਾਂ ਜ਼ਿਆਦਾ ਡੇਟਾ ਪ੍ਰੋਸੈਸ ਕਰਨ ਦੀ ਸਮਰੱਥਾ ਹੁੰਦੀ ਹੈ। ਜਿਵੇਂ-ਜਿਵੇਂ ਆਧੁਨਿਕ GPU ਹਾਰਡਵੇਅਰ FP16 ਆਪਰੇਸ਼ਨਾਂ ਨੂੰ ਸਮਰਥਨ ਕਰਦਾ ਜਾ ਰਿਹਾ ਹੈ, FP16 ਫਾਰਮੈਟ ਦੀ ਵਰਤੋਂ ਨਾਲ ਗਣਨਾ ਦੀ ਗਤੀ ਵਿੱਚ ਸੁਧਾਰ ਵੀ ਹੋ ਸਕਦਾ ਹੈ। ਪਰ FP16 ਫਾਰਮੈਟ ਦੀਆਂ ਕੁਝ ਆਪਣੀਆਂ ਨੁਕਸਾਨੀਆਂ ਵੀ ਹਨ, ਜਿਵੇਂ ਘੱਟ ਸਹੀਤਾ, ਜਿਸ ਨਾਲ ਕੁਝ ਹਾਲਤਾਂ ਵਿੱਚ ਗਣਿਤੀ ਅਸਥਿਰਤਾ ਜਾਂ ਸਹੀਤਾ ਦਾ ਨੁਕਸਾਨ ਹੋ ਸਕਦਾ ਹੈ।

### **FP32**

FP32 ਫਾਰਮੈਟ ਵੱਧ ਸਹੀਤਾ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ ਅਤੇ ਮੁੱਲਾਂ ਦੀ ਵਿਆਪਕ ਰੇਂਜ ਨੂੰ ਸਹੀ ਤਰੀਕੇ ਨਾਲ ਦਰਸਾ ਸਕਦਾ ਹੈ। ਜਿੱਥੇ ਜਟਿਲ ਗਣਿਤੀ ਕਾਰਵਾਈਆਂ ਕੀਤੀਆਂ ਜਾਂਦੀਆਂ ਹਨ ਜਾਂ ਉੱਚ-ਸਹੀਤਾ ਵਾਲੇ ਨਤੀਜੇ ਲੋੜੀਂਦੇ ਹਨ, ਉੱਥੇ FP32 ਫਾਰਮੈਟ ਨੂੰ ਤਰਜੀਹ ਦਿੱਤੀ ਜਾਂਦੀ ਹੈ। ਪਰ, ਉੱਚ ਸਹੀਤਾ ਦਾ ਮਤਲਬ ਵੱਧ ਯਾਦਾਸ਼ਤ ਦੀ ਵਰਤੋਂ ਅਤੇ ਲੰਮਾ ਗਣਨਾ ਸਮਾਂ ਵੀ ਹੁੰਦਾ ਹੈ। ਵੱਡੇ ਪੱਧਰ ਦੇ ਡੀਪ ਲਰਨਿੰਗ ਮਾਡਲਾਂ ਵਿੱਚ, ਖਾਸ ਕਰਕੇ ਜਦੋਂ ਮਾਡਲ ਪੈਰਾਮੀਟਰਾਂ ਦੀ ਗਿਣਤੀ ਜ਼ਿਆਦਾ ਹੋਵੇ ਅਤੇ ਡੇਟਾ ਦਾ ਬਹੁਤ ਵੱਡਾ ਭੰਡਾਰ ਹੋਵੇ, FP32 ਫਾਰਮੈਟ GPU ਯਾਦਾਸ਼ਤ ਦੀ ਘਾਟ ਜਾਂ ਇਨਫਰੰਸ ਦੀ ਗਤੀ ਵਿੱਚ ਕਮੀ ਦਾ ਕਾਰਨ ਬਣ ਸਕਦਾ ਹੈ।

ਮੋਬਾਈਲ ਡਿਵਾਈਸਾਂ ਜਾਂ IoT ਡਿਵਾਈਸਾਂ 'ਤੇ ਅਸੀਂ Phi-3.x ਮਾਡਲਾਂ ਨੂੰ INT4 ਵਿੱਚ ਬਦਲ ਸਕਦੇ ਹਾਂ, ਜਦਕਿ AI PC / Copilot PC ਵੱਧ ਸਹੀਤਾ ਵਾਲੇ ਤਰੀਕੇ ਵਰਤ ਸਕਦੇ ਹਨ ਜਿਵੇਂ ਕਿ INT8, FP16, FP32।

ਫਿਲਹਾਲ, ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਨਿਰਮਾਤਿਆਂ ਕੋਲ ਜਨਰੇਟਿਵ ਮਾਡਲਾਂ ਨੂੰ ਸਮਰਥਨ ਦੇਣ ਲਈ ਵੱਖ-ਵੱਖ ਫਰੇਮਵਰਕ ਹਨ, ਜਿਵੇਂ Intel ਦਾ OpenVINO, Qualcomm ਦਾ QNN, Apple ਦਾ MLX, ਅਤੇ Nvidia ਦਾ CUDA ਆਦਿ, ਜੋ ਮਾਡਲ ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਨਾਲ ਮਿਲ ਕੇ ਸਥਾਨਕ ਤੈਅਕਰਨ ਨੂੰ ਪੂਰਾ ਕਰਦੇ ਹਨ।

ਤਕਨੀਕੀ ਪੱਖ ਤੋਂ, ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਤੋਂ ਬਾਅਦ ਸਾਡੇ ਕੋਲ ਵੱਖ-ਵੱਖ ਫਾਰਮੈਟ ਸਹਾਇਤਾ ਹੈ, ਜਿਵੇਂ PyTorch / Tensorflow ਫਾਰਮੈਟ, GGUF, ਅਤੇ ONNX। ਮੈਂ GGUF ਅਤੇ ONNX ਵਿਚਕਾਰ ਫਾਰਮੈਟ ਤੁਲਨਾ ਅਤੇ ਐਪਲੀਕੇਸ਼ਨ ਸਥਿਤੀਆਂ ਕਰ ਚੁੱਕਾ ਹਾਂ। ਇੱਥੇ ਮੈਂ ONNX ਕੁਆਨਟਾਈਜੇਸ਼ਨ ਫਾਰਮੈਟ ਦੀ ਸਿਫਾਰਸ਼ ਕਰਦਾ ਹਾਂ, ਜਿਸਨੂੰ ਮਾਡਲ ਫਰੇਮਵਰਕ ਤੋਂ ਲੈ ਕੇ ਹਾਰਡਵੇਅਰ ਤੱਕ ਚੰਗਾ ਸਮਰਥਨ ਮਿਲਦਾ ਹੈ। ਇਸ ਅਧਿਆਇ ਵਿੱਚ ਅਸੀਂ ONNX Runtime for GenAI, OpenVINO, ਅਤੇ Apple MLX ਦੇ ਜ਼ਰੀਏ ਮਾਡਲ ਕੁਆਨਟਾਈਜੇਸ਼ਨ 'ਤੇ ਧਿਆਨ ਦੇਵਾਂਗੇ (ਜੇ ਤੁਹਾਡੇ ਕੋਲ ਕੋਈ ਵਧੀਆ ਤਰੀਕਾ ਹੈ, ਤਾਂ ਤੁਸੀਂ PR ਸਬਮਿਟ ਕਰਕੇ ਸਾਨੂੰ ਦੇ ਸਕਦੇ ਹੋ)।

**ਇਸ ਅਧਿਆਇ ਵਿੱਚ ਸ਼ਾਮਲ ਹੈ**

1. [llama.cpp ਦੀ ਵਰਤੋਂ ਕਰਕੇ Phi-3.5 / 4 ਨੂੰ ਕੁਆਨਟਾਈਜ਼ ਕਰਨਾ](./UsingLlamacppQuantifyingPhi.md)

2. [onnxruntime ਲਈ Generative AI ਐਕਸਟੇਂਸ਼ਨ ਦੀ ਵਰਤੋਂ ਕਰਕੇ Phi-3.5 / 4 ਨੂੰ ਕੁਆਨਟਾਈਜ਼ ਕਰਨਾ](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO ਦੀ ਵਰਤੋਂ ਕਰਕੇ Phi-3.5 / 4 ਨੂੰ ਕੁਆਨਟਾਈਜ਼ ਕਰਨਾ](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX Framework ਦੀ ਵਰਤੋਂ ਕਰਕੇ Phi-3.5 / 4 ਨੂੰ ਕੁਆਨਟਾਈਜ਼ ਕਰਨਾ](./UsingAppleMLXQuantifyingPhi.md)

**ਅਸਵੀਕਾਰੋक्ति**:  
ਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀਤਾ ਲਈ ਕੋਸ਼ਿਸ਼ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਵਿੱਚ ਰੱਖੋ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਣਸਹੀਤੀਆਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼ ਆਪਣੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਪ੍ਰਮਾਣਿਕ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਜਰੂਰੀ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਅਸੀਂ ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੀਆਂ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀਆਂ ਜਾਂ ਭ੍ਰਮਾਂ ਲਈ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।