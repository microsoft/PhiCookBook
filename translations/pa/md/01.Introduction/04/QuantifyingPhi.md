# **ਫਾਈ ਪਰਿਵਾਰ ਨੂੰ ਮਾਪਣਾ**

ਮਾਡਲ ਕਵਾਂਟੀਜੇਸ਼ਨ ਦਾ ਅਰਥ ਹੈ ਕਿਸੇ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਮਾਡਲ ਵਿੱਚ ਪੈਰਾਮੀਟਰਾਂ (ਜਿਵੇਂ ਕਿ ਵਜ਼ਨ ਅਤੇ ਸਰਗਰਮੀ ਮੁੱਲ) ਨੂੰ ਇੱਕ ਵੱਡੇ ਮੁੱਲ ਸੀਮਾ (ਆਮਤੌਰ 'ਤੇ ਲਗਾਤਾਰ ਮੁੱਲ ਸੀਮਾ) ਤੋਂ ਛੋਟੀ ਸੀਮਤ ਮੁੱਲ ਸੀਮਾ ਵਿੱਚ ਮੈਪ ਕਰਨ ਦੀ ਪ੍ਰਕਿਰਿਆ। ਇਹ ਤਕਨੀਕ ਮਾਡਲ ਦੇ ਆਕਾਰ ਅਤੇ ਗਣਨਾਤਮਕ ਜટਿਲਤਾ ਨੂੰ ਘਟਾ ਸਕਦੀ ਹੈ ਅਤੇ ਮਾਡਲ ਨੂੰ ਰਿਸੋਰਸ-ਸੀਮਿਤ ਵਾਤਾਵਰਨਾਂ ਜਿਵੇਂ ਕਿ ਮੋਬਾਈਲ ਡਿਵਾਈਸز ਜਾਂ ਐম্বੈੱਡਿਡ ਸਿਸਟਮਾਂ ਵਿੱਚ ਚਲਾਉਣ ਦੀ ਕੁਸ਼ਲਤਾ ਨੂੰ ਸੁਧਾਰ ਸਕਦੀ ਹੈ। ਮਾਡਲ ਕਵਾਂਟੀਜੇਸ਼ਨ ਪੈਰਾਮੀਟਰਾਂ ਦੀ ਸਹੀਤਾ ਨੂੰ ਘਟਾ ਕੇ ਕੰਪ੍ਰੈਸ਼ਨ ਪ੍ਰਾਪਤ ਕਰਦੀ ਹੈ, ਪਰ ਇਸ ਨਾਲ ਕੁਝ ਨੁਕਸਾਨ ਵੀ ਹੁੰਦਾ ਹੈ। ਇਸ ਲਈ, ਕਵਾਂਟੀਜੇਸ਼ਨ ਪ੍ਰਕਿਰਿਆ ਵਿੱਚ, ਮਾਡਲ ਦੇ ਆਕਾਰ, ਗਣਨਾਤਮਕ ਜਟਿਲਤਾ, ਅਤੇ ਸਹੀਤਾ ਵਿਚ ਸੰਤੁਲਨ ਬਣਾਇਆ ਜਾਣਾ ਜ਼ਰੂਰੀ ਹੈ। ਆਮ ਕਵਾਂਟੀਜੇਸ਼ਨ ਵਿਧੀਆਂ ਵਿੱਚ ਫਿਕਸਡ-ਪੋਇੰਟ ਕਵਾਂਟੀਜੇਸ਼ਨ, ਫਲੋਟਿੰਗ-ਪੌਇੰਟ ਕਵਾਂਟੀਜੇਸ਼ਨ ਆਦਿ ਸ਼ਾਮਲ ਹਨ। ਤੁਸੀਂ ਵਿਸ਼ੇਸ਼ ਸੰਦਰਭ ਅਤੇ ਲੋੜਾਂ ਦੇ ਅਨੁਸਾਰ ਉਚਿਤ ਕਵਾਂਟੀਜੇਸ਼ਨ ਰਣਨੀਤੀ ਚੁਣ ਸਕਦੇ ਹੋ। 

ਅਸੀਂ ਚਾਹੁੰਦੇ ਹਾਂ ਕਿ GenAI ਮਾਡਲ ਨੂੰ ਐਜ ਡਿਵਾਈਸز 'ਤੇ ਨਿਯੁਕਤ ਕੀਤਾ ਜਾਵੇ ਅਤੇ ਹੋਰ ਡਿਵਾਈਸਜ਼ ਨੂੰ GenAI ਸੰਦਰਭਾਂ ਵਿਚ ਸ਼ਾਮਿਲ ਕੀਤਾ ਜਾਵੇ, ਜਿਵੇਂ ਮੋਬਾਈਲ ਡਿਵਾਈਸਜ਼, AI PC/Copilot+PC, ਅਤੇ ਪਰੰਪਰਾਗਤ IoT ਡਿਵਾਈਸਜ਼। ਕਵਾਂਟੀਜੇਸ਼ਨ ਮਾਡਲ ਰਾਹੀਂ, ਅਸੀਂ ਇਸ ਨੂੰ ਵੱਖ-ਵੱਖ ਐਜ ਡਿਵਾਈਸਜ਼ 'ਤੇ ਅਲੱਗ-ਅਲੱਗ ਡਿਵਾਈਸ ਦੇ ਆਧਾਰ 'ਤੇ ਤੈਨਾਤ ਕਰ ਸਕਦੇ ਹਾਂ। ਹਾਰਡਵੇਅਰ ਨਿਰਮਾਤਾ ਵੱਲੋਂ ਦਿੱਤੇ ਗਏ ਮਾਡਲ ਤੇਜ਼ੀ ਫਰੇਮਵਰਕ ਅਤੇ ਕਵਾਂਟੀਜੇਸ਼ਨ ਮਾਡਲ ਨਾਲ ਮਿਲਾਕੇ, ਅਸੀਂ ਵਧੀਆ SLM ਐਪਲੀਕੇਸ਼ਨ ਸੰਦਰਭ बना ਸਕਦੇ ਹਾਂ। 

ਕਵਾਂਟੀਜੇਸ਼ਨ ਸੰਦਰਭ ਵਿੱਚ, ਸਾਡੇ ਕੋਲ ਵੱਖ-ਵੱਖ ਸਹੀਤਾਵਾਂ ਹਨ (INT4, INT8, FP16, FP32)। ਹੇਠਾਂ ਆਮ ਤੌਰ 'ਤੇ ਵਰਤੀਆਂ ਜਾਣ ਵਾਲੀਆਂ ਕਵਾਂਟੀਜੇਸ਼ਨ ਸਹੀਤਾਵਾਂ ਦੀ ਵਿਆਖਿਆ ਦਿੱਤੀ ਗਈ ਹੈ।

### **INT4**

INT4 ਕਵਾਂਟੀਜੇਸ਼ਨ ਇੱਕ ਬਹੁਤ ਹੀ ਗੰਭੀਰ ਕਵਾਂਟੀਜੇਸ਼ਨ ਵਿਧੀ ਹੈ ਜੋ ਮਾਡਲ ਦੇ ਵਜ਼ਨਾਂ ਅਤੇ ਸਰਗਰਮੀ ਮੁੱਲਾਂ ਨੂੰ 4-ਬਿੱਟ ਪੂਰਨਾਂਕਾਂ ਵਿੱਚ ਕਵਾਂਟੀਕਾਰ ਕਰਦੀ ਹੈ। ਛੋਟੀ ਪ੍ਰਤੀਨਿਧਿੱਤਾ ਸੀਮਾ ਅਤੇ ਘੱਟ ਸਹੀਤਾ ਦੇ ਕਾਰਨ ਆਮ ਤੌਰ 'ਤੇ INT4 ਕਵਾਂਟੀਜੇਸ਼ਨ ਸਹੀਤਾ ਵਿੱਚ ਵੱਧ ਨੁਕਸਾਨ ਪੈਦਾ ਕਰਦਾ ਹੈ। ਇਸਦੇ ਬਾਵਜੂਦ, INT8 ਨਾਲੋਂ ਤੁਲਨਾਤਮਕ ਤੌਰ 'ਤੇ, INT4 ਮਾਡਲ ਦੀ ਸਟੋਰੇਜ਼ ਦੀ ਲੋੜ ਅਤੇ ਗਣਨਾਤਮਕ ਜਟਿਲਤਾ ਨੂੰ ਹੋਰ ਘਟਾ ਸਕਦਾ ਹੈ। ਇਸ ਗੱਲ ਦਾ ਧਿਆਨ ਰੱਖਣਾ ਜਰੂਰੀ ਹੈ ਕਿ ਕਾਰਗੁਜ਼ਾਰੀ ਪ੍ਰਯੋਗਾਂ ਵਿੱਚ INT4 ਕਵਾਂਟੀਜੇਸ਼ਨ ਕਾਫੀ ਘੱਟ ਵਰਤੀ ਜਾਂਦੀ ਹੈ, ਕਿਉਂਕਿ ਬਹੁਤ ਘੱਟ ਸਹੀਤਾ ਮਾਡਲ ਦੀ ਕਾਰਗੁਜ਼ਾਰੀ ਨੂੰ ਕਾਫੀ ਹੱਦ ਤੱਕ ਖ਼ਰਾਬ ਕਰ ਸਕਦੀ ਹੈ। ਇਸਦੇ ਨਾਲ ਨਾਲ, ਸਾਰਾ ਹਾਰਡਵੇਅਰ INT4 ਓਪਰੇਸ਼ਨਾਂ ਦਾ ਸਮਰਥਨ ਨਹੀਂ ਕਰਦਾ, ਇਸ ਕਰਕੇ ਕਵਾਂਟੀਜੇਸ਼ਨ ਵਿਧੀ ਚੁਣਦਿਆਂ ਹਾਰਡਵੇਅਰ ਅਨੁਕੂਲਤਾ 'ਤੇ ਧਿਆਨ ਦੇਣਾ ਪੈਂਦਾ ਹੈ।

### **INT8**

INT8 ਕਵਾਂਟੀਜੇਸ਼ਨ ਇੱਕ ਪ੍ਰਕਿਰਿਆ ਹੈ ਜਿਸ ਵਿੱਚ ਮਾਡਲ ਦੇ ਵਜ਼ਨ ਅਤੇ ਸਰਗਰਮੀ ਮੁੱਲ ਫਲੋਟਿੰਗ ਪੌਇੰਟ ਨੰਬਰਾਂ ਤੋਂ 8-ਬਿੱਟ ਪੂਰਨਾਂਕਾਂ ਵਿੱਚ ਬਦਲੇ ਜਾਂਦੇ ਹਨ। ਜਦਕਿ INT8 ਪੂਰਨਾਂਕਾਂ ਦੁਆਰਾ ਦਰਸਾਈ ਗਈ ਸੰਖਿਆਵਾਂ ਦੀ ਸੀਮਾ ਛੋਟੀ ਅਤੇ ਸਹੀਤਾ ਘੱਟ ਹੁੰਦੀ ਹੈ, ਇਹ ਸਟੋਰੇਜ਼ ਅਤੇ ਗਣਨਾ ਦੀ ਲੋੜ ਨੂੰ ਮਹੱਤਵਪੂਰਕ ਤੌਰ 'ਤੇ ਘਟਾ ਸਕਦਾ ਹੈ। INT8 ਕਵਾਂਟੀਜੇਸ਼ਨ ਵਿੱਚ, ਮਾਡਲ ਦੇ ਵਜ਼ਨ ਅਤੇ ਸਰਗਰਮੀ ਮੁੱਲ ਕਵਾਂਟੀਜੇਸ਼ਨ ਪ੍ਰਕਿਰਿਆ ਤੋਂ ਗੁਜਰਦੇ ਹਨ, ਜਿਸ ਵਿੱਚ ਸਕੇਲਿੰਗ ਅਤੇ ਆਫ਼ਸੈਟ ਸ਼ਾਮਲ ਹੁੰਦੇ ਹਨ, ਤਾਂ ਜੋ ਮੂਲ ਫਲੋਟਿੰਗ ਪੌਇੰਟ ਜਾਣਕਾਰੀ ਜ਼ਿਆਦਾ ਤੋਂ ਜ਼ਿਆਦਾ ਸੰਭਾਲੀ ਜਾ ਸਕੇ। ਇੰਫਰੈਂਸ ਦੌਰਾਨ, ਇਹ Quantized ਕੀਤੀਆਂ ਮੁੱਲਾਂ ਫਿਰ ਫਲੋਟਿੰਗ ਪੌਇੰਟ ਨੰਬਰਾਂ ਵਿੱਚ ਪਰਤਾਈਆਂ ਜਾਂਦੀਆਂ ਹਨ ਗਣਨਾ ਲਈ, ਅਤੇ ਫਿਰ ਅੱਗਲੇ ਕਦਮ ਲਈ ਫਿਰ ਤੋ INT8 ਵਿੱਚ Quantize ਕਰ ਦਿੱਤੀਆਂ ਜਾਂਦੀਆਂ ਹਨ। ਇਹ ਵਿਧੀ ਜ਼ਿਆਦਾਤਰ ਐਪਲੀਕੇਸ਼ਨਾਂ ਵਿੱਚ ਯਥਾਥਤਾ ਪ੍ਰਦਾਨ ਕਰਦੀ ਹੈ ਅਤੇ ਉੱਚ ਗਣਨਾਤਮਕ ਕੁਸ਼ਲਤਾ ਨੂੰ ਬਰਕਰਾਰ ਰੱਖਦੀ ਹੈ।

### **FP16**

FP16 ਫਾਰਮੈਟ, ਜਾਂ 16-ਬਿੱਟ ਫਲੋਟਿੰਗ ਪੌਇੰਟ ਨੰਬਰ (float16), 32-ਬਿੱਟ ਫਲੋਟਿੰਗ ਪੌਇੰਟ ਨੰਬਰਾਂ (float32) ਨਾਲੋਂ ਅੱਧਾ ਯਾਦਸ਼ਾਸ਼ਤਾ ਖਪਤ ਕਰਦਾ ਹੈ, ਜੋ ਵੱਡੇ ਪੱਧਰ ਦੇ ਡੀਪ ਲਰਨਿੰਗ ਐਪਲੀਕੇਸ਼ਨਾਂ ਵਿੱਚ ਮਹੱਤਵਪੂਰਕ ਲਾਭ ਦਿੰਦਾ ਹੈ। FP16 ਫਾਰਮੈਟ ਇੱਕੋ ਜਿਹੇ GPU ਯਾਦਸ਼ਾਸ਼ਤਾ ਸੀਮਾਵਾਂ ਵਿੱਚ ਵੱਡੇ ਮਾਡਲਾਂ ਨੂੰ ਲੋਡ ਕਰਨ ਜਾਂ ਜ਼ਿਆਦਾ ਡੇਟਾ ਪ੍ਰਕਿਰਿਆ ਕਰਨ ਦੀ ਆਗਿਆ ਦਿੰਦਾ ਹੈ। ਜਿਵੇਂ ਕਿ ਆਧੁਨਿਕ GPU ਹਾਰਡਵੇਅਰ FP16 ਓਪਰੇਸ਼ਨਾਂ ਦਾ ਸਮਰਥਨ ਜਾਰੀ ਰੱਖਦਾ ਹੈ, FP16 ਫਾਰਮੈਟ ਦੀ ਵਰਤੋਂ ਨਾਲ ਕੰਪਿਊਟਿੰਗ ਸਪੀਡ ਵਿੱਚ ਸੁਧਾਰ ਵੀ ਆ ਸਕਦਾ ਹੈ। ਇਸਦੇ ਬਾਵਜੂਦ, FP16 ਫਾਰਮੈਟ ਵਿੱਚ ਕੁਝ ਅੰਦਰੂਨੀ ਘਟਕਾਂ ਹਨ, ਜਿਵੇਂ ਘੱਟ ਸਹੀਤਾ, ਜੋ ਕੁਝ ਮਾਮਲਿਆਂ ਵਿੱਚ ਗਿਣਤੀ ਦੀ ਅਸਥਿਰਤਾ ਜਾਂ ਸਹੀਤਾਂ ਦੇ ਘਾਟੇ ਦਾ ਕਾਰਨ ਬਨ ਸਕਦਾ ਹੈ।

### **FP32**

FP32 ਫਾਰਮੈਟ ਉੱਚ ਸਹੀਤਾ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ ਅਤੇ ਮੁੱਲਾਂ ਦੀ ਵਿਆਪਕ ਸੀਮਾ ਨੂੰ ਢੁੱਕਵਾਂ ਤਰ੍ਹਾਂ ਦਰਸਾ ਸਕਦਾ ਹੈ। ਜਿੱਥੇ ਦੁਸ਼ਵਾਰ ਗਣਿਤੀ ਕਾਰਜ ਕੀਤੇ ਜਾਂਦੇ ਹਨ ਜਾਂ ਉੱਚ-ਸਹੀਤਾ ਵਾਲੇ ਨਤੀਜੇ ਲੋੜੀਂਦੇ ਹਨ, ਉੱਥੇ FP32 ਫਾਰਮੈਟ ਪ੍ਰਮੁੱਖ ਰਹਿੰਦਾ ਹੈ। ਇਸਦੇ ਬਾਵਜੂਦ, ਉੱਚ ਸਹੀਤਾ ਦਾ ਮਤਲਬ ਵੱਧ ਯਾਦਸ਼ਾਸ਼ਤਾ ਦੀ ਵਰਤੋਂ ਅਤੇ ਲੰਮੀ ਗਣਨਾ ਸਮਾਂ ਹੈ। ਵੱਡੇ ਪੱਧਰ ਦੇ ਡੀਪ ਲਰਨਿੰਗ ਮਾਡਲਾਂ ਲਈ, ਖਾਸ ਕਰਕੇ ਜਦ ਮਾਡਲ ਪੈਰਾਮੀਟਰ ਬਹੁਤ ਹਨ ਅਤੇ ਡੇਟਾ ਦੀ ਮਾਤਰਾ ਜ਼ਿਆਦਾ ਹੈ, FP32 ਫਾਰਮੈਟ GPU ਯਾਦਸ਼ਾਸ਼ਤਾ ਦੀ ਘਾਟ ਜਾਂ ਇੰਫਰੈਂਸ ਗਤੀ ਵਿੱਚ ਕਮੀ ਪੈਦਾ ਕਰ ਸਕਦਾ ਹੈ।

ਮੋਬਾਈਲ ਜਾਂ IoT ਡਿਵਾਈਸਜ਼ 'ਤੇ, ਅਸੀਂ Phi-3.x ਮਾਡਲਾਂ ਨੂੰ INT4 ਵਿੱਚ ਬਦਲ ਸਕਦੇ ਹਾਂ, ਜਦਕਿ AI PC / Copilot PC ਉੱਚ ਸਹੀਤਾ ਜਿਵੇਂ ਕਿ INT8, FP16, FP32 ਵਰਤ ਸਕਦਾ ਹੈ। 

ਵਰਤਮਾਨ ਵਿੱਚ, ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਨਿਰਮਾਤਾ ਵੱਖ-ਵੱਖ ਫਰੇਮਵਰਕ ਸਹਿਯੋਗ ਦਿੰਦੇ ਹਨ ਜਿਵੇਂ ਕਿ Intel ਦਾ OpenVINO, Qualcomm ਦਾ QNN, Apple ਦਾ MLX, ਅਤੇ Nvidia ਦਾ CUDA ਆਦਿ, ਜੋ ਮਾਡਲ ਕਵਾਂਟੀਜੇਸ਼ਨ ਨਾਲ ਮਿਲਾਕੇ ਸਥਾਨਕ ਤੈਨਾਤੀ ਕਰਦੇ ਹਨ। 

ਤਕਨੀਕੀ ਤੌਰ 'ਤੇ, ਸਾਡੇ ਕੋਲ ਕਵਾਂਟੀਜੇਸ਼ਨ ਤੋਂ ਬਾਅਦ ਵੱਖ-ਵੱਖ ਫਾਰਮੈਟ ਸਹਿਯੋਗ ਹਨ, ਜਿਵੇਂ PyTorch / TensorFlow ਫਾਰਮੈਟ, GGUF, ਅਤੇ ONNX। ਮੈਂ GGUF ਅਤੇ ONNX ਦੇ ਵਿਚਕਾਰ ਫਾਰਮੈਟ ਮੁਕਾਬਲਾ ਅਤੇ ਐਪਲੀਕੇਸ਼ਨ ਸੰਦਰਭ ਕੀਤੇ ਹਨ। ਇੱਥੇ ਮੈਂ ONNX ਕਵਾਂਟੀਜੇਸ਼ਨ ਫਾਰਮੈਟ ਦੀ ਸਿਫਾਰਸ਼ ਕਰਦਾ ਹਾਂ, ਜੋ ਮਾਡਲ ਫਰੇਮਵਰਕ ਤੋਂ ਲੈ ਕੇ ਹਾਰਡਵੇਅਰ ਤੱਕ ਚੰਗਾ ਸਮਰਥਨ ਰੱਖਦਾ ਹੈ। ਇਸ ਅਧਿਆਇ ਵਿੱਚ ਅਸੀਂ ONNX Runtime for GenAI, OpenVINO, ਅਤੇ Apple MLX ਨੂੰ ਮਾਡਲ ਕਵਾਂਟੀਜੇਸ਼ਨ ਕਰਨ ਲਈ ਕੇਂਦਰਿਤ ਕਰਾਂਗੇ (ਜੇ ਤੁਹਾਡੇ ਕੋਲ ਕੋਈ ਬਿਹਤਰ ਤਰੀਕਾ ਹੈ, ਤਾਂ ਤੁਸੀਂ ਸਾਨੂੰ PR ਜਮ੍ਹਾਂ ਕਰਕੇ ਦੇ ਸਕਦੇ ਹੋ)। 

**ਇਸ ਅਧਿਆਇ ਵਿੱਚ ਸ਼ਾਮਲ ਹਨ**

1. [llama.cpp ਦੀ ਵਰਤੋਂ ਕਰਕੇ Phi-3.5 / 4 ਨੂੰ ਕਵਾਂਟੀਕਾਰ ਕਰਨਾ](./UsingLlamacppQuantifyingPhi.md) 

2. [onnxruntime ਲਈ ਜਨਰੇਟਿਵ AI ਐਕਸਟੈਨਸ਼ਨ ਦੀ ਵਰਤੋਂ ਕਰਕੇ Phi-3.5 / 4 ਨੂੰ ਕਵਾਂਟੀਕਾਰ ਕਰਨਾ](./UsingORTGenAIQuantifyingPhi.md)

3. [Intel OpenVINO ਦੀ ਵਰਤੋਂ ਕਰਕੇ Phi-3.5 / 4 ਨੂੰ ਕਵਾਂਟੀਕਾਰ ਕਰਨਾ](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Apple MLX Framework ਦੀ ਵਰਤੋਂ ਕਰਕੇ Phi-3.5 / 4 ਨੂੰ ਕਵਾਂਟੀਕਾਰ ਕਰਨਾ](./UsingAppleMLXQuantifyingPhi.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**ਦਿਆਨ ਦਿਵਾਉਣੀ**:  
ਇਹ ਦਸਤਾਵੇਜ਼ ਏਆਈ ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀਗ੍ਰਹਿ ਲਈ ਯਤਨ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਜਾਣੋ ਕਿ ਆਟੋਮੈਟਿਕ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸਮਰਥਤਾਵਾਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼ ਉਸ ਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿਚ ਅਧਿਕਾਰਕ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਗੰਭੀਰ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫ਼ਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਅਸੀਂ ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਨਾਲ ਹੋਣ ਵਾਲੀਆਂ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀਆਂ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆਵਾਂ ਲਈ ਜਵਾਬਦੇਹ ਨਹੀਂ ਹਾਂ।
<!-- CO-OP TRANSLATOR DISCLAIMER END -->