<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-05-09T08:19:06+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "pa"
}
-->
# ਮੁੱਖ ਤਕਨਾਲੋਜੀਆਂ ਜਿਨ੍ਹਾਂ ਦਾ ਜ਼ਿਕਰ ਕੀਤਾ ਗਿਆ ਹੈ

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - ਇੱਕ ਨੀਵੀਂ ਸਤਰ ਦੀ API ਜੋ DirectX 12 ਦੇ ਉੱਤੇ ਬਣਾਈ ਗਈ ਹੈ ਅਤੇ ਹਾਰਡਵੇਅਰ ਤੇ ਤੇਜ਼ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਲਈ ਵਰਤੀ ਜਾਂਦੀ ਹੈ।  
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia ਵੱਲੋਂ ਵਿਕਸਤ ਕੀਤੀ ਗਈ ਇੱਕ ਪੈਰਲਲ ਕੰਪਿਊਟਿੰਗ ਪਲੇਟਫਾਰਮ ਅਤੇ ਐਪਲੀਕੇਸ਼ਨ ਪ੍ਰੋਗ੍ਰਾਮਿੰਗ ਇੰਟਰਫੇਸ (API) ਮਾਡਲ, ਜੋ ਗ੍ਰਾਫਿਕਸ ਪ੍ਰੋਸੈਸਿੰਗ ਯੂਨਿਟਸ (GPUs) 'ਤੇ ਜਨਰਲ ਪਰਪਜ਼ ਪ੍ਰੋਸੈਸਿੰਗ ਨੂੰ ਯੋਗ ਬਣਾਉਂਦਾ ਹੈ।  
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - ਇੱਕ ਖੁੱਲ੍ਹਾ ਫਾਰਮੈਟ ਜੋ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਮਾਡਲਾਂ ਨੂੰ ਦਰਸਾਉਂਦਾ ਹੈ ਅਤੇ ਵੱਖ-ਵੱਖ ML ਫਰੇਮਵਰਕਾਂ ਵਿਚਕਾਰ ਇੰਟਰਓਪਰੇਬਿਲਿਟੀ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ।  
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - ਇੱਕ ਫਾਰਮੈਟ ਜੋ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਮਾਡਲਾਂ ਨੂੰ ਦਰਸਾਉਣ ਅਤੇ ਅਪਡੇਟ ਕਰਨ ਲਈ ਵਰਤਿਆ ਜਾਂਦਾ ਹੈ, ਖਾਸ ਕਰਕੇ ਛੋਟੇ ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਲਈ ਜੋ CPU ਤੇ 4-8bit ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ ਨਾਲ ਪ੍ਰਭਾਵਸ਼ਾਲੀ ਤਰੀਕੇ ਨਾਲ ਚੱਲ ਸਕਦੇ ਹਨ।

## DirectML

DirectML ਇੱਕ ਨੀਵੀਂ ਸਤਰ ਦੀ API ਹੈ ਜੋ ਹਾਰਡਵੇਅਰ ਤੇ ਤੇਜ਼ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਨੂੰ ਯੋਗ ਬਣਾਉਂਦੀ ਹੈ। ਇਹ DirectX 12 ਦੇ ਉੱਤੇ ਬਣਾਈ ਗਈ ਹੈ ਤਾਂ ਜੋ GPU ਤੀਜ਼ੀ ਨਾਲ ਕੰਮ ਕਰ ਸਕੇ ਅਤੇ ਇਹ ਵੈਂਡਰ-ਅਗਨੋਸਟਿਕ ਹੈ, ਜਿਸਦਾ ਮਤਲਬ ਹੈ ਕਿ ਵੱਖ-ਵੱਖ GPU ਵੈਂਡਰਾਂ ਲਈ ਕੋਡ ਵਿੱਚ ਕੋਈ ਬਦਲਾਅ ਕਰਨ ਦੀ ਲੋੜ ਨਹੀਂ। ਇਹ ਮੁੱਖ ਤੌਰ 'ਤੇ ਮਾਡਲ ਟ੍ਰੇਨਿੰਗ ਅਤੇ ਇਨਫਰੰਸਿੰਗ ਕੰਮਾਂ ਲਈ GPU ਤੇ ਵਰਤੀ ਜਾਂਦੀ ਹੈ।

ਹਾਰਡਵੇਅਰ ਸਹਾਇਤਾ ਦੇ ਤੌਰ 'ਤੇ, DirectML ਵੱਖ-ਵੱਖ GPUs ਨਾਲ ਕੰਮ ਕਰਨ ਲਈ ਡਿਜ਼ਾਈਨ ਕੀਤੀ ਗਈ ਹੈ, ਜਿਸ ਵਿੱਚ AMD ਇੰਟੀਗ੍ਰੇਟਿਡ ਅਤੇ ਡਿਸਕ੍ਰੀਟ GPUs, Intel ਇੰਟੀਗ੍ਰੇਟਿਡ GPUs, ਅਤੇ NVIDIA ਡਿਸਕ੍ਰੀਟ GPUs ਸ਼ਾਮਲ ਹਨ। ਇਹ Windows AI ਪਲੇਟਫਾਰਮ ਦਾ ਹਿੱਸਾ ਹੈ ਅਤੇ Windows 10 & 11 'ਤੇ ਸਪੋਰਟ ਕੀਤੀ ਜਾਂਦੀ ਹੈ, ਜਿਸ ਨਾਲ ਕਿਸੇ ਵੀ Windows ਡਿਵਾਈਸ 'ਤੇ ਮਾਡਲ ਟ੍ਰੇਨਿੰਗ ਅਤੇ ਇਨਫਰੰਸਿੰਗ ਸੰਭਵ ਹੈ।

DirectML ਨਾਲ ਜੁੜੇ ਅਪਡੇਟ ਅਤੇ ਮੌਕੇ ਹਨ, ਜਿਵੇਂ ਕਿ 150 ONNX ਓਪਰੇਟਰਾਂ ਤੱਕ ਸਹਾਇਤਾ ਅਤੇ ONNX ਰਨਟਾਈਮ ਅਤੇ WinML ਵੱਲੋਂ ਇਸਦੀ ਵਰਤੋਂ। ਇਹ ਵੱਡੇ Integrated Hardware Vendors (IHVs) ਵੱਲੋਂ ਸਮਰਥਿਤ ਹੈ, ਜੋ ਵੱਖ-ਵੱਖ ਮੈਟਾਕਮਾਂਡ ਲਾਗੂ ਕਰਦੇ ਹਨ।

## CUDA

CUDA, ਜਿਸਦਾ ਪੂਰਾ ਨਾਮ Compute Unified Device Architecture ਹੈ, Nvidia ਵੱਲੋਂ ਬਣਾਇਆ ਗਿਆ ਇੱਕ ਪੈਰਲਲ ਕੰਪਿਊਟਿੰਗ ਪਲੇਟਫਾਰਮ ਅਤੇ API ਮਾਡਲ ਹੈ। ਇਹ ਸਾਫਟਵੇਅਰ ਡਿਵੈਲਪਰਾਂ ਨੂੰ CUDA-ਸਮਰਥਿਤ GPU ਵਰਤ ਕੇ ਜਨਰਲ ਪਰਪਜ਼ ਪ੍ਰੋਸੈਸਿੰਗ ਕਰਨ ਦੀ ਆਗਿਆ ਦਿੰਦਾ ਹੈ — ਜਿਸਨੂੰ GPGPU (ਜਨਰਲ-ਪਰਪਜ਼ ਕੰਪਿਊਟਿੰਗ ਆਨ ਗ੍ਰਾਫਿਕਸ ਪ੍ਰੋਸੈਸਿੰਗ ਯੂਨਿਟਸ) ਕਿਹਾ ਜਾਂਦਾ ਹੈ। CUDA Nvidia ਦੀ GPU ਤੀਜ਼ੀ ਨੂੰ ਯੋਗ ਬਣਾਉਂਦਾ ਹੈ ਅਤੇ ਇਹ ਮਸ਼ੀਨ ਲਰਨਿੰਗ, ਵਿਗਿਆਨਕ ਕੰਪਿਊਟਿੰਗ ਅਤੇ ਵੀਡੀਓ ਪ੍ਰੋਸੈਸਿੰਗ ਵਰਗੇ ਖੇਤਰਾਂ ਵਿੱਚ ਵਿਆਪਕ ਤੌਰ 'ਤੇ ਵਰਤਿਆ ਜਾਂਦਾ ਹੈ।

CUDA ਲਈ ਹਾਰਡਵੇਅਰ ਸਹਾਇਤਾ ਖਾਸ ਕਰਕੇ Nvidia ਦੇ GPUs ਲਈ ਹੈ, ਕਿਉਂਕਿ ਇਹ Nvidia ਦੀ ਖਾਸ ਤਕਨਾਲੋਜੀ ਹੈ। ਹਰ ਆਰਕੀਟੈਕਚਰ CUDA ਟੂਲਕਿਟ ਦੇ ਖਾਸ ਵਰਜਨਾਂ ਨੂੰ ਸਪੋਰਟ ਕਰਦਾ ਹੈ, ਜੋ ਡਿਵੈਲਪਰਾਂ ਨੂੰ ਲਾਇਬ੍ਰੇਰੀਆਂ ਅਤੇ ਟੂਲਜ਼ ਮੁਹੱਈਆ ਕਰਵਾਉਂਦਾ ਹੈ ਤਾਂ ਜੋ ਉਹ CUDA ਐਪਲੀਕੇਸ਼ਨਾਂ ਨੂੰ ਬਣਾਉਣ ਅਤੇ ਚਲਾਉਣ ਸਕਣ।

## ONNX

ONNX (Open Neural Network Exchange) ਇੱਕ ਖੁੱਲ੍ਹਾ ਫਾਰਮੈਟ ਹੈ ਜੋ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਮਾਡਲਾਂ ਨੂੰ ਦਰਸਾਉਣ ਲਈ ਬਣਾਇਆ ਗਿਆ ਹੈ। ਇਹ ਇੱਕ ਵਿਸਥਾਰਯੋਗ ਕਮਪਿਊਟੇਸ਼ਨ ਗ੍ਰਾਫ ਮਾਡਲ ਦੀ ਪਰਿਭਾਸ਼ਾ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ, ਨਾਲ ਹੀ ਬਿਲਟ-ਇਨ ਓਪਰੇਟਰਾਂ ਅਤੇ ਸਟੈਂਡਰਡ ਡੇਟਾ ਟਾਈਪਾਂ ਦੀ ਪਰਿਭਾਸ਼ਾ ਵੀ। ONNX ਡਿਵੈਲਪਰਾਂ ਨੂੰ ਵੱਖ-ਵੱਖ ML ਫਰੇਮਵਰਕਾਂ ਵਿਚਕਾਰ ਮਾਡਲਾਂ ਨੂੰ ਬਦਲਣ ਦੀ ਆਗਿਆ ਦਿੰਦਾ ਹੈ, ਜਿਸ ਨਾਲ ਇੰਟਰਓਪਰੇਬਿਲਿਟੀ ਆਉਂਦੀ ਹੈ ਅਤੇ AI ਐਪਲੀਕੇਸ਼ਨਾਂ ਨੂੰ ਬਣਾਉਣਾ ਅਤੇ ਤਿਆਰ ਕਰਨਾ ਆਸਾਨ ਹੁੰਦਾ ਹੈ।

Phi3 ਮਿਨੀ ONNX Runtime ਨਾਲ CPU ਅਤੇ GPU 'ਤੇ ਵੱਖ-ਵੱਖ ਡਿਵਾਈਸਾਂ ਉੱਤੇ ਚੱਲ ਸਕਦਾ ਹੈ, ਜਿਸ ਵਿੱਚ ਸਰਵਰ ਪਲੇਟਫਾਰਮ, Windows, Linux ਅਤੇ Mac ਡੈਸਕਟਾਪ ਅਤੇ ਮੋਬਾਈਲ CPU ਸ਼ਾਮਲ ਹਨ।  
ਅਸੀਂ ਜੋ ਅਪਟੀਮਾਈਜ਼ਡ ਕੰਫਿਗਰੇਸ਼ਨ ਸ਼ਾਮਲ ਕੀਤੀਆਂ ਹਨ, ਉਹ ਹਨ:

- ONNX ਮਾਡਲ int4 DML ਲਈ: AWQ ਰਾਹੀਂ int4 ਵਿੱਚ ਕਵਾਂਟਾਈਜ਼ਡ  
- ONNX ਮਾਡਲ fp16 CUDA ਲਈ  
- ONNX ਮਾਡਲ int4 CUDA ਲਈ: RTN ਰਾਹੀਂ int4 ਵਿੱਚ ਕਵਾਂਟਾਈਜ਼ਡ  
- ONNX ਮਾਡਲ int4 CPU ਅਤੇ ਮੋਬਾਈਲ ਲਈ: RTN ਰਾਹੀਂ int4 ਵਿੱਚ ਕਵਾਂਟਾਈਜ਼ਡ  

## Llama.cpp

Llama.cpp ਇੱਕ ਖੁੱਲ੍ਹਾ ਸਰੋਤ ਸਾਫਟਵੇਅਰ ਲਾਇਬ੍ਰੇਰੀ ਹੈ ਜੋ C++ ਵਿੱਚ ਲਿਖੀ ਗਈ ਹੈ। ਇਹ ਵੱਖ-ਵੱਖ ਵੱਡੇ ਭਾਸ਼ਾ ਮਾਡਲਾਂ (LLMs), ਜਿਵੇਂ ਕਿ Llama, 'ਤੇ ਇਨਫਰੰਸ ਕਰਦੀ ਹੈ। ggml ਲਾਇਬ੍ਰੇਰੀ (ਜੋ ਇੱਕ ਜਨਰਲ-ਪਰਪਜ਼ ਟੈਂਸਰ ਲਾਇਬ੍ਰੇਰੀ ਹੈ) ਦੇ ਨਾਲ ਵਿਕਸਤ ਕੀਤੀ ਗਈ, llama.cpp ਦਾ ਮਕਸਦ ਮੂਲ Python ਇੰਪਲੀਮੈਂਟੇਸ਼ਨ ਨਾਲੋਂ ਤੇਜ਼ ਇਨਫਰੰਸ ਅਤੇ ਘੱਟ ਮੈਮੋਰੀ ਖਪਤ ਪ੍ਰਦਾਨ ਕਰਨਾ ਹੈ। ਇਹ ਹਾਰਡਵੇਅਰ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ, ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ ਨੂੰ ਸਪੋਰਟ ਕਰਦਾ ਹੈ ਅਤੇ ਇੱਕ ਸਧਾਰਣ API ਅਤੇ ਉਦਾਹਰਣਾਂ ਦਿੰਦਾ ਹੈ। ਜੇ ਤੁਸੀਂ ਪ੍ਰਭਾਵਸ਼ਾਲੀ LLM ਇਨਫਰੰਸ ਵਿੱਚ ਦਿਲਚਸਪੀ ਰੱਖਦੇ ਹੋ, ਤਾਂ llama.cpp ਨੂੰ ਜ਼ਰੂਰ ਦੇਖੋ ਕਿਉਂਕਿ Phi3 Llama.cpp ਚਲਾ ਸਕਦਾ ਹੈ।

## GGUF

GGUF (Generic Graph Update Format) ਇੱਕ ਫਾਰਮੈਟ ਹੈ ਜੋ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਮਾਡਲਾਂ ਨੂੰ ਦਰਸਾਉਣ ਅਤੇ ਅਪਡੇਟ ਕਰਨ ਲਈ ਵਰਤਿਆ ਜਾਂਦਾ ਹੈ। ਇਹ ਖਾਸ ਕਰਕੇ ਛੋਟੇ ਭਾਸ਼ਾ ਮਾਡਲਾਂ (SLMs) ਲਈ ਲਾਭਦਾਇਕ ਹੈ ਜੋ CPU 'ਤੇ 4-8bit ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ ਨਾਲ ਪ੍ਰਭਾਵਸ਼ਾਲੀ ਤਰੀਕੇ ਨਾਲ ਚੱਲ ਸਕਦੇ ਹਨ। GGUF ਤੇਜ਼ ਪ੍ਰੋਟੋਟਾਈਪਿੰਗ ਅਤੇ ਐਜ ਡਿਵਾਈਸਾਂ ਜਾਂ CI/CD ਪਾਈਪਲਾਈਨਾਂ ਵਰਗੇ ਬੈਚ ਕੰਮਾਂ ਵਿੱਚ ਮਾਡਲ ਚਲਾਉਣ ਲਈ ਫਾਇਦੇਮੰਦ ਹੈ।

**ਅਸਵੀਕਾਰੋक्ति**:  
ਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀਅਤ ਲਈ ਕੋਸ਼ਿਸ਼ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਰੱਖੋ ਕਿ ਆਟੋਮੇਟਿਕ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸਹੀਤਾਵਾਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼ ਆਪਣੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਹੀ ਅਧਿਕਾਰਤ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਅਸੀਂ ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਉਤਪੰਨ ਕਿਸੇ ਵੀ ਗਲਤਫਹਮੀ ਜਾਂ ਭ੍ਰਮ ਲਈ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।