<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "9841486ba4cf2590fabe609b925b00eb",
  "translation_date": "2025-07-16T18:43:12+00:00",
  "source_file": "md/01.Introduction/01/01.Understandingtech.md",
  "language_code": "pa"
}
-->
# ਮੁੱਖ ਤਕਨਾਲੋਜੀਆਂ ਵਿੱਚ ਸ਼ਾਮਲ ਹਨ

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - ਇੱਕ ਨੀਵੀਂ ਪੱਧਰੀ API ਜੋ DirectX 12 ਦੇ ਉੱਪਰ ਬਣਾਈ ਗਈ ਹੈ ਅਤੇ ਹਾਰਡਵੇਅਰ-ਤੇਜ਼ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਲਈ ਹੈ।
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia ਵੱਲੋਂ ਵਿਕਸਿਤ ਇੱਕ ਪੈਰਲੇਲ ਕੰਪਿਊਟਿੰਗ ਪਲੇਟਫਾਰਮ ਅਤੇ ਐਪਲੀਕੇਸ਼ਨ ਪ੍ਰੋਗ੍ਰਾਮਿੰਗ ਇੰਟਰਫੇਸ (API) ਮਾਡਲ, ਜੋ ਗ੍ਰਾਫਿਕਸ ਪ੍ਰੋਸੈਸਿੰਗ ਯੂਨਿਟਸ (GPUs) 'ਤੇ ਜਨਰਲ-ਪਰਪਜ਼ ਪ੍ਰੋਸੈਸਿੰਗ ਦੀ ਆਗਿਆ ਦਿੰਦਾ ਹੈ।
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - ਇੱਕ ਖੁੱਲ੍ਹਾ ਫਾਰਮੈਟ ਜੋ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਮਾਡਲਾਂ ਨੂੰ ਦਰਸਾਉਂਦਾ ਹੈ ਅਤੇ ਵੱਖ-ਵੱਖ ML ਫਰੇਮਵਰਕਾਂ ਵਿਚਕਾਰ ਇੰਟਰਓਪਰੇਬਿਲਿਟੀ ਮੁਹੱਈਆ ਕਰਵਾਉਂਦਾ ਹੈ।
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - ਇੱਕ ਫਾਰਮੈਟ ਜੋ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਮਾਡਲਾਂ ਨੂੰ ਦਰਸਾਉਣ ਅਤੇ ਅਪਡੇਟ ਕਰਨ ਲਈ ਵਰਤਿਆ ਜਾਂਦਾ ਹੈ, ਖਾਸ ਕਰਕੇ ਛੋਟੇ ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਲਈ ਜੋ CPUs 'ਤੇ 4-8ਬਿਟ ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ ਨਾਲ ਪ੍ਰਭਾਵਸ਼ਾਲੀ ਤਰੀਕੇ ਨਾਲ ਚੱਲ ਸਕਦੇ ਹਨ।

## DirectML

DirectML ਇੱਕ ਨੀਵੀਂ ਪੱਧਰੀ API ਹੈ ਜੋ ਹਾਰਡਵੇਅਰ-ਤੇਜ਼ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਨੂੰ ਯੋਗ ਬਣਾਉਂਦੀ ਹੈ। ਇਹ DirectX 12 ਦੇ ਉੱਪਰ ਬਣਾਈ ਗਈ ਹੈ ਤਾਂ ਜੋ GPU ਤੇਜ਼ੀ ਦਾ ਲਾਭ ਲਿਆ ਜਾ ਸਕੇ ਅਤੇ ਇਹ ਵੈਂਡਰ-ਅਗਨੋਸਟਿਕ ਹੈ, ਜਿਸਦਾ ਮਤਲਬ ਹੈ ਕਿ ਇਹ ਵੱਖ-ਵੱਖ GPU ਵੈਂਡਰਾਂ 'ਤੇ ਕੰਮ ਕਰਨ ਲਈ ਕੋਡ ਵਿੱਚ ਕੋਈ ਬਦਲਾਅ ਨਹੀਂ ਮੰਗਦੀ। ਇਹ ਮੁੱਖ ਤੌਰ 'ਤੇ GPU 'ਤੇ ਮਾਡਲ ਟ੍ਰੇਨਿੰਗ ਅਤੇ ਇੰਫਰੈਂਸਿੰਗ ਵਰਕਲੋਡ ਲਈ ਵਰਤੀ ਜਾਂਦੀ ਹੈ।

ਹਾਰਡਵੇਅਰ ਸਹਿਯੋਗ ਦੇ ਮਾਮਲੇ ਵਿੱਚ, DirectML ਵੱਖ-ਵੱਖ GPUs ਨਾਲ ਕੰਮ ਕਰਨ ਲਈ ਬਣਾਈ ਗਈ ਹੈ, ਜਿਸ ਵਿੱਚ AMD ਇੰਟੀਗ੍ਰੇਟਿਡ ਅਤੇ ਡਿਸਕ੍ਰੀਟ GPUs, Intel ਇੰਟੀਗ੍ਰੇਟਿਡ GPUs, ਅਤੇ NVIDIA ਡਿਸਕ੍ਰੀਟ GPUs ਸ਼ਾਮਲ ਹਨ। ਇਹ Windows AI ਪਲੇਟਫਾਰਮ ਦਾ ਹਿੱਸਾ ਹੈ ਅਤੇ Windows 10 ਅਤੇ 11 'ਤੇ ਸਹਿਯੋਗਿਤ ਹੈ, ਜਿਸ ਨਾਲ ਕਿਸੇ ਵੀ Windows ਡਿਵਾਈਸ 'ਤੇ ਮਾਡਲ ਟ੍ਰੇਨਿੰਗ ਅਤੇ ਇੰਫਰੈਂਸਿੰਗ ਕਰਨਾ ਸੰਭਵ ਹੈ।

DirectML ਨਾਲ ਜੁੜੇ ਅਪਡੇਟ ਅਤੇ ਮੌਕੇ ਵੀ ਹਨ, ਜਿਵੇਂ ਕਿ 150 ONNX ਓਪਰੇਟਰਾਂ ਤੱਕ ਸਹਿਯੋਗ ਅਤੇ ONNX ਰਨਟਾਈਮ ਅਤੇ WinML ਦੁਆਰਾ ਇਸਦਾ ਇਸਤੇਮਾਲ। ਇਹ ਵੱਡੇ ਇੰਟੀਗ੍ਰੇਟਿਡ ਹਾਰਡਵੇਅਰ ਵੈਂਡਰਾਂ (IHVs) ਦੁਆਰਾ ਸਮਰਥਿਤ ਹੈ, ਜੋ ਵੱਖ-ਵੱਖ ਮੈਟਾਕਮਾਂਡਾਂ ਨੂੰ ਲਾਗੂ ਕਰਦੇ ਹਨ।

## CUDA

CUDA, ਜਿਸਦਾ ਪੂਰਾ ਨਾਮ Compute Unified Device Architecture ਹੈ, Nvidia ਵੱਲੋਂ ਬਣਾਇਆ ਗਿਆ ਇੱਕ ਪੈਰਲੇਲ ਕੰਪਿਊਟਿੰਗ ਪਲੇਟਫਾਰਮ ਅਤੇ ਐਪਲੀਕੇਸ਼ਨ ਪ੍ਰੋਗ੍ਰਾਮਿੰਗ ਇੰਟਰਫੇਸ (API) ਮਾਡਲ ਹੈ। ਇਹ ਸਾਫਟਵੇਅਰ ਡਿਵੈਲਪਰਾਂ ਨੂੰ CUDA-ਸਮਰਥਿਤ ਗ੍ਰਾਫਿਕਸ ਪ੍ਰੋਸੈਸਿੰਗ ਯੂਨਿਟ (GPU) ਦੀ ਵਰਤੋਂ ਜਨਰਲ ਪਰਪਜ਼ ਪ੍ਰੋਸੈਸਿੰਗ ਲਈ ਕਰਨ ਦੀ ਆਗਿਆ ਦਿੰਦਾ ਹੈ — ਜਿਸਨੂੰ GPGPU (General-Purpose computing on Graphics Processing Units) ਕਿਹਾ ਜਾਂਦਾ ਹੈ। CUDA Nvidia ਦੇ GPU ਤੇਜ਼ੀ ਲਈ ਇੱਕ ਮੁੱਖ ਸਹਾਇਕ ਹੈ ਅਤੇ ਇਹ ਮਸ਼ੀਨ ਲਰਨਿੰਗ, ਵਿਗਿਆਨਕ ਕੰਪਿਊਟਿੰਗ ਅਤੇ ਵੀਡੀਓ ਪ੍ਰੋਸੈਸਿੰਗ ਸਮੇਤ ਕਈ ਖੇਤਰਾਂ ਵਿੱਚ ਵਿਆਪਕ ਤੌਰ 'ਤੇ ਵਰਤਿਆ ਜਾਂਦਾ ਹੈ।

CUDA ਲਈ ਹਾਰਡਵੇਅਰ ਸਹਿਯੋਗ ਖਾਸ ਤੌਰ 'ਤੇ Nvidia ਦੇ GPUs ਲਈ ਹੈ, ਕਿਉਂਕਿ ਇਹ Nvidia ਦੀ ਮਲਕੀਅਤ ਵਾਲੀ ਤਕਨਾਲੋਜੀ ਹੈ। ਹਰ ਆਰਕੀਟੈਕਚਰ CUDA ਟੂਲਕਿਟ ਦੇ ਖਾਸ ਵਰਜਨਾਂ ਨੂੰ ਸਹਿਯੋਗ ਦਿੰਦੀ ਹੈ, ਜੋ ਡਿਵੈਲਪਰਾਂ ਨੂੰ CUDA ਐਪਲੀਕੇਸ਼ਨਾਂ ਬਣਾਉਣ ਅਤੇ ਚਲਾਉਣ ਲਈ ਲਾਇਬ੍ਰੇਰੀਆਂ ਅਤੇ ਟੂਲਜ਼ ਮੁਹੱਈਆ ਕਰਵਾਉਂਦਾ ਹੈ।

## ONNX

ONNX (Open Neural Network Exchange) ਇੱਕ ਖੁੱਲ੍ਹਾ ਫਾਰਮੈਟ ਹੈ ਜੋ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਮਾਡਲਾਂ ਨੂੰ ਦਰਸਾਉਣ ਲਈ ਬਣਾਇਆ ਗਿਆ ਹੈ। ਇਹ ਇੱਕ ਵਧਾਇਆ ਜਾ ਸਕਣ ਵਾਲੇ ਕੰਪਿਊਟੇਸ਼ਨ ਗ੍ਰਾਫ ਮਾਡਲ ਦੀ ਪਰਿਭਾਸ਼ਾ ਦੇਂਦਾ ਹੈ, ਨਾਲ ਹੀ ਬਿਲਟ-ਇਨ ਓਪਰੇਟਰਾਂ ਅਤੇ ਮਿਆਰੀ ਡੇਟਾ ਟਾਈਪਾਂ ਦੀ ਪਰਿਭਾਸ਼ਾ ਵੀ। ONNX ਡਿਵੈਲਪਰਾਂ ਨੂੰ ਵੱਖ-ਵੱਖ ML ਫਰੇਮਵਰਕਾਂ ਵਿਚਕਾਰ ਮਾਡਲਾਂ ਨੂੰ ਸਾਂਝਾ ਕਰਨ ਦੀ ਆਗਿਆ ਦਿੰਦਾ ਹੈ, ਜਿਸ ਨਾਲ ਇੰਟਰਓਪਰੇਬਿਲਿਟੀ ਆਸਾਨ ਹੁੰਦੀ ਹੈ ਅਤੇ AI ਐਪਲੀਕੇਸ਼ਨਾਂ ਨੂੰ ਬਣਾਉਣਾ ਅਤੇ ਤਿਆਰ ਕਰਨਾ ਸੌਖਾ ਹੋ ਜਾਂਦਾ ਹੈ।

Phi3 ਮਿਨੀ ONNX Runtime ਨਾਲ CPU ਅਤੇ GPU 'ਤੇ ਚੱਲ ਸਕਦਾ ਹੈ, ਜਿਹੜਾ ਸਰਵਰ ਪਲੇਟਫਾਰਮਾਂ, Windows, Linux ਅਤੇ Mac ਡੈਸਕਟਾਪਾਂ ਅਤੇ ਮੋਬਾਈਲ CPUs ਸਮੇਤ ਕਈ ਡਿਵਾਈਸਾਂ 'ਤੇ ਕੰਮ ਕਰਦਾ ਹੈ।  
ਅਸੀਂ ਜੋ ਅਪਟੀਮਾਈਜ਼ਡ ਕਨਫਿਗਰੇਸ਼ਨਜ਼ ਸ਼ਾਮਲ ਕੀਤੀਆਂ ਹਨ ਉਹ ਹਨ

- int4 DML ਲਈ ONNX ਮਾਡਲ: AWQ ਰਾਹੀਂ int4 ਵਿੱਚ ਕਵਾਂਟਾਈਜ਼ ਕੀਤਾ ਗਿਆ  
- fp16 CUDA ਲਈ ONNX ਮਾਡਲ  
- int4 CUDA ਲਈ ONNX ਮਾਡਲ: RTN ਰਾਹੀਂ int4 ਵਿੱਚ ਕਵਾਂਟਾਈਜ਼ ਕੀਤਾ ਗਿਆ  
- int4 CPU ਅਤੇ ਮੋਬਾਈਲ ਲਈ ONNX ਮਾਡਲ: RTN ਰਾਹੀਂ int4 ਵਿੱਚ ਕਵਾਂਟਾਈਜ਼ ਕੀਤਾ ਗਿਆ  

## Llama.cpp

Llama.cpp ਇੱਕ ਖੁੱਲ੍ਹਾ ਸਰੋਤ ਸਾਫਟਵੇਅਰ ਲਾਇਬ੍ਰੇਰੀ ਹੈ ਜੋ C++ ਵਿੱਚ ਲਿਖੀ ਗਈ ਹੈ। ਇਹ ਵੱਖ-ਵੱਖ ਵੱਡੇ ਭਾਸ਼ਾ ਮਾਡਲਾਂ (LLMs) 'ਤੇ ਇੰਫਰੈਂਸ ਕਰਦੀ ਹੈ, ਜਿਸ ਵਿੱਚ Llama ਵੀ ਸ਼ਾਮਲ ਹੈ। ggml ਲਾਇਬ੍ਰੇਰੀ (ਜੋ ਇੱਕ ਜਨਰਲ-ਪਰਪਜ਼ ਟੈਂਸਰ ਲਾਇਬ੍ਰੇਰੀ ਹੈ) ਦੇ ਨਾਲ ਵਿਕਸਿਤ, llama.cpp ਦਾ ਮਕਸਦ ਮੂਲ Python ਇੰਪਲੀਮੈਂਟੇਸ਼ਨ ਨਾਲੋਂ ਤੇਜ਼ ਇੰਫਰੈਂਸ ਅਤੇ ਘੱਟ ਮੈਮੋਰੀ ਖਪਤ ਪ੍ਰਦਾਨ ਕਰਨਾ ਹੈ। ਇਹ ਹਾਰਡਵੇਅਰ ਅਪਟੀਮਾਈਜ਼ੇਸ਼ਨ, ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ ਨੂੰ ਸਹਿਯੋਗ ਦਿੰਦਾ ਹੈ ਅਤੇ ਇੱਕ ਸਧਾਰਣ API ਅਤੇ ਉਦਾਹਰਣਾਂ ਮੁਹੱਈਆ ਕਰਵਾਉਂਦਾ ਹੈ। ਜੇ ਤੁਸੀਂ ਪ੍ਰਭਾਵਸ਼ਾਲੀ LLM ਇੰਫਰੈਂਸ ਵਿੱਚ ਦਿਲਚਸਪੀ ਰੱਖਦੇ ਹੋ, ਤਾਂ llama.cpp ਨੂੰ ਵੇਖਣਾ ਲਾਇਕ ਹੈ ਕਿਉਂਕਿ Phi3 Llama.cpp ਚਲਾ ਸਕਦਾ ਹੈ।

## GGUF

GGUF (Generic Graph Update Format) ਇੱਕ ਫਾਰਮੈਟ ਹੈ ਜੋ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਮਾਡਲਾਂ ਨੂੰ ਦਰਸਾਉਣ ਅਤੇ ਅਪਡੇਟ ਕਰਨ ਲਈ ਵਰਤਿਆ ਜਾਂਦਾ ਹੈ। ਇਹ ਖਾਸ ਕਰਕੇ ਛੋਟੇ ਭਾਸ਼ਾ ਮਾਡਲਾਂ (SLMs) ਲਈ ਲਾਭਦਾਇਕ ਹੈ ਜੋ CPUs 'ਤੇ 4-8ਬਿਟ ਕਵਾਂਟਾਈਜ਼ੇਸ਼ਨ ਨਾਲ ਪ੍ਰਭਾਵਸ਼ਾਲੀ ਤਰੀਕੇ ਨਾਲ ਚੱਲ ਸਕਦੇ ਹਨ। GGUF ਤੇਜ਼ ਪ੍ਰੋਟੋਟਾਈਪਿੰਗ ਅਤੇ ਐਜ ਡਿਵਾਈਸਾਂ ਜਾਂ CI/CD ਪਾਈਪਲਾਈਨਾਂ ਵਰਗੀਆਂ ਬੈਚ ਨੌਕਰੀਆਂ ਵਿੱਚ ਮਾਡਲ ਚਲਾਉਣ ਲਈ ਫਾਇਦੇਮੰਦ ਹੈ।

**ਅਸਵੀਕਾਰੋਪਣ**:  
ਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦਿਤ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀਤਾ ਲਈ ਕੋਸ਼ਿਸ਼ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਰੱਖੋ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸਮਰਥਤਾਵਾਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼ ਆਪਣੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਪ੍ਰਮਾਣਿਕ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਉਤਪੰਨ ਕਿਸੇ ਵੀ ਗਲਤਫਹਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।